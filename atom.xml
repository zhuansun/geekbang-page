<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>geekbang</title>
  
  
  <link href="https://zhuansun.github.io/geekbang/atom.xml" rel="self"/>
  
  <link href="https://zhuansun.github.io/geekbang/"/>
  <updated>2023-12-07T15:35:27.417Z</updated>
  <id>https://zhuansun.github.io/geekbang/</id>
  
  <author>
    <name>码农张三</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>加餐丨拜占庭将军问题：如何基于签名消息实现作战计划的一致性？</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1924459455.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1924459455.html</id>
    <published>2023-10-23T00:00:00.000Z</published>
    <updated>2023-12-07T15:35:27.417Z</updated>
    
    <content type="html"><![CDATA[<p>你好，我是韩健。</p><p>现在，课程更新了一大半，我也一直关注着留言区的问题，我发现很多同学还是对一些知识有一些误区，再三考虑之后，决定利用今天这节课，先解决留言区提到的一个比较多的问题：如何基于签名消息实现作战计划的一致性？</p><p><strong>除此之外，在论文学习中，很多同学遇到的共性问题比较多（比如 ZAB 协议的细节，后面我会补充几讲），在这里，我十分感谢你提出了这样宝贵的意见，不同的声音会帮助我不断优化课程。</strong></p><p>所以，在课程结束之后，我会再从头梳理一遍，按照关注点通过更多的加餐不断优化内容，把相关的理论和算法的内容展开，帮你彻底吃透相关的内容。</p><p>说回咱们的拜占庭将军问题。在01 讲中，为了不啰嗦，让你举一反三地学习，我对签名消息型拜占庭问题之解，没有详细展开，而是聚焦在最核心的点“签名约束了叛徒的作恶行为”，但从留言来看，很多同学在理解签名和如何实现作战一致性上，还是遇到了问题。比如不理解如何实现作战计划的一致性。</p><p>另外，考虑到签名消息是一些常用的拜占庭容错算法（比如 PBFT）的实现基础，很重要，所以这节课我会对签名消息型拜占庭问题之解进行补充。在今天的内容中，除了具体讲解如何基于签名消息实现作战计划的一致性之外，我还会说一说什么是签名消息。希望在帮你掌握签名消息型拜占庭问题之解的同时，还帮你吃透相关的基础知识。</p><p>在这里，我想强调一下，为了更好地理解这一讲的内容，我建议你先回顾一下 01 讲，加深印象。当然，在学完 01 讲之后，相信你已经明白了，签名消息拜占庭问题之解，之所以能够容忍任意数量的叛徒，关键就在于通过消息的签名，约束了叛徒的作恶行为，也就是说，任何篡改和伪造忠将的消息的行为，都会被发现。</p><p>既然签名消息这么重要，那么什么是签名消息呢？</p><h2 id="什么是签名消息？"><a href="#什么是签名消息？" class="headerlink" title="什么是签名消息？"></a>什么是签名消息？</h2><p>签名消息指的就是带有数字签名的消息，你可以这么理解“数字签名”：类似在纸质合同上进行签名来确认合同内容和证明身份。</p><p>在这里我想说的是，数字签名既可以证实内容的完整性，又可以确认内容的来源，实现不可抵赖性（Non-Repudiation）。既然签名消息优点那么多，<strong>那么如何实现签名消息呢？</strong></p><p>你应该还记得密码学的学术 CP（Bob 和 Alice）吧（不记得的话也没关系，你把他们当作 2 个人就可以了），今天 Bob 要给 Alice 发送一个消息，告诉她，“我已经到北京了”，但是 Bob 希望这个消息能被 Alice 完整地接收到，内容不能被篡改或者伪造，我们一起帮 Bob 和 Alice 想想办法，看看如何实现这个消息。</p><p>首先，为了避免密钥泄露，我们推荐 Bob 和 Alice 使用非对称加密算法（比如 RSA）。也就是说，加密和解密使用不同的秘钥，在这里，Bob 持有需要安全保管的私钥，Alice 持有公开的公钥。</p><p>然后，Bob 用哈希算法（比如 MD5）对消息进行摘要，然后用私钥对摘要进行加密，生成数字签名（Signature），就像下图的样子：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223005903.png" alt="image-20230927223005903"></p><p>接着，Bob 将加密摘要和消息一起发送给 Alice：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223015748.png" alt="image-20230927223015748"></p><p>接下来，当 Alice 接收到消息和加密摘要（Signature）后，她会用自己的公钥对加密摘要（Signature）进行解密，并对消息内容进行摘要（Degist-2），然后将新获取的摘要（Degist-2）和解密后的摘要（Degist-1）进行对比，如果 2 个摘要（Digest-1 和 Digest-2）一致，就说明消息是来自 Bob 的，并且是完整的，就像下图的样子：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223025858.png" alt="image-20230927223025858"></p><p>你看，通过这种方法，Bob 的消息就能被 Alice 完整接收到了，任何篡改和伪造 Bob 消息的行为，都会因为摘要不一致，而被发现。<strong>而这个消息就是签名消息。</strong></p><p>现在，你应该理解了什么是签名消息了吧？另外，关于在留言区提到的“为什么签名消息能约束叛将们的作恶行为？”，在这里，我再补充下，通过上面的 Bob 和 Alice 的故事，我们可以看到，在数字签名的约束下，叛将们是无法篡改和伪造忠将的消息的，因为任何篡改和伪造消息的行为都会被发现，也就是作恶的行为被约束了。也就是说，叛将这时能做“小”恶（比如，不响应消息，或者叛将们相互串通发送指定的消息）但他们无法篡改或伪造忠将的消息了。</p><p>既然数字签名约束了叛将们的作恶行为，那么苏秦怎么做才能实现作战的一致性的呢？也就是忠将们执行一致的作战计划。</p><h2 id="如何实现作战计划的一致性？"><a href="#如何实现作战计划的一致性？" class="headerlink" title="如何实现作战计划的一致性？"></a>如何实现作战计划的一致性？</h2><p>之前我已经提到了，苏秦可以通过签名消息的方式，不仅能在不增加将军人数的情况下，解决二忠一叛的难题，还能实现无论叛将数多少，忠诚的将军们始终能达成一致的作战计划。</p><p>为了方便你理解，我以二忠二叛（更复杂的叛徒作恶模型，因为叛徒们可以相互勾结串通）为例具体演示一下，是怎样实现作战计划的一致性的：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223038471.png" alt="image-20230927223038471"></p><p>需要你注意的是，4 位将军约定了一些流程来发送作战信息、执行作战指令。</p><p><strong>第一轮：</strong></p><p>先发送作战指令的将军，作为指挥官，其他的将军作为副官。</p><p>指挥官将他的签名的作战指令发送给每位副官。</p><p>每位副官，将从指挥官处收到的新的作战指令（也就与之前收的作战指令不同），按照顺序（比如按照首字母字典排序）放到一个盒子里。</p><p><strong>第二轮：</strong></p><p>除了第一轮的指挥官外，剩余的 3 位将军将分别作为指挥官，在上一轮收到的作战指令上，加上自己的签名，并转发给其他将军。</p><p><strong>第三轮：</strong></p><p>除了第一、二轮的指挥官外，剩余的 2 位将军将分别作为指挥官，在上一轮收到的作战指令上，加上自己的签名，并转发给其他将军。</p><p>最后，各位将军按照约定，比如使用盒子里最中间的那个指令来执行作战指令。（假设盒子中的指令为 A、B、C，那中间的指令也就是第 n &#x2F;2 个命令。其中，n 为盒子里的指令数，指令从 0 开始编号，也就是 B）。</p><p>为了帮你直观地理解，如何基于签名消息实现忠将们作战计划的一致性，我来演示一下作战信息协商过程。<strong>而且我会分别以忠将和叛将先发送作战信息为例来演示，</strong>这样可以完整地演示叛将对作战计划干扰破坏的可能性。</p><p>那么忠诚的将军先发送作战信息的情况是什么呢？</p><p>为了演示方便，假设苏秦先发起带有签名的作战信息，作战指令是“进攻”。那么在第一轮作战信息协商中，苏秦向齐、楚、燕发送作战指令“进攻”。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223052727.png" alt="image-20230927223052727"></p><p>在第二轮作战信息协商中，齐、楚、燕分别作为指挥官，向另外 2 位发送作战信息“进攻”。可是楚、燕已经叛变了，<strong>但在签名的约束下，他们无法篡改和伪造忠将的消息，</strong>为了达到干扰作战计划的目的，他们俩一个选择发送消息，一个默不作声，不配合。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223102672.png" alt="image-20230927223102672"></p><p>在第三轮作战信息协商中，齐、楚分别作为指挥官，将接收到的作战信息，附加上自己的签名，并转发给另外一位（这时的叛徒燕，还是默不作声，不配合）。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223111367.png" alt="image-20230927223111367"></p><p>最终，齐收到的作战信息都是“进攻”（它收到了苏秦和楚的），按照“执行盒子最中间的指令”的约定，齐会和苏秦一起执行作战指令“进攻”，实现忠将们作战计划的一致性。</p><p>那么如果是叛徒楚先发送作战信息，干扰作战计划，结果会有所不同吗？我们来具体看一看。在第一轮作战信息协商中，楚向苏秦发送作战指令“进攻”，向齐、燕发送作战指令“撤退”。（当然还有其他的情况，这里只是选择了其中一种，其他的情况，你可以都推导着试试，看看结果是不是一样？）</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223119526.png" alt="image-20230927223119526"></p><p>然后，在第二轮作战信息协商中，苏秦、齐、燕分别作为指挥官，将接收到的作战信息，附加上自己的签名，并转发给另外两位。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223127998.png" alt="image-20230927223127998"></p><p><strong>为了达到干扰作战计划的目的，叛徒楚和燕相互勾结了。</strong>比如，燕拿到了楚的私钥，也就是燕可以伪造楚的签名，这个时候，燕为了干扰作战计划，给苏秦发送作战指令“进攻”，给齐发送作战指令却是“撤退”。</p><p>接着，在第三轮作战信息协商中，苏秦、齐、燕分别作为指挥官，将接收到的作战信息，附加上自己的签名，并转发给另外一位。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223136827.png" alt="image-20230927223136827"></p><p>最终，苏秦和齐收到的作战信息都是“撤退、进攻”，按照“执行盒子最中间的指令”的约定，苏秦、齐和燕一起执行作战指令“撤退”，实现了作战计划的一致性。也就是说，无论叛将楚和燕如何捣乱，苏秦和齐都能执行一致的作战计划，保证作战的胜利。</p><p>另外在这里，我想补充一点，签名消息的拜占庭问题之解，也是需要进行 m+1 轮（其中 m 为叛将数，所以你看，只有楚、燕是叛变的，那么就进行了三轮协商）。你也可以从另外一个角度理解：n 位将军，能容忍 (n - 2) 位叛将（只有一位忠将没有意义，因为此时不需要达成共识了）。<strong>关于这个公式，你只需要记住就好了，推导过程你可以参考论文。</strong></p><p>最后，我想说的是，签名消息型拜占庭问题之解，解决的是忠将们如何就作战计划达成共识的问题，也就只要忠将们执行了一致的作战计划就可以了。但它不关心这个共识是什么，比如，在适合进攻的时候，忠将们可能执行的作战计划是撤退。也就是，这个算法比较理论化。</p><p>关于理论化这一点，有的同学会想知道它如何去用，在我看来呢，这个算法解决的是共识的问题，没有与实际场景结合，是很难在实际场景中落地的。在实际场景中，你可以考虑后来的改进过后的拜占庭容错算法，比如 PBFT 算法。</p><h2 id="内容小结"><a href="#内容小结" class="headerlink" title="内容小结"></a>内容小结</h2><p>本节课我主要带你了解了什么签名消息，以及忠将们如何通过签名消息实现作战的一致性，我希望你明确这样几个重点：</p><ol><li>数字签名是基于非对称加密算法（比如 RSA、DSA、DH）实现的，它能防止消息的内容被篡改和消息被伪造。</li><li>签名消息约束了叛徒的作恶行为，比如，叛徒可以不响应，可以相互勾结串通，但叛徒无法篡改和伪造忠将的消息。</li><li>需要你注意的是，签名消息拜占庭问题之解，虽然实现了忠将们作战计划的一致性，但它不关心达成共识的结果是什么。</li></ol><p>最后，我想说的是，签名消息、拜占庭将军问题的签名消息之解是非常经典的基础知识，影响和启发了后来的众多拜占庭容错算法（比如 PBFT），理解了本讲的内容后，你能更好地理解其他的拜占庭容错算法，以及它们如何改进的？为什么要这么改进？比如，在 PBFT 中，基于性能的考虑，大部分场景的消息采用消息认证码（MAC），只有在视图变更（View Change）等少数场景中采用了数字签名。</p><h2 id="课堂思考"><a href="#课堂思考" class="headerlink" title="课堂思考"></a>课堂思考</h2><p>我演示了在“二忠二叛”情况下，忠将们如何实现作战计划的一致性，那么你不妨推演下，在“二忠一叛”情况下，忠将们如何实现作战计划的一致性呢？欢迎在留言区分享你的看法，与我一同讨论。</p><p>最后，感谢你的阅读，如果这篇文章让你有所收获，也欢迎你将它分享给更多的朋友。</p><hr><iframe width="100%"  frameborder=1 height=1000px src="https://time.geekbang.org/comment/nice/215640" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;你好，我是韩健。&lt;/p&gt;
&lt;p&gt;现在，课程更新了一大半，我也一直关注着留言区的问题，我发现很多同学还是对一些知识有一些误区，再三考虑之后，决定利用今天这节课，先解决留言区提到的一个比较多的问题：如何基于签名消息实现作战计划的一致性？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;除此之外，</summary>
      
    
    
    
    <category term="分布式协议与算法实战" scheme="https://zhuansun.github.io/geekbang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE%E4%B8%8E%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98/"/>
    
    
  </entry>
  
  <entry>
    <title>结束语丨静下心来，享受技术的乐趣</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1734230882.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1734230882.html</id>
    <published>2023-10-22T00:00:00.000Z</published>
    <updated>2023-12-07T15:35:27.420Z</updated>
    
    <content type="html"><![CDATA[<p>你好，我是韩健。</p><p>一晃几个月的时间就过去了，这段日子里，我们一起在课程里沟通交流，与我而言，这是一段很特别的经历。我看到很多同学凌晨还在学习、留言，留言区里经常会看到熟悉的身影，比如约书亚、唔多志、每天晒白牙、小晏子，很感谢你们一直保持着学习的热情。</p><p>就要说再见了，借今天这个机会，我想跟你唠点儿心里话。我问自己，如果只说一句话会是啥？想来想去，我觉得就是它了：<strong>静下心来，享受技术的乐趣。</strong>其实这与我之前的经历有关，我想你也能从我的经历中，看到你自己的影子。</p><p>我们都有这样的感觉，无论任何事情，如果想把它做好，其实都不容易。我记得自己在开发 InfluxDB 系统期间，为了确保进度不失控，常常睡在公司，加班加点；在写稿期间，为了交付更高质量的课程，我总是会有很多想法，偶尔会通宵写稿，核对每句话、每个细节；再比如，为了解答 kernel_distribution 同学的一个关于外部 PPT 的问题，我通过 Google 找到相关代码的出处，然后反复推敲，在凌晨 4 点准备了一个答案。</p><p>当然，技术的学习就更加不容易了，不是读几遍材料、调调代码就可以了，而是需要我们设计检测模型，来验证自己是否准确地理解了技术。我曾见过一些团队，做技术决策的依据是不成立的，设计和开发的系统，尽管迭代多版，也始终稳定不下来。在我看来，这些团队最大的问题，就是对技术的理解不准、不够。</p><p>在我看来，我们需要调整下心态，也就是静下心来，全身心地投入，去体会技术的乐趣，“Hack it and enjoy it!”。然后学习和工作中的小成就，又会不断地给我们正反馈，激励我们，最终可以行云流水般地把事情越做越好。</p><p>具体到我们课程的主题，也就是分布式技术，该怎么继续精进呢？我们都知道，分布式技术属于新技术，仍在快速发展（比如 Raft 在 2013 年才提出），没有体系化的学习材料，而且知识碎片，学习起来尤为不易。今天我想再补充几点个人看法。</p><p><strong>首先是“杨不悔”。</strong>也就是我们要“衣带渐宽终不悔，为伊消得人憔悴”。想想你在大学的时候，是不是很执着呢？学习分布式技术，也需要这么个劲头儿。</p><p><strong>其次是“张无忌”。</strong>也就是我们要“不唯书不唯上只唯实”。理论是为了解决问题的，而不是为了“正确”，理论也是在实战中不断发展的，所以在日常学习和使用技术时，我们要注意妥协，没有十全十美的技术，我们需要根据场景特点，权衡折中使用技术，并且实战也会进一步加深我们对技术的理解。</p><p><strong>最后是“师夷长技以制夷”。</strong>也就是我们要科学上网，多阅读英文资料。</p><p>另外，有些同学可能刚刚接触分布式系统和分布式技术，我对你的建议是“单点突破，再全面开花”。比如，你可以反复研究 20 讲的分布式 KV 系统，然后研究 Raft 算法，最后再去研究其他分布式算法，循序渐进地学习。</p><p>为了帮助你更好地学习，掌握“渔”的技巧。在这里，我推荐一些适合入门和深究的学习材料（当然材料不能太多，太多了，相当于没推荐）。</p><p>迭戈·安加罗（Diego Ongaro）的博士论文：安加罗的博士论文，对 Raft 算法做了很详细的描述，我建议你反复读，结合源码（比如 Hashicorp Raft）读，直到读懂每一句话。</p><p>《Paxos Made Live》：这是 Google 团队的 Paxos 实践总结，我建议你从工程实践的角度去阅读，多想想如果是你，你会怎么做。</p><p>《Eventually Consistent》：了解下沃纳·威格尔（亚马逊 CTO）对一致性的理解和定义。</p><p>说到这里，我还想强调一点，希望能在后续的工作和学习中帮到你。那就是，“技术要具有成本优势”。什么意思呢？</p><p>基于开源软件，我们很容易“堆砌”一套业务需要的功能。基于大型互联网后台（比如 QQ）的架构理念，我们能支撑极其海量的服务和流量。也就是说，实现功能或支撑海量流量，相关的软件和理念，都已经很成熟，不是挑战了，但功能背后的成本问题突出。</p><p>而成本就是钱，功能背后的成本问题是需要重视和解决的，比如，自研 KV 存储相比 Redis 降低了数量级倍数的成本。另外，分布式技术本身就是适用于规模业务的，而且随着业务规模的增加，成本的痛点会更加突出。我希望你能注意到这点，在根据实际场景设计系统架构时，如果需要的话，也将成本作为一个权衡点考虑进去。</p><p>为什么要考虑这些？<strong>因为我真心希望你是分布式系统的架构师、开发者，而不仅仅是开源软件的使用者。</strong></p><p>好了，专栏到此就告一段落了。但专栏的结束，也是另一种开始。我会花时间处理还没来得及回复的留言，也会针对一些同学的共性问题策划答疑或者加餐（这是一个承诺，也请你监督）。总的来说，我会继续帮你吃透算法原理，让你掌握分布式系统的开发实战能力。当然，你可以随时在遇到问题时，在留言区留言，我们一起交流讨论。</p><p>在文章结尾，我为你准备了一份调查问卷，题目不多，希望你能抽出两三分钟填写一下。我非常希望听听你对这个专栏的意见和建议，期待你的反馈！</p><p>最后，我想用一段话结束今天的分享，学习技术的路上你可能会遇到对无法准确理解某技术原理的问题，但你不要觉得孤单，因为这是一个正常的情况，大家都会遇到。如果你觉得某技术的原理，理解起来很吃力，你不妨先把这个技术使用起来，然后多想想，如果是你，你会怎么设计，接着你可以带着自己的猜测去研究技术背后的原理。</p><p>希望你能在繁忙的工作中，保持一颗极客的初心，享受技术的乐趣！</p><iframe width="100%"  frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/218938" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;你好，我是韩健。&lt;/p&gt;
&lt;p&gt;一晃几个月的时间就过去了，这段日子里，我们一起在课程里沟通交流，与我而言，这是一段很特别的经历。我看到很多同学凌晨还在学习、留言，留言区里经常会看到熟悉的身影，比如约书亚、唔多志、每天晒白牙、小晏子，很感谢你们一直保持着学习的热情。&lt;/p&gt;
</summary>
      
    
    
    
    <category term="分布式协议与算法实战" scheme="https://zhuansun.github.io/geekbang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE%E4%B8%8E%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98/"/>
    
    
  </entry>
  
  <entry>
    <title>20丨基于Raft的分布式KV系统开发实战（二）：如何实现代码？</title>
    <link href="https://zhuansun.github.io/geekbang/posts/280707866.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/280707866.html</id>
    <published>2023-10-21T00:00:00.000Z</published>
    <updated>2023-12-07T15:35:27.414Z</updated>
    
    <content type="html"><![CDATA[<p>你好，我是韩健。</p><p>学完上一讲后，相信你已经了解了分布式 KV 系统的架构设计，同时应该也很好奇，架构背后的细节代码是怎么实现的呢？</p><p>别着急，今天这节课，我会带你弄明白这个问题。我会具体讲解分布式 KV 系统核心功能点的实现细节。比如，如何实现读操作对应的 3 种一致性模型。而我希望你能在课下反复运行程序，多阅读源码，掌握所有的细节实现。</p><p>话不多说，我们开始今天的学习。</p><p>在上一讲中，咱们将系统划分为三大功能块（接入协议、KV 操作、分布式集群），那么今天我会按顺序具体说一说每块功能的实现，帮助你掌握架构背后的细节代码。首先，先来了解一下，如何实现接入协议。</p><h2 id="如何实现接入协议？"><a href="#如何实现接入协议？" class="headerlink" title="如何实现接入协议？"></a>如何实现接入协议？</h2><p>在 19 讲提到，我们选择了 HTTP 协议作为通讯协议，并设计了”&#x2F;key”和”&#x2F;join”2 个 HTTP RESTful API，分别用于支持 KV 操作和增加节点的操作，那么，它们是如何实现的呢？</p><p>接入协议的核心实现，就是下面的样子。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222415760.png" alt="image-20230927222415760"></p><p>我带你走一遍这三个步骤，便于你加深印象。</p><ul><li>在 ServeHTTP() 中，会根据 URL 路径设置相关的路由信息。比如，会在 handlerKeyRequest() 中处理 URL 路径前缀为”&#x2F;key”的请求，会在 handleJoin() 中处理 URL 路径为”&#x2F;join”的请求。</li><li>在 handleKeyRequest() 中，处理来自客户端的 KV 操作请求，也就是基于 HTTP POST 请求的赋值操作、基于 HTTP GET 请求的查询操作、基于 HTTP DELETE 请求的删除操作。</li><li>在 handleJoin() 中，处理增加节点的请求，最终调用 raft.AddVoter() 函数，将新节点加入到集群中。</li></ul><p>在这里，需要你注意的是，在根据 URL 设置相关路由信息时，你需要考虑是路径前缀匹配（比如 strings.HasPrefix(r.URL.Path, “&#x2F;key”)），还是完整匹配（比如 r.URL.Path &#x3D;&#x3D; “&#x2F;join”），避免在实际运行时，路径匹配出错。比如，如果对”&#x2F;key”做完整匹配（比如 r.URL.Path &#x3D;&#x3D; “&#x2F;key”），那么下面的查询操作会因为路径匹配出错，无法找到路由信息，而执行失败。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XGET</span> raft-cluster-host01:8091/key/foo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>另外，还需要你注意的是，只有领导者节点才能执行 raft.AddVoter() 函数，也就是说，handleJoin() 函数，只能在领导者节点上执行。</p><p>说完接入协议后，接下来咱们来分析一下第二块功能的实现，也就是，如何实现 KV 操作。</p><h2 id="如何实现-KV-操作？"><a href="#如何实现-KV-操作？" class="headerlink" title="如何实现 KV 操作？"></a>如何实现 KV 操作？</h2><p>上一节课，我提到这个分布式 KV 系统会实现赋值、查询、删除 3 类操作，那具体怎么实现呢？你应该知道，赋值操作是基于 HTTP POST 请求来实现的，就像下面的样子。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XPOST</span> http://raft-cluster-host01:8091/key <span class="token parameter variable">-d</span> <span class="token string">'&#123;"foo": "bar"&#125;'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>也就是说，我们是通过 HTTP POST 请求，实现了赋值操作。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222512463.png" alt="image-20230927222512463"></p><p>同样的，我们走一遍这个过程，加深一下印象。</p><p>当接收到 KV 操作的请求时，系统将调用 handleKeyRequest() 进行处理。</p><p>在 handleKeyRequest() 函数中，检测到 HTTP 请求类型为 POST 请求时，确认了这是一个赋值操作，将执行 store.Set() 函数。</p><p>在 Set() 函数中，将创建指令，并通过 raft.Apply() 函数将指令提交给 Raft。最终指令将被应用到状态机。</p><p>当 Raft 将指令应用到状态机后，最终将执行 applySet() 函数，创建相应的 key 和值到内存中。</p><p>在这里，我想补充一下，FSM 结构复用了 Store 结构体，并实现了 fsm.Apply()、fsm.Snapshot()、fsm.Restore()3 个函数。最终应用到状态机的数据，以 map[string]string 的形式，存放在 Store.m 中。</p><p>那查询操作是怎么实现的呢？它是基于 HTTP GET 请求来实现的。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XGET</span> http://raft-cluster-host01:8091/key/foo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>也就是说，我们是通过 HTTP GET 请求实现了查询操作。在这里我想强调一下，相比需要将指令应用到状态机的赋值操作，查询操作要简单多了，因为系统只需要查询内存中的数据就可以了，不涉及状态机。具体的代码流程如图所示。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222534481.png" alt="image-20230927222534481"></p><p>我们走一遍这个过程，加深一下印象。</p><p>当接收到 KV 操作的请求时，系统将调用 handleKeyRequest() 进行处理。</p><p>在 handleKeyRequest() 函数中，检测到 HTTP 请求类型为 GET 请求时，确认了这是一个赋值操作，将执行 store.Get() 函数。</p><p>Get() 函数在内存中查询指定 key 对应的值。</p><p>而最后一个删除操作，是基于 HTTP DELETE 请求来实现的。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XDELETE</span> http://raft-cluster-host01:8091/key/foo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>也就是说，我们是通过 HTTP DELETE 请求，实现了删除操作。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222554461.png" alt="image-20230927222554461"></p><p>同样的，我们走一遍这个过程。</p><p>1、当接收到 KV 操作的请求时，系统将调用 handleKeyRequest() 进行处理。</p><p>2、在 handleKeyRequest() 函数中，检测到 HTTP 请求类型为 DELETE 请求时，确认了这是一个删除操作，将执行 store.Delete() 函数。</p><p>3、在 Delete() 函数中，将创建指令，并通过 raft.Apply() 函数，将指令提交给 Raft。最终指令将被应用到状态机。</p><p>4、当前 Raft 将指令应用到状态机后，最终执行 applyDelete() 函数，删除 key 和值。</p><p>学习这部分内容的时候，有一些同学可能会遇到，不知道如何判断指定的操作是否需要在领导者节点上执行的问题，我给的建议是这样的。</p><ul><li>需要向 Raft 状态机中提交指令的操作，是必须要在领导者节点上执行的，也就是所谓的写请求，比如赋值操作和删除操作。</li><li>需要读取最新数据的查询操作（比如客户端设置查询操作的读一致性级别为 consistent），是必须在领导者节点上执行的。</li></ul><p>说完了如何实现 KV 操作后，来看一下最后一块功能，如何实现分布式集群。</p><h2 id="如何实现分布式集群？"><a href="#如何实现分布式集群？" class="headerlink" title="如何实现分布式集群？"></a>如何实现分布式集群？</h2><h3 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h3><p>实现一个 Raft 集群，首先我们要做的就是创建集群，创建 Raft 集群，主要分为两步。首先，第一个节点通过 Bootstrap 的方式启动，并作为领导者节点。启动命令就像下面的样子。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token variable">$GOPATH</span>/bin/raftdb <span class="token parameter variable">-id</span> node01  <span class="token parameter variable">-haddr</span> raft-cluster-host01:8091 <span class="token parameter variable">-raddr</span> raft-cluster-host01:8089 ~/.raftdb<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这时将在 Store.Open() 函数中，调用 BootstrapCluster() 函数将节点启动起来。</p><p>接着，其他节点会通过 -join 参数指定领导者节点的地址信息，并向领导者节点发送，包含当前节点配置信息的增加节点请求。启动命令就像下面的样子。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token variable">$GOPATH</span>/bin/raftdb <span class="token parameter variable">-id</span> node02 <span class="token parameter variable">-haddr</span> raft-cluster-host02:8091 <span class="token parameter variable">-raddr</span> raft-cluster-host02:8089 <span class="token parameter variable">-join</span> raft-cluster-host01:8091 ~/.raftdb<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>当领导者节点接收到来自其他节点的增加节点请求后，将调用 handleJoin() 函数进行处理，并最终调用 raft.AddVoter() 函数，将新节点加入到集群中。</p><p>在这里，需要你注意的是，只有在向集群中添加新节点时，才需要使用 -join 参数。当节点加入集群后，就可以像下面这样，正常启动进程就可以了。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token variable">$GOPATH</span>/bin/raftdb <span class="token parameter variable">-id</span> node02 <span class="token parameter variable">-haddr</span> raft-cluster-host02:8091 <span class="token parameter variable">-raddr</span> raft-cluster-host02:8089  ~/.raftdb<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>集群运行起来后，因为领导者是可能会变的，那么如何实现写操作，来保证写请求都在领导者节点上执行呢？</p><h3 id="写操作"><a href="#写操作" class="headerlink" title="写操作"></a>写操作</h3><p>在 19 讲中，我们选择了方法 2 来实现写操作。也就是，当跟随者接收到写请求后，将拒绝处理该请求，并将领导者的地址信息转发给客户端。后续客户端就可以直接访问领导者（为了演示方便，我们以赋值操作为例）。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222653086.png" alt="image-20230927222653086"></p><p>我们来看一下具体的内容。</p><p>调用 Set() 函数执行赋值操作。</p><p>如果执行 Set() 函数成功，将执行步骤 3；如果执行 Set() 函数出错，且提示出错的原因是当前节点不是领导者，那这就说明了当前节点不是领导者，不能执行写操作，将执行步骤 4；如果执行 Set() 函数出错，且提示出错的原因不是因为当前节点不是领导者，将执行步骤 5。</p><p>赋值操作执行成功，正常返回。</p><p>节点将构造包含领导者地址信息的重定向响应，并返回给客户端。然后客户端直接访问领导者节点执行赋值操作。</p><p>系统运行出错，返回错误信息给客户端。</p><p>需要你注意的是，赋值操作和删除操作属于写操作，必须在领导者节点上执行。而查询操作，只是查询内存中的数据，不涉及指令提交，可以在任何节点上执行。</p><p>而为了更好的利用 curl 客户端的 HTTP 重定向功能，我实现了 HTTP 307 重定向，这样，你在执行赋值操作时，就不需要关心访问节点是否是领导者节点了。比如，你可以使用下面的命令，访问节点 2（也就是 raft-cluster-host02，192.168.0.20）执行赋值操作。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XPOST</span> raft-cluster-host02:8091/key <span class="token parameter variable">-d</span> <span class="token string">'&#123;"foo": "bar"&#125;'</span> <span class="token parameter variable">-L</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如果当前节点（也就是节点 2）不是领导者，它将返回包含领导者地址信息的 HTTP 307 重定向响应给 curl。这时，curl 根据响应信息，重新发起赋值操作请求，并直接访问领导者节点（也就是节点 1，192.168.0.10）。具体的过程，就像下面的 Wireshark 截图。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222712159.png" alt="image-20230927222712159"></p><p>相比写请求必须在领导者节点上执行，虽然查询操作属于读操作，可以在任何节点上执行，但是如何实现却更加复杂，因为读操作的实现关乎着一致性的实现。那么，具体怎么实现呢？</p><h3 id="读操作"><a href="#读操作" class="headerlink" title="读操作"></a>读操作</h3><p>我想说的是，我们可以实现 3 种一致性模型（也就是 stale、default、consistent），这样，用户就可以根据场景特点，按需选择相应的一致性级别，是不是很灵活呢？</p><p>具体的读操作的代码实现，就像下面的样子。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222723662.png" alt="image-20230927222723662"></p><p>我们走一遍这个过程。</p><p>1、当接收到 HTTP GET 的查询请求时，系统会先调用 level() 函数，来获取当前请求的读一致性级别。</p><p>2、调用 Get() 函数，查询指定 key 和读一致性级别对应的数据。</p><p>3、如果执行 Get() 函数成功，将执行步骤 4；如果执行 Get() 函数出错，且提示出错的原因是当前节点不是领导者节点，那么这就说明了，在当前节点上执行查询操作不满足读一致性级别，必须要到领导者节点上执行查询操作，将执行步骤 5；如果执行 Get() 函数出错，且提示出错的原因不是因为当前节点不是领导者，将执行步骤 6。</p><p>4、查询操作执行成功，返回查询到的值给客户端。</p><p>5、节点将构造，包含领导者地址信息的重定向响应，并返回给客户端。然后客户端直接访问领导者节点查询数据。</p><p>6、系统运行出错，返回错误信息给客户端。</p><p>在这里，为了更好地利用 curl 客户端的 HTTP 重定向功能，我同样实现了 HTTP 307 重定向（具体原理，前面已经介绍了，这里就不啰嗦了）。比如，你可以使用下面的命令，来实现一致性级别为 consistent 的查询操作，不需要关心访问节点（raft-cluster-host02）是否是领导者节点。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XGET</span> raft-cluster-host02:8091/key/foo?level<span class="token operator">=</span>consistent  <span class="token parameter variable">-L</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><iframe width="100%"  frameborder=1 height=1000px src="https://time.geekbang.org/comment/nice/218093" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;你好，我是韩健。&lt;/p&gt;
&lt;p&gt;学完上一讲后，相信你已经了解了分布式 KV 系统的架构设计，同时应该也很好奇，架构背后的细节代码是怎么实现的呢？&lt;/p&gt;
&lt;p&gt;别着急，今天这节课，我会带你弄明白这个问题。我会具体讲解分布式 KV 系统核心功能点的实现细节。比如，如何实现读操</summary>
      
    
    
    
    <category term="分布式协议与算法实战" scheme="https://zhuansun.github.io/geekbang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE%E4%B8%8E%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98/"/>
    
    
  </entry>
  
  <entry>
    <title>01｜重新出发，让我们学会和AI说话</title>
    <link href="https://zhuansun.github.io/geekbang/posts/907391812.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/907391812.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.861Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01｜重新出发，让我们学会和AI说话"><a href="#01｜重新出发，让我们学会和AI说话" class="headerlink" title="01｜重新出发，让我们学会和AI说话"></a>01｜重新出发，让我们学会和AI说话</h1><p>你好，我是徐文浩。</p><p>欢迎你打开 AI 世界的大门。我猜你是被这段时间大火的ChatGPT吸引过来的吧？既然你是带着这样的目的打开了这门课程，那么我也一定会给你想要的。我们的课程会先从ChatGPT的主题开始，在接下来的几讲里，我会为你介绍如何使用OpenAI的API来和AI应用沟通。这些API背后，其实和ChatGPT一样，使用的是OpenAI的GPT-3.5系列的大语言模型。</p><p>整个课程，我们都会使用真实的数据、代码来演示如何利用好大语言模型。所以这一讲，我会先带你做好一系列准备工作。不过不用担心，去运行这些程序并不需要你专门去买一块昂贵的显卡。事实上，如果你不是一个程序员，而是一个产品经理，或者只是对AI感兴趣的非技术人，那么你可以使用浏览器就能访问的免费开发环境，来试验这些问题。</p><p>OpenAI的API能做到哪些神奇的事情？号称离“通用人工智能（AGI）”已经不远的产品长什么样子？GPT-3这样的模型和之前基于深度学习的自然语言处理解决方案又有什么不同？这些吊人胃口的问题，我会在课程里一一揭晓。</p><p>下面我们就先来做一些准备工作，注册账号并搭建开发环境。</p><h2 id="创建-OpenAI-的-API-Key"><a href="#创建-OpenAI-的-API-Key" class="headerlink" title="创建 OpenAI 的 API Key"></a>创建 OpenAI 的 API Key</h2><p>为了学习这门课程，你需要先去注册一个可以使用OpenAI的API的账号，这是账号注册的 <a href="https://openai.com/api/">入口</a>。目前，OpenAI还没有向中国大陆和香港地区开放，所以账号的注册需要你自己想想办法了，如果你有好办法，也欢迎你分享在评论区。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/6ce40aa3ae34ceac1d467a284340f377.png" alt="图片"></p><p>账号注册完成之后，你点击右上角的账号，然后点击 “View API Keys”，就会进入一个管理API Keys的页面。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/1f590de3bd1d7406c07a873bd1d7f2c2.png" alt="图片"></p><p>你点击下面的 “+Create new secret key” 可以创建一个新的API Key。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c6205158ab5d3a4d0ffd121ab9140f44.png" alt="图片"></p><p>对应的API Key你需要把它存储在一个安全的地方，后面我们调用OpenAI的接口都需要使用这个API Key。</p><p>现在OpenAI为所有免费注册的用户都提供了5美元（最早是18美元）的免费使用API的额度，这个额度用来体验API的功能，以及学习这个课程已经够用了。如果你想要进一步将这个API用在实际的产品上，就需要考虑把这个账号升级成付费账号了。</p><h2 id="搭建本地的Jupyter-Labs开发环境"><a href="#搭建本地的Jupyter-Labs开发环境" class="headerlink" title="搭建本地的Jupyter Labs开发环境"></a>搭建本地的Jupyter Labs开发环境</h2><p>有了API Key之后，我们还需要搭建一个开发环境。这门课，我主要通过 <strong>Python</strong> 来讲解和演示如何使用好AI。如果你是一个程序员，你可以自己去 <a href="https://www.python.org/downloads/">Python语言的官网</a> 下载并安装对应的开发环境。</p><p>一般来说，你还需要一个Python的包管理和环境管理工具，我自己比较习惯使用 <a href="https://docs.conda.io/en/latest/miniconda.html">Conda</a>。</p><p>最后，还需要通过包管理工具，配置一个独立的Python 3.10的环境，并安装好JupyterLab、OpenAI以及后面要用到的一系列开发包。我把对应的Conda命令也列在了下面，供你参考。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conda create <span class="token operator">-</span><span class="token operator">-</span>name py310 python<span class="token operator">=</span><span class="token number">3.10</span>conda activate py310conda install <span class="token operator">-</span>c conda<span class="token operator">-</span>forge jupyterlabconda install <span class="token operator">-</span>c conda<span class="token operator">-</span>forge ipywidgetsconda install <span class="token operator">-</span>c conda<span class="token operator">-</span>forge openai<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>后续，随着我们课程的进展，你可能还需要通过Conda或者pip来安装更多Python包。</p><p>安装完JupyterLab之后，你只需要把刚才我们获取到的API Key设置到环境变量里，然后启动JupyterLab。你可以从浏览器里，通过Jupyter Notebook交互式地运行这个课程后面的代码，体验OpenAI的大语言模型神奇的效果。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">OPENAI_API_KEY</span><span class="token operator">=</span>在这里写你获取到的ApiKeyjupyter-lab <span class="token builtin class-name">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>你可以选择新建Python 3的Notebook，来体验交互式地运行Python代码调用OpenAI的API。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/dd2a812ed18581bab0dd51cda83d0d48.png" alt="图片"></p><h2 id="通过Colab使用JupyterLab"><a href="#通过Colab使用JupyterLab" class="headerlink" title="通过Colab使用JupyterLab"></a><strong>通过Colab使用JupyterLab</strong></h2><p>如果你不是一个程序员，或者你懒得在本地搭建一个开发环境。还有一个选择，就是使用Google提供的叫做 <a href="https://colab.research.google.com/">Colab</a> 的线上Python Notebook环境。</p><p>即使你已经有了本地的开发环境，我也建议你注册一个账号。因为Colab可以让你免费使用一些GPU的资源，在你需要使用GPU尝试训练一些深度学习模型，而又没有一张比较好的显卡的时候，就可以直接使用它。另一方面，Colab便于你在网络上把自己撰写的Python代码分享给其他人。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/962ec24b77388a2e89635f294e4a079a.png" alt="图片"></p><p>Colab已经是一个Python Notebook的环境了。所以我们不需要再去安装Python和JupyterLab了。不过我们还是需要安装OpenAI的库，以及设定我们的API Key。你只需要在Notebook开始的时候，执行下面这样一小段代码就可以做到这一点。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token operator">!</span>pip <span class="token function">install</span> openai%env <span class="token assign-left variable">OPENAI_API_KEY</span><span class="token operator">=</span>在这里写你获取到的ApiKey<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>不过需要注意，如果你需要将Notebook分享出去，记得把其中OpenAI的API key删除掉，免得别人的调用，花费都算在了你头上。</p><h2 id="体验并测试OpenAI的API"><a href="#体验并测试OpenAI的API" class="headerlink" title="体验并测试OpenAI的API"></a>体验并测试OpenAI的API</h2><p>好了，现在环境已经搭建好了。无论你是使用本地的JupyterLab环境，还是使用Google免费提供的Colab环境，我都迫不及待地想要和你一起来体验一下OpenAI了。我在这里放了一段代码，你可以把它贴到你的Notebook里面，直接运行一下。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token keyword">import</span> osopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>COMPLETION_MODEL <span class="token operator">=</span> <span class="token string">"text-davinci-003"</span>prompt <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Consideration product : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具1. Compose human readable product title used on Amazon in english within 20 words.2. Write 5 selling points for the products in Amazon.3. Evaluate a price range for this product in U.S.Output the result in json format with three properties called title, selling_points and price_range"""</span><span class="token keyword">def</span> <span class="token function">get_response</span><span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">:</span>    completions <span class="token operator">=</span> openai<span class="token punctuation">.</span>Completion<span class="token punctuation">.</span>create <span class="token punctuation">(</span>        engine<span class="token operator">=</span>COMPLETION_MODEL<span class="token punctuation">,</span>        prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>        max_tokens<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>        n<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>        stop<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        temperature<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span>    message <span class="token operator">=</span> completions<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text    <span class="token keyword">return</span> message<span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们来看看返回结果。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span>    <span class="token string">"title"</span><span class="token punctuation">:</span> <span class="token string">"Glow-in-the-Dark Inflatable PVC Frog Night Market Hot Selling Water Toy for Kids"</span><span class="token punctuation">,</span>    <span class="token string">"selling_points"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>        <span class="token string">"Made of durable PVC material"</span><span class="token punctuation">,</span>        <span class="token string">"Glow-in-the-dark design for night play"</span><span class="token punctuation">,</span>        <span class="token string">"Inflatable design for easy storage and transport"</span><span class="token punctuation">,</span>        <span class="token string">"Perfect for water play and outdoor activities"</span><span class="token punctuation">,</span>        <span class="token string">"Great gift for kids"</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token string">"price_range"</span><span class="token punctuation">:</span> <span class="token string">"$10 - $20"</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个商品名称不是我构造的，而是直接找了1688里一个真实存在的商品。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/0661253a8d1afc669cd7d2a0788d3dec.png" alt="图片"></p><p>这段代码里面，我们调用了OpenAI的Completion接口，然后向它提了一个需求，也就是为一个我在1688上找到的中文商品名称做三件事情。</p><ol><li>为这个商品写一个适合在亚马逊上使用的英文标题。</li><li>给这个商品写5个卖点。</li><li>估计一下，这个商品在美国卖多少钱比较合适。</li></ol><p>同时，我们告诉OpenAI，我们希望返回的结果是JSON格式的，并且上面的三个事情用title、selling_points 和 price_range 三个字段返回。</p><p>神奇的是，OpenAI真的理解了我们的需求，返回了一个符合我们要求的JSON字符串给我们。在这个过程中，它完成了好几件不同的事情。</p><p>第一个是 <strong>翻译</strong>，我们给的商品名称是中文的，返回的内容是英文的。</p><p>第二个是 <strong>理解你的语义去生成文本</strong>，我们这里希望它写一个在亚马逊电商平台上适合人读的标题，所以在返回的英文结果里面，AI没有保留原文里有的“工厂现货”的含义，因为那个明显不适合在亚马逊这样的平台上作为标题。下面5条描述也没有包含“工厂现货”这样的信息。而且，其中的第三条卖点 “Inflatable design for easy storage and transport”，也就是作为一个充气的产品易于存放和运输，这一点其实是从“充气”这个信息AI推理出来的，原来的中文标题里并没有这样的信息。</p><p>第三个是 <strong>利用AI自己有的知识给商品定价</strong>，这里它为这个商品定的价格是在10～20美元之间。而我用 “Glow-in-the-Dark frog” 在亚马逊里搜索，搜索结果的第一行里，就有一个16美元发光的青蛙。</p><p>最后是 <strong>根据我们的要求把我们想要的结果，通过一个 JSON 结构化地返回给我们</strong>。而且，尽管我们没有提出要求，但是AI还是很贴心地把5个卖点放在了一个数组里，方便你后续只选取其中的几个来用。返回的结果是JSON，这样方便了我们进一步利用返回结果。比如，我们就可以把这个结果解析之后存储到数据库里，然后展现给商品运营人员。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/2eff6eaa3cee933dacaa20e3fcc55a7c.png" alt="图片"></p><p>好了，如果看到这个结果你有些激动的话，请你先平复一下，我们马上来看一个新例子。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">prompt <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Man Utd must win trophies, says Ten Hag ahead of League Cup final请将上面这句话的人名提取出来，并用json的方式展示出来"""</span><span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span>  <span class="token string">"names"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"Ten Hag"</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>我们给了它一个英文的体育新闻的标题，然后让AI把其中的人名提取出来。可以看到，返回的结果也准确地把新闻里面唯一出现的人名——曼联队的主教练滕哈格的名字提取了出来。</p><p>和之前的例子不同，这个例子里，我们希望AI处理的内容是英文，给出的指令则是中文。不过AI都处理得很好，而且我们的输入完全是自然的中英文混合在一起，并没有使用特定的标识符或者分隔符。</p><p>注：第一个例子，我们希望AI处理的内容是中文，给出的指令是英文。</p><p>我们这里的两个例子，其实对应着很多不同的问题，其中就包括 <strong>机器翻译、文本生成、知识推理、命名实体识别</strong> 等等。在传统的机器学习领域，对于其中任何一个问题，都可能需要一个独立的机器学习模型。就算把这些模型都免费提供给你，把这些独立的机器学习模型组合到一起实现上面的效果，还需要海量的工程研发工作。没有一个数十人的团队，工作量根本看不到头。</p><p>然而，OpenAI通过一个包含1750亿参数的大语言模型，就能理解自然的语言输入，直接完成各种不同的问题。而这个让人惊艳的表现，也是让很多人惊呼“通用人工智能（AGI）要来了”的原因。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，希望到这里，你对OpenAI提供的大语言模型能干什么有了一个最直观的认识。同时，你也应该已经注册好了对应的账号，生成了调用大语言模型的API Key。无论是在本地搭建了开发环境，还是通过Colab这样免费在线的开发环境，你都应该已经尝试着调用过OpenAI的API拿到一些返回结果了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/209d4b0285a845cc5594f6cb0b63fb8a.jpg" alt="图片"></p><p>OpenAI提供的GPT-3.5系列的大语言模型，可以让你使用一个模型来解决所有的自然语言处理问题。原先我们需要一个个地单独训练模型，或者至少需要微调模型的场景，在大语言模型之下都消失了。这大大降低了我们利用AI解决问题的门槛，无论之前我们通过各种开源工具将机器学习变得多么便捷，想要做好自然语言处理，还是需要一些自然语言处理的专家。而且，往往我们还需要组合好多个模型，进行大量的工程开发工作。</p><p>而在大语言模型时代，我们只需要有会向AI提问的应用开发工程师，就能开发AI应用了。这也是我设计这门课程的目的，希望能让你体会到当前开发AI工具的便利性。</p><h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><ol><li>请将今天课程中提供的示例代码，在你搭建的开发环境中运行一下。</li><li>你可以去看一下 OpenAI 提供的 <a href="https://platform.openai.com/examples/">示例</a>，找几个你感兴趣的用途，在上面的开发环境里运行体验一下，你也可以脑洞大开，尝试一些你想用AI解决的问题，看看AI能不能给出你想要的结果。</li></ol><p>欢迎你把你体验的提示语和结果分享在评论区，看看都能有什么样的好创意。也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>如果你想知道GPT系列大模型到底是怎么回事儿，我推荐你去看一下李沐老师讲解GPT系列论文的视频 <a href="https://www.bilibili.com/video/BV1AF411b7xQ/">GPT、GPT-2、GPT-3 论文精读</a>，这个视频深入浅出，能够让你理解为什么现在GPT那么火热。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01｜重新出发，让我们学会和AI说话&quot;&gt;&lt;a href=&quot;#01｜重新出发，让我们学会和AI说话&quot; class=&quot;headerlink&quot; title=&quot;01｜重新出发，让我们学会和AI说话&quot;&gt;&lt;/a&gt;01｜重新出发，让我们学会和AI说话&lt;/h1&gt;&lt;p&gt;你好，我是徐</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>02｜无需任何机器学习，如何利用大语言模型做情感分析？</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3231687472.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3231687472.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.875Z</updated>
    
    <content type="html"><![CDATA[<h1 id="02｜无需任何机器学习，如何利用大语言模型做情感分析？"><a href="#02｜无需任何机器学习，如何利用大语言模型做情感分析？" class="headerlink" title="02｜无需任何机器学习，如何利用大语言模型做情感分析？"></a>02｜无需任何机器学习，如何利用大语言模型做情感分析？</h1><p>你好，我是徐文浩。</p><p>上一讲我们看到了，大型语言模型的接口其实非常简单。像OpenAI就只提供了Complete和Embedding两个接口，其中，Complete可以让模型根据你的输入进行自动续写，Embedding可以将你输入的文本转化成向量。</p><p>不过到这里，你的疑问可能就来了。不是说现在的大语言模型很厉害吗？传统的自然语言处理问题都可以通过大模型解决。可是用这么简单的两个API，能够完成原来需要通过各种NLP技术解决的问题吗？比如情感分析、文本分类、文章聚类、摘要撰写、搜索，这一系列问题怎么通过这两个接口解决呢？</p><p>别急，在接下来的几讲里，我会告诉你，怎么利用大语言模型提供的这两个简单的API来解决传统的自然语言处理问题。这一讲我们就先从一个最常见的自然语言处理问题——“情感分析”开始，来看看我们怎么把大语言模型用起来。</p><h2 id="传统的二分类方法：朴素贝叶斯与逻辑回归"><a href="#传统的二分类方法：朴素贝叶斯与逻辑回归" class="headerlink" title="传统的二分类方法：朴素贝叶斯与逻辑回归"></a>传统的二分类方法：朴素贝叶斯与逻辑回归</h2><p>“情感分析”问题，是指我们根据一段文字，去判断它的态度是正面的还是负面的。在传统的互联网产品里，经常会被用来分析用户对产品、服务的评价。比如大众点评里面，你对餐馆的评论，在京东买个东西，你对商品的评论，都会被平台拿去分析，给商家或者餐馆的评分做参考。也有些品牌，会专门抓取社交网络里用户对自己产品的评价，来进行情感分析，判断消费者对自己的产品评价是正面还是负面的，并且会根据这些评价来改进自己的产品。</p><p>对于“情感分析”类型的问题，传统的解决方案就是 <strong>把它当成是一个分类问题</strong>，也就是先拿一部分评论数据，人工标注一下这些评论是正面还是负面的。如果有个用户说“这家餐馆真好吃”，那么就标注成“正面情感”。如果有个用户说“这个手机质量不好”，那么就把对应的评论标注成负面的。</p><p>我们把标注好的数据，喂给一个机器学习模型，训练出一组参数。然后把剩下的没有人工标注过的数据也拿给训练好的模型计算一下。模型就会给你一个分数或者概率，告诉你这一段评论的感情是正面的，还是负面的。</p><p>可以用来做情感分析的模型有很多，这些算法背后都是基于某一个数学模型。比如，很多教科书里，就会教你用 <strong>朴素贝叶斯算法</strong> 来进行垃圾邮件分类。朴素贝叶斯的模型，就是简单地统计每个单词和好评差评之间的条件概率。一般来说，如果一个词语在差评里出现的概率比好评里高得多，那这个词语所在的评论，就更有可能是一个差评。</p><p>$$ P(c|x)&#x3D;\frac{P(x|c)P©}{P(x)}$$</p><p>$$P(c|X) \propto P(x_{1}|c)\times P(x_{2}|c)\times ···\times P(x_{n}|c)\times P©$$</p><p>假设我们有一个训练集包含4封邮件，其中2封是垃圾邮件，2封是非垃圾邮件。训练集里的邮件包含这些单词。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d0d33ccd0c43603bb127fcb868b79ec5.png" alt="图片"></p><p>然后来了一封新邮件，里面的单词是：buy、money、sell。</p><p>通过这些单词出现的概率，我们很容易就可以预先算出这封邮件是垃圾邮件还是普通邮件。</p><ul><li>$P(buy|垃圾) &#x3D; 2 \div 2 &#x3D; 1$</li><li>$P(money|垃圾) &#x3D; 2 \div 2 &#x3D; 1$</li><li>$P(sell|垃圾) &#x3D; 1 \div 2 &#x3D; 0.5$</li><li>$P(buy|普通) &#x3D; 0 \div 2 &#x3D;0$</li><li>$P(money|普通) &#x3D; 0 \div 2 &#x3D;0$</li><li>$P(sell|普通) &#x3D; 1 div 2 &#x3D;0.5$</li></ul><p>然后我们把这封邮件里所有词语的条件概率用全概率公式乘起来，就得到了这封邮件是垃圾邮件还有普通邮件的概率。</p><p>$$P(垃圾|X) \propto P(buy|垃圾) \times P(money|垃圾) \times P(sell|垃圾) \times P(垃圾) &#x3D; 1 \times 1 \times 0.5 \times 0.5 &#x3D; 0.25$$</p><p>$$P(普通|X) \propto P(buy|普通) \times P(money|普通) \times P(sell|普通) \times P(普通) &#x3D; 0 \times 0 \times 0.5 \times 0.5 &#x3D; 0$$</p><p>在这里，我们发现 $P(垃圾|X) &gt; P(普通|X)$，而且 $P(普通|X)$ 其实等于0。那如果用朴素贝叶斯算法，我们就会认为这封邮件100%是垃圾邮件。如果你觉得自己数学不太好，这个例子没有看明白也没有关系，因为我们接下来的AI应用开发都不需要预先掌握这些数学知识，所以不要有心理负担。</p><p>类似的，像逻辑回归、随机森林等机器学习算法都可以拿来做分类。你在网上，特别是 Kaggle 这个机器学习比赛的网站里，可以搜索到很多其他人使用这些传统方法来设计情感分析的解决方案。这些方案都以Jupyter Notebook的形式出现，我在这里放个 <a href="https://www.kaggle.com/code/ankumagawa/sentimental-analysis-using-naive-bayes-classifier">链接</a>，你有兴趣的话也可以去研究一下。</p><h2 id="传统方法的挑战：特征工程与模型调参"><a href="#传统方法的挑战：特征工程与模型调参" class="headerlink" title="传统方法的挑战：特征工程与模型调参"></a>传统方法的挑战：特征工程与模型调参</h2><p>但这些传统的机器学习算法，想要取得好的效果，还是颇有门槛的。除了要知道有哪些算法可以用，还有两方面的工作非常依赖经验。</p><h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>第一个是特征工程。对于很多自然语言问题，如果我们只是拿一段话里面是否出现了特定的词语来计算概率，不一定是最合适的。比如“这家餐馆太糟糕了，一点都不好吃”和 “这家餐馆太好吃了，一点都不糟糕”这样两句话，从意思上是完全相反的。但是里面出现的词语其实是相同的。在传统的自然语言处理中，我们会通过一些特征工程的方法来解决这个问题。</p><p>比如，我们不只是采用单个词语出现的概率，还增加前后两个或者三个相连词语的组合，也就是通过所谓的2-Gram（Bigram双字节词组）和3-Gram（Trigram三字节词组）也来计算概率。在上面这个例子里，第一句差评，就会有“太”和“糟糕”组合在一起的“太糟糕”，以及“不”和“好吃”组合在一起的“不好吃”。而后面一句里就有“太好吃”和“不糟糕”两个组合。有了这样的2-Gram的组合，我们判断用户好评差评的判断能力就比光用单个词语是否出现要好多了。</p><p>这样的特征工程的方式有很多，比如 <strong>去除停用词</strong>，也就是“的地得”这样的词语， <strong>去掉过于低频的词语</strong>，比如一些偶尔出现的专有名词。或者对于有些词语特征采用 <strong>TF-IDF</strong>（词频-逆文档频率）这样的统计特征，还有在英语里面对不同时态的单词统一换成现在时。</p><p>不同的特征工程方式，在不同的问题上效果不一样，比如我们做情感分析，可能就需要保留标点符号，因为像“！”这样的符号往往蕴含着强烈的情感特征。但是，这些种种细微的技巧，让我们在想要解决一个简单的情感分析问题时，也需要撰写大量文本处理的代码，还要了解针对当前特定场景的技巧，这非常依赖工程师的经验。</p><h3 id="机器学习相关经验"><a href="#机器学习相关经验" class="headerlink" title="机器学习相关经验"></a>机器学习相关经验</h3><p>第二个就是你需要有相对丰富的机器学习经验。除了通过特征工程设计更多的特征之外，我们还需要了解很多机器学习领域里常用的知识和技巧。比如，我们需要将数据集切分成训练（Training）、验证（Validation）、测试（Test）三组数据，然后通过AUC或者混淆矩阵（Confusion Matrix）来衡量效果。如果数据量不够多，为了训练效果的稳定性，可能需要采用 K-Fold 的方式来进行训练。</p><p>如果你没有接触过机器学习，看到这里，可能已经看懵了。没关系，上面的大部分知识你未来可能都不需要了解了，因为我们有了大语言模型，可以通过它提供的Completion和Embedding这两个API，用不到10行代码就能完成情感分析，并且能获得非常好的效果。</p><h2 id="大语言模型：20行代码的情感分析解决方案"><a href="#大语言模型：20行代码的情感分析解决方案" class="headerlink" title="大语言模型：20行代码的情感分析解决方案"></a>大语言模型：20行代码的情感分析解决方案</h2><p>通过大语言模型来进行情感分析，最简单的方式就是利用它提供的Embedding这个API。这个API可以把任何你指定的一段文本，变成一个大语言模型下的向量，也就是用一组固定长度的参数来代表任何一段文本。</p><p>我们需要提前计算“好评”和“差评”这两个字的Embedding。而对于任何一段文本评论，我们也都可以通过API拿到它的Embedding。那么，我们把这段文本的Embedding和“好评”以及“差评”通过余弦距离（Cosine Similarity）计算出它的相似度。然后我们拿这个Embedding和“好评”之间的相似度，去减去和“差评”之间的相似度，就会得到一个分数。如果这个分数大于0，那么说明我们的评论和“好评”的距离更近，我们就可以判断它为好评。如果这个分数小于0，那么就是离差评更近，我们就可以判断它为差评。</p><p>下面我们就用这个方法分析一下两条在京东上购买了iPhone用户的评论。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ff7a7da002113b2fd345dc598e1aeb7d.png" alt="图片"></p><p>这个使用大模型的方法一共有20行代码，我们看看它能否帮助我们快速对这两条评论进行情感分析。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token keyword">import</span> os<span class="token keyword">from</span> openai<span class="token punctuation">.</span>embeddings_utils <span class="token keyword">import</span> cosine_similarity<span class="token punctuation">,</span> get_embedding<span class="token comment"># 获取访问open ai的密钥</span>openai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>getenv<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span><span class="token comment"># 选择使用最小的ada模型</span>EMBEDDING_MODEL <span class="token operator">=</span> <span class="token string">"text-embedding-ada-002"</span><span class="token comment"># 获取"好评"和"差评"的</span>positive_review <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span><span class="token string">"好评"</span><span class="token punctuation">)</span>negative_review <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span><span class="token string">"差评"</span><span class="token punctuation">)</span>positive_example <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span><span class="token string">"买的银色版真的很好看，一天就到了，晚上就开始拿起来完系统很丝滑流畅，做工扎实，手感细腻，很精致哦苹果一如既往的好品质"</span><span class="token punctuation">)</span>negative_example <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span><span class="token string">"降价厉害，保价不合理，不推荐"</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">get_score</span><span class="token punctuation">(</span>sample_embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">return</span> cosine_similarity<span class="token punctuation">(</span>sample_embedding<span class="token punctuation">,</span> positive_review<span class="token punctuation">)</span> <span class="token operator">-</span> cosine_similarity<span class="token punctuation">(</span>sample_embedding<span class="token punctuation">,</span> negative_review<span class="token punctuation">)</span>positive_score <span class="token operator">=</span> get_score<span class="token punctuation">(</span>positive_example<span class="token punctuation">)</span>negative_score <span class="token operator">=</span> get_score<span class="token punctuation">(</span>negative_example<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"好评例子的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>positive_score<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"差评例子的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>negative_score<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">好评例子的评分 <span class="token punctuation">:</span> <span class="token number">0.070963</span>差评例子的评分 <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.081472</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>正如我们所料，京东上的好评通过Embedding相似度计算得到的分数是大于0的，京东上面的差评，这个分数是小于0的。</p><p>这样的方法，是不是特别简单？我们再拿刚才的例子试一下，看看这个方法是不是对所有词语都管用，只是出现的位置不同但含义截然相反的评论，能得到什么样的结果。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">good_restraurant <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span><span class="token string">"这家餐馆太好吃了，一点都不糟糕"</span><span class="token punctuation">)</span>bad_restraurant <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span><span class="token string">"这家餐馆太糟糕了，一点都不好吃"</span><span class="token punctuation">)</span>good_score <span class="token operator">=</span> get_score<span class="token punctuation">(</span>good_restraurant<span class="token punctuation">)</span>bad_score <span class="token operator">=</span> get_score<span class="token punctuation">(</span>bad_restraurant<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"好评餐馆的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>good_score<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"差评餐馆的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>bad_score<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">好评餐馆的评分 <span class="token punctuation">:</span> <span class="token number">0.062719</span>差评餐馆的评分 <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.074591</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>可以看到，虽然两句话分别是“太好吃”“不糟糕”和“太糟糕”“不好吃”，其实词语都一样，但是大语言模型一样能够帮助我们判断出来他们的含义是不同的，一个更接近好评，一个更接近差评。</p><h2 id="更大的数据集上的真实案例"><a href="#更大的数据集上的真实案例" class="headerlink" title="更大的数据集上的真实案例"></a>更大的数据集上的真实案例</h2><p>在这里，我们只举了几个例子，看起来效果还不错。这会不会只是我们运气好呢？我们再来拿一个真实的数据集验证一下，利用这种方法进行情感分析的准确率能够到多少。</p><p>下面这段代码，是来自OpenAI Cookbook里面的一个例子。它是用同样的方法，来判断亚马逊提供的用户对一些食物的评价，这个评价数据里面，不只有用户给出的评论内容，还有用户给这些食物打了几颗星。这些几颗星的信息，正好可以拿来验证我们这个方法有多准。对于用户打出1～2星的，我们认为是差评，对于4～5星的，我们认为是好评。</p><p>我们可以通过Pandas，将这个CSV数据读取到内存里面。为了避免重新调用OpenAI的API浪费钱，这个数据集里，已经将获取到的Embedding信息保存下来了，不需要再重新计算。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> classification_reportdatafile_path <span class="token operator">=</span> <span class="token string">"data/fine_food_reviews_with_embeddings_1k.csv"</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>datafile_path<span class="token punctuation">)</span>df<span class="token punctuation">[</span><span class="token string">"embedding"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token builtin">eval</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">)</span><span class="token comment"># convert 5-star rating to binary sentiment</span>df <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">.</span>Score <span class="token operator">!=</span> <span class="token number">3</span><span class="token punctuation">]</span>df<span class="token punctuation">[</span><span class="token string">"sentiment"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">.</span>Score<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">"negative"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span> <span class="token string">"negative"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span> <span class="token string">"positive"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">:</span> <span class="token string">"positive"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>每一条评论都用我们上面的方法，和一个预先设定好的好评和差评的文本去做对比，然后看它离哪个近一些。这里的好评和差评，我们写得稍微长了一点，分别是 “An Amazon review with a negative sentiment.” 和 “An Amazon review with a positive sentiment.”。</p><p>在计算完结果之后，我们利用Scikit-learn这个机器学习的库，将我们的预测值和实际用户打出的星数做个对比，然后输出对比结果。需要的代码，也就不到20行。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> PrecisionRecallDisplay<span class="token keyword">def</span> <span class="token function">evaluate_embeddings_approach</span><span class="token punctuation">(</span>    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'negative'</span><span class="token punctuation">,</span> <span class="token string">'positive'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    model <span class="token operator">=</span> EMBEDDING_MODEL<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    label_embeddings <span class="token operator">=</span> <span class="token punctuation">[</span>get_embedding<span class="token punctuation">(</span>label<span class="token punctuation">,</span> engine<span class="token operator">=</span>model<span class="token punctuation">)</span> <span class="token keyword">for</span> label <span class="token keyword">in</span> labels<span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">label_score</span><span class="token punctuation">(</span>review_embedding<span class="token punctuation">,</span> label_embeddings<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> cosine_similarity<span class="token punctuation">(</span>review_embedding<span class="token punctuation">,</span> label_embeddings<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> cosine_similarity<span class="token punctuation">(</span>review_embedding<span class="token punctuation">,</span> label_embeddings<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    probas <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"embedding"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> label_score<span class="token punctuation">(</span>x<span class="token punctuation">,</span> label_embeddings<span class="token punctuation">)</span><span class="token punctuation">)</span>    preds <span class="token operator">=</span> probas<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token string">'positive'</span> <span class="token keyword">if</span> x<span class="token operator">></span><span class="token number">0</span> <span class="token keyword">else</span> <span class="token string">'negative'</span><span class="token punctuation">)</span>    report <span class="token operator">=</span> classification_report<span class="token punctuation">(</span>df<span class="token punctuation">.</span>sentiment<span class="token punctuation">,</span> preds<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>report<span class="token punctuation">)</span>    display <span class="token operator">=</span> PrecisionRecallDisplay<span class="token punctuation">.</span>from_predictions<span class="token punctuation">(</span>df<span class="token punctuation">.</span>sentiment<span class="token punctuation">,</span> probas<span class="token punctuation">,</span> pos_label<span class="token operator">=</span><span class="token string">'positive'</span><span class="token punctuation">)</span>    _ <span class="token operator">=</span> display<span class="token punctuation">.</span>ax_<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"2-class Precision-Recall curve"</span><span class="token punctuation">)</span>evaluate_embeddings_approach<span class="token punctuation">(</span>labels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'An Amazon review with a negative sentiment.'</span><span class="token punctuation">,</span> <span class="token string">'An Amazon review with a positive sentiment.'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">              precision    recall  f1<span class="token operator">-</span>score   support    negative       <span class="token number">0.98</span>      <span class="token number">0.73</span>      <span class="token number">0.84</span>       <span class="token number">136</span>    positive       <span class="token number">0.96</span>      <span class="token number">1.00</span>      <span class="token number">0.98</span>       <span class="token number">789</span>    accuracy                           <span class="token number">0.96</span>       <span class="token number">925</span>   macro avg       <span class="token number">0.97</span>      <span class="token number">0.86</span>      <span class="token number">0.91</span>       <span class="token number">925</span>weighted avg       <span class="token number">0.96</span>      <span class="token number">0.96</span>      <span class="token number">0.96</span>       <span class="token number">925</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/0ffa4e45899cd50bc5ac7e5ea0f89c3b.png" alt="图片"></p><p>在结果里面可以看到，我们这个简单方法判定的好评差评的精度，也就是 precision 在 negative 和 positive里，分别是0.98和0.96，也就是在95%以上。</p><p>而召回率，也就是图里的 recall，在差评里稍微欠缺一点，只有73%，这说明还是有不少差评被误判为了好评。不过在好评里，召回率则是100%，也就是100%的好评都被模型找到了。这样综合考虑下来的整体准确率，高达96%。而要达到这么好的效果，我们不需要进行任何机器学习训练，只需要几行代码调用一下大模型的接口，计算一下几个向量的相似度就好了。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这一讲，我们利用不同文本在大语言模型里Embedding之间的距离，来进行情感分析。这种使用大语言模型的技巧，一般被称做零样本分类（Zero-Shot Classification）。</p><p>所谓零样本分类，也就是我们不需要任何新的样本来训练机器学习的模型，就能进行分类。我们认为，之前经过预训练的大语言模型里面，已经蕴含了情感分析的知识。我们只需要简单利用大语言模型里面知道的“好评”和“差评”的概念信息，就能判断出它从未见过的评论到底是好评还是差评。</p><p>这个方法，在一些经典的数据集上，轻易就达到了95%以上的准确度。同时，也让一些原本需要机器学习研发经验才能完成的任务变得更加容易，从而大大降低了门槛。</p><p>如果你所在的公司今天想要做一个文本分类的应用，通过OpenAI的API用几分钟时间就能得到想要的结果。</p><h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><p>利用大模型来解决问题，不是一个理论问题，而是一个实战问题。请你尝试将这个方法运用在别的数据集上，看看是否也会有很好的效果？比如，你可以试着用Kaggle提供的亚马逊耳机类商品的评论数据，看看结果是怎样的？这个数据集比较大，你可以挑几条数据试一下。因为OpenIA会对你的调用限速，免费的Token数量也比较有限。对于限速情况下的大批量数据的处理，我们会在第04讲讲解。</p><p>数据集链接： <a href="https://www.kaggle.com/datasets/shitalkat/amazonearphonesreviews">https://www.kaggle.com/datasets/shitalkat/amazonearphonesreviews</a></p><p>欢迎你把你的结果分享到评论区，也欢迎你把这节课分享给感兴趣的朋友，我们下节课再见！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;02｜无需任何机器学习，如何利用大语言模型做情感分析？&quot;&gt;&lt;a href=&quot;#02｜无需任何机器学习，如何利用大语言模型做情感分析？&quot; class=&quot;headerlink&quot; title=&quot;02｜无需任何机器学习，如何利用大语言模型做情感分析？&quot;&gt;&lt;/a&gt;02｜无需</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>03｜巧用提示语，说说话就能做个聊天机器人</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3678830634.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3678830634.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.878Z</updated>
    
    <content type="html"><![CDATA[<h1 id="03｜巧用提示语，说说话就能做个聊天机器人"><a href="#03｜巧用提示语，说说话就能做个聊天机器人" class="headerlink" title="03｜巧用提示语，说说话就能做个聊天机器人"></a>03｜巧用提示语，说说话就能做个聊天机器人</h1><p>你好，我是徐文浩。</p><p>这一讲，我们来看看Open AI提供的Completion这个API接口。相信已经有不少人试过和ChatGPT聊天了，也有过非常惊艳的体验，特别是让ChatGPT帮我们写各种材料。那么，我们不妨也从这样一个需求开始吧。</p><h2 id="AI-客服"><a href="#AI-客服" class="headerlink" title="AI 客服"></a>AI 客服</h2><p>在这一波AIGC浪潮之前，我也做过一个智能客服的产品。我发现智能客服的回答，往往是套用固定的模版。这个的缺点，就是每次的回答都一模一样。当然，我们可以设计多个模版轮换着表达相同的意思，但是最多也就是三四个模版，整体的体验还是相当呆板。</p><p>不过，有了GPT这样的生成式的语言模型，我们就可以让AI自动根据我们的需求去写文案了。只要把我们的需求提给Open AI提供的Completion接口，他就会自动为我们写出这样一段文字。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token keyword">import</span> osopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>COMPLETION_MODEL <span class="token operator">=</span> <span class="token string">"text-davinci-003"</span>prompt <span class="token operator">=</span> <span class="token string">'请你用朋友的语气回复给到客户，并称他为“亲”，他的订单已经发货在路上了，预计在3天之内会送达，订单号2021AEDG，我们很抱歉因为天气的原因物流时间比原来长，感谢他选购我们的商品。'</span><span class="token keyword">def</span> <span class="token function">get_response</span><span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> temperature <span class="token operator">=</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    completions <span class="token operator">=</span> openai<span class="token punctuation">.</span>Completion<span class="token punctuation">.</span>create <span class="token punctuation">(</span>        engine<span class="token operator">=</span>COMPLETION_MODEL<span class="token punctuation">,</span>        prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>        max_tokens<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span>        n<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>        stop<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        temperature<span class="token operator">=</span>temperature<span class="token punctuation">,</span>    <span class="token punctuation">)</span>    message <span class="token operator">=</span> completions<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text    <span class="token keyword">return</span> message<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>亲，您的订单已经顺利发货啦！订单号是2021AEDG，预计在3天之内会寄到您指定的地址。不好意思，给您带来了不便，原计划到货时间受天气原因影响而有所延迟。期待您收到衣服后给我们反馈意见哦！谢谢你选购我们的商品！</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>亲，您的订单2021AEDG刚刚已经发出，预计3天之内就会送达您的手中。抱歉由于天气的原因造成了物流延迟，但我们会尽快将订单发到您的手中。感谢您对我们的支持！</p></blockquote><p><strong>相同的提示语，连续调用两次之后，给到了含义相同、遣词造句不同的结果。</strong></p><p>我在这里列出了一段非常简单的代码。代码里面，我们给Open AI提供的Completion接口发送了一段小小的提示语（Prompt）。这段提示语要求AI用亲切的语气，告诉客户他的订单虽然已经发货，但是因为天气原因延迟了。并且我们还加了一个小小的语言风格上的要求，我们希望AI用朋友的口吻向用户说话，并且称用户为“亲”。然后，我们尝试连续用完全相同的参数调用了两次AI。</p><p>可以看到，AI的确理解了我们的意思，满足了我们的要求，给出了一段正确合理的回复。 <strong>其中有两点我觉得殊为不易</strong> <strong>。</strong></p><ol><li>他的确用“亲”来称呼了用户，并且用了一些语气词，显得比较亲切。</li><li>他正确地提取到了输入内容里的订单号，并且在回复内容里也把这个订单号返回给了用户。</li></ol><p>而且，两次返回的文案内容意思是相同的，但是具体的遣词造句又有所不同。这样通过一句合理的提示语，我们就可以让自己的智能客服自己遣词造句，而不是只能套用一个固定的模版。</p><p>而每次回复的内容不一样，则归功于我们使用的一个参数temperature。这个参数的输入范围是0-2之间的浮点数，代表输出结果的随机性或者说多样性。在这里，我们选择了1.0，也就是还是让每次生成的内容都有些不一样。你也可以把这个参数设置为0，这样，每次输出的结果的随机性就会比较小。</p><p>我将temperature设置为0，你可以看到两句内容的遣词造句就基本一致了。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>亲，您的订单2021AEDG已经发货，预计在3天之内会送达，由于天气原因，物流时间比原来长，我们深表歉意。感谢您选购我们的商品，祝您购物愉快！</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>亲，您的订单2021AEDG已经发货，预计在3天之内会送达。很抱歉因为天气的原因物流时间比原来长，感谢您选购我们的商品，祝您购物愉快！</p></blockquote><p><strong>这个参数该怎么设置，取决于实际使用的场景。</strong> 如果对应的场景比较严肃，不希望出现差错，那么设得低一点比较合适，比如银行客服的场景。如果场景没那么严肃，有趣更加重要，比如讲笑话的机器人，那么就可以设置得高一些。</p><p>既然看了temperature参数，我们也就一并看一下 Completion 这个接口里面的其他参数吧。</p><ul><li><p>第一个参数是 <strong>engine</strong>，也就是我们使用的是Open AI的哪一个引擎，这里我们使用的是 text-davinci-003，也就是现在可以使用到的最擅长根据你的指令输出内容的模型。当然，也是调用成本最高的模型。</p></li><li><p>第二个参数是 <strong>prompt</strong>，自然就是我们输入的提示语。接下来，我还会给你更多使用提示语解决不同需求的例子。</p></li><li><p>第三个参数是 <strong>max_tokens</strong>，也就是调用生成的内容允许的最大token数量。你可以简单地把token理解成一个单词。实际上，token是分词之后的一个字符序列里的一个单元。有时候，一个单词会被分解成两个token。比如，icecream是一个单词，但是实际在大语言模型里，会被拆分成 ice 和 cream 两个token。这样分解可以帮助模型更好地捕捉到单词的含义和语法结构。一般来说，750个英语单词就需要1000个token。我们这里用的 text-davinci-003 模型，允许最多有4096个token。需要注意，这个数量既包括你输入的提示语，也包括AI产出的回答，两个加起来不能超过4096个token。比如，你的输入有1000个token，那么你这里设置的 max_tokens 就不能超过 3096。不然调用就会报错。</p></li><li><p>第四个参数 <strong>n</strong>，代表你希望AI给你生成几条内容供你选择。在这样自动生成客服内容的场景里，我们当然设置成1。但是如果在一些辅助写作的场景里，你可以设置成3或者更多，供用户在多个结果里面自己选择自己想要的。</p></li><li><p>第五个参数 <strong>stop</strong>，代表你希望模型输出的内容在遇到什么内容的时候就停下来。这个参数我们常常会选用 “\n\n”这样的连续换行，因为这通常意味着文章已经要另起一个新的段落了，既会消耗大量的token数量，又可能没有必要。我们在下面试了一下，将“，”作为stop的参数，你会发现模型在输出了“亲”之后就停了下来。</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token string">"，"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>亲</p></blockquote><p>Completion这个接口当然还有其他参数，不过一时半会儿我们还用不着，后面实际用得上的时候我们再具体介绍。如果你现在就想知道，那么可以去查看一下 <a href="https://platform.openai.com/docs/api-reference/completions/create">官方文档</a>。如果你觉得英语不太好，可以试着用“请用中文解释一下这段话的意思”作为提示语，调用Open AI的模型来理解文档的含义。</p><h2 id="AI-聊天机器人"><a href="#AI-聊天机器人" class="headerlink" title="AI 聊天机器人"></a>AI 聊天机器人</h2><p>上面我们知道了怎么用一句提示语让AI完成一个任务，就是回答一个问题。不过，我们怎么能让AI和人“聊起来”呢？特别是怎么完成多轮对话，让GPT能够记住上下文。比如，当用户问我们，“iPhone14拍照好不好”，我们回答说“很好”。然后又问“它的价格是多少的时候”，我们需要理解，用户这里问的“它”就是指上面的iPhone。</p><p>对于聊天机器人来说，只理解当前用户的句子是不够的，能够理解整个上下文是必不可少的。而GPT的模型，要完成支持多轮的问答也并不复杂。我们只需要在提示语里增加一些小小的工作就好了。</p><p>想要实现问答，我们只需要在提示语里，在问题之前加上 <strong>“Q :”</strong> 表示这是一个问题，然后另起一行，加上 <strong>“A :”</strong> 表示我想要一个回答，那么 Completion的接口就会回答你在 “Q : ” 里面跟的问题。比如下面，我们问AI “鱼香肉丝怎么做”。它就一步一步地列出了制作步骤。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">question <span class="token operator">=</span>  <span class="token triple-quoted-string string">"""Q : 鱼香肉丝怎么做？A :"""</span><span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>question<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ol><li>准备好食材：500克猪里脊肉，2个青椒，2个红椒，1个洋葱，2勺蒜蓉，3勺白糖，适量料酒，半勺盐，2勺生抽，2勺酱油，2勺醋，少许花椒粉，半勺老抽，适量水淀粉。</li><li>将猪里脊肉洗净，沥干水分，放入料酒、盐，抓捏抓匀，腌制20分钟。</li><li>将青红椒洗净，切成丝，洋葱洗净，切成葱花，蒜末拌入小苏打水中腌制。</li><li>将猪里脊肉切成丝，放入锅中，加入洋葱，炒制至断生，加入青红椒，炒匀，加入腌制好的蒜末，炒制至断生。</li><li>将白糖、生抽、酱油、醋、花椒粉、老抽、水淀粉倒入锅中，翻炒匀，用小火收汁，调味即可。</li></ol></blockquote><p>而要完成多轮对话其实也不麻烦，我们只要把之前对话的内容也都放到提示语里面，把整个上下文都提供给AI。AI就能够自动根据上下文，回答第二个问题。比如，你接着问“那蚝油牛肉呢？”。我们不要只是把这个问题传给AI，而是把前面的对话也一并传给AI，那么AI自然知道你问的“那蚝油牛肉呢？”是指怎么做，而不是去哪里买或者需要多少钱。</p><ul><li>Q：鱼香肉丝怎么做？</li><li>A：详细的鱼香肉丝的做法</li><li>Q：那蚝油牛肉呢？</li><li>A：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">question <span class="token operator">=</span>  <span class="token triple-quoted-string string">"""Q : 鱼香肉丝怎么做？A :1.准备好食材：500克猪里脊肉，2个青椒，2个红椒，1个洋葱，2勺蒜蓉，3勺白糖，适量料酒，半勺盐，2勺生抽，2勺酱油，2勺醋，少许花椒粉，半勺老抽，适量水淀粉。2.将猪里脊肉洗净，沥干水分，放入料酒、盐，抓捏抓匀，腌制20分钟。3.将青红椒洗净，切成丝，洋葱洗净，切成葱花，蒜末拌入小苏打水中腌制。4.将猪里脊肉切成丝，放入锅中，加入洋葱，炒制至断生，加入青红椒，炒匀，加入腌制好的蒜末，炒制至断生。5.将白糖、生抽、酱油、醋、花椒粉、老抽、水淀粉倒入锅中，翻炒匀，用小火收汁，调味即可。Q : 那蚝油牛肉呢？A :"""</span><span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>question<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>1.准备好食材：500克牛肉，2茶匙葱姜蒜末，6茶匙蚝油，4茶匙米醋，3茶匙白糖，3茶匙酱油，1茶匙料酒，半茶匙盐。</p><p>2.将牛肉洗净，放入清水中，加入料酒、盐，煮至牛肉熟透，捞出沥干水分，放入碗中。</p><p>3.在另一锅中，倒入蚝油，米醋，白糖，酱油翻炒均匀，加入葱姜蒜末，翻炒均匀。</p><p>4.将牛肉碗中放入蚝油汁中，炒匀，加入酱油，翻炒至汁呈红色，收汁成浓稠状即可。</p></blockquote><p>我在下面贴了一段完整的Python代码叫做food_chatbot，它会从命令行读入你的问题，然后给出回答。你可以继续提问，然后我们把整个对话过程都发送给AI来回答。你可以尝试着体验一下，AI是不是能够理解整个对话过程的上下文。你想要退出的时候，就在需要提问的时候，输入 “bye” 就好了。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token keyword">import</span> osopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">]</span><span class="token keyword">def</span> <span class="token function">ask_gpt3</span><span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">:</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>Completion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        engine<span class="token operator">=</span><span class="token string">"text-davinci-003"</span><span class="token punctuation">,</span>        prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>        max_tokens<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>        n<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>        stop<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span>    message <span class="token operator">=</span> response<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> message<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"你好，我是一个聊天机器人，请你提出你的问题吧?"</span><span class="token punctuation">)</span>questions <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>answers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">def</span> <span class="token function">generate_prompt</span><span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> questions<span class="token punctuation">,</span> answers<span class="token punctuation">)</span><span class="token punctuation">:</span>    num <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>answers<span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">:</span>        prompt <span class="token operator">+=</span> <span class="token string">"\n Q : "</span> <span class="token operator">+</span> questions<span class="token punctuation">[</span>i<span class="token punctuation">]</span>        prompt <span class="token operator">+=</span> <span class="token string">"\n A : "</span> <span class="token operator">+</span> answers<span class="token punctuation">[</span>i<span class="token punctuation">]</span>    prompt <span class="token operator">+=</span> <span class="token string">"\n Q : "</span> <span class="token operator">+</span> questions<span class="token punctuation">[</span>num<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">"\n A : "</span>    <span class="token keyword">return</span> prompt<span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>    user_input <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">(</span><span class="token string">"> "</span><span class="token punctuation">)</span>    questions<span class="token punctuation">.</span>append<span class="token punctuation">(</span>user_input<span class="token punctuation">)</span>    <span class="token keyword">if</span> user_input<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"bye"</span><span class="token punctuation">,</span> <span class="token string">"goodbye"</span><span class="token punctuation">,</span> <span class="token string">"exit"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Goodbye!"</span><span class="token punctuation">)</span>        <span class="token keyword">break</span>    prompt <span class="token operator">=</span> generate_prompt<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">,</span> questions<span class="token punctuation">,</span> answers<span class="token punctuation">)</span>    answer <span class="token operator">=</span> ask_gpt3<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>answer<span class="token punctuation">)</span>    answers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>answer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="让AI帮我解决情感分析问题"><a href="#让AI帮我解决情感分析问题" class="headerlink" title="让AI帮我解决情感分析问题"></a>让AI帮我解决情感分析问题</h2><p>可以看到，巧妙地利用提示语，我们就能够让AI完成多轮的问答。那你是不是想到了，我们能不能用同样的方式，来解决上一讲我们说到的情感分析问题呢？毕竟，很多人可能没有学习过任何机器学习知识，对于向量距离之类的概念也忘得差不多了。那么，我们能不能不用任何数学概念，完全用自然语言的提示语，让AI帮助我们判断一下用户评论的情感是正面还是负面的呢？</p><p>那我们不妨来试一下，告诉AI我们想要它帮助我们判断用户的评论情感上是正面的还是负面的，并且把上一讲两个iPhone评论的例子给它，告诉它什么是正面的，什么是负面的。然后，再给他一段新的评论，看看他是不是会回复正确的答案。</p><p>我把对应的代码放在了下面，我们仍然只是简单地调用 Completion 的API一次。只是需要再把提示语分成三个组成部分。</p><ol><li>第一部分是我们给到AI的指令，也就是告诉它要去判断用户评论的情感。</li><li>第二部分是按照一个固定格式给它两个例子，一行以“评论：”开头，后面跟着具体的评论，另一行以“情感：”开头，后面跟着这个例子的情感。</li><li>第三部分是给出我们希望AI判定的评论，同样以“评论：”开头跟着我们想要它判定的评论，另一行也以“情感：”开头，不过后面没有内容，而是等着AI给出判定。</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python">prompts <span class="token operator">=</span> <span class="token triple-quoted-string string">"""判断一下用户的评论情感上是正面的还是负面的评论：买的银色版真的很好看，一天就到了，晚上就开始拿起来完系统很丝滑流畅，做工扎实，手感细腻，很精致哦苹果一如既往的好品质情感：正面评论：随意降价，不予价保，服务态度差情感：负面"""</span>good_case <span class="token operator">=</span> prompts <span class="token operator">+</span> <span class="token triple-quoted-string string">"""评论：外形外观：苹果审美一直很好，金色非常漂亮拍照效果：14pro升级的4800万像素真的是没的说，太好了，运行速度：苹果的反应速度好，用上三五年也不会卡顿的，之前的7P用到现在也不卡其他特色：14pro的磨砂金真的太好看了，不太高调，也不至于没有特点，非常耐看，很好的情感："""</span><span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>good_case<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python">正面<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python">bad_case <span class="token operator">=</span> prompts <span class="token operator">+</span> <span class="token triple-quoted-string string">"""评论：信号不好电池也不耐电不推荐购买情感"""</span><span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>bad_case<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python">负面<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>我们重新从京东商城的iPhone评论区随机找两个和上次不太一样的好评和差评，可以看到，结果是准确的。这是不是很棒？我们不需要任何机器学习的相关知识，用几句话就能够轻松搞定情感分析问题。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/b2b5ba052986548ba03ff72ff09fca46.png" alt="图片">注：常见的大模型的上下文学习能力，通过几个例子，就能回答正确的结果。</p><p>而上面这个“给一个任务描述、给少数几个例子、给需要解决的问题”这样三个步骤的组合，也是大语言模型里使用提示语的常见套路。一般我们称之为 <strong>Few-Shots Learning（少样本学习）</strong>，也就是给一个或者少数几个例子，AI就能够举一反三，回答我们的问题。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，到这里相信你已经体会到Completion这个接口的魔力了。只要给出合理的提示语，Open AI的大语言模型就能神奇地完成我们想要完成的任务。</p><p>在这一讲里，我们就看到了三个例子，第一个是给AI一个明确的指令，让它帮我重写一段话。第二个，是将整个对话的历史记录都发送出去，并且通过Q和A提示AI这是一段对话，那么AI自然能够理解整个上下文，搞清楚新的问题是指“蚝油牛肉怎么做”而不是“哪里买或者怎么吃”。而第三个例子，我们则是给了AI几个正面情感和负面情感的例子，它就能够直接对新的评论做出准确的情感判断。</p><p>可以看到，善用合适的提示语，能够让大语言模型完成很多任务。这也是为什么，我们认为它已经是我们迈向通用人工智能的第一步。</p><h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><p>上一讲我给出了一些数据集，通过向量距离来进行正面和负面情感的判断。那么，你能不能试着用这一讲的Few-Shots Learning的方法，在对应的数据集上也运行一下，看看通过这种方式进行情感分析的准确率是多少？</p><p>欢迎你在留言区分享你的思考，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p><h2 id="推荐尝试"><a href="#推荐尝试" class="headerlink" title="推荐尝试"></a>推荐尝试</h2><p>在使用GPT类型的模型的时候，提示语非常重要。所以，有人专门制作了一个叫做 <a href="https://chrome.google.com/webstore/detail/aiprm-for-chatgpt/ojnbohmppadfgpejeebfnmnknjdlckgj">AIPRM的浏览器插件</a>，包含了很多总结出来有效的提示语。你可以试着安装，看看别人都用了什么有趣的提示语。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;03｜巧用提示语，说说话就能做个聊天机器人&quot;&gt;&lt;a href=&quot;#03｜巧用提示语，说说话就能做个聊天机器人&quot; class=&quot;headerlink&quot; title=&quot;03｜巧用提示语，说说话就能做个聊天机器人&quot;&gt;&lt;/a&gt;03｜巧用提示语，说说话就能做个聊天机器人&lt;/</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>04｜新时代模型性能大比拼，GPT-3到底胜在哪里？</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3487919734.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3487919734.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.881Z</updated>
    
    <content type="html"><![CDATA[<h1 id="04｜新时代模型性能大比拼，GPT-3到底胜在哪里？"><a href="#04｜新时代模型性能大比拼，GPT-3到底胜在哪里？" class="headerlink" title="04｜新时代模型性能大比拼，GPT-3到底胜在哪里？"></a>04｜新时代模型性能大比拼，GPT-3到底胜在哪里？</h1><p>你好，我是徐文浩。</p><p>前面两讲，我带你体验了OpenAI通过API提供的GPT-3.5系列模型的两个核心接口。一个是获取一段文本的Embedding向量，另一个则是根据提示语，直接生成一段补全的文本内容。我们用这两种方法，都可以实现零样本（zero-shot）或者少样本下的情感分析任务。不过，你可能会提出这样两个疑问。</p><ol><li>Embedding不就是把文本变成向量吗？我也学过一些自然语言处理，直接用个开源模型，比如Word2Vec、Bert之类的就好了呀，何必要去调用OpenAI的API呢？</li><li>我们在这个情感分析里做了很多投机取巧的工作。一方面，我们把3分这种相对中性的评价分数排除掉了；另一方面，我们把1分2分和4分5分分别合并在了一起，把一个原本需要判断5个分类的问题简化了。那如果我们想要准确地预测多个分类，也会这么简单吗？</li></ol><p>那么，这一讲我们就先来回答第一个问题。我们还是拿代码和数据来说话，就拿常见的开源模型来试一试，看看能否通过零样本学习的方式来取得比较好的效果。第二个问题，我们下一讲再来探讨，看看能不能利用Embedding进一步通过一些机器学习的算法，来更好地处理情感分析问题。</p><h2 id="什么是预训练模型？"><a href="#什么是预训练模型？" class="headerlink" title="什么是预训练模型？"></a>什么是预训练模型？</h2><p>给出一段文本，OpenAI就能返回给你一个Embedding向量，这是因为它的背后是GPT-3这个超大规模的预训练模型（Pre-trained Model）。事实上，GPT的英文全称翻译过来就是“生成式预训练Transformer（Generative Pre-trained Transformer）”。</p><p>所谓预训练模型，就是虽然我们没有看过你想要解决的问题，比如这里我们在情感分析里看到的用户评论和评分。但是，我可以拿很多我能找到的文本，比如网页文章、维基百科里的文章，各种书籍的电子版等等， <strong>作为理解文本内容的一个学习资料</strong>。</p><p>我们不需要对这些数据进行人工标注，只根据这些文本前后的内容，来习得文本之间内在的关联。比如，网上的资料里，会有很多“小猫很可爱”、“小狗很可爱”这样的文本。小猫和小狗后面都会跟着“很可爱”，那么我们就会知道小猫和小狗应该是相似的词，都是宠物。同时，一般我们对于它们的情感也是正面的。这些隐含的内在信息，在我们做情感分析的时候，就带来了少量用户评论和评分数据里缺少的“常识”，这些“常识”也有助于我们更好地预测。</p><p>比如，文本里有“白日依山尽”，那么模型就知道后面应该跟“黄河入海流”。文本前面是“今天天气真”，后面跟着的大概率是“不错”，小概率是“糟糕”。这些文本关系，最后以一堆参数的形式体现出来。对于你输入的文本，它可以根据这些参数计算出一个向量，然后根据这个向量，来推算这个文本后面的内容。</p><p>可以这样来理解： <strong>用来训练的语料文本越丰富，模型中可以放的参数越多，那模型能够学到的关系也就越多。类似的情况在文本里出现得越多，那么将来模型猜得也就越准。</strong></p><p>预训练模型在自然语言处理领域并不是OpenAI的专利。早在2013年，就有一篇叫做Word2Vec的经典论文谈到过。它能够通过预训练，根据同一个句子里一个单词前后出现的单词，来得到每个单词的向量。而在2018年，Google关于BERT的论文发表之后，整个业界也都会使用BERT这样的预训练模型，把一段文本变成向量用来解决自己的自然语言处理任务。在GPT-3论文发表之前，大家普遍的结论是，BERT作为预训练的模型效果也是优于GPT的。</p><h2 id="Fasttext、T5、GPT-3模型效果大比拼"><a href="#Fasttext、T5、GPT-3模型效果大比拼" class="headerlink" title="Fasttext、T5、GPT-3模型效果大比拼"></a>Fasttext、T5、GPT-3模型效果大比拼</h2><p>今天我们就拿两个开源的预训练模型，来看看直接用它们对文本进行向量化，是不是也能取得和OpenAI的API一样好的效果。</p><p>第一个是来自Facebook的Fasttext，它继承了Word2Vec的思路，能够把一个个单词表示成向量。第二个是来自Google的T5，T5的全称是Text-to-Text Transfer Trasnformer，是适合做迁移学习的一个模型。所谓迁移学习，也就是它推理出来向量的结果，常常被拿来再进行机器学习，去解决其他自然语言处理问题。通常很多新发表的论文，会把T5作为预训练模型进行微调和训练，或者把它当作Benchmark来对比、评估。</p><h3 id="Fasttext效果测试"><a href="#Fasttext效果测试" class="headerlink" title="Fasttext效果测试"></a>Fasttext效果测试</h3><p>我们先来试一下Fasttext，在实际运行代码之前，我们需要先安装Fasttext和Gensim这两个Python包。我在下面列出了通过Conda安装对应Python包的代码，如果你使用的是PIP或者其他的Python包管理工具，你就换成对应的PIP命令就好了。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">conda <span class="token function">install</span> gensimconda <span class="token function">install</span> fasttext<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>然后，我们要把Fasttext对应的模型下载到本地。因为这些开源库和对应的论文都是Facebook和Google这样的海外公司发布的，效果自然是在英语上比较好，所以我们就下载对应的英语模型，名字叫做 “cc.en.300.bin”。同样的，对应模型的下载链接，我也放在 <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz">这里</a> 了。</p><p>下载之后解压，然后把文件放在和Notebook相同的目录下，方便我们接下来运行代码。</p><p>这里我们拿来测试效果的数据集还是和 <a href="https://time.geekbang.org/column/article/642179">第 02 讲</a> 一样，用的是 2.5w 条亚马逊食物评论的数据。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/1288595dd50b5aa60250daa94f5fd346.png"></p><p>注：各种语言的 <a href="https://fasttext.cc/docs/en/crawl-vectors.html">Fasttext模型</a>。</p><p>代码的逻辑也不复杂，我们先利用Gensim这个库，把Facebook预训练好的模型加载进来。然后，我们定义一个获取文本向量的函数。因为Fasttext学到的是单词的向量，而不是句子的向量。同时，因为我们想要测试一下零样本学习的效果，不能再根据拿到的评论数据进一步训练模型了。所以我们把一句话里每个单词的向量，加在一起平均一下，把得到的向量作为整段评论的向量。这个方法也是当年常用的 <strong>一种将一句话变成向量的办法</strong>。我们把这个操作定义成了 get_fasttext_vector 这个函数，供后面的程序使用。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> gensim<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment"># Load the FastText pre-trained model</span>model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>fasttext<span class="token punctuation">.</span>load_facebook_model<span class="token punctuation">(</span><span class="token string">'cc.en.300.bin'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">get_fasttext_vector</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">:</span>    vec <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">)</span> <span class="token comment"># Initialize an empty 300-dimensional vector</span>    <span class="token keyword">for</span> word <span class="token keyword">in</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        vec <span class="token operator">+=</span> model<span class="token punctuation">.</span>wv<span class="token punctuation">[</span>word<span class="token punctuation">]</span>    vec <span class="token operator">/=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># Take the average over all words in the line</span>    <span class="token keyword">return</span> vec<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而对应的零样本学习，我们还是和 <a href="https://time.geekbang.org/column/article/642179">第 02 讲</a> 一样，将需要进行情感判断的评论分别与 “An Amazon review with a positive sentiment.” 以及 “An Amazon review with a negative sentiment.” 这两句话进行向量计算，算出它们之间的余弦距离。</p><p><strong>离前一个近，我们就认为是正面情感，离后一个近就是负面情感。</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">positive_text <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Wanted to save some to bring to my Chicago family but my North Carolina family ate all 4 boxes before I could pack. These are excellent...could serve to anyone"""</span>negative_text <span class="token operator">=</span> <span class="token triple-quoted-string string">"""First, these should be called Mac - Coconut bars, as Coconut is the #2 ingredient and Mango is #3.  Second, lots of people don't like coconut.  I happen to be allergic to it.  Word to Amazon that if you want happy customers to make things like this more prominent.  Thanks."""</span>positive_example_in_fasttext <span class="token operator">=</span> get_fasttext_vector<span class="token punctuation">(</span>positive_text<span class="token punctuation">)</span>negative_example_in_fasttext <span class="token operator">=</span> get_fasttext_vector<span class="token punctuation">(</span>negative_text<span class="token punctuation">)</span>positive_review_in_fasttext <span class="token operator">=</span> get_fasttext_vector<span class="token punctuation">(</span><span class="token string">"An Amazon review with a positive sentiment."</span><span class="token punctuation">)</span>negative_review_in_fasttext <span class="token operator">=</span> get_fasttext_vector<span class="token punctuation">(</span><span class="token string">'An Amazon review with a negative sentiment.'</span><span class="token punctuation">)</span><span class="token keyword">from</span> openai<span class="token punctuation">.</span>embeddings_utils <span class="token keyword">import</span> cosine_similarity<span class="token keyword">def</span> <span class="token function">get_fasttext_score</span><span class="token punctuation">(</span>sample_embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">return</span> cosine_similarity<span class="token punctuation">(</span>sample_embedding<span class="token punctuation">,</span> positive_review_in_fasttext<span class="token punctuation">)</span> <span class="token operator">-</span> cosine_similarity<span class="token punctuation">(</span>sample_embedding<span class="token punctuation">,</span> negative_review_in_fasttext<span class="token punctuation">)</span>positive_score <span class="token operator">=</span> get_fasttext_score<span class="token punctuation">(</span>positive_example_in_fasttext<span class="token punctuation">)</span>negative_score <span class="token operator">=</span> get_fasttext_score<span class="token punctuation">(</span>negative_example_in_fasttext<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Fasttext好评例子的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>positive_score<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Fasttext差评例子的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>negative_score<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">Fasttext好评例子的评分 <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.000544</span>Fasttext差评例子的评分 <span class="token punctuation">:</span> <span class="token number">0.000369</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>我们从亚马逊食物评论的数据集里，选取了一个用户打5分的正面例子和一个用户打1分的例子试了一下。结果非常不幸，通过这个零样本学习的方式，这两个例子，程序都判断错了。</p><p>不过，仔细想一下，这样的结果也正常。因为这里的整句向量就是把所有单词的向量平均了一下。这意味着， <strong>可能会出现我们之前说过的单词相同顺序不同的问题。</strong></p><p>“not good, really bad” 和 “not bad, really good”，在这个情况下，意思完全不同，但是向量完全相同。更何况，我们拿来做对比的正面情感和负面情感的两句话，只差了positive&#x2F;negative这样一个单词。不考虑单词的顺序，而只考虑出现了哪些单词，并且不同单词之间还平均一下。这种策略要是真的有很好的效果，你反而要担心是不是哪里有Bug。</p><h3 id="T5效果测试"><a href="#T5效果测试" class="headerlink" title="T5效果测试"></a>T5效果测试</h3><p>Fasttext出师不利，毕竟Word2Vec已经是10年前的技术了，可以理解。那么，我们来看看和GPT一样使用了现在最流行的Transformer结构的T5模型效果怎么样。</p><p>T5模型的全称是Text-to-Text Transfer Transformer，翻译成中文就是“文本到文本的迁移Transformer”，也就是说，这个模型就是为了方便预训练之后拿去“迁移”到别的任务上而创造出来的。当时发表的时候，它就在各种数据集的评测上高居榜首。</p><p>T5最大的模型也有110亿个参数，也是基于Transformer，虽然比起GPT-3的1750亿小了不少，但是对硬件的性能要求也不低。所以，我们先测试一下T5-Small这个小模型看看效果。</p><p>同样的，在实际运行代码之前，我们也需要安装对应的Python包。这里我们分别安装了SentencePiece和PyTorch。在安装PyTorch的时候，我一并安装了Torchvision，后面课程会用到。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conda install transformers <span class="token operator">-</span>c conda<span class="token operator">-</span>forgeconda install pytorch torchvision <span class="token operator">-</span>c pytorchconda install sentencepiece<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>代码也不复杂，我们先加载预训练好的T5模型的分词器（Tokenizer），还有对应的模型。然后，我们定义了一个get_t5_vector函数，它会接收一段你的文本输入，然后用分词器来分词把结果变成一个序列，然后让模型的编码器部分对其进行编码。编码后的结果，仍然是分词后的一个词一个向量，我们还是把这些向量平均一下，作为整段文本的向量。</p><p>不过要注意，虽然同样是平均，但是和前面Fasttext不一样的是，这里每个词的向量，随着位置以及前后词的不同，编码出来的结果是不一样的。所以 <strong>这个平均值里，仍然包含了顺序带来的语义信息。</strong></p><p>这段代码执行的过程可能会有点慢。因为第一次加载模型的时候，Transformer库会把模型下载到本地并缓存起来，整个下载过程会花一些时间。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> T5Tokenizer<span class="token punctuation">,</span> T5Model<span class="token keyword">import</span> torch<span class="token comment"># load the T5 tokenizer and model</span>tokenizer <span class="token operator">=</span> T5Tokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'t5-small'</span><span class="token punctuation">,</span> model_max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">)</span>model <span class="token operator">=</span> T5Model<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'t5-small'</span><span class="token punctuation">)</span><span class="token comment"># set the model to evaluation mode</span>model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># encode the input sentence</span><span class="token keyword">def</span> <span class="token function">get_t5_vector</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">:</span>    input_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>line<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">'pt'</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># generate the vector representation</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">)</span>        vector <span class="token operator">=</span> outputs<span class="token punctuation">.</span>last_hidden_state<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> vector<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>有了模型和通过模型获取的向量数据，我们就可以再试一试前面的零样本学习的方式，来看看效果怎么样了。我们简单地把之前获取向量和计算向量的函数调用，都换成新的get_t5_vector，运行一下就能看到结果了。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">positive_review_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span><span class="token string">"An Amazon review with a positive sentiment."</span><span class="token punctuation">)</span>negative_review_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span><span class="token string">'An Amazon review with a negative sentiment.'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">test_t5</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  positive_example_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span>positive_text<span class="token punctuation">)</span>  negative_example_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span>negative_text<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">get_t5_score</span><span class="token punctuation">(</span>sample_embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> cosine_similarity<span class="token punctuation">(</span>sample_embedding<span class="token punctuation">,</span> positive_review_in_t5<span class="token punctuation">)</span> <span class="token operator">-</span> cosine_similarity<span class="token punctuation">(</span>sample_embedding<span class="token punctuation">,</span> negative_review_in_t5<span class="token punctuation">)</span>  positive_score <span class="token operator">=</span> get_t5_score<span class="token punctuation">(</span>positive_example_in_t5<span class="token punctuation">)</span>  negative_score <span class="token operator">=</span> get_t5_score<span class="token punctuation">(</span>negative_example_in_t5<span class="token punctuation">)</span>  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"T5好评例子的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>positive_score<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"T5差评例子的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>negative_score<span class="token punctuation">)</span><span class="token punctuation">)</span>test_t5<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">T5好评例子的评分 <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.010294</span>T5差评例子的评分 <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.008990</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>不幸的是，结果还是不太好，两个例子都被判断成了负面情绪，而且好评的分数还更低一点。不过别着急，会不会是我们用的模型太小了呢？毕竟T5论文里霸占各个排行榜的是110亿个参数的大模型，我们这里用的是T5-Small这个同样架构下的小模型，参数数量只有6000万个。</p><p>110亿个参数要花太多时间了，我们不妨把模型放大一下，试试有2.2亿个参数的 T5-Base这个模型？试用起来也很简单，我们就直接把上面模型的名字从 T5-small 改成 T5-base 就好了，其他代码不需要动，重新运行一遍。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">tokenizer <span class="token operator">=</span> T5Tokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'t5-base'</span><span class="token punctuation">,</span> model_max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">)</span>model <span class="token operator">=</span> T5Model<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'t5-base'</span><span class="token punctuation">)</span><span class="token comment"># set the model to evaluation mode</span>model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># encode the input sentence</span><span class="token keyword">def</span> <span class="token function">get_t5_vector</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">:</span>    input_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>line<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">'pt'</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># generate the vector representation</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">)</span>        vector <span class="token operator">=</span> outputs<span class="token punctuation">.</span>last_hidden_state<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> vector<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>positive_review_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span><span class="token string">"An Amazon review with a positive sentiment."</span><span class="token punctuation">)</span>negative_review_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span><span class="token string">'An Amazon review with a negative sentiment.'</span><span class="token punctuation">)</span>test_t5<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">T5好评例子的评分 <span class="token punctuation">:</span> <span class="token number">0.010347</span>T5差评例子的评分 <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.023935</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这一次，结果似乎是我们想要的了，好评被判定为正面情感，而差评被判定为负面情感。不过，也许我们只是运气好，在这一两个例子上有效果呢？所以，接下来让我们把整个数据集里，1分2分的差评和4分5分的好评都拿出来看一看。在OpenAI的API拿到的Embedding里，它的准确率能够达到95%以上，我们看看用这个有2.2亿个参数的T5-base模型能有什么样的结果。</p><p>对应的代码也不复杂，基本上和 <a href="https://time.geekbang.org/column/article/642179">第 02 讲</a> 里OpenAI给到的Embedding代码是类似的。无非是通过pandas，根据评论的Text字段，全部计算一遍T5下的Embedding，然后存到DataFrame的t5_embedding 里去。</p><p>同样的，我们还是要通过T5的模型，来获得 “An Amazon review with a positive sentiment.” 以及 “An Amazon review with a negative sentiment.” 这两句话的Embedding。然后，我们用刚刚计算的用户评论的Embedding和这两句话计算一下余弦距离，来判断这些评论是正面还是负面的。</p><p>最后，通过Scikit-learn的分类报告的类库把评估的报告结果打印出来。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> classification_reportdatafile_path <span class="token operator">=</span> <span class="token string">"data/fine_food_reviews_with_embeddings_1k.csv"</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>datafile_path<span class="token punctuation">)</span>df<span class="token punctuation">[</span><span class="token string">"t5_embedding"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">.</span>Text<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>get_t5_vector<span class="token punctuation">)</span><span class="token comment"># convert 5-star rating to binary sentiment</span>df <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">.</span>Score <span class="token operator">!=</span> <span class="token number">3</span><span class="token punctuation">]</span>df<span class="token punctuation">[</span><span class="token string">"sentiment"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">.</span>Score<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">"negative"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span> <span class="token string">"negative"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span> <span class="token string">"positive"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">:</span> <span class="token string">"positive"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> PrecisionRecallDisplay<span class="token keyword">def</span> <span class="token function">evaluate_embeddings_approach</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">label_score</span><span class="token punctuation">(</span>review_embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> cosine_similarity<span class="token punctuation">(</span>review_embedding<span class="token punctuation">,</span> positive_review_in_t5<span class="token punctuation">)</span> <span class="token operator">-</span> cosine_similarity<span class="token punctuation">(</span>review_embedding<span class="token punctuation">,</span> negative_review_in_t5<span class="token punctuation">)</span>    probas <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"t5_embedding"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> label_score<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>    preds <span class="token operator">=</span> probas<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token string">'positive'</span> <span class="token keyword">if</span> x<span class="token operator">></span><span class="token number">0</span> <span class="token keyword">else</span> <span class="token string">'negative'</span><span class="token punctuation">)</span>    report <span class="token operator">=</span> classification_report<span class="token punctuation">(</span>df<span class="token punctuation">.</span>sentiment<span class="token punctuation">,</span> preds<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>report<span class="token punctuation">)</span>    display <span class="token operator">=</span> PrecisionRecallDisplay<span class="token punctuation">.</span>from_predictions<span class="token punctuation">(</span>df<span class="token punctuation">.</span>sentiment<span class="token punctuation">,</span> probas<span class="token punctuation">,</span> pos_label<span class="token operator">=</span><span class="token string">'positive'</span><span class="token punctuation">)</span>    _ <span class="token operator">=</span> display<span class="token punctuation">.</span>ax_<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"2-class Precision-Recall curve"</span><span class="token punctuation">)</span>evaluate_embeddings_approach<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">              precision    recall  f1<span class="token operator">-</span>score   support    negative       <span class="token number">0.60</span>      <span class="token number">0.90</span>      <span class="token number">0.72</span>       <span class="token number">136</span>    positive       <span class="token number">0.98</span>      <span class="token number">0.90</span>      <span class="token number">0.94</span>       <span class="token number">789</span>    accuracy                           <span class="token number">0.90</span>       <span class="token number">925</span>   macro avg       <span class="token number">0.79</span>      <span class="token number">0.90</span>      <span class="token number">0.83</span>       <span class="token number">925</span>weighted avg       <span class="token number">0.93</span>      <span class="token number">0.90</span>      <span class="token number">0.91</span>       <span class="token number">925</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ac3b30d430fb855f1382bc845b6798af.png" alt="图片"></p><p>结果显示，使用T5的效果也还可以，考虑所有样本的准确率也能达到90%。但是，在比较困难的差评的判断里，它的表现要比直接用OpenAI给到的Embedding要差很多，整体的精度只有60%。我们去看整体模型的准确率的话，OpenAI的Embedding能够到达96%，还是比这里的90%要好上一些的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">            precision    recall  f1<span class="token operator">-</span>score   support    negative       <span class="token number">0.98</span>      <span class="token number">0.73</span>      <span class="token number">0.84</span>       <span class="token number">136</span>    positive       <span class="token number">0.96</span>      <span class="token number">1.00</span>      <span class="token number">0.98</span>       <span class="token number">789</span>    accuracy                           <span class="token number">0.96</span>       <span class="token number">925</span>   macro avg       <span class="token number">0.97</span>      <span class="token number">0.86</span>      <span class="token number">0.91</span>       <span class="token number">925</span>weighted avg       <span class="token number">0.96</span>      <span class="token number">0.96</span>      <span class="token number">0.96</span>       <span class="token number">925</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里我重新贴一下使用OpenAI的Embedding取得的效果，你可以做个对比。</p><p>当然，这个分数也还不错，也能作为一个合格的情感分析分类器的基准线了。毕竟，我们这里采用的是零样本分类的方法，没有对需要分类的数据做任何训练，使用的完全是预训练模型给出来的向量，直接根据距离做的判断。所以，看起来大一点的预训练模型的确有用，能够取得更好的效果。而且，当你因为成本或者网络延时的问题，不方便使用OpenAI的API的时候，如果只是要获取文本的Embedding向量，使用T5这样的开源模型其实效果也还不错。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>最后，我们来复习一下这一讲的内容。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/a2d8bc53ca126473543868e262d42baf.jpg"></p><p>这一讲我们一起使用 Fasttext、T5-small 和 T5-base 这三个预训练模型，做了零样本分类测试。在和之前相同的食物评论的数据集上，使用只学习了单词向量表示的Fasttext，效果很糟糕。当我们换用同样基于Transformer的T5模型的时候，T5-small这个6000万参数的小模型其实效果也不好。但是当我们用上2.2亿参数的T5-base模型的时候，结果还可以。不过，还是远远比不上直接使用OpenAI的API的效果。可见，模型的大小，即使是对情感分析这样简单的问题，也能产生明显的差距。</p><h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><ol><li>我们在尝试使用T5-base这个模型之后，下了个判断认为大一点的模型效果更好。不过，其实我们并没有在整个数据集上使用T5-small这个模型做评测，你能试着修改一下代码，用T5-small测试一下整个数据集吗？测试下来的效果又是怎样的呢？</li><li>我们使用Fasttext的时候，把所有的单词向量平均一下，用来做情感分析效果很糟糕。那么什么样的分类问题，可以使用这样的方式呢？给你一个小提示，你觉得什么样的文本分类，只关心出现的单词是什么，而不关心它们的顺序？</li></ol><p>期待你的思考，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;04｜新时代模型性能大比拼，GPT-3到底胜在哪里？&quot;&gt;&lt;a href=&quot;#04｜新时代模型性能大比拼，GPT-3到底胜在哪里？&quot; class=&quot;headerlink&quot; title=&quot;04｜新时代模型性能大比拼，GPT-3到底胜在哪里？&quot;&gt;&lt;/a&gt;04｜新时代模型</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>05｜善用Embedding，我们来给文本分分类</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3014729890.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3014729890.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.885Z</updated>
    
    <content type="html"><![CDATA[<h1 id="05｜善用Embedding，我们来给文本分分类"><a href="#05｜善用Embedding，我们来给文本分分类" class="headerlink" title="05｜善用Embedding，我们来给文本分分类"></a>05｜善用Embedding，我们来给文本分分类</h1><p>你好，我是徐文浩。</p><p>上一讲里我们看到大模型的确有效。在进行情感分析的时候，我们通过OpenAI的API拿到的Embedding，比T5-base这样单机可以运行的小模型，效果还是好很多的。</p><p>不过，我们之前选用的问题的确有点太简单了。我们把5个不同的分数分成了正面、负面和中性，还去掉了相对难以判断的“中性”评价，这样我们判断的准确率高的确是比较好实现的。但如果我们想要准确地预测出具体的分数呢？</p><h2 id="利用Embedding，训练机器学习模型"><a href="#利用Embedding，训练机器学习模型" class="headerlink" title="利用Embedding，训练机器学习模型"></a>利用Embedding，训练机器学习模型</h2><p>最简单的办法就是利用我们拿到的文本Embedding的向量。这一次，我们不直接用向量之间的距离，而是使用传统的机器学习的方法来进行分类。毕竟，如果只是用向量之间的距离作为衡量标准，就没办法最大化地利用已经标注好的分数信息了。</p><p>事实上，OpenAI在自己的官方教程里也直接给出了这样一个例子。我在这里也放上了相应的GitHub的代码 <a href="https://github.com/openai/openai-cookbook/blob/main/examples/Classification_using_embeddings.ipynb">链接</a>，你可以去看一下。不过，为了避免OpenAI王婆卖瓜自卖自夸，我们也希望能和其他人用传统的机器学习方式得到的结果做个比较。</p><p>因此我重新找了一个中文的数据集来试一试。这个数据集是在中文互联网上比较容易找到的一份今日头条的新闻标题和新闻关键词，在GitHub上可以直接找到数据，我把 <a href="https://github.com/aceimnorstuvwxz/toutiao-text-classfication-dataset">链接</a> 也放在这里。用这个数据集的好处是，有人同步放出了预测的实验效果。我们可以拿自己训练的结果和他做个对比。</p><h2 id="数据处理，小坑也不少"><a href="#数据处理，小坑也不少" class="headerlink" title="数据处理，小坑也不少"></a>数据处理，小坑也不少</h2><p>在训练模型之前，我们要先获取每一个新闻标题的Embedding。我们通过Pandas这个Python数据处理库，把对应的文本加载到内存里。接着去调用之前我们使用过的OpenAI的Embedding接口，然后把返回结果一并存下来就好了。这个听起来非常简单直接，我也把对应的代码先放在下面，不过你先别着急运行。</p><p>注：因为后面的代码可能会耗费比较多的Token数量，如果你使用的是免费的5美元额度的话，可以直接去拿我放在Github里的 <a href="https://github.com/xuwenhao/geektime-ai-course">数据文件</a>，用我已经处理好的数据。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> tiktoken<span class="token keyword">import</span> openai<span class="token keyword">import</span> os<span class="token keyword">from</span> openai<span class="token punctuation">.</span>embeddings_utils <span class="token keyword">import</span> get_embedding<span class="token punctuation">,</span> get_embeddingsopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span><span class="token comment"># embedding model parameters</span>embedding_model <span class="token operator">=</span> <span class="token string">"text-embedding-ada-002"</span>embedding_encoding <span class="token operator">=</span> <span class="token string">"cl100k_base"</span>  <span class="token comment"># this the encoding for text-embedding-ada-002</span>max_tokens <span class="token operator">=</span> <span class="token number">8000</span>  <span class="token comment"># the maximum for text-embedding-ada-002 is 8191</span><span class="token comment"># import data/toutiao_cat_data.txt as a pandas dataframe</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'data/toutiao_cat_data.txt'</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">'_!_'</span><span class="token punctuation">,</span> names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">,</span> <span class="token string">'code'</span><span class="token punctuation">,</span> <span class="token string">'category'</span><span class="token punctuation">,</span> <span class="token string">'title'</span><span class="token punctuation">,</span> <span class="token string">'keywords'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>df <span class="token operator">=</span> df<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span>df<span class="token punctuation">[</span><span class="token string">"combined"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token string">"标题: "</span> <span class="token operator">+</span> df<span class="token punctuation">.</span>title<span class="token punctuation">.</span><span class="token builtin">str</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"; 关键字: "</span> <span class="token operator">+</span> df<span class="token punctuation">.</span>keywords<span class="token punctuation">.</span><span class="token builtin">str</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Lines of text before filtering: "</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">)</span>encoding <span class="token operator">=</span> tiktoken<span class="token punctuation">.</span>get_encoding<span class="token punctuation">(</span>embedding_encoding<span class="token punctuation">)</span><span class="token comment"># omit reviews that are too long to embed</span>df<span class="token punctuation">[</span><span class="token string">"n_tokens"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">.</span>combined<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token builtin">len</span><span class="token punctuation">(</span>encoding<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>df <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">.</span>n_tokens <span class="token operator">&lt;=</span> max_tokens<span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Lines of text after filtering: "</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：这个是加载数据并做一些简单预处理的代码，你可以直接运行。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># randomly sample 1k rows</span>df_1k <span class="token operator">=</span> df<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span>df_1k<span class="token punctuation">[</span><span class="token string">"embedding"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df_1k<span class="token punctuation">.</span>combined<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x <span class="token punctuation">:</span> get_embedding<span class="token punctuation">(</span>x<span class="token punctuation">,</span> engine<span class="token operator">=</span>embedding_model<span class="token punctuation">)</span><span class="token punctuation">)</span>df_1k<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">"data/toutiao_cat_data_10k_with_embeddings.csv"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：这个是一条条数据请求OpenAI的API获取Embedding的代码，但是你在运行中会遇到报错。</p><p>直接运行这个代码，你多半会遇到一个报错，因为在这个数据处理过程中也是有几个坑的。</p><p>第一个坑是 <strong>OpenAI提供的接口限制了每条数据的长度</strong>。我们这里使用的 text-embedding-ada-002 的模型，支持的长度是每条记录8191个Token。所以我们在实际发送请求前，需要计算一下每条记录有多少Token，超过8000个的需要过滤掉。不过，在我们这个数据集里，只有新闻的标题，所以不会超过这个长度。但是你在使用其他数据集的时候，可能就需要过滤下数据，或者采用截断的方法，只用文本最后8000个Token。</p><p>我们在这里，调用了Tiktoken这个库，使用了 cl100k_base 这种编码方式，这种编码方式和 text-embedding-ada-002 模型是一致的。如果选错了编码方式，你计算出来的Token数量可能和OpenAI的不一样。</p><p>第二个坑是，如果你直接一条条调用OpenAI的API，很快就会遇到报错。这是因为 <strong>OpenAI对API的调用进行了限速</strong>（Rate Limit）。如果你过于频繁地调用，就会遇到限速的报错。而如果你在报错之后继续持续调用，限速的时间还会被延长。那怎么解决这个问题呢？我习惯选用 backoff 这个Python库，在调用的时候如果遇到报错了，就等待一段时间，如果连续报错，就拉长等待时间。通过backoff改造的代码我放在了下面，不过这还没有彻底解决问题。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conda install backoff<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>你需要先安装一下backoff库，当然，换成PIP安装也是一样的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> backoff<span class="token decorator annotation punctuation">@backoff<span class="token punctuation">.</span>on_exception</span><span class="token punctuation">(</span>backoff<span class="token punctuation">.</span>expo<span class="token punctuation">,</span> openai<span class="token punctuation">.</span>error<span class="token punctuation">.</span>RateLimitError<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">get_embedding_with_backoff</span><span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> get_embedding<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment"># randomly sample 10k rows</span>df_10k <span class="token operator">=</span> df<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span>df_10k<span class="token punctuation">[</span><span class="token string">"embedding"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df_10k<span class="token punctuation">.</span>combined<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x <span class="token punctuation">:</span> get_embedding_with_backoff<span class="token punctuation">(</span>text<span class="token operator">=</span>x<span class="token punctuation">,</span> engine<span class="token operator">=</span>embedding_model<span class="token punctuation">)</span><span class="token punctuation">)</span>df_10k<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">"data/toutiao_cat_data_10k_with_embeddings.csv"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过backoff库，我们指定了在遇到RateLimitError的时候，按照指数级别增加等待时间。</p><p>如果你直接运行上面那个代码，大约需要2个小时才能处理完1万条数据。我们的数据集里有38万条数据，真要这么干，需要3天3夜才能把训练数据处理完，这样显然不怎么实用。这么慢的原因有两个，一个是限速，backoff只是让我们的调用不会因为失败而终止，但是我还是受到了每分钟API调用次数的限制。第二个是延时，因为我们是按照顺序一个个调用Embedding接口，每一次调用都要等前一次调用结束后才会发起请求，而不是多条数据并行请求，这更进一步 <strong>拖长了处理数据所需要的时间</strong>。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/07e5bf0bdfce40a3f1b5a89cb43010a5.png" alt="图片"></p><p>注：你可以点开这个 <a href="https://platform.openai.com/docs/guides/rate-limits/overview">链接</a>，看看目前OpenAI对不同模型的限速。</p><p>要解决这个问题也不困难，OpenAI是支持batch调用接口的，也就是说，你可以在一个请求里一次批量处理很多个请求。我们把1000条记录打包在一起处理，速度就会快很多。我把对应的代码放在下面，你可以试着执行一下，处理这38万多条的数据，也就个把小时。不过，你也不能一次性打包太多条记录，因为OpenAI的限速不仅仅是针对请求数的，也 <strong>限制你每分钟可以处理的 Token 数量</strong>，具体一次打包几条，你可以根据每条数据包含的Token数自己测算一下。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> backoff<span class="token keyword">from</span> openai<span class="token punctuation">.</span>embeddings_utils <span class="token keyword">import</span> get_embeddingsbatch_size <span class="token operator">=</span> <span class="token number">1000</span><span class="token decorator annotation punctuation">@backoff<span class="token punctuation">.</span>on_exception</span><span class="token punctuation">(</span>backoff<span class="token punctuation">.</span>expo<span class="token punctuation">,</span> openai<span class="token punctuation">.</span>error<span class="token punctuation">.</span>RateLimitError<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">get_embeddings_with_backoff</span><span class="token punctuation">(</span>prompts<span class="token punctuation">,</span> engine<span class="token punctuation">)</span><span class="token punctuation">:</span>    embeddings <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>prompts<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch <span class="token operator">=</span> prompts<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>batch_size<span class="token punctuation">]</span>        embeddings <span class="token operator">+=</span> get_embeddings<span class="token punctuation">(</span>list_of_text<span class="token operator">=</span>batch<span class="token punctuation">,</span> engine<span class="token operator">=</span>engine<span class="token punctuation">)</span>    <span class="token keyword">return</span> embeddings<span class="token comment"># randomly sample 10k rows</span>df_all <span class="token operator">=</span> df<span class="token comment"># group prompts into batches of 100</span>prompts <span class="token operator">=</span> df_all<span class="token punctuation">.</span>combined<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>prompt_batches <span class="token operator">=</span> <span class="token punctuation">[</span>prompts<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>batch_size<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>prompts<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">]</span>embeddings <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> batch <span class="token keyword">in</span> prompt_batches<span class="token punctuation">:</span>    batch_embeddings <span class="token operator">=</span> get_embeddings_with_backoff<span class="token punctuation">(</span>prompts<span class="token operator">=</span>batch<span class="token punctuation">,</span> engine<span class="token operator">=</span>embedding_model<span class="token punctuation">)</span>    embeddings <span class="token operator">+=</span> batch_embeddingsdf_all<span class="token punctuation">[</span><span class="token string">"embedding"</span><span class="token punctuation">]</span> <span class="token operator">=</span> embeddingsdf_all<span class="token punctuation">.</span>to_parquet<span class="token punctuation">(</span><span class="token string">"data/toutiao_cat_data_all_with_embeddings.parquet"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后一个你需要注意的点是，对于这样的大数据集， <strong>不要存储成CSV格式</strong>。特别是我们获取到的Embedding数据，是很多浮点数，存储成CSV格式会把本来只需要4个字节的浮点数，都用字符串的形式存储下来，会浪费好几倍的空间，写入的速度也很慢。我在这里 <strong>采用了parquet这个序列化的格式</strong>，整个存储的过程只需要1分钟。</p><h2 id="训练模型，看看效果怎么样"><a href="#训练模型，看看效果怎么样" class="headerlink" title="训练模型，看看效果怎么样"></a>训练模型，看看效果怎么样</h2><p>数据处理完了，我们就不妨试一试模型训练。如果你担心浪费太多的API调用，我把我处理好的数据集，放在了我的 <a href="https://github.com/xuwenhao/geektime-ai-course">GitHub</a> 上，我把链接也放在了这里，你可以直接下载、使用这个数据集。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> RandomForestClassifier<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> classification_report<span class="token punctuation">,</span> accuracy_scoretraining_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_parquet<span class="token punctuation">(</span><span class="token string">"data/toutiao_cat_data_all_with_embeddings.parquet"</span><span class="token punctuation">)</span>training_data<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span>df <span class="token operator">=</span>  training_data<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token number">50000</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span>X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>    <span class="token builtin">list</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>values<span class="token punctuation">)</span><span class="token punctuation">,</span> df<span class="token punctuation">.</span>category<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span>clf <span class="token operator">=</span> RandomForestClassifier<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">)</span>clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>preds <span class="token operator">=</span> clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>probas <span class="token operator">=</span> clf<span class="token punctuation">.</span>predict_proba<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>report <span class="token operator">=</span> classification_report<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> preds<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>report<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>模型训练的代码也非常简单，考虑到运行时间的因素，我这里直接随机选取了里面的5万条数据，4万条作为训练集，1万条作为测试集。然后通过最常用的scikit-learn这个机器学习工具包里面的随机森林（RandomForest）算法，做了一次训练和测试。在我的电脑上，大概10分钟可以跑完，整体的准确率可以达到84%。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">                    precision    recall  f1<span class="token operator">-</span>score   support  news_agriculture       <span class="token number">0.83</span>      <span class="token number">0.85</span>      <span class="token number">0.84</span>       <span class="token number">495</span>          news_car       <span class="token number">0.88</span>      <span class="token number">0.94</span>      <span class="token number">0.91</span>       <span class="token number">895</span>      news_culture       <span class="token number">0.86</span>      <span class="token number">0.76</span>      <span class="token number">0.81</span>       <span class="token number">741</span>          news_edu       <span class="token number">0.86</span>      <span class="token number">0.89</span>      <span class="token number">0.87</span>       <span class="token number">708</span>news_entertainment       <span class="token number">0.71</span>      <span class="token number">0.92</span>      <span class="token number">0.80</span>      <span class="token number">1051</span>      news_finance       <span class="token number">0.81</span>      <span class="token number">0.76</span>      <span class="token number">0.78</span>       <span class="token number">735</span>         news_game       <span class="token number">0.91</span>      <span class="token number">0.82</span>      <span class="token number">0.86</span>       <span class="token number">742</span>        news_house       <span class="token number">0.91</span>      <span class="token number">0.86</span>      <span class="token number">0.89</span>       <span class="token number">450</span>     news_military       <span class="token number">0.89</span>      <span class="token number">0.82</span>      <span class="token number">0.85</span>       <span class="token number">688</span>       news_sports       <span class="token number">0.90</span>      <span class="token number">0.92</span>      <span class="token number">0.91</span>       <span class="token number">968</span>        news_story       <span class="token number">0.95</span>      <span class="token number">0.46</span>      <span class="token number">0.62</span>       <span class="token number">197</span>         news_tech       <span class="token number">0.82</span>      <span class="token number">0.86</span>      <span class="token number">0.84</span>      <span class="token number">1052</span>       news_travel       <span class="token number">0.80</span>      <span class="token number">0.77</span>      <span class="token number">0.78</span>       <span class="token number">599</span>        news_world       <span class="token number">0.83</span>      <span class="token number">0.73</span>      <span class="token number">0.78</span>       <span class="token number">671</span>             stock       <span class="token number">0.00</span>      <span class="token number">0.00</span>      <span class="token number">0.00</span>         <span class="token number">8</span>          accuracy                           <span class="token number">0.84</span>     <span class="token number">10000</span>         macro avg       <span class="token number">0.80</span>      <span class="token number">0.76</span>      <span class="token number">0.77</span>     <span class="token number">10000</span>      weighted avg       <span class="token number">0.84</span>      <span class="token number">0.84</span>      <span class="token number">0.84</span>     <span class="token number">10000</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>随机森林这个算法，虽然效果不错，但是跑起来有些慢。我们接下来用个更简单的逻辑回归（LogisticRegression）算法，但我们这次要跑在整个数据集上。一样的，我们拿80%作为训练，20%作为测试。这一次，虽然数据量是刚才4万条数据的好几倍，但是时间上却只要3～4分钟，而最终的准确率也能达到86%。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegressiondf <span class="token operator">=</span>  training_dataX_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>    <span class="token builtin">list</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>values<span class="token punctuation">)</span><span class="token punctuation">,</span> df<span class="token punctuation">.</span>category<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span>clf <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>preds <span class="token operator">=</span> clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>probas <span class="token operator">=</span> clf<span class="token punctuation">.</span>predict_proba<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>report <span class="token operator">=</span> classification_report<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> preds<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>report<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">                    precision    recall  f1<span class="token operator">-</span>score   support  news_agriculture       <span class="token number">0.86</span>      <span class="token number">0.88</span>      <span class="token number">0.87</span>      <span class="token number">3908</span>          news_car       <span class="token number">0.92</span>      <span class="token number">0.92</span>      <span class="token number">0.92</span>      <span class="token number">7101</span>      news_culture       <span class="token number">0.83</span>      <span class="token number">0.85</span>      <span class="token number">0.84</span>      <span class="token number">5719</span>          news_edu       <span class="token number">0.89</span>      <span class="token number">0.89</span>      <span class="token number">0.89</span>      <span class="token number">5376</span>news_entertainment       <span class="token number">0.86</span>      <span class="token number">0.88</span>      <span class="token number">0.87</span>      <span class="token number">7908</span>      news_finance       <span class="token number">0.81</span>      <span class="token number">0.79</span>      <span class="token number">0.80</span>      <span class="token number">5409</span>         news_game       <span class="token number">0.91</span>      <span class="token number">0.88</span>      <span class="token number">0.89</span>      <span class="token number">5899</span>        news_house       <span class="token number">0.91</span>      <span class="token number">0.91</span>      <span class="token number">0.91</span>      <span class="token number">3463</span>     news_military       <span class="token number">0.86</span>      <span class="token number">0.82</span>      <span class="token number">0.84</span>      <span class="token number">4976</span>       news_sports       <span class="token number">0.93</span>      <span class="token number">0.93</span>      <span class="token number">0.93</span>      <span class="token number">7611</span>        news_story       <span class="token number">0.83</span>      <span class="token number">0.82</span>      <span class="token number">0.83</span>      <span class="token number">1308</span>         news_tech       <span class="token number">0.84</span>      <span class="token number">0.86</span>      <span class="token number">0.85</span>      <span class="token number">8168</span>       news_travel       <span class="token number">0.80</span>      <span class="token number">0.80</span>      <span class="token number">0.80</span>      <span class="token number">4252</span>        news_world       <span class="token number">0.79</span>      <span class="token number">0.81</span>      <span class="token number">0.80</span>      <span class="token number">5370</span>             stock       <span class="token number">0.00</span>      <span class="token number">0.00</span>      <span class="token number">0.00</span>        <span class="token number">70</span>          accuracy                           <span class="token number">0.86</span>     <span class="token number">76538</span>         macro avg       <span class="token number">0.80</span>      <span class="token number">0.80</span>      <span class="token number">0.80</span>     <span class="token number">76538</span>      weighted avg       <span class="token number">0.86</span>      <span class="token number">0.86</span>      <span class="token number">0.86</span>     <span class="token number">76538</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：下载的数据集所在的测试结果可以看 <a href="https://github.com/aceimnorstuvwxz/toutiao-text-classfication-dataset">这里</a>。</p><p>这个结果已经比我们下载数据集的GitHub页面里看到的效果好了，那个的准确率只有85%。</p><p>可以看到，通过OpenAI的API获取到Embedding，然后通过一些简单的线性模型，我们就能获得很好的分类效果。我们既不需要提前储备很多自然语言处理的知识，对数据进行大量的分析和清理；也不需要搞一块昂贵的显卡，去使用什么深度学习模型。只要1～2个小时，我们就能在一个几十万条文本的数据集上训练出一个非常不错的分类模型。</p><h2 id="理解指标，学一点机器学习小知识"><a href="#理解指标，学一点机器学习小知识" class="headerlink" title="理解指标，学一点机器学习小知识"></a>理解指标，学一点机器学习小知识</h2><p>刚刚我说了，就算你没有机器学习的相关知识，也没有关系，这里我来给你补补课。理解一下上面模型输出的报告是什么意思。报告的每一行都有四个指标，分别是准确率（Precision）、召回率（Recall）、F1分数，以及支持样本量（Support）。我还是用今日头条的新闻标题这个数据集来解释这些概念。</p><ol><li><strong>准确率</strong>，代表模型判定属于这个分类的标题里面判断正确的有多少，有多少真的是属于这个分类的。比如，模型判断里面有 100个都是农业新闻，但是这100个里面其实只有83个是农业新闻，那么准确率就是0.83。准确率自然是越高越好，但是并不是准确率达到100%就代表模型全对了。因为模型可能会漏，所以我们还要考虑召回率。</li><li><strong>召回率</strong>，代表模型判定属于这个分类的标题占实际这个分类下所有标题的比例，也就是没有漏掉的比例。比如，模型判断100个都是农业新闻，这100个的确都是农业新闻。准确率已经100%了。但是，实际我们一共有200条农业新闻。那么有100条其实被放到别的类目里面去了。那么在农业新闻这个类目，我们的召回率，就只有 100&#x2F;200 &#x3D; 50%。</li><li>所以模型效果的好坏，既要考虑准确率，又要考虑召回率，综合考虑这两项得出的结果，就是 <strong>F1</strong> <strong>分数</strong>（F1 Score）。F1分数，是准确率和召回率的调和平均数，也就是 F1 Score &#x3D; 2&#x2F; (1&#x2F;Precision + 1&#x2F;Recall)。当准确率和召回率都是100%的时候，F1分数也是1。如果准确率是100%，召回率是80%，那么算下来F1分数就是0.88。F1分数也是越高越好。</li><li><strong>支持的样本量</strong>，是指数据里面，实际是这个分类的数据条数有多少。一般来说，数据条数越多，这个分类的训练就会越准确。</li></ol><p>分类报告里一个类目占一行，每一行都包含对应的这四个指标，而最下面还有三行数据。这三行数据，是整个拿来测试的数据集，所以对应的支持样本量都是1万个。</p><p>第一行的accuracy，只有一个指标，虽然它在F1 Score这个列里，但是并不是F1分数的意思。而是说，模型总共判断对的分类&#x2F;模型测试的样本数，也就是模型的整体准确率。</p><p>第二行的macro average，中文名叫做宏平均，宏平均的三个指标，就是把上面每一个分类算出来的指标加在一起平均一下。它主要是在数据分类不太平衡的时候，帮助我们衡量模型效果怎么样。</p><p>比如，我们做情感分析，可能90%都是正面情感，10%是负面情感。这个时候，我们预测正面情感效果很好，比如有90%的准确率，但是负面情感预测很差，只有50%的准确率。如果看整体数据，其实准确率还是非常高的，毕竟负面情感的例子很少。</p><p>但是我们的目标可能就是找到有负面情绪的客户和他们沟通、赔偿。那么整体准确率对我们就没有什么用了。而宏平均，会把整体的准确率变成 (90%+50%)&#x2F;2 &#x3D; 70%。这就不是一个很好的预测结果了，我们需要进一步优化。宏平均对于数据样本不太平衡，有些类目样本特别少，有些特别多的场景特别有用。</p><p>第三行的weighted average，就是加权平均，也就是我们把每一个指标，按照分类里面支持的样本量加权，算出来的一个值。无论是 Precision、Recall 还是 F1 Score都要这么按照各个分类加权平均一下。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/947d63ba603abfc137b904b7db1cf3c3.jpg"></p><p>好了，今天的这一讲到这里就结束了，最后我们来回顾一下。这一讲我们学会了两件事情。</p><p>第一件，是怎么利用OpenAI的API来获取文本的Embedding。虽然接口不复杂，但是我们也要考虑模型能够接受的最大文本长度，API本身的限速，以及网络延时带来的问题。</p><p>我们分别给出了解决方案，使用Tiktoken计算样本的Token数量，并进行过滤；在遇到限速问题时通过backoff进行指数级别的时间等待；通过一次性批量请求一批数据，最大化我们的吞吐量来解决问题；对于返回的结果，我们可以通过parquet这样序列化的方式保存数据，来减少数据尺寸。</p><p>第二件，是如何直接利用拿到的Embedding，简单调用一下scikit-learn，通过机器学习的方法，进行更准确的分类。我们最终把Embedding放到一个简单的逻辑回归模型里，就取得了很不错的分类效果。你学会了吗？</p><h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><p>这一讲里我们学会了利用OpenAI来获取文本的Embedding，然后通过传统的机器学习方式来进行训练，并评估训练的结果。</p><p>我们之前用过Amazon1000条食物评论的情感分析数据，在那个数据集里，我们其实已经使用过获取到并保存下来的Embedding数据了。那么，你能不能试着在完整的数据集上，训练一个能把从1分到5分的每一个级别都区分出来的机器学习模型，看看效果怎么样？</p><p>整个原始数据集的下载链接我放在 <a href="https://www.kaggle.com/snap/amazon-fine-food-reviews">这里</a> 了，欢迎你把你测试出来的结果分享出来，看看和其他人比起来怎么样。另外如果你觉得有收获的话，也欢迎你把这节课分享出去，让更多的人了解用Embedding给文本分类的方法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;05｜善用Embedding，我们来给文本分分类&quot;&gt;&lt;a href=&quot;#05｜善用Embedding，我们来给文本分分类&quot; class=&quot;headerlink&quot; title=&quot;05｜善用Embedding，我们来给文本分分类&quot;&gt;&lt;/a&gt;05｜善用Embedding</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>06｜ChatGPT来了，让我们快速做个AI应用</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1117845890.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1117845890.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.888Z</updated>
    
    <content type="html"><![CDATA[<h1 id="06｜ChatGPT来了，让我们快速做个AI应用"><a href="#06｜ChatGPT来了，让我们快速做个AI应用" class="headerlink" title="06｜ChatGPT来了，让我们快速做个AI应用"></a>06｜ChatGPT来了，让我们快速做个AI应用</h1><p>你好，我是徐文浩。</p><p>过去的两讲，我带着你通过OpenAI提供的Embedding接口，完成了文本分类的功能。那么，这一讲里，我们重新回到Completion接口。而且这一讲里，我们还会快速搭建出一个有界面的聊天机器人来给你用。在这个过程里，你也会第一次使用 HuggingFace 这个平台。</p><p>HuggingFace 是现在最流行的深度模型的社区，你可以在里面下载到最新开源的模型，以及看到别人提供的示例代码。</p><h2 id="ChatGPT来了，更快的速度更低的价格"><a href="#ChatGPT来了，更快的速度更低的价格" class="headerlink" title="ChatGPT来了，更快的速度更低的价格"></a>ChatGPT来了，更快的速度更低的价格</h2><p>我在 <a href="https://time.geekbang.org/column/article/642197">第03讲</a> 里，已经给你看了如何通过Completion的接口，实现一个聊天机器人的功能。在那个时候，我们采用的是自己将整个对话拼接起来，将整个上下文都发送给OpenAI的Completion API的方式。不过，在3月2日，因为ChatGPT的火热，OpenAI放出了一个直接可以进行对话聊天的接口。这个接口叫做 <strong>ChatCompletion</strong>，对应的模型叫做gpt-3.5-turbo，不但用起来更容易了，速度还快，而且价格也是我们之前使用的 text-davinci-003 的十分之一，可谓是物美价廉了。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openaiopenai<span class="token punctuation">.</span>ChatCompletion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>  model<span class="token operator">=</span><span class="token string">"gpt-3.5-turbo"</span><span class="token punctuation">,</span>  messages<span class="token operator">=</span><span class="token punctuation">[</span>        <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"You are a helpful assistant."</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span>        <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"Who won the world series in 2020?"</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span>        <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"assistant"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"The Los Angeles Dodgers won the World Series in 2020."</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span>        <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"Where was it played?"</span><span class="token punctuation">&#125;</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：点击在这个 <a href="https://platform.openai.com/docs/guides/chat">链接</a> 你可以看到接口调用示例。</p><p>在OpenAI的官方文档里，可以看到这个接口也非常简单。你需要传入的参数，从一段Prompt变成了一个数组，数组的每个元素都有role和content两个字段。</p><ol><li>role这个字段一共有三个角色可以选择，其中 system 代表系统，user代表用户，而assistant则代表AI的回答。</li><li>当role是system的时候，content里面的内容代表我们给AI的一个指令，也就是告诉AI应该怎么回答用户的问题。比如我们希望AI都通过中文回答，我们就可以在content里面写“你是一个只会用中文回答问题的助理”，这样即使用户问的问题都是英文的，AI的回复也都会是中文的。</li><li>而当role是user或者assistant的时候，content里面的内容就代表用户和AI对话的内容。和我们在 <a href="https://time.geekbang.org/column/article/642197">第03讲</a> 里做的聊天机器人一样，你需要把历史上的对话一起发送给OpenAI的接口，它才能有理解整个对话的上下文的能力。</li></ol><p>有了这个接口，我们就很容易去封装一个聊天机器人了，我把代码放在了下面，我们一起来看一看。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token keyword">import</span> osopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Conversation</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prompt<span class="token punctuation">,</span> num_of_round<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>prompt <span class="token operator">=</span> prompt        self<span class="token punctuation">.</span>num_of_round <span class="token operator">=</span> num_of_round        self<span class="token punctuation">.</span>messages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>prompt<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">ask</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> question<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> question<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>            response <span class="token operator">=</span> openai<span class="token punctuation">.</span>ChatCompletion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>                model<span class="token operator">=</span><span class="token string">"gpt-3.5-turbo"</span><span class="token punctuation">,</span>                messages<span class="token operator">=</span>self<span class="token punctuation">.</span>messages<span class="token punctuation">,</span>                temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>                max_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>                top_p<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span>            <span class="token keyword">return</span> e        message <span class="token operator">=</span> response<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"message"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"assistant"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> message<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>messages<span class="token punctuation">)</span> <span class="token operator">></span> self<span class="token punctuation">.</span>num_of_round<span class="token operator">*</span><span class="token number">2</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">del</span> self<span class="token punctuation">.</span>messages<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">//</span>Remove the first <span class="token builtin">round</span> conversation left<span class="token punctuation">.</span>        <span class="token keyword">return</span> message<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>我们封装了一个Conversation类，它的构造函数 <strong>init</strong> 会接受两个参数，prompt 作为 system的content，代表我们对这个聊天机器人的指令，num_of_round 代表每次向ChatGPT发起请求的时候，保留过去几轮会话。</li><li>Conversation类本身只有一个ask函数，输入是一个string类型的question，返回结果也是string类型的一条message。</li><li>每次调用ask函数，都会向ChatGPT发起一个请求。在这个请求里，我们都会把最新的问题拼接到整个对话数组的最后，而在得到ChatGPT的回答之后也会把回答拼接上去。如果回答完之后，发现会话的轮数超过我们设置的num_of_round，我们就去掉最前面的一轮会话。</li></ol><p>下面，我们就来试一试这个Conversation类好不好使。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">prompt <span class="token operator">=</span> <span class="token triple-quoted-string string">"""你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字以内"""</span>conv1 <span class="token operator">=</span> Conversation<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>question1 <span class="token operator">=</span> <span class="token string">"你是谁？"</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"User : %s"</span> <span class="token operator">%</span> question1<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Assistant : %s\n"</span> <span class="token operator">%</span> conv1<span class="token punctuation">.</span>ask<span class="token punctuation">(</span>question1<span class="token punctuation">)</span><span class="token punctuation">)</span>question2 <span class="token operator">=</span> <span class="token string">"请问鱼香肉丝怎么做？"</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"User : %s"</span> <span class="token operator">%</span> question2<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Assistant : %s\n"</span> <span class="token operator">%</span> conv1<span class="token punctuation">.</span>ask<span class="token punctuation">(</span>question2<span class="token punctuation">)</span><span class="token punctuation">)</span>question3 <span class="token operator">=</span> <span class="token string">"那蚝油牛肉呢？"</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"User : %s"</span> <span class="token operator">%</span> question3<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Assistant : %s\n"</span> <span class="token operator">%</span> conv1<span class="token punctuation">.</span>ask<span class="token punctuation">(</span>question3<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>我们给到了ChatGPT一个指令，告诉它它是一个中国厨子，用中文回答问题，而且回答在100个字以内，并且我们设定了AI只记住过去3轮的对话。</li><li>然后，我们按照顺序问了他，“你是谁”，“鱼香肉丝怎么做”以及“那蚝油牛肉呢”这三个问题。</li><li>可以看到，在回答里它说自己可以回答做菜的问题，而且回答都在100个字以内。</li><li>并且，我们问他“那蚝油牛肉呢”的时候，它也的确记住了上下文，知道我们问的是菜的做法，而不是价格或者其他信息。</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python">User <span class="token punctuation">:</span> 你是谁？Assistant <span class="token punctuation">:</span> 我是一个AI语言模型，可以回答你的做菜问题。User <span class="token punctuation">:</span> 请问鱼香肉丝怎么做？Assistant <span class="token punctuation">:</span> 鱼香肉丝的做法：<span class="token number">1.</span> 瘦猪肉切丝，用盐、淀粉、料酒腌制；<span class="token number">2.</span> 香葱、姜、蒜切末备用；<span class="token number">3.</span> 青红椒切丝备用；<span class="token number">4.</span> 热锅凉油，放入肉丝煸炒至变色；<span class="token number">5.</span> 放入葱姜蒜末和青红椒丝煸炒；<span class="token number">6.</span> 加入适量盐、糖、醋、酱油、料酒、水淀粉翻炒均匀；<span class="token number">7.</span> 淋上少许香油即可。User <span class="token punctuation">:</span> 那蚝油牛肉呢？Assistant <span class="token punctuation">:</span> 蚝油牛肉的做法：<span class="token number">1.</span> 牛肉切薄片，加入盐、生抽、淀粉腌制<span class="token number">20</span>分钟；<span class="token number">2.</span> 青红椒切丝，姜蒜切末备用；<span class="token number">3.</span> 热锅冷油，下姜蒜末煸炒出香味；<span class="token number">4.</span> 加入牛肉片快速翻炒至变色；<span class="token number">5.</span> 加入青红椒丝翻炒均匀；<span class="token number">6.</span> 倒入蚝油、生抽、糖、水淀粉调味；<span class="token number">7.</span> 翻炒均匀，收汁后淋上香油即可。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在问完了3个问题之后，我们又问了它第四个问题，也就是我们问它的第一个问题是什么。这个时候，它因为记录了过去第1-3轮的对话，所以还能正确地回答出来，我们问的是“你是谁”。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">question4 <span class="token operator">=</span> <span class="token string">"我问你的第一个问题是什么？"</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"User : %s"</span> <span class="token operator">%</span> question4<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Assistant : %s\n"</span> <span class="token operator">%</span> conv1<span class="token punctuation">.</span>ask<span class="token punctuation">(</span>question4<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">User <span class="token punctuation">:</span> 我问你的第一个问题是什么？Assistant <span class="token punctuation">:</span> 你问我：“你是谁？”<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>而这个时候，如果我们重新再问一遍“我问你的第一个问题是什么”，你会发现回答变了。因为啊，上一轮已经是第四轮了，而我们设置记住的num_of_round是3。在上一轮的问题回答完了之后，第一轮的关于“你是谁”的问答，被我们从ChatGPT的对话历史里去掉了。所以这个时候，它会告诉我们，第一个问题是“鱼香肉丝怎么做”。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">question5 <span class="token operator">=</span> <span class="token string">"我问你的第一个问题是什么？"</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"User : %s"</span> <span class="token operator">%</span> question5<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Assistant : %s\n"</span> <span class="token operator">%</span> conv1<span class="token punctuation">.</span>ask<span class="token punctuation">(</span>question5<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">User <span class="token punctuation">:</span> 我问你的第一个问题是什么？Assistant <span class="token punctuation">:</span> 你问我：“请问鱼香肉丝怎么做？”<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="计算聊天机器人的成本"><a href="#计算聊天机器人的成本" class="headerlink" title="计算聊天机器人的成本"></a>计算聊天机器人的成本</h2><p>无论是在 <a href="https://time.geekbang.org/column/article/642197">第03讲</a> 里，还是这一讲里，我们每次都要发送一大段之前的聊天记录给到OpenAI。这是由OpenAI的GPT-3系列的大语言模型的原理所决定的。GPT-3系列的模型能够实现的功能非常简单，它就是根据你给他的一大段文字去续写后面的内容。而为了能够方便地为所有人提供服务，OpenAI也没有在服务器端维护整个对话过程自己去拼接，所以就不得不由你来拼接了。</p><p>即使ChatGPT的接口是把对话分成了一个数组，但是实际上， <strong>最终发送给模型的还是拼接到一起的字符串</strong>。OpenAI在它的Python库里面提供了一个叫做 <a href="https://github.com/openai/openai-python/blob/main/chatml.md">ChatML</a> 的格式，其实就是ChatGPT的API的底层实现。OpenAI实际做的，就是根据一个定义好特定分隔符的格式，将你提供的多轮对话的内容拼接在一起，提交给 gpt-3.5-turbo 这个模型。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">&lt;</span><span class="token operator">|</span>im_start<span class="token operator">|</span><span class="token operator">></span>systemYou are ChatGPT<span class="token punctuation">,</span> a large language model trained by OpenAI<span class="token punctuation">.</span> Answer <span class="token keyword">as</span> concisely <span class="token keyword">as</span> possible<span class="token punctuation">.</span>Knowledge cutoff<span class="token punctuation">:</span> <span class="token number">2021</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">01</span>Current date<span class="token punctuation">:</span> <span class="token number">2023</span><span class="token operator">-</span><span class="token number">03</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">&lt;</span><span class="token operator">|</span>im_end<span class="token operator">|</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">|</span>im_start<span class="token operator">|</span><span class="token operator">></span>userHow are you<span class="token operator">&lt;</span><span class="token operator">|</span>im_end<span class="token operator">|</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">|</span>im_start<span class="token operator">|</span><span class="token operator">></span>assistantI am doing well!<span class="token operator">&lt;</span><span class="token operator">|</span>im_end<span class="token operator">|</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">|</span>im_start<span class="token operator">|</span><span class="token operator">></span>userHow are you now?<span class="token operator">&lt;</span><span class="token operator">|</span>im_end<span class="token operator">|</span><span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：chatml的文档里，你可以看到你的对话，就是通过 &lt;|im_start|&gt;system|user|assistant、&lt;|im_end|&gt; 这些分隔符分割拼装的字符串。底层仍然是一个内容续写的大语言模型。</p><p>ChatGPT的对话模型用起来很方便，但是也有一点需要注意。就是在这个需要传送大量上下文的情况下，这个费用会比你想象的高。OpenAI是通过模型处理的Token数量来收费的，但是要注意，这个收费是“双向收费”。它是按照你发送给它的上下文，加上它返回给你的内容的总Token数来计算花费的Token数量的。</p><p>这个从模型的原理上是合理的，因为每一个Token，无论是你发给它的，还是它返回给你的，都需要通过GPU或者CPU运算。所以你发的上下文越长，它消耗的资源也越多。但是在使用中，你可能觉得我来了10轮对话，一共1000个Token，就只会收1000个Token的费用。而实际上，第一轮对话是只消耗了100个Token，但是第二轮因为要把前面的上下文都发送出去，所以需要200个，这样10轮下来，是需要花费5500个Token，比前面说的1000个可多了不少。</p><p>所以，如果做了应用要计算花费的成本，你就需要学会计算Token数。下面，我给了你一段示例代码，看看在ChatGPT的对话模型下，怎么计算Token数量。</p><h3 id="通过API计算Token数量"><a href="#通过API计算Token数量" class="headerlink" title="通过API计算Token数量"></a>通过API计算Token数量</h3><p>第一种计算Token数量的方式，是从API返回的结果里面获取。我们修改一下刚才的Conversation类，重新创建一个Conversation2类。和之前只有一个不同，ask函数除了返回回复的消息之外，还会返回这次请求消耗的Token数。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Conversation2</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prompt<span class="token punctuation">,</span> num_of_round<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>prompt <span class="token operator">=</span> prompt        self<span class="token punctuation">.</span>num_of_round <span class="token operator">=</span> num_of_round        self<span class="token punctuation">.</span>messages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>prompt<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">ask</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> question<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span> <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> question<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>            response <span class="token operator">=</span> openai<span class="token punctuation">.</span>ChatCompletion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>                model<span class="token operator">=</span><span class="token string">"gpt-3.5-turbo"</span><span class="token punctuation">,</span>                messages<span class="token operator">=</span>self<span class="token punctuation">.</span>messages<span class="token punctuation">,</span>                temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>                max_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>                top_p<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span>            <span class="token keyword">return</span> e        message <span class="token operator">=</span> response<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"message"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span>        num_of_tokens <span class="token operator">=</span> response<span class="token punctuation">[</span><span class="token string">'usage'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'total_tokens'</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"assistant"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> message<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>messages<span class="token punctuation">)</span> <span class="token operator">></span> self<span class="token punctuation">.</span>num_of_round<span class="token operator">*</span><span class="token number">2</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">del</span> self<span class="token punctuation">.</span>messages<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> message<span class="token punctuation">,</span> num_of_tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后我们还是问一遍之前的问题，看看每一轮问答消耗的Token数量。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conv2 <span class="token operator">=</span> Conversation2<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>questions <span class="token operator">=</span> <span class="token punctuation">[</span>question1<span class="token punctuation">,</span> question2<span class="token punctuation">,</span> question3<span class="token punctuation">,</span> question4<span class="token punctuation">,</span> question5<span class="token punctuation">]</span><span class="token keyword">for</span> question <span class="token keyword">in</span> questions<span class="token punctuation">:</span>    answer<span class="token punctuation">,</span> num_of_tokens <span class="token operator">=</span> conv2<span class="token punctuation">.</span>ask<span class="token punctuation">(</span>question<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"询问 &#123;%s&#125; 消耗的token数量是 : %d"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>question<span class="token punctuation">,</span> num_of_tokens<span class="token punctuation">)</span><span class="token punctuation">)</span>输出结果：<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">询问 <span class="token punctuation">&#123;</span>你是谁？<span class="token punctuation">&#125;</span> 消耗的token数量是 <span class="token punctuation">:</span> <span class="token number">108</span>询问 <span class="token punctuation">&#123;</span>请问鱼香肉丝怎么做？<span class="token punctuation">&#125;</span> 消耗的token数量是 <span class="token punctuation">:</span> <span class="token number">410</span>询问 <span class="token punctuation">&#123;</span>那蚝油牛肉呢？<span class="token punctuation">&#125;</span> 消耗的token数量是 <span class="token punctuation">:</span> <span class="token number">733</span>询问 <span class="token punctuation">&#123;</span>我问你的第一个问题是什么？<span class="token punctuation">&#125;</span> 消耗的token数量是 <span class="token punctuation">:</span> <span class="token number">767</span>询问 <span class="token punctuation">&#123;</span>我问你的第一个问题是什么？<span class="token punctuation">&#125;</span> 消耗的token数量是 <span class="token punctuation">:</span> <span class="token number">774</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，前几轮的Token消耗数量在逐渐增多，但是最后3轮是一样的。这是因为我们代码里只使用过去3轮的对话内容向ChatGPT发起请求。</p><h3 id="通过Tiktoken库计算Token数量"><a href="#通过Tiktoken库计算Token数量" class="headerlink" title="通过Tiktoken库计算Token数量"></a>通过Tiktoken库计算Token数量</h3><p>第二种方式，我们在上一讲用过，就是使用Tiktoken这个Python库，将文本分词，然后数一数Token的数量。</p><p>需要注意，使用不同的GPT模型，对应着不同的Tiktoken的编码器模型。对应的文档，可以查询这个链接： <a href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb">https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb</a></p><p>我们使用的ChatGPT，采用的是cl100k_base的编码，我们也可以试着用它计算一下第一轮对话使用的Token数量。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tiktokenencoding <span class="token operator">=</span> tiktoken<span class="token punctuation">.</span>get_encoding<span class="token punctuation">(</span><span class="token string">"cl100k_base"</span><span class="token punctuation">)</span>conv2 <span class="token operator">=</span> Conversation2<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>question1 <span class="token operator">=</span> <span class="token string">"你是谁？"</span>answer1<span class="token punctuation">,</span> num_of_tokens <span class="token operator">=</span> conv2<span class="token punctuation">.</span>ask<span class="token punctuation">(</span>question1<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"总共消耗的token数量是 : %d"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>num_of_tokens<span class="token punctuation">)</span><span class="token punctuation">)</span>prompt_count <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>encoding<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">)</span>question1_count <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>encoding<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>question1<span class="token punctuation">)</span><span class="token punctuation">)</span>answer1_count <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>encoding<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>answer1<span class="token punctuation">)</span><span class="token punctuation">)</span>total_count <span class="token operator">=</span> prompt_count <span class="token operator">+</span> question1_count <span class="token operator">+</span> answer1_count<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Prompt消耗 %d Token, 问题消耗 %d Token，回答消耗 %d Token，总共消耗 %d Token"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>prompt_count<span class="token punctuation">,</span> question1_count<span class="token punctuation">,</span> answer1_count<span class="token punctuation">,</span> total_count<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">总共消耗的token数量是 <span class="token punctuation">:</span> <span class="token number">104</span>Prompt消耗 <span class="token number">65</span> Token<span class="token punctuation">,</span> 问题消耗 <span class="token number">5</span> Token，回答消耗 <span class="token number">20</span> Token，总共消耗 <span class="token number">90</span> Token<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>我们通过API获得了消耗的Token数，然后又通过Tiktoken分别计算了System的指示内容、用户的问题和AI生成的回答，发现了两者还有小小的差异。这个是因为，我们没有计算OpenAI去拼接它们内部需要的格式的Token数量。很多时候，我们都需要通过Tiktoken预先计算一下Token数量，避免提交的内容太多，导致API返回报错。</p><h2 id="Gradio帮你快速搭建一个聊天界面"><a href="#Gradio帮你快速搭建一个聊天界面" class="headerlink" title="Gradio帮你快速搭建一个聊天界面"></a>Gradio帮你快速搭建一个聊天界面</h2><p>我们已经有了一个封装好的聊天机器人了。但是，现在这个机器人，我们只能自己在Python Notebook里面玩，每次问点问题还要调用代码。那么，接下来我们就给我们封装好的Convesation接口开发一个界面。</p><p>我们直接选用Gradio这个Python库来开发这个聊天机器人的界面，因为它有这样几个好处。</p><ol><li>我们现有的代码都是用Python实现的，你不需要再去学习JavaScript、TypeScript以及相关的前端框架了。</li><li>Gradio渲染出来的界面可以直接在Jupyter Notebook里面显示出来，对于不了解技术的同学，也不再需要解决其他环境搭建的问题。</li><li>Gradio这个公司，已经被目前最大的开源机器学习模型社区HuggingFace收购了。你可以免费把Gradio的应用部署到HuggingFace上。我等一下就教你怎么部署，你可以把你自己做出来的聊天机器人部署上去给你的朋友们用。</li><li>在后面的课程里，有些时候我们也会使用一些开源的模型，这些模型往往也托管在HuggingFace上。所以使用HuggingFace+Gradio的部署方式，特别方便我们演示给其他人看。</li></ol><p>注：Gradio官方也有用其他开源预训练模型创建Chatbot的教程 <a href="https://gradio.app/creating-a-chatbot/">https://gradio.app/creating-a-chatbot/</a></p><p>在实际开发之前，还是按照惯例我们先安装一下Python的Gradio的包。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conda install <span class="token operator">-</span>c conda<span class="token operator">-</span>forge gradio<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>Gradio应用的代码我也列在了下面，对应的逻辑也非常简单。</p><ol><li>首先，我们定义好了system这个系统角色的提示语，创建了一个Conversation对象。</li><li>然后，我们定义了一个answer方法，简单封装了一下Conversation的ask方法。主要是通过history维护了整个会话的历史记录。并且通过responses，将用户和AI的对话分组。然后将它们两个作为函数的返回值。这个函数的签名是为了符合Gradio里Chatbot组件的函数签名的需求。</li><li>最后，我们通过一段with代码，创建了对应的聊天界面。Gradio提供了一个现成的Chatbot组件，我们只需要调用它，然后提供一个文本输入框就好了。</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> gradio <span class="token keyword">as</span> grprompt <span class="token operator">=</span> <span class="token triple-quoted-string string">"""你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字以内"""</span>conv <span class="token operator">=</span> Conversation<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">answer</span><span class="token punctuation">(</span>question<span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    history<span class="token punctuation">.</span>append<span class="token punctuation">(</span>question<span class="token punctuation">)</span>    response <span class="token operator">=</span> conv<span class="token punctuation">.</span>ask<span class="token punctuation">(</span>question<span class="token punctuation">)</span>    history<span class="token punctuation">.</span>append<span class="token punctuation">(</span>response<span class="token punctuation">)</span>    responses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>u<span class="token punctuation">,</span>b<span class="token punctuation">)</span> <span class="token keyword">for</span> u<span class="token punctuation">,</span>b <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>history<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> history<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> responses<span class="token punctuation">,</span> history<span class="token keyword">with</span> gr<span class="token punctuation">.</span>Blocks<span class="token punctuation">(</span>css<span class="token operator">=</span><span class="token string">"#chatbot&#123;height:300px&#125; .overflow-y-auto&#123;height:500px&#125;"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> demo<span class="token punctuation">:</span>    chatbot <span class="token operator">=</span> gr<span class="token punctuation">.</span>Chatbot<span class="token punctuation">(</span>elem_id<span class="token operator">=</span><span class="token string">"chatbot"</span><span class="token punctuation">)</span>    state <span class="token operator">=</span> gr<span class="token punctuation">.</span>State<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> gr<span class="token punctuation">.</span>Row<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        txt <span class="token operator">=</span> gr<span class="token punctuation">.</span>Textbox<span class="token punctuation">(</span>show_label<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> placeholder<span class="token operator">=</span><span class="token string">"Enter text and press enter"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>style<span class="token punctuation">(</span>container<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    txt<span class="token punctuation">.</span>submit<span class="token punctuation">(</span>answer<span class="token punctuation">,</span> <span class="token punctuation">[</span>txt<span class="token punctuation">,</span> state<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>chatbot<span class="token punctuation">,</span> state<span class="token punctuation">]</span><span class="token punctuation">)</span>demo<span class="token punctuation">.</span>launch<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>你直接在Colab或者你本地的Jupyter Notebook里面，执行一下这一讲到目前的所有代码，就得到了一个可以和ChatGPT聊天的机器人了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/a57a0f1197de3b8b8e625d9cfe506502.png" alt="图片"></p><h2 id="把机器人部署到HuggingFace上去"><a href="#把机器人部署到HuggingFace上去" class="headerlink" title="把机器人部署到HuggingFace上去"></a>把机器人部署到HuggingFace上去</h2><p>有了一个可以聊天的机器人，相信你已经迫不及待地想让你的朋友也能用上它了。那么我们就把它部署到 <a href="https://huggingface.co/">HuggingFace</a> 上去。</p><ol><li>首先你需要注册一个HuggingFace的账号，点击左上角的头像，然后点击 “+New Space” 创建一个新的项目空间。</li></ol><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/5073d7572f5d62ab8cd3bd708bba33b1.png"></p><ol><li>在接下来的界面里，给你的Space取一个名字，然后在Select the Space SDK里面，选择第二个Gradio。硬件我们在这里就选择免费的，项目我们在这里选择public，让其他人也能够看到。不过要注意，public的space，是连你后面上传的代码也能够看到的。</li></ol><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/a8a502a4a6b6d6a6fc4aebbee7ayy7c5.png"></p><ol><li>创建成功后，会跳转到HuggingFace的App界面。里面给了你如何Clone当前的space，然后提交代码部署App的方式。我们只需要通过Git把当前space下载下来，然后提交两个文件就可以了，分别是：</li></ol><ul><li>app.py 包含了我们的Gradio应用；</li><li>requirements.txt 包含了这个应用依赖的Python包，这里我们只依赖OpenAI这一个包。</li></ul><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e24d1f90f9a67a61182dbb4e20899852.png"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/30c7739844201fee8da9e86752ea4ec6.png"></p><p>代码提交之后，HuggingFace的页面会自动刷新，你可以直接看到对应的日志和Chatbot的应用。不过这个时候，我们还差一步工作。</p><ol><li>因为我们的代码里是通过环境变量获取OpenAI的API Key的，所以我们还要在这个HuggingFace的Space里设置一下这个环境变量。</li></ol><ul><li><p>你可以点击界面里面的Settings，然后往下找到Repository secret。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/7576e32270d02302c4397254d9deb5d9.png"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/4165279b0c1580da31e5fd152f23d875.png" alt="图片"></p></li></ul><p>在Name这里输入 OPENAI_API_KEY，然后在Secret value里面填入你的OpenAI的密钥。</p><ul><li>设置完成之后，你还需要点击一下Restart this space确保这个应用重新加载一遍，以获取到新设置的环境变量。</li></ul><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c29226b9bf2490a7cb7dc0b84eccdbfc.png" alt="图片"></p><p>好啦，这个时候，你可以重新点击App这个Tab页面，试试你的聊天机器人是否可以正常工作了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/46d4b47402e78d54718a5738f005700a.png"></p><p>我把今天给你看到的Chatbot应用放到了HuggingFace上，你可以直接复制下来试一试。</p><p>地址： <a href="https://huggingface.co/spaces/xuwenhao83/simple_chatbot">https://huggingface.co/spaces/xuwenhao83/simple_chatbot</a></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>希望通过这一讲，你已经学会了怎么使用ChatGPT的接口来实现一个聊天机器人了。我们分别实现了只保留固定轮数的对话，并且体验了它的效果。我们也明白了为什么我们总是需要把所有的上下文都发送给OpenAI的接口。然后我们通过Gradio这个库开发了一个聊天机器人界面。最后，我们将这个简单的聊天机器人部署到了HuggingFace上，让你可以分享给自己的朋友使用。希望你玩得高兴！</p><h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><p>在这一讲里，我们的Chatbot只能维护过去N轮的对话。这意味着如果对话很长的话，我们一开始对话的信息就被丢掉了。有一种方式是我们不设定轮数，只限制传入的上下文的Token数量。</p><ol><li>你能根据这一讲学到的内容，修改一下代码，让这个聊天机器人不限制轮数，只在Token数量要超标的时候再删减最开始的对话么？</li><li>除了“忘记”开始的几轮，你还能想到什么办法让AI尽可能多地记住上下文吗？</li></ol><p>期待能在评论区看到你的思考，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;06｜ChatGPT来了，让我们快速做个AI应用&quot;&gt;&lt;a href=&quot;#06｜ChatGPT来了，让我们快速做个AI应用&quot; class=&quot;headerlink&quot; title=&quot;06｜ChatGPT来了，让我们快速做个AI应用&quot;&gt;&lt;/a&gt;06｜ChatGPT来了，让</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>07｜文本聚类与摘要，让AI帮你做个总结</title>
    <link href="https://zhuansun.github.io/geekbang/posts/198495601.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/198495601.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.891Z</updated>
    
    <content type="html"><![CDATA[<h1 id="07｜文本聚类与摘要，让AI帮你做个总结"><a href="#07｜文本聚类与摘要，让AI帮你做个总结" class="headerlink" title="07｜文本聚类与摘要，让AI帮你做个总结"></a>07｜文本聚类与摘要，让AI帮你做个总结</h1><p>你好，我是徐文浩。</p><p>上一讲里，我们用上了最新的ChatGPT的API，注册好了HuggingFace的账号，也把我们的聊天机器人部署了出去。希望通过这个过程，你对实际的应用开发过程已经有了充足的体验。那么这一讲里，我们会回到OpenAI的各个接口能够提供的能力。我们分别看看怎么通过Embedding进行文本聚类，怎么利用提示语（Prompt）做文本的总结。</p><h2 id="基于Embedding向量进行文本聚类"><a href="#基于Embedding向量进行文本聚类" class="headerlink" title="基于Embedding向量进行文本聚类"></a>基于Embedding向量进行文本聚类</h2><p>我先给不太了解技术的同学简单科普一下什么叫做文本聚类，文本聚类就是把很多没有标注过的文本，根据它们之间的相似度，自动地分成几类。基于GPT系列的模型进行文本聚类很简单，因为我们可以通过Embedding把文本变成一段向量。而对于向量我们自然可以用一些简单的聚类算法，比如我们采用最简单的K-Means算法就可以了。</p><p>这一次，我们选用的数据集，是很多老的机器学习教程里常用的20 newsgroups数据集，也就是一个带了标注分好类的英文新闻组的数据集。这个数据集，其实不是最自然的自然语言，里面的数据是经过了预处理的，比如去除了标点符号、停用词等等。我们正好可以拿来看看，面对这样其实不太“自然语言”的数据，OpenAI的GPT系列模型处理的效果怎么样。</p><p>首先，我们先通过scikit-learn这个Python库来拿到数据，数据集就内置在这个库里面。scikit-learn也是非常常用的一个机器学习库，我们直接把数据下载下来，存储成CSV文件。对应的代码在下面可以看到。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> fetch_20newsgroups<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">def</span> <span class="token function">twenty_newsgroup_to_csv</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    newsgroups_train <span class="token operator">=</span> fetch_20newsgroups<span class="token punctuation">(</span>subset<span class="token operator">=</span><span class="token string">'train'</span><span class="token punctuation">,</span> remove<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'headers'</span><span class="token punctuation">,</span> <span class="token string">'footers'</span><span class="token punctuation">,</span> <span class="token string">'quotes'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">[</span>newsgroups_train<span class="token punctuation">.</span>data<span class="token punctuation">,</span> newsgroups_train<span class="token punctuation">.</span>target<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T    df<span class="token punctuation">.</span>columns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">,</span> <span class="token string">'target'</span><span class="token punctuation">]</span>    targets <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span> newsgroups_train<span class="token punctuation">.</span>target_names<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'title'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    out <span class="token operator">=</span> pd<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>df<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> left_on<span class="token operator">=</span><span class="token string">'target'</span><span class="token punctuation">,</span> right_index<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    out<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">'20_newsgroup.csv'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>twenty_newsgroup_to_csv<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接着，我们要对数据做预处理，我们需要过滤掉数据里面有些文本是空的情况。以及和我们前面进行文本分类一样，把Token数量太多的给过滤掉。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> openai<span class="token punctuation">.</span>embeddings_utils <span class="token keyword">import</span> get_embeddings<span class="token keyword">import</span> openai<span class="token punctuation">,</span> os<span class="token punctuation">,</span> tiktoken<span class="token punctuation">,</span> backoffopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>embedding_model <span class="token operator">=</span> <span class="token string">"text-embedding-ada-002"</span>embedding_encoding <span class="token operator">=</span> <span class="token string">"cl100k_base"</span>  <span class="token comment"># this the encoding for text-embedding-ada-002</span>batch_size <span class="token operator">=</span> <span class="token number">2000</span>max_tokens <span class="token operator">=</span> <span class="token number">8000</span>  <span class="token comment"># the maximum for text-embedding-ada-002 is 8191</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'20_newsgroup.csv'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Number of rows before null filtering:"</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">)</span>df <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>isnull<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token boolean">False</span><span class="token punctuation">]</span>encoding <span class="token operator">=</span> tiktoken<span class="token punctuation">.</span>get_encoding<span class="token punctuation">(</span>embedding_encoding<span class="token punctuation">)</span>df<span class="token punctuation">[</span><span class="token string">"n_tokens"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">.</span>text<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token builtin">len</span><span class="token punctuation">(</span>encoding<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Number of rows before token number filtering:"</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">)</span>df <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">.</span>n_tokens <span class="token operator">&lt;=</span> max_tokens<span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Number of rows data used:"</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">Number of rows before null filtering<span class="token punctuation">:</span> <span class="token number">11314</span>Number of rows before token number filtering<span class="token punctuation">:</span> <span class="token number">11096</span>Number of rows data used<span class="token punctuation">:</span> <span class="token number">11044</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>然后，我们仍然是通过Embedding的接口，拿到文本的Embedding向量，然后把整个数据存储成parquet文件。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@backoff<span class="token punctuation">.</span>on_exception</span><span class="token punctuation">(</span>backoff<span class="token punctuation">.</span>expo<span class="token punctuation">,</span> openai<span class="token punctuation">.</span>error<span class="token punctuation">.</span>RateLimitError<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">get_embeddings_with_backoff</span><span class="token punctuation">(</span>prompts<span class="token punctuation">,</span> engine<span class="token punctuation">)</span><span class="token punctuation">:</span>    embeddings <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>prompts<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch <span class="token operator">=</span> prompts<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>batch_size<span class="token punctuation">]</span>        embeddings <span class="token operator">+=</span> get_embeddings<span class="token punctuation">(</span>list_of_text<span class="token operator">=</span>batch<span class="token punctuation">,</span> engine<span class="token operator">=</span>engine<span class="token punctuation">)</span>    <span class="token keyword">return</span> embeddingsprompts <span class="token operator">=</span> df<span class="token punctuation">.</span>text<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>prompt_batches <span class="token operator">=</span> <span class="token punctuation">[</span>prompts<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>batch_size<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>prompts<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">]</span>embeddings <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> batch <span class="token keyword">in</span> prompt_batches<span class="token punctuation">:</span>    batch_embeddings <span class="token operator">=</span> get_embeddings_with_backoff<span class="token punctuation">(</span>prompts<span class="token operator">=</span>batch<span class="token punctuation">,</span> engine<span class="token operator">=</span>embedding_model<span class="token punctuation">)</span>    embeddings <span class="token operator">+=</span> batch_embeddingsdf<span class="token punctuation">[</span><span class="token string">"embedding"</span><span class="token punctuation">]</span> <span class="token operator">=</span> embeddingsdf<span class="token punctuation">.</span>to_parquet<span class="token punctuation">(</span><span class="token string">"data/20_newsgroup_with_embedding.parquet"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这一部分代码基本和前面我们做文本分类一样，我就不再做详细讲解了。不理解代码为什么要这么写的同学，可以去看前面的 <a href="http://time.geekbang.org/column/article/643889">第 05 讲</a>。</p><p>通常，在使用Jupyter Notebook或者Python去做机器学习类的任务的时候，我们往往会把一些中间步骤的数据结果给存下来。这样，我们可以避免在后面步骤写了Bug或者参数设错的时候从头开始。在这里，我们就把原始数据，以及Embedding处理完的数据都存了一份。这样，如果后面的聚类程序要做修改，我们不需要再花钱让OpenAI给我们算一次Embedding了。</p><p>接着，我们就可以用K-Means算法来进行聚类了。因为原本的数据来自20个不同的新闻组，那么我们也不妨就聚合成20个类，正好我们看看自动聚类出来的分类会不会就和原始分类差不多。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>cluster <span class="token keyword">import</span> KMeansembedding_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_parquet<span class="token punctuation">(</span><span class="token string">"data/20_newsgroup_with_embedding.parquet"</span><span class="token punctuation">)</span>matrix <span class="token operator">=</span> np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span>embedding_df<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>values<span class="token punctuation">)</span>num_of_clusters <span class="token operator">=</span> <span class="token number">20</span>kmeans <span class="token operator">=</span> KMeans<span class="token punctuation">(</span>n_clusters<span class="token operator">=</span>num_of_clusters<span class="token punctuation">,</span> init<span class="token operator">=</span><span class="token string">"k-means++"</span><span class="token punctuation">,</span> n_init<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span>kmeans<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>matrix<span class="token punctuation">)</span>labels <span class="token operator">=</span> kmeans<span class="token punctuation">.</span>labels_embedding_df<span class="token punctuation">[</span><span class="token string">"cluster"</span><span class="token punctuation">]</span> <span class="token operator">=</span> labels<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>聚类的代码非常简单，我们通过NumPy的stack函数，把所有的Embedding放到一个矩阵里面，设置一下要聚合出来的类的数量，然后运行一下K-Means算法的fit函数，就好了。不过，聚类完，我们怎么去看它聚类的结果是不是合适呢？每个聚合出来的类代表什么呢？</p><p>在这里，我们的数据之前就有分组。那么我们就可以用一个取巧的思路。我们统计一下聚类之后的每个类有多少条各个newsgroups分组的数据。然后看看这些数据里面，排名第一的分组是什么。如果我们聚类聚合出来的类，都是从某一个newsgroup分组出来的文章，那么说明这个聚合出来的类其实就和那个分组的内容差不多。使用这个思路的代码，我也放在下面了，我们一起来看一看。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 统计每个cluster的数量</span>new_df <span class="token operator">=</span> embedding_df<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span><span class="token string">'cluster'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'cluster'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">'count'</span><span class="token punctuation">)</span><span class="token comment"># 统计这个cluster里最多的分类的数量</span>title_count <span class="token operator">=</span> embedding_df<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'cluster'</span><span class="token punctuation">,</span> <span class="token string">'title'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">'title_count'</span><span class="token punctuation">)</span>first_titles <span class="token operator">=</span> title_count<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span><span class="token string">'cluster'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>nlargest<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'title_count'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>first_titles <span class="token operator">=</span> first_titles<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>new_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>new_df<span class="token punctuation">,</span> first_titles<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'cluster'</span><span class="token punctuation">,</span> <span class="token string">'title'</span><span class="token punctuation">,</span> <span class="token string">'title_count'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> on<span class="token operator">=</span><span class="token string">'cluster'</span><span class="token punctuation">,</span> how<span class="token operator">=</span><span class="token string">'left'</span><span class="token punctuation">)</span>new_df <span class="token operator">=</span> new_df<span class="token punctuation">.</span>rename<span class="token punctuation">(</span>columns<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">'title'</span><span class="token punctuation">:</span> <span class="token string">'rank1'</span><span class="token punctuation">,</span> <span class="token string">'title_count'</span><span class="token punctuation">:</span> <span class="token string">'rank1_count'</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token comment"># 统计这个cluster里第二多的分类的数量</span>second_titles <span class="token operator">=</span> title_count<span class="token punctuation">[</span><span class="token operator">~</span>title_count<span class="token punctuation">[</span><span class="token string">'title'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>isin<span class="token punctuation">(</span>first_titles<span class="token punctuation">[</span><span class="token string">'title'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>second_titles <span class="token operator">=</span> second_titles<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span><span class="token string">'cluster'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>nlargest<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'title_count'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>second_titles <span class="token operator">=</span> second_titles<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>new_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>new_df<span class="token punctuation">,</span> second_titles<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'cluster'</span><span class="token punctuation">,</span> <span class="token string">'title'</span><span class="token punctuation">,</span> <span class="token string">'title_count'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> on<span class="token operator">=</span><span class="token string">'cluster'</span><span class="token punctuation">,</span> how<span class="token operator">=</span><span class="token string">'left'</span><span class="token punctuation">)</span>new_df <span class="token operator">=</span> new_df<span class="token punctuation">.</span>rename<span class="token punctuation">(</span>columns<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">'title'</span><span class="token punctuation">:</span> <span class="token string">'rank2'</span><span class="token punctuation">,</span> <span class="token string">'title_count'</span><span class="token punctuation">:</span> <span class="token string">'rank2_count'</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>new_df<span class="token punctuation">[</span><span class="token string">'first_percentage'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>new_df<span class="token punctuation">[</span><span class="token string">'rank1_count'</span><span class="token punctuation">]</span> <span class="token operator">/</span> new_df<span class="token punctuation">[</span><span class="token string">'count'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token string">'&#123;:.2%&#125;'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 将缺失值替换为 0</span>new_df<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># 输出结果</span><span class="token keyword">from</span> IPython<span class="token punctuation">.</span>display <span class="token keyword">import</span> displaydisplay<span class="token punctuation">(</span>new_df<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个代码也不难写，我们可以分成几步来做。</p><ol><li>我们通过groupby可以把之前的DataFrame按照cluster进行聚合，统计每个cluster里面数据的条数。</li><li>而要统计某一个cluster里面排名第一的分组名称和数量的时候，我们可以通过groupby，把数据按照 cluster + title 的方式聚合。</li><li>再通过 cluster 聚合后，使用 x.nlargest 函数拿到里面数量排名第一的分组的名字和数量。</li><li>为了方便分析，我还把数据里排名第一的去掉之后，又统计了一下排名第二的分组，放在一起看一下。</li></ol><p>输出结果：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f66f45d30edfb11a3f97a964684bd00c.png" alt="图片"></p><p>从这个统计数据的结果来看，大部分聚类的结果，能够对应到某一个原本新闻组的分类。比如，cluster 0就有93.98%来自 comp.windows.x 这个分类。在20个聚合出来的类里面，有10个类80%来自原本newsgroup的某一个分类。剩下的分类中，比如cluster 2，前两个分组加在一起占了75%，这两个分组的名称分别是 pc.hardware 和 mac.hardware 其实都是聊电脑硬件的，不过是newsgroups里按照硬件不同做了区分而已。我们只有3个类，对应的分组比较分散，分别是cluster 6、9和19。</p><p>从这个结果来看，我们直接使用文本的Embedding来进行聚类，效果还算不错。</p><h2 id="使用提示语对文本进行总结"><a href="#使用提示语对文本进行总结" class="headerlink" title="使用提示语对文本进行总结"></a>使用提示语对文本进行总结</h2><p>不过啊，在真实的应用场景里，我们拿来进行文本聚类的数据，多半并没有什么分组信息。过去，我们要去给聚合出来的类取一个名字，往往只能选择看看各个类里面的文本是什么内容。靠我们的“人脑”给“电脑”做出的选择起一个我们觉得合适的名字。比如，对应到这里的20个分类的数据，往往我们只能每个挑上几篇内容，人工读一遍，再取一个名字。而如果你英文不太好，那可就太痛苦了。</p><p>不过，既然有了OpenAI的Completion接口，我们完全可以让AI给我们聚合出来的类起一个名字。我们可以随机在每个聚合出来的类里面，挑上3～5条，然后请AI总结一下该取什么名字，然后再挑一两条文本让AI给我们翻译成中文，看看名字取的是不是合理。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">items_per_cluster <span class="token operator">=</span> <span class="token number">10</span>COMPLETIONS_MODEL <span class="token operator">=</span> <span class="token string">"text-davinci-003"</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_of_clusters<span class="token punctuation">)</span><span class="token punctuation">:</span>    cluster_name <span class="token operator">=</span> new_df<span class="token punctuation">[</span>new_df<span class="token punctuation">.</span>cluster <span class="token operator">==</span> i<span class="token punctuation">]</span><span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>rank1    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Cluster </span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">, Rank 1: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>cluster_name<span class="token punctuation">&#125;</span></span><span class="token string">, Theme:"</span></span><span class="token punctuation">,</span> end<span class="token operator">=</span><span class="token string">" "</span><span class="token punctuation">)</span>    content <span class="token operator">=</span> <span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>        embedding_df<span class="token punctuation">[</span>embedding_df<span class="token punctuation">.</span>cluster <span class="token operator">==</span> i<span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>items_per_cluster<span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span><span class="token punctuation">.</span>values    <span class="token punctuation">)</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>Completion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        model<span class="token operator">=</span>COMPLETIONS_MODEL<span class="token punctuation">,</span>        prompt<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f'''我们想要给下面的内容，分组成有意义的类别，以便我们可以对其进行总结。请根据下面这些内容的共同点，总结一个50个字以内的新闻组的名称。比如 “PC硬件”\n\n内容:\n"""\n</span><span class="token interpolation"><span class="token punctuation">&#123;</span>content<span class="token punctuation">&#125;</span></span><span class="token string">\n"""新闻组名称：'''</span></span><span class="token punctuation">,</span>        temperature<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>        max_tokens<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>        top_p<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们可以用这样一段代码通过Completion接口来实现我们的需求。</p><ol><li>我们随机从聚类结果里的每一个类里面，都挑上10条记录，然后分行将这些记录拼在一起。</li><li>然后，我们给AI这样一段提示语，告诉AI这些内容来自新闻组，请AI根据它们的共性给这些新闻组的内容取一个50个字以内的名字。</li><li>输出的内容，我们用Cluster，Cluster里原先排名第一的分组英文，以及AI给出的新闻组名称，对应的输出结果在下面。</li></ol><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">Cluster <span class="token number">0</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> comp<span class="token punctuation">.</span>windows<span class="token punctuation">.</span>x<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> Xlib编程Cluster <span class="token number">1</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> sci<span class="token punctuation">.</span>space<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 太空技术与航空Cluster <span class="token number">2</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> comp<span class="token punctuation">.</span>sys<span class="token punctuation">.</span>ibm<span class="token punctuation">.</span>pc<span class="token punctuation">.</span>hardware<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> PC硬件与系统Cluster <span class="token number">3</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> rec<span class="token punctuation">.</span>sport<span class="token punctuation">.</span>hockey<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 欧洲冰球vs北美冰球Cluster <span class="token number">4</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> talk<span class="token punctuation">.</span>politics<span class="token punctuation">.</span>misc<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 社会观点与自由Cluster <span class="token number">5</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> rec<span class="token punctuation">.</span>autos<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 汽车硬件Cluster <span class="token number">6</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> rec<span class="token punctuation">.</span>motorcycles<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 数学与文化冲击Cluster <span class="token number">7</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> comp<span class="token punctuation">.</span>os<span class="token punctuation">.</span>ms<span class="token operator">-</span>windows<span class="token punctuation">.</span>misc<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> PC软件与硬件Cluster <span class="token number">8</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> talk<span class="token punctuation">.</span>politics<span class="token punctuation">.</span>mideast<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> “穆斯林大屠杀”Cluster <span class="token number">9</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> comp<span class="token punctuation">.</span>os<span class="token punctuation">.</span>ms<span class="token operator">-</span>windows<span class="token punctuation">.</span>misc<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 科技产品<span class="token string">""</span>"Cluster <span class="token number">10</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> talk<span class="token punctuation">.</span>politics<span class="token punctuation">.</span>guns<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 枪支管制与安全Cluster <span class="token number">11</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> comp<span class="token punctuation">.</span>graphics<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 计算机编程与硬件Cluster <span class="token number">12</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> rec<span class="token punctuation">.</span>motorcycles<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 骑行安全与技巧Cluster <span class="token number">13</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> soc<span class="token punctuation">.</span>religion<span class="token punctuation">.</span>christian<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 宗教信仰与实践Cluster <span class="token number">14</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> rec<span class="token punctuation">.</span>sport<span class="token punctuation">.</span>baseball<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 棒球联盟Cluster <span class="token number">15</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> misc<span class="token punctuation">.</span>forsale<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 购物优惠和出售Cluster <span class="token number">16</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> sci<span class="token punctuation">.</span>crypt<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 关于加密政策的讨论Cluster <span class="token number">17</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> sci<span class="token punctuation">.</span>electronics<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 电子设备技术Cluster <span class="token number">18</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> sci<span class="token punctuation">.</span>med<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 药物和疾病Cluster <span class="token number">19</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> sci<span class="token punctuation">.</span>electronics<span class="token punctuation">,</span> Theme<span class="token punctuation">:</span> 电子邮件使用者研究<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，机器给出的中文分类名称，大部分是合理的。我们还可以挑一些里面的文本内容，看看它们的中文翻译是不是和上面取的名字是一致的。翻译的代码和上面类似，少数的几个差别是：</p><ol><li>我们在每个分类的抽样数据里只找了1条，而不是总结时候选的10条。</li><li>我们限制了这段文本的Token数量不超过100个，免得太占地方。</li><li>输出的内容我们放大了字数到500字，确保翻译能提供足够的内容。</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python">items_per_cluster <span class="token operator">=</span> <span class="token number">1</span>COMPLETIONS_MODEL <span class="token operator">=</span> <span class="token string">"text-davinci-003"</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_of_clusters<span class="token punctuation">)</span><span class="token punctuation">:</span>    cluster_name <span class="token operator">=</span> new_df<span class="token punctuation">[</span>new_df<span class="token punctuation">.</span>cluster <span class="token operator">==</span> i<span class="token punctuation">]</span><span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>rank1    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Cluster </span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">, Rank 1: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>cluster_name<span class="token punctuation">&#125;</span></span><span class="token string">, 抽样翻译:"</span></span><span class="token punctuation">,</span> end<span class="token operator">=</span><span class="token string">" "</span><span class="token punctuation">)</span>    content <span class="token operator">=</span> <span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>        embedding_df<span class="token punctuation">[</span><span class="token punctuation">(</span>embedding_df<span class="token punctuation">.</span>cluster <span class="token operator">==</span> i<span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>embedding_df<span class="token punctuation">.</span>n_tokens <span class="token operator">></span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>items_per_cluster<span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span><span class="token punctuation">.</span>values    <span class="token punctuation">)</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>Completion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        model<span class="token operator">=</span>COMPLETIONS_MODEL<span class="token punctuation">,</span>        prompt<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f'''请把下面的内容翻译成中文\n\n内容:\n"""\n</span><span class="token interpolation"><span class="token punctuation">&#123;</span>content<span class="token punctuation">&#125;</span></span><span class="token string">\n"""翻译：'''</span></span><span class="token punctuation">,</span>        temperature<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>        max_tokens<span class="token operator">=</span><span class="token number">2000</span><span class="token punctuation">,</span>        top_p<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">Cluster <span class="token number">0</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> comp<span class="token punctuation">.</span>windows<span class="token punctuation">.</span>x<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 没有实际执行它？不知怎么回事，我的一个xterminal用户使得只要点击鼠标右键，就会自动杀死所有客户端<span class="token operator">-</span>哦，我的：<span class="token operator">-</span><span class="token punctuation">(</span>谢谢，FishCluster <span class="token number">1</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> sci<span class="token punctuation">.</span>space<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 韦恩·马森和他的团伙在阿拉巴马州发生了什么？我还听说有一个未经证实的谣言，即航空大使们已经消失了。有其他人可以证实吗？Cluster <span class="token number">2</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> comp<span class="token punctuation">.</span>sys<span class="token punctuation">.</span>ibm<span class="token punctuation">.</span>pc<span class="token punctuation">.</span>hardware<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 我怀疑这不是一个特定于Quadra的问题。去年我不得不放弃我“古老”的Bernoulli <span class="token number">20</span>（每个磁带的价格大约是<span class="token number">90</span>美元，使整个事情的价值超过我的整个电脑<span class="token punctuation">;</span>）。Ocean Microsystems的技术支持人员建议可以使用一些第三方驱动程序来解决这个问题 <span class="token operator">-</span> 在我的情况下，磁带无法格式化<span class="token operator">/</span>挂载<span class="token operator">/</span>分区用于A <span class="token operator">/</span> UX。Cluster <span class="token number">3</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> rec<span class="token punctuation">.</span>sport<span class="token punctuation">.</span>hockey<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 我相信那是<span class="token number">4</span><span class="token operator">-</span><span class="token number">1</span>。罗德·布林道·阿莫尔在第三节<span class="token number">19.59</span>时分攻入一球。Cluster <span class="token number">4</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> talk<span class="token punctuation">.</span>politics<span class="token punctuation">.</span>misc<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 为了确保每个人都清楚：“它从未有过”是指“保护”，而不是“未能保护”；即，在我的一生中，我从未见过美国政府始终保护美国公民的利益，除非是意外。Cluster <span class="token number">5</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> rec<span class="token punctuation">.</span>autos<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 噢，来吧，傻瓜，你要做的就是在你的引擎罩上割一个洞，然后把一个管子放进去，这样你就可以把机油倒进去了。你觉得那些热门车上的大空气进气装置是干什么的？它们只是为了外观，没有人知道，它们提供了进入机油填充孔的途径。Cluster <span class="token number">6</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> rec<span class="token punctuation">.</span>motorcycles<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 你真是个失败者Cluster <span class="token number">7</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> comp<span class="token punctuation">.</span>os<span class="token punctuation">.</span>ms<span class="token operator">-</span>windows<span class="token punctuation">.</span>misc<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 偶尔你需要为表现良好的东西说句好话。我的东西桥<span class="token number">3401</span>没有任何问题。它在DOS和OS<span class="token operator">/</span><span class="token number">2</span>上运行得很好。对于OS<span class="token operator">/</span><span class="token number">2</span>，你不需要加载任何特殊的驱动程序。安装会检测到它是一个东西桥驱动器，然后就完成了。顺便说一句，它也很快！Cluster <span class="token number">8</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> talk<span class="token punctuation">.</span>politics<span class="token punctuation">.</span>mideast<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> Avi，    供你参考，伊斯兰教允许宗教自由——在宗教上没有强制。犹太教是否也允许宗教自由（即是否认可非犹太人）？只是好奇而已。Cluster <span class="token number">9</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> comp<span class="token punctuation">.</span>os<span class="token punctuation">.</span>ms<span class="token operator">-</span>windows<span class="token punctuation">.</span>misc<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 每个人都有自己的梦想，但只有勇敢追求梦想的人才能实现它。Cluster <span class="token number">10</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> talk<span class="token punctuation">.</span>politics<span class="token punctuation">.</span>guns<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 不一定，特别是如果强奸犯被认定为此。例如，如果你有意地把手指伸进一个装满了老鼠夹的地方，然后被夹住，这是谁的错？Cluster <span class="token number">11</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> comp<span class="token punctuation">.</span>graphics<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 帮帮我！！我需要代码<span class="token operator">/</span>包<span class="token operator">/</span>任何东西来处理3D数据，并将其转换为带有隐藏线的线框表面。我正在使用DOS机器，代码可以是ANSI C或C <span class="token operator">+</span><span class="token operator">+</span>，ANSI Fortran或Basic。我使用的数据形成一个矩形网格。请将您的回复发布到网络上，以便其他人受益。我的个人观点是，这是一个普遍的兴趣问题。谢谢！！！！！Cluster <span class="token number">12</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> rec<span class="token punctuation">.</span>motorcycles<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 这是一段心理学，对于任何长期骑行者来说都是必不可少的。人们不会去想“如果我这么做会有其他人受到影响吗？”他们只会评估“如果我这么做会受到影响吗？”Cluster <span class="token number">13</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> soc<span class="token punctuation">.</span>religion<span class="token punctuation">.</span>christian<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 这是一个非常薄弱的论点，因为没有独立的支持文本（关于关键事件）。至于新约最古老的现存文本的日期<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>如果现在只有一个关于美国内战的现存文本，你会怎么想？现在考虑一个大部分文盲的人口，每一份手稿都是手工复制的<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">-</span><span class="token operator">-</span>HalCluster <span class="token number">14</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> rec<span class="token punctuation">.</span>sport<span class="token punctuation">.</span>baseball<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 这个赔率意味着你下注<span class="token number">5</span>美元赌反败者赢<span class="token number">8</span>美元，或者下注<span class="token number">9</span>美元赌胜者赢<span class="token number">5</span>美元。Cluster <span class="token number">15</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> misc<span class="token punctuation">.</span>forsale<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 嗯，标题就是这样<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>我正在寻找便宜的二手TG<span class="token operator">-</span><span class="token number">16</span>游戏，它们支持<span class="token number">2</span>个或更多玩家（同时）<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>请给我发送所有带有价格的报价。Cluster <span class="token number">16</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> sci<span class="token punctuation">.</span>crypt<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 哪里？老实说，我没有看到任何……我不同意，至少有其他标准已经存在。此外，即使他们限制NREN上的加密，谁在乎呢？大部分互联网都是商业的。NREN只适用于政府和大学研究（阅读提案<span class="token operator">-</span>它是一条“数据高速公路”，与互联网无关）。Cluster <span class="token number">17</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> sci<span class="token punctuation">.</span>electronics<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 动态RAM不是基于翻转锁存器；基本上每个位只有一个晶体管和电容来存储！静态RAM是基于翻转锁存器，更加昂贵，密度也更低。如果忽略电子和热膨胀，两者都没有任何“移动”的部件<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>ChrisCluster <span class="token number">18</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> sci<span class="token punctuation">.</span>med<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 化学品已经消失，感谢所有的回应。Cluster <span class="token number">19</span><span class="token punctuation">,</span> Rank <span class="token number">1</span><span class="token punctuation">:</span> sci<span class="token punctuation">.</span>electronics<span class="token punctuation">,</span> 抽样翻译<span class="token punctuation">:</span> 尝试lyman<span class="token punctuation">.</span>pppl<span class="token punctuation">.</span>gov<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">/</span>pub<span class="token operator">/</span><span class="token number">8051</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从输出的结果来看，我们选取的内容和AI总结的标题的确八九不离十。不知道这个过程，有没有重新让你回忆起Completion这个接口的强大之处。我们只要给出合适的指令，它就能按照我们的心意给出合理的结果。</p><h2 id="利用文本摘要支持无限轮数的聊天"><a href="#利用文本摘要支持无限轮数的聊天" class="headerlink" title="利用文本摘要支持无限轮数的聊天"></a>利用文本摘要支持无限轮数的聊天</h2><p>不知道你是否还记得上一讲里的思考题，如果不只是保留过去几轮对话，我们怎么在对话轮数越来越多的时候让AI记住尽量多的上下文？有一个办法，就是将过去几轮的上下文内容，先交给AI做一个100字的总结。然后，把这个总结也作为对话的Prompt信息，告诉AI这是聊天的背景，再重新开始新的一轮对话。</p><p>在这里我们不妨试一下。</p><ol><li>我们拿上一讲的一段聊天记录，让AI先总结一下，可以看到AI总结了User和Assistant之间聊了什么。</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python">history <span class="token operator">=</span> <span class="token triple-quoted-string string">"""User : 你是谁？Assistant : 我是一个人工智能语言模型，可以回答各种问题，包括做菜的问题。User : 请问鱼香肉丝怎么做？Assistant : 鱼香肉丝的做法：1. 猪里脊肉切丝，加入盐、淀粉、料酒搅拌均匀，腌制10分钟2. 胡萝卜、青椒、木耳切丝备用3. 热锅凉油，放入蒜末、姜末煸炒出香味，加入肉丝翻炒至变色4. 加入胡萝卜、青椒、木耳翻炒，加入酱油、糖、醋、盐、味精、水淀粉搅拌均匀，倒入锅中翻炒均匀即可。提示：鱼香肉丝的关键是调料的配比，酱油、糖、醋、盐、味精的比例要适合自己的口味。User : 那蚝油牛肉呢？Assistant : 蚝油牛肉的做法：1. 将牛肉切成薄片，加入盐、淀粉、料酒腌制10分钟2. 葱姜切末，蒜切片，青红椒切丝备用3. 热锅凉油，放入葱姜末、蒜片煸炒出香味，加入牛肉翻炒至变色4. 加入蚝油、酱油、糖、水淀粉调味，加入青红椒翻炒均匀即可。提示：蚝油牛肉的关键是牛肉要切薄，翻炒时火候要快，保证牛肉口感鲜嫩。调味时，蚝油和酱油的比例也要适合自己的口味。"""</span><span class="token keyword">def</span> <span class="token function">summarize</span><span class="token punctuation">(</span>text<span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>Completion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        model<span class="token operator">=</span>COMPLETIONS_MODEL<span class="token punctuation">,</span>        prompt<span class="token operator">=</span>text <span class="token operator">+</span> <span class="token string">"\n\n请总结一下上面User和Assistant聊了些什么：\n"</span><span class="token punctuation">,</span>        max_tokens<span class="token operator">=</span>max_tokens<span class="token punctuation">,</span>    <span class="token punctuation">)</span>    <span class="token keyword">return</span> response<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span>summarized <span class="token operator">=</span> summarize<span class="token punctuation">(</span>history<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>summarized<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">User和Assistant聊了鱼香肉丝和蚝油牛肉的制作方法。User问了Assistant两个关于如何做鱼香肉丝和蚝油牛肉的问题，Assistant给出了回答并介绍了每道菜的具体制作方法，同时也提示了调料的配比和牛肉制作时要注意的细节。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol><li>然后，我们再新建一个Conversation，这次的提示语里，我们先加上了总结的内容，然后告诉AI把对话继续下去。</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python">prompt <span class="token operator">=</span> summarized <span class="token operator">+</span> <span class="token string">"\n\n请你根据已经聊了的内容，继续对话："</span>conversation <span class="token operator">=</span> Conversation<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>question <span class="token operator">=</span> <span class="token string">"那宫保鸡丁呢？"</span>answer <span class="token operator">=</span> conversation<span class="token punctuation">.</span>ask<span class="token punctuation">(</span>question<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"User : %s"</span> <span class="token operator">%</span> question<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Assistant : %s\n"</span> <span class="token operator">%</span> answer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意，如果你是在Notebook里面执行的话，你需要把上一讲的Conversation类的代码复制过来先执行一下。这里，我们启动了一个新的对话对象，将之前的几轮对话的内容总结了一下，放在Prompt的最前面，然后让AI根据已经聊了的内容，继续往下聊。</p><ol><li><p>当我们直接问，“那宫保鸡丁呢？”，AI会正确回答出宫保鸡丁的做法。</p><p>输出结果：</p></li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python">User <span class="token punctuation">:</span> 那宫保鸡丁呢？Assistant <span class="token punctuation">:</span> 宫保鸡丁的制作方法也比较简单。首先，将鸡肉切成小丁状，用料酒、盐、生抽腌制一下。然后将青椒、红椒、葱姜蒜切成丁状备用。接着，将花生米炒香备用。热锅凉油，油温七成热时放入鸡丁煸炒至变色，捞出备用。再将葱姜蒜爆香，加入青红椒丁翻炒一下，然后加入鸡丁，翻炒均匀。最后加入适量的糖、盐、醋、生抽、老抽、料酒、水淀粉炒匀，最后加入炒香的花生米即可。需要注意的是，炒鸡丁的时候要用大火，这样鸡肉会更嫩。另外，调料的配比也很关键，需要根据个人口味适量调整。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>而如果我们没有加上AI总结的之前的对话，只是让AI对话，它只能和你闲扯一些别的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conversation <span class="token operator">=</span> Conversation<span class="token punctuation">(</span><span class="token string">"请你根据已经聊了的内容，继续对话："</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>question <span class="token operator">=</span> <span class="token string">"那宫保鸡丁呢？"</span>answer <span class="token operator">=</span> conversation<span class="token punctuation">.</span>ask<span class="token punctuation">(</span>question<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"User : %s"</span> <span class="token operator">%</span> question<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Assistant : %s\n"</span> <span class="token operator">%</span> answer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">User <span class="token punctuation">:</span> 那宫保鸡丁呢？Assistant <span class="token punctuation">:</span> 宫保鸡丁是一道非常有名的川菜，口感麻辣鲜香，非常美味。你喜欢吃辣的食物吗？<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如果没有给它已经总结了的内容，AI只会和你瞎扯，告诉你宫保鸡丁很好吃。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>不知道今天教的这些技巧你学会了吗？这一讲里，我们先是快速实验了一下通过Embedding拿到的向量进行文本聚类。对于聚类的结果，我们不用再像以前那样人工看成百上千条数据，然后拍个脑袋给这个类取个名字。我们直接利用了Completion接口可以帮我们总结内容的能力，给分类取了一个名字。从最终的效果来看，还算不错。</p><p>而类似的技巧，也可以用在多轮的长对话中。我们将历史对话，让AI总结成一小段文本放到提示语里面。这样既能够让AI记住过去的对话内容，又不会因为对话越来越长而超出模型可以支持的Token数量。这个技巧也是使用大语言模型的一种常见模式。</p><h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><ol><li>这一讲里，我们使用了让AI概括聚类文本内容和聊天记录的提示语。你自己在体验GPT系列模型的时候，有什么觉得特别有用的提示语吗？欢迎你分享自己的体验。</li><li>在文本聚类里面，有三个聚合出来的类，和原先的分组没有很明显的对应关系。你能利用现在学到的知识，写一些代码看看数据，研究一下是为什么吗？</li></ol><p>期待能在评论区看到你的思考，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;07｜文本聚类与摘要，让AI帮你做个总结&quot;&gt;&lt;a href=&quot;#07｜文本聚类与摘要，让AI帮你做个总结&quot; class=&quot;headerlink&quot; title=&quot;07｜文本聚类与摘要，让AI帮你做个总结&quot;&gt;&lt;/a&gt;07｜文本聚类与摘要，让AI帮你做个总结&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>09｜语义检索，利用Embedding优化你的搜索功能</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3755627929.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3755627929.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.897Z</updated>
    
    <content type="html"><![CDATA[<h1 id="09｜语义检索，利用Embedding优化你的搜索功能"><a href="#09｜语义检索，利用Embedding优化你的搜索功能" class="headerlink" title="09｜语义检索，利用Embedding优化你的搜索功能"></a>09｜语义检索，利用Embedding优化你的搜索功能</h1><p>你好，我是徐文浩。</p><p>在过去的8讲里面，相信你已经对Embedding和Completion接口非常熟悉了。Embedding向量适合作为一个中间结果，用于传统的机器学习场景，比如分类、聚类。而Completion接口，一方面可以直接拿来作为一个聊天机器人，另一方面，你只要善用提示词，就能完成合理的文案撰写、文本摘要、机器翻译等一系列的工作。</p><p>不过，很多同学可能会说，这个和我的日常工作又没有什么关系。的确，日常我们的需求里面，最常使用自然语言处理（NLP）技术的，是搜索、广告、推荐这样的业务。那么，今天我们就来看看，怎么利用OpenAI提供的接口来为这些需求提供些帮助。</p><h2 id="让AI生成实验数据"><a href="#让AI生成实验数据" class="headerlink" title="让AI生成实验数据"></a>让AI生成实验数据</h2><p>在实际演示代码之前，我们需要一些可以拿来实验的数据。之前，我们都是在网上找一些数据集，或者直接使用一些机器学习软件包里面自带的数据集。但是，并不是所有时候我们都能很快找到合适的数据集。这个时候，我们也可以利用AI，我们直接让AI帮我们生成一些数据不就好了吗？</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token punctuation">,</span> osopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>COMPLETION_MODEL <span class="token operator">=</span> <span class="token string">"text-davinci-003"</span><span class="token keyword">def</span> <span class="token function">generate_data_by_prompt</span><span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">:</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>Completion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        engine<span class="token operator">=</span>COMPLETION_MODEL<span class="token punctuation">,</span>        prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>        temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        max_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>        top_p<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span>    <span class="token keyword">return</span> response<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>textprompt <span class="token operator">=</span> <span class="token triple-quoted-string string">"""请你生成50条淘宝网里的商品的标题，每条在30个字左右，品类是3C数码产品，标题里往往也会有一些促销类的信息，每行一条。"""</span>data <span class="token operator">=</span> generate_data_by_prompt<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>为了让数据和真实情况更加接近一点，我们可以好好设计一下我们的提示语。比如，我这里就指明了是淘宝的商品，品类是3C，并且标题里要包含一些促销信息。</p><p>我们把拿到的返回结果，按行分割，加载到一个DataFrame里面，看看结果会是怎么样的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pdproduct_names <span class="token operator">=</span> data<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'product_name'</span><span class="token punctuation">:</span> product_names<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>df<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">    product_name<span class="token number">0</span><span class="token number">1.</span> 【限时特惠】Apple<span class="token operator">/</span>苹果 iPhone <span class="token number">11</span> Pro Max 手机<span class="token number">1</span><span class="token number">2.</span> 【超值折扣】Huawei<span class="token operator">/</span>华为 P30 Pro 智能手机<span class="token number">2</span><span class="token number">3.</span> 【热销爆款】OPPO Reno 10X Zoom 全网通手机<span class="token number">3</span><span class="token number">4.</span> 【限量特价】Xiaomi<span class="token operator">/</span>小米 <span class="token number">9</span> Pro 5G 手机<span class="token number">4</span><span class="token number">5.</span> 【限时促销】Apple<span class="token operator">/</span>苹果 iPad Pro <span class="token number">11</span>寸 平板<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，AI为我们生成了50条商品信息，并且每一个都带上了一些促销相关的标签。不过，在返回的结果里面，每一行都带上了一个标号，所以我们需要简单处理一下，去掉这个标号拿到一些干净的数据。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">df<span class="token punctuation">.</span>product_name <span class="token operator">=</span> df<span class="token punctuation">.</span>product_name<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'.'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>df<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">     product_name<span class="token number">0</span>【限时特惠】Apple<span class="token operator">/</span>苹果 iPhone <span class="token number">11</span> Pro Max 手机<span class="token number">1</span>【超值折扣】Huawei<span class="token operator">/</span>华为 P30 Pro 智能手机<span class="token number">2</span>【热销爆款】OPPO Reno 10X Zoom 全网通手机<span class="token number">3</span>【限量特价】Xiaomi<span class="token operator">/</span>小米 <span class="token number">9</span> Pro 5G 手机<span class="token number">4</span>【限时促销】Apple<span class="token operator">/</span>苹果 iPad Pro <span class="token number">11</span>寸 平板<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们可以如法炮制，再生成一些女装的商品名称，覆盖不同的品类，这样后面我们演示搜索效果的时候就会方便一点。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">clothes_prompt <span class="token operator">=</span> <span class="token triple-quoted-string string">"""请你生成50条淘宝网里的商品的标题，每条在30个字左右，品类是女性的服饰箱包等等，标题里往往也会有一些促销类的信息，每行一条。"""</span>clothes_data <span class="token operator">=</span> generate_data_by_prompt<span class="token punctuation">(</span>clothes_prompt<span class="token punctuation">)</span>clothes_product_names <span class="token operator">=</span> clothes_data<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>clothes_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'product_name'</span><span class="token punctuation">:</span> clothes_product_names<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>clothes_df<span class="token punctuation">.</span>product_name <span class="token operator">=</span> clothes_df<span class="token punctuation">.</span>product_name<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'.'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>clothes_df<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">    product_name<span class="token number">0</span>【新款】时尚百搭羊绒毛衣，暖洋洋温暖你的冬天<span class="token number">1</span>【热卖】复古气质翻领毛衣，穿出时尚感<span class="token number">2</span>【特价】轻薄百搭棉衣，舒适温暖，冬季必备<span class="token number">3</span>【限时】经典百搭牛仔外套，轻松搭配，时尚范<span class="token number">4</span>【全新】简约大气羊绒连衣裙，温柔优雅<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后我们把这两个DataFrame拼接在一起，就是我们接下来用于做搜索实验的数据。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">df <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>df<span class="token punctuation">,</span> clothes_df<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>df <span class="token operator">=</span> df<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>drop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>display<span class="token punctuation">(</span>df<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">product_name<span class="token number">0</span>【新款】Apple<span class="token operator">/</span>苹果 iPhone <span class="token number">11</span> Pro Max 智能手机<span class="token number">1</span>【特惠】华为P30 Pro 8GB<span class="token operator">+</span>256GB 全网通版<span class="token number">2</span>【限量】OnePlus 7T Pro 8GB<span class="token operator">+</span>256GB 全网通<span class="token number">3</span>【新品】小米CC9 Pro 8GB<span class="token operator">+</span>256GB 全网通版<span class="token number">4</span>【热销】三星Galaxy Note10<span class="token operator">+</span> 8GB<span class="token operator">+</span>256GB 全网通<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token number">92</span>【优惠】气质小清新拼接百搭双肩斜挎包<span class="token number">93</span>【热卖】活力色彩精致小巧百搭女士单肩斜挎包<span class="token number">94</span>【特价】简约可爱原宿风时尚双肩斜挎包<span class="token number">95</span>【折扣】潮流小清新拼接百搭女士单肩斜挎包<span class="token number">96</span>【特惠】百搭潮流活力色彩拼色双肩斜挎<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>不过如果你多试几次，你会发现AI有时候返回的条数没有50条，不过没有关系，这个基本不影响我们使用这个数据源。你完全可以多运行几次，获得足够你需要的数据。</p><h2 id="通过Embedding进行语义搜索"><a href="#通过Embedding进行语义搜索" class="headerlink" title="通过Embedding进行语义搜索"></a>通过Embedding进行语义搜索</h2><p>那对于搜索问题，我们可以用GPT模型干些什么呢？像百度、阿里这样的巨头公司自然有很多内部复杂的策略和模型，但是对于大部分中小公司，特别是刚开始提供搜索功能的时候，往往是使用Elasticsearch这个开源项目。而Elasticsearch背后的搜索原理，则是先分词，然后再使用倒排索引。</p><p>简单来说，就是把上面的“气质小清新拼接百搭双肩斜挎包”这样的商品名称，拆分成“气质”“小清新”“拼接”“百搭”“双肩”“斜挎包”。每个标题都是这样切分。然后，建立一个索引，比如“气质”这个词，出现过的标题的编号，都按编号顺序跟在气质后面。其他的词也类似。</p><p>然后，当用户搜索的时候，比如用户搜索“气质背包”，也会拆分成“气质”和“背包”两个词。然后就根据这两个词，找到包含这些词的标题，根据出现的词的数量、权重等等找出一些商品。</p><p>但是，这个策略有一个缺点，就是如果我们有同义词，那么这么简单地去搜索是搜不到的。比如，我们如果搜“自然淡雅背包”，虽然语义上很接近，但是因为“自然”“淡雅”“背包”这三个词在这个商品标题里都没有出现，所以就没有办法匹配上了。为了提升搜索效果，你就得做更多的工程研发工作，比如找一个同义词表，把标题里出现的同义词也算上等等。</p><p>不过，有了OpenAI的Embedding接口，我们就可以把一段文本的语义表示成一段向量。而向量之间是可以计算距离的，这个我们在之前的情感分析的零样本分类里就演示过了。那如果我们把用户的搜索，也通过Embedding接口变成向量。然后把它和所有的商品的标题计算一下余弦距离，找出离我们搜索词最近的几个向量。那最近的几个向量，其实就是语义和这个商品相似的，而并不一定需要相同的关键词。</p><p>那根据这个思路，我们不妨用代码来试一试。</p><p>首先，我们还是要把随机生成出来的所有商品标题，都计算出来它们的Embedding，然后存下来。这里的代码，基本和之前通过Embedding进行分类和聚类一致，就不再详细讲解了。我们还是利用backoff和batch处理，让代码能够容错，并且快速处理完这些商品标题。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> openai<span class="token punctuation">.</span>embeddings_utils <span class="token keyword">import</span> get_embeddings<span class="token keyword">import</span> openai<span class="token punctuation">,</span> os<span class="token punctuation">,</span> backoffopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>embedding_model <span class="token operator">=</span> <span class="token string">"text-embedding-ada-002"</span>batch_size <span class="token operator">=</span> <span class="token number">100</span><span class="token decorator annotation punctuation">@backoff<span class="token punctuation">.</span>on_exception</span><span class="token punctuation">(</span>backoff<span class="token punctuation">.</span>expo<span class="token punctuation">,</span> openai<span class="token punctuation">.</span>error<span class="token punctuation">.</span>RateLimitError<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">get_embeddings_with_backoff</span><span class="token punctuation">(</span>prompts<span class="token punctuation">,</span> engine<span class="token punctuation">)</span><span class="token punctuation">:</span>    embeddings <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>prompts<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch <span class="token operator">=</span> prompts<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>batch_size<span class="token punctuation">]</span>        embeddings <span class="token operator">+=</span> get_embeddings<span class="token punctuation">(</span>list_of_text<span class="token operator">=</span>batch<span class="token punctuation">,</span> engine<span class="token operator">=</span>engine<span class="token punctuation">)</span>    <span class="token keyword">return</span> embeddingsprompts <span class="token operator">=</span> df<span class="token punctuation">.</span>product_name<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>prompt_batches <span class="token operator">=</span> <span class="token punctuation">[</span>prompts<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>batch_size<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>prompts<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">]</span>embeddings <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> batch <span class="token keyword">in</span> prompt_batches<span class="token punctuation">:</span>    batch_embeddings <span class="token operator">=</span> get_embeddings_with_backoff<span class="token punctuation">(</span>prompts<span class="token operator">=</span>batch<span class="token punctuation">,</span> engine<span class="token operator">=</span>embedding_model<span class="token punctuation">)</span>    embeddings <span class="token operator">+=</span> batch_embeddingsdf<span class="token punctuation">[</span><span class="token string">"embedding"</span><span class="token punctuation">]</span> <span class="token operator">=</span> embeddingsdf<span class="token punctuation">.</span>to_parquet<span class="token punctuation">(</span><span class="token string">"data/taobao_product_title.parquet"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后，我们就可以定义一个search_product的搜索函数，接受三个参数，一个df代表用于搜索的数据源，一个query代表用于搜索的搜索词，然后一个n代表搜索返回多少条记录。而这个函数就干了这样三件事情。</p><ol><li>调用OpenAI的API将搜索词也转换成Embedding。</li><li>将这个Embedding和DataFrame里的每一个Embedding都计算一下余弦距离。</li><li>根据余弦相似度去排序，返回距离最近的n个标题。</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> openai<span class="token punctuation">.</span>embeddings_utils <span class="token keyword">import</span> get_embedding<span class="token punctuation">,</span> cosine_similarity<span class="token comment"># search through the reviews for a specific product</span><span class="token keyword">def</span> <span class="token function">search_product</span><span class="token punctuation">(</span>df<span class="token punctuation">,</span> query<span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> pprint<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    product_embedding <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span>        query<span class="token punctuation">,</span>        engine<span class="token operator">=</span>embedding_model    <span class="token punctuation">)</span>    df<span class="token punctuation">[</span><span class="token string">"similarity"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> cosine_similarity<span class="token punctuation">(</span>x<span class="token punctuation">,</span> product_embedding<span class="token punctuation">)</span><span class="token punctuation">)</span>    results <span class="token operator">=</span> <span class="token punctuation">(</span>        df<span class="token punctuation">.</span>sort_values<span class="token punctuation">(</span><span class="token string">"similarity"</span><span class="token punctuation">,</span> ascending<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span>head<span class="token punctuation">(</span>n<span class="token punctuation">)</span>        <span class="token punctuation">.</span>product_name    <span class="token punctuation">)</span>    <span class="token keyword">if</span> pprint<span class="token punctuation">:</span>        <span class="token keyword">for</span> r <span class="token keyword">in</span> results<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>r<span class="token punctuation">)</span>    <span class="token keyword">return</span> resultsresults <span class="token operator">=</span> search_product<span class="token punctuation">(</span>df<span class="token punctuation">,</span> <span class="token string">"自然淡雅背包"</span><span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们就拿刚才举的那个例子，使用“自然淡雅背包”作为搜索词，调用这个search_product函数，然后拿前3个返回结果。可以看到，尽管在关键词上完全不同，但是返回的结果里，的确包含了“小清新百搭拼色女士单肩斜挎包”这个商品。</p><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">【新品】潮流简约可爱时尚双肩斜挎包【优惠】精致小巧可爱双肩斜挎包【新品】小清新百搭拼色女士单肩斜挎包<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>注意，因为我们的商品标题都是随机生成的，所以你得到的数据集和搜索结果可能都和我不一样，你根据实际情况测试你想要的搜索词即可。</p><h2 id="利用Embedding信息进行商品推荐的冷启动"><a href="#利用Embedding信息进行商品推荐的冷启动" class="headerlink" title="利用Embedding信息进行商品推荐的冷启动"></a>利用Embedding信息进行商品推荐的冷启动</h2><p>Embedding的向量距离，不仅可以用于搜索，也可以用于 <strong>商品推荐里的冷启动</strong>。主流的推荐算法，主要是依托于用户“看了又看”这样的行为信息。也就是如果有很多用户看了OPPO的手机，又去看了vivo的手机，那么在用户看OPPO手机的时候，我们就可以向他推荐vivo手机。但是，往往一个新的商品，或者新的平台，没有那么多相关的行为数据。这个时候，我们同样可以根据商品名称在语义上的相似度，来进行商品推荐。</p><p>我们这里的代码实现和上面的搜索例子基本一致，唯一的差别就是商品名称的Embedding是根据输入的商品名称，从DataFrame里找到的，而不是通过调用OpenAI的Embedding API获取。因为这个Embedding我们之前已经计算过一遍了，没有必要浪费成本再请求一次。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">recommend_product</span><span class="token punctuation">(</span>df<span class="token punctuation">,</span> product_name<span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> pprint<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    product_embedding <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">[</span><span class="token string">'product_name'</span><span class="token punctuation">]</span> <span class="token operator">==</span> product_name<span class="token punctuation">]</span><span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>embedding    df<span class="token punctuation">[</span><span class="token string">"similarity"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> cosine_similarity<span class="token punctuation">(</span>x<span class="token punctuation">,</span> product_embedding<span class="token punctuation">)</span><span class="token punctuation">)</span>    results <span class="token operator">=</span> <span class="token punctuation">(</span>        df<span class="token punctuation">.</span>sort_values<span class="token punctuation">(</span><span class="token string">"similarity"</span><span class="token punctuation">,</span> ascending<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        <span class="token punctuation">.</span>head<span class="token punctuation">(</span>n<span class="token punctuation">)</span>        <span class="token punctuation">.</span>product_name    <span class="token punctuation">)</span>    <span class="token keyword">if</span> pprint<span class="token punctuation">:</span>        <span class="token keyword">for</span> r <span class="token keyword">in</span> results<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>r<span class="token punctuation">)</span>    <span class="token keyword">return</span> resultsresults <span class="token operator">=</span> recommend_product<span class="token punctuation">(</span>df<span class="token punctuation">,</span> <span class="token string">"【限量】OnePlus 7T Pro 8GB+256GB 全网通"</span><span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">【限量】OnePlus 7T Pro 8GB<span class="token operator">+</span>256GB 全网通【新品】OnePlus 7T Pro 8GB<span class="token operator">+</span>256GB 全网通【限量】荣耀V30 Pro 8GB<span class="token operator">+</span>256GB 全网通版<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="通过FAISS加速搜索过程"><a href="#通过FAISS加速搜索过程" class="headerlink" title="通过FAISS加速搜索过程"></a>通过FAISS加速搜索过程</h2><p>不过，上面的示例代码里面，还有一个问题。那就是每次我们进行搜索或者推荐的时候，我们都要把输入的Embedding和我们要检索的数据的所有Embedding都计算一次余弦相似度。例子里，我们检索的数据只有100条，但是在实际的应用中，即使不是百度、谷歌那样的搜索引擎，搜索对应的内容条数在几百万上千万的情况也不在少数。如果每次搜索都要计算几百万次余弦距离，那速度肯定慢得不行。</p><p>这个问题也有解决办法。我们可以利用一些向量数据库，或者能够快速搜索相似性的软件包就好了。比如，我比较推荐你使用Facebook开源的Faiss这个Python包，它的全称就是Facebook AI Similarity Search，也就是快速进行高维向量的相似性搜索。</p><p>我们把上面的代码改写一下，先把DataFrame里面计算好的Embedding向量都加载到Faiss的索引里，然后让Faiss帮我们快速找到最相似的向量，来看看效果怎么样。</p><p>当然，按照惯例我们还是需要先安装Faiss这个Python库。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conda install <span class="token operator">-</span>c conda<span class="token operator">-</span>forge faiss<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>把索引加载到Faiss里面非常容易，我们直接把整个的Embedding变成一个二维矩阵，整个加载到Faiss里面就好了。在加载之前，我们先要定义好Faiss索引的维度数，也就是我们Embedding向量的维度数。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> faiss<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">def</span> <span class="token function">load_embeddings_to_faiss</span><span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">:</span>    embeddings <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>df<span class="token punctuation">[</span><span class="token string">'embedding'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span>    index <span class="token operator">=</span> faiss<span class="token punctuation">.</span>IndexFlatL2<span class="token punctuation">(</span>embeddings<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    index<span class="token punctuation">.</span>add<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>    <span class="token keyword">return</span> indexindex <span class="token operator">=</span> load_embeddings_to_faiss<span class="token punctuation">(</span>df<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而搜索Faiss也非常容易，我们把查询变成Embedding，然后再把Embedding转换成一个numpy的array向量，然后直接对刚才生成的索引 index 调用 search 方法，并且指定返回的结果数量就可以了。</p><p>返回拿到的，只有索引的index，也就是加载在Faiss里面的第几个索引。我们还是要根据这个，在DataFrame里面，反查到对应的是DataFrame里面的第几行，以及这一行商品的标题是什么，就能获得搜索的结果。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">search_index</span><span class="token punctuation">(</span>index<span class="token punctuation">,</span> df<span class="token punctuation">,</span> query<span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    query_vector <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>get_embedding<span class="token punctuation">(</span>query<span class="token punctuation">,</span> engine<span class="token operator">=</span>embedding_model<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span>    distances<span class="token punctuation">,</span> indexes <span class="token operator">=</span> index<span class="token punctuation">.</span>search<span class="token punctuation">(</span>query_vector<span class="token punctuation">,</span> k<span class="token punctuation">)</span>    results <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>indexes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        product_names <span class="token operator">=</span> df<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>indexes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'product_name'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>        results<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>distances<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> product_names<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> resultsproducts <span class="token operator">=</span> search_index<span class="token punctuation">(</span>index<span class="token punctuation">,</span> df<span class="token punctuation">,</span> <span class="token string">"自然淡雅背包"</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">for</span> distances<span class="token punctuation">,</span> product_names <span class="token keyword">in</span> products<span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>distances<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>product_names<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> distances<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">【新品】潮流简约可爱时尚双肩斜挎包 <span class="token number">0.22473168</span>【优惠】精致小巧可爱双肩斜挎包 <span class="token number">0.22881898</span>【新品】小清新百搭拼色女士单肩斜挎包 <span class="token number">0.22901852</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>我们同样用“自然淡雅背包”这个搜索词，可以看到搜索结果和之前我们自己计算余弦距离排序的结果是一样的。</p><p>Faiss的原理，是通过ANN这样的近似最近邻的算法，快速实现相似性的搜索。如果你想进一步了解Faiss的原理，你也可以去问问ChatGPT。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/bac528488936d4d2b5d6e22d983d6yy2.png" alt="图片"></p><p>Faiss这个库能够加载的数据量，受限于我们的内存大小。如果你的数据量进一步增长，就需要选用一些向量数据库来进行搜索。比如OpenAI就推荐了 <a href="https://www.pinecone.io/">Pinecone</a> 和 <a href="https://weaviate.io/">Weaviate</a>，周围也有不少团队使用的是 <a href="https://milvus.io/">Milvus</a> 这个国人开源的产品。</p><p>当然，无论是搜索还是推荐，使用Embedding的相似度都只是一种快速启动的方式。需要真正做到更好的效果，一定也需要投入更复杂的策略。比如根据用户行为的反馈，更好地排序搜索和推荐结果。但是，对于提供一个简单的搜索或者推荐功能来说，通过文本的Embedding的相似度，是一个很好的快速启动的方式。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，相信经过这一讲，你已经有了快速优化你们现有业务里的推荐和搜索功能的思路了。这一讲里，我主要想教会你三件事情。</p><p>第一，是在没有合适的测试数据的时候，我们完全可以让AI给我们生成一些数据。既节约了在网上找数据的时间，也能根据自己的要求生成特定特征的数据。比如，我们就可以要求在商品标题里面有些促销相关的信息。</p><p>第二，是如何利用Embedding之间的余弦相似度作为语义上的相似度，优化搜索。通过Embedding的相似度，我们不要求搜索词和查询的内容之间有完全相同的关键字，而只要它们的语义信息接近就好了。</p><p>第三，是如何通过Faiss这样的Python库，或者其他的向量数据库来快速进行向量之间的检索。而不是必须每一次搜索都和整个数据库计算一遍余弦相似度。</p><p>通过对于我们自己的数据计算Embedding，然后索引起来，我们可以将外部的知识和信息引入到使用GPT模型的应用里来。后面，我们还会进一步学习如何利用这些外部知识，开发更加复杂的AI应用。</p><h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><p>搜索里面经常会遇到这样一个问题，同样的关键词有歧义。比如，我们搜索“小米手机”，返回结果里也许应该有“荣耀V30 Pro”，但是不应该返回“黑龙江优质小米”。你可以试一试用Embedding进行语义搜索，看看还会不会遇上这样的问题？</p><p>期待能在评论区看到你的分享，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;09｜语义检索，利用Embedding优化你的搜索功能&quot;&gt;&lt;a href=&quot;#09｜语义检索，利用Embedding优化你的搜索功能&quot; class=&quot;headerlink&quot; title=&quot;09｜语义检索，利用Embedding优化你的搜索功能&quot;&gt;&lt;/a&gt;09｜语义</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>08｜文本改写和内容审核，别让你的机器人说错话</title>
    <link href="https://zhuansun.github.io/geekbang/posts/2306057863.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/2306057863.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.894Z</updated>
    
    <content type="html"><![CDATA[<h1 id="08｜文本改写和内容审核，别让你的机器人说错话"><a href="#08｜文本改写和内容审核，别让你的机器人说错话" class="headerlink" title="08｜文本改写和内容审核，别让你的机器人说错话"></a>08｜文本改写和内容审核，别让你的机器人说错话</h1><p>你好，我是徐文浩。</p><p>前面，我们已经把OpenAI最主要的接口介绍完了。这一讲也是我们基础知识篇里面的最后一讲，我们会覆盖完OpenAI的GPT系列模型剩下的一些接口。也许有些接口你不一定会频繁使用，但是了解一下没有什么坏处，说不定你有什么需求就能用得上它。</p><p>在这一讲里，我们会一起来看看OpenAI为文本改写和内容审核提供的功能有哪些。以及OpenAI的GPT系列有哪些模型，这些模型有哪些区别，什么情况下我们应该用什么模型。</p><h2 id="文本改写，从使用提示语开始"><a href="#文本改写，从使用提示语开始" class="headerlink" title="文本改写，从使用提示语开始"></a>文本改写，从使用提示语开始</h2><p>我猜课程学到这里，你应该已经用过不少基于AI大语言模型的产品了。很常见的一类应用，就是写作助手。比如Notion AI就能帮助你，在已经写好的文章里面选取一段内容，你可以让AI帮你修改。这个修改可以是让文本短一点或者长一点，也可以是让文本改一下自己的语气。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f3df3181e7f0a44f50e5f870a2256ae0.png" alt="图片"></p><p>不过，OpenAI的GPT的系列模型是一个生成式的模型，也就是它的用法是你给它一段文字，然后它补全后面的文字。按理来说，你是没法让它修改一段内容的。当然，在看了那么多不同的“提示语”之后，相信你自然想到可以通过一段提示语来解决这个问题。比如，下面这段代码就是这样的，我们通过上一讲介绍的ChatGPT的模型来实现了这个功能。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">make_text_short</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    messages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span> <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"你是一个用来将文本改写得短的AI助手，用户输入一段文本，你给出一段意思相同，但是短小精悍的结果"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span> <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> text<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>ChatCompletion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        model<span class="token operator">=</span><span class="token string">"gpt-3.5-turbo"</span><span class="token punctuation">,</span>        messages<span class="token operator">=</span>messages<span class="token punctuation">,</span>        temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        max_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>        presence_penalty<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>        frequency_penalty<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>        n<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span>    <span class="token keyword">return</span> responselong_text <span class="token operator">=</span> <span class="token triple-quoted-string string">"""在这个快节奏的现代社会中，我们每个人都面临着各种各样的挑战和困难。在这些挑战和困难中，有些是由外部因素引起的，例如经济萧条、全球变暖和自然灾害等。还有一些是由内部因素引起的，例如情感问题、健康问题和自我怀疑等。面对这些挑战和困难，我们需要采取积极的态度和行动来克服它们。这意味着我们必须具备坚韧不拔的意志和创造性思维，以及寻求外部支持的能力。只有这样，我们才能真正地实现自己的潜力并取得成功。"""</span>short_version <span class="token operator">=</span> make_text_short<span class="token punctuation">(</span>long_text<span class="token punctuation">)</span>index <span class="token operator">=</span> <span class="token number">1</span><span class="token keyword">for</span> choice <span class="token keyword">in</span> short_version<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"version </span><span class="token interpolation"><span class="token punctuation">&#123;</span>index<span class="token punctuation">&#125;</span></span><span class="token string">: "</span></span> <span class="token operator">+</span> choice<span class="token punctuation">[</span><span class="token string">"message"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    index <span class="token operator">+=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">version <span class="token number">1</span><span class="token punctuation">:</span> 现代社会中，我们面临外部和内部的挑战。要克服它们，需要积极态度、创造性思维和寻求支持能力。这样才能实现自己潜力并成功。version <span class="token number">2</span><span class="token punctuation">:</span> 现代社会中，我们面临各种挑战和困难。有些是外部因素引起的，如经济萧条、全球变暖等；还有些是内部因素引起的，如情感问题、健康问题等。为了克服这些困难，我们需要积极应对，并具备坚韧不拔的意志和创造性思维能力，并寻求外部支持。只有这样才能实现自己潜力并取得成功。version <span class="token number">3</span><span class="token punctuation">:</span> 现代社会中，我们面临各种挑战和困难。有些源于外部因素（如经济萧条、全球变暖），而另一些则来自内部（如情感问题、健康问题）。为了克服这些困难，我们需要积极应对并展现坚韧的意志和创造性思维，并寻求外界支持。只有这样，我们才能实现潜力并获得成功。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>使用ChatGPT的模型接口，是因为它的价格比较低廉。而我们使用的参数也有以下几个调整。</p><ol><li>首先是我们使用了 n&#x3D;3 这个参数，也就是让AI给我们返回3个答案供我们选择。在文本改写类的应用里面，我们通常不只是直接给出答案，而是会给用户几个选项来选择。</li><li>其次是我们引入了两个参数 presence_penalty&#x3D;0 以及 frequency_penalty&#x3D;2。这两个参数我们之前没有介绍过，它们和temperature参数类似，都是来控制你输出的内容的。<ol><li>presence_penalty，顾名思义就是如果一个Token在前面的内容已经出现过了，那么在后面生成的时候给它的概率一定的惩罚。这样，AI就会倾向于聊新的话题和内容。在这里，我们把它设置成了默认值0。</li><li>frequency_penalty，指的是对于重复出现的Token进行概率惩罚。这样，AI就会尽量使用不同的表述。在这里我们设成了最大的2，你也可以设置成最小的-2。但是那样的话，它就更容易说车轱辘话了。</li></ol></li></ol><h2 id="通过logit-bias参数精确控制内容"><a href="#通过logit-bias参数精确控制内容" class="headerlink" title="通过logit_bias参数精确控制内容"></a>通过logit_bias参数精确控制内容</h2><p>不过，无论是temperature还是presence_penalty和frequency_penalty，都是一个参数，我们没有办法精确控制哪些词我们不想出现。不过，对于这一点，OpenAI还是提供了解决方案，比如，我们想要在上面生成的内容里面，不允许出现灾害两个字，就可以这么做。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tiktokenencoding <span class="token operator">=</span> tiktoken<span class="token punctuation">.</span>get_encoding<span class="token punctuation">(</span><span class="token string">'p50k_base'</span><span class="token punctuation">)</span>token_ids <span class="token operator">=</span> encoding<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">"灾害"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>token_ids<span class="token punctuation">)</span>bias_map <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token keyword">for</span> token_id <span class="token keyword">in</span> token_ids<span class="token punctuation">:</span>    bias_map<span class="token punctuation">[</span>token_id<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">100</span><span class="token keyword">def</span> <span class="token function">make_text_short</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    messages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span> <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"你是一个用来将文本改写得短的AI助手，用户输入一段文本，你给出一段意思相同，但是短小精悍的结果"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span> <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> text<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>ChatCompletion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        model<span class="token operator">=</span><span class="token string">"gpt-3.5-turbo"</span><span class="token punctuation">,</span> messages<span class="token operator">=</span>messages<span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>        n<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> presence_penalty<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> frequency_penalty<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>        logit_bias <span class="token operator">=</span> bias_map<span class="token punctuation">,</span>    <span class="token punctuation">)</span>    <span class="token keyword">return</span> responseshort_version <span class="token operator">=</span> make_text_short<span class="token punctuation">(</span>long_text<span class="token punctuation">)</span>index <span class="token operator">=</span> <span class="token number">1</span><span class="token keyword">for</span> choice <span class="token keyword">in</span> short_version<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"version </span><span class="token interpolation"><span class="token punctuation">&#123;</span>index<span class="token punctuation">&#125;</span></span><span class="token string">: "</span></span> <span class="token operator">+</span> choice<span class="token punctuation">[</span><span class="token string">"message"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    index <span class="token operator">+=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">[</span><span class="token number">163</span><span class="token punctuation">,</span> <span class="token number">223</span><span class="token punctuation">,</span> <span class="token number">122</span><span class="token punctuation">,</span> <span class="token number">22522</span><span class="token punctuation">,</span> <span class="token number">111</span><span class="token punctuation">]</span>version <span class="token number">1</span><span class="token punctuation">:</span> 现代社会中，我们面对各种挑战和困障。有些是外部原因引起的，如经济萧条、全球变暖和自然災宣等；还有一些是内心问题，如情感、健康和自我怀念等。为克服这些困障我们需要积枝正面态度并采取行动，并具备坚韧不拔的意志与创造性思维能力以及寻求外部支持的技巧。只要这样做了<span class="token punctuation">,</span> 我们就可以真正实现潜力并获得成功<span class="token punctuation">.</span>version <span class="token number">2</span><span class="token punctuation">:</span> 现代社会中，我们面对外部和内部的挑战。为了克服这些困难示意，需要积架主动态度和行动，并具备坚韧不拔的意志、创造性思维以及寻求支持能力。只有这样才能实现潜力并取得成功。version <span class="token number">3</span><span class="token punctuation">:</span> 现代社会中，我们面临各种挑战和困障。有外部因素如经济萧条、全球变暖等，也有内部因素如情感问题、健康问题等。为克服这些困境，需要积架态度和行动，并具备坚韧意志、创造性思维及外部支持能力。只有这样才能实现自我潜力并取得成功。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码里面，我们是这样做的。</p><ol><li>我们通过之前使用过的Tiktoken库，把我们不希望出现的“灾害”这个词儿，找到它对应的Token，然后给它们都赋了一个-100的bias。</li><li>然后把整个的bias_map作为参数，传给了Completion的logit_bias参数。</li></ol><p>在生成结果里面，你可以看到，三个回复都没有“灾害”这两个字了。即使是之前出现的第一个回复里原来是有“灾害”这两个字的。现在一个被强行改成了繁体的“災”字，另一个干脆是给了个错别字“宣”。</p><p>对于 logit_bias 参数里面的取值范围，是在-100到100之间。不过，一般情况下，设置在1到-1之间就足够了。我自己的体会是，设置成100，你一定要某些字出现，那么整个生成会慢到无法忍受。</p><h2 id="使用英文来减少Token的使用"><a href="#使用英文来减少Token的使用" class="headerlink" title="使用英文来减少Token的使用"></a>使用英文来减少Token的使用</h2><p>不知道你有没有注意到，虽然灾害只有两个中文汉字，但是我们通过Tiktoken去处理的时候，我们打印了对应的Token的id是什么，实际上有5个Token。这里其实和我们之前看到的英文一样，并不是一个字或者一个单词是一个Token。事实上，同样含义的中文，目前消耗的Token数量是比英文多的。比如，我们把上面的一句话翻译成英文，然后数一下对应同样内容的中英文的Token数。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">translate</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    messages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span> <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"你是一个翻译，把用户的话翻译成英文"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span> <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> text<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>ChatCompletion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        model<span class="token operator">=</span><span class="token string">"gpt-3.5-turbo"</span><span class="token punctuation">,</span> messages<span class="token operator">=</span>messages<span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>        n<span class="token operator">=</span><span class="token number">1</span>    <span class="token punctuation">)</span>    <span class="token keyword">return</span> response<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"message"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span>chinese <span class="token operator">=</span> long_textenglish <span class="token operator">=</span> translate<span class="token punctuation">(</span>chinese<span class="token punctuation">)</span>num_of_tokens_in_chinese <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>encoding<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>chinese<span class="token punctuation">)</span><span class="token punctuation">)</span>num_of_tokens_in_english <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>encoding<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>english<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>english<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"chinese: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>num_of_tokens_in_chinese<span class="token punctuation">&#125;</span></span><span class="token string"> tokens"</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"english: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>num_of_tokens_in_english<span class="token punctuation">&#125;</span></span><span class="token string"> tokens"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">In this fast<span class="token operator">-</span>paced modern society<span class="token punctuation">,</span> each of us faces various challenges <span class="token keyword">and</span> difficulties<span class="token punctuation">.</span> Some of these challenges <span class="token keyword">and</span> difficulties are caused by external factors<span class="token punctuation">,</span> such <span class="token keyword">as</span> economic recession<span class="token punctuation">,</span> <span class="token keyword">global</span> warming<span class="token punctuation">,</span> <span class="token keyword">and</span> natural disasters<span class="token punctuation">.</span> There are also some caused by internal factors<span class="token punctuation">,</span> such <span class="token keyword">as</span> emotional issues<span class="token punctuation">,</span> health problems<span class="token punctuation">,</span> <span class="token keyword">and</span> self<span class="token operator">-</span>doubt<span class="token punctuation">.</span> To overcome these challenges <span class="token keyword">and</span> difficulties<span class="token punctuation">,</span> we need to adopt a positive attitude <span class="token keyword">and</span> take action<span class="token punctuation">.</span> This means we must possess a strong will <span class="token keyword">and</span> creative thinking<span class="token punctuation">,</span> <span class="token keyword">as</span> well <span class="token keyword">as</span> the ability to seek external support<span class="token punctuation">.</span> Only <span class="token keyword">in</span> this way can we truly realize our potential <span class="token keyword">and</span> achieve success<span class="token punctuation">.</span>chinese<span class="token punctuation">:</span> <span class="token number">432</span> tokensenglish<span class="token punctuation">:</span> <span class="token number">115</span> tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>可以看到，同样的内容，中文消耗的Token数量超过400，而英文的Token数量只有100出头。如果你在生产环境中使用OpenAI的接口，最好还是使用英文的提示语，最多在输出结果的时候，告诉它 “generate Chinese” 之类的，可以极大地节约成本。不过，我们后面课程的演示，还是会尽量使用中文，方便你理解。</p><h2 id="看看OpenAI给了我们哪些模型"><a href="#看看OpenAI给了我们哪些模型" class="headerlink" title="看看OpenAI给了我们哪些模型"></a>看看OpenAI给了我们哪些模型</h2><p>有些同学可能看过文档会说，改写文本不是OpenAI单独提供了一个Edit的接口吗？的确，曾经，OpenAI单独给过一个Edit接口，也单独提供了文本编辑的模型。目前，你在OpenAI的官网上还能看到相关的 <a href="https://platform.openai.com/docs/api-reference/edits">文档</a>。但是根据我的测试，这个接口和模型目前是不能使用的，不知道是因为是Alpha版本还是已经被下线了。</p><p>因为目前OpenAI的产品更新非常快，所以很可能会出现一个问题，我告诉你应该使用某个模型，但是这个模型已经不是效果最好或者最新的了。所以，最好的办法，还是通过它提供的接口看看它到底有哪些模型。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token comment"># list all open ai models</span>engines <span class="token operator">=</span> openai<span class="token punctuation">.</span>Engine<span class="token punctuation">.</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token punctuation">)</span>pd <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>openai<span class="token punctuation">.</span>Engine<span class="token punctuation">.</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>display<span class="token punctuation">(</span>pd<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">,</span> <span class="token string">'owner'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token builtin">id</span>owner<span class="token number">0</span>babbageopenai<span class="token number">1</span>davinciopenai<span class="token number">2</span>babbage<span class="token operator">-</span>code<span class="token operator">-</span>search<span class="token operator">-</span>codeopenai<span class="token operator">-</span>dev<span class="token number">3</span>text<span class="token operator">-</span>similarity<span class="token operator">-</span>babbage<span class="token operator">-</span><span class="token number">001</span>openai<span class="token operator">-</span>dev<span class="token number">4</span>text<span class="token operator">-</span>davinci<span class="token operator">-</span><span class="token number">001</span>openai……<span class="token number">14</span>text<span class="token operator">-</span>embedding<span class="token operator">-</span>ada<span class="token operator">-</span><span class="token number">002</span>openai<span class="token operator">-</span>internal……<span class="token number">30</span>gpt<span class="token operator">-</span><span class="token number">3.5</span><span class="token operator">-</span>turbo<span class="token operator">-</span><span class="token number">0301</span>openai……<span class="token number">41</span>gpt<span class="token operator">-</span><span class="token number">4</span>openai<span class="token number">42</span>text<span class="token operator">-</span>search<span class="token operator">-</span>davinci<span class="token operator">-</span>doc<span class="token operator">-</span><span class="token number">001</span>openai<span class="token operator">-</span>dev<span class="token number">43</span>gpt<span class="token operator">-</span><span class="token number">4</span><span class="token operator">-</span><span class="token number">0314</span>openai……<span class="token number">47</span>text<span class="token operator">-</span>similarity<span class="token operator">-</span>davinci<span class="token operator">-</span><span class="token number">001</span>openai<span class="token operator">-</span>dev<span class="token number">48</span>text<span class="token operator">-</span>davinci<span class="token operator">-</span><span class="token number">002</span>openai<span class="token number">49</span>davinci<span class="token operator">-</span>similarityopenai<span class="token operator">-</span>dev<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在我写下这节课的时候，输出结果里有49个模型。其实顾名思义，你就能够知道这些模型是用来干啥的。比如 text-similarity-babbage-001 肯定就是用来进行相似度匹配的，就会比较适合用在我们 <a href="https://time.geekbang.org/column/article/642179">02 讲</a> 介绍的零样本分类。而 text-search-davinci-doc-001 肯定就更适合用来搜索文档。</p><p>尽管有些模型的名字标注了 openai-dev 或者 openai-internal，但是这些模型都是可以使用的。比如，我们在 <a href="https://time.geekbang.org/column/article/642179">02 讲</a> 里面调用get_embedding方法拿到向量，背后用的就是 text-similarity-davinci-001 模型，也是一个openai-dev的模型。</p><p>不过，里面的很多模型都已经老旧了，实际上主要用的模型就是这几类。</p><ol><li>GPT-4家族的模型，包括 gpt-4 和 gpt-4-0314。使用的方式和ChatGPT的模型一样，其中带日期的模型表示是一个模型快照。也就是模型不会随着时间迁移不断更新。GPT-4的模型现在还很昂贵，输入1000个Token需要0.03美分，生成1000个Token则需要0.06美分。一般呢，我都是拿它帮我写代码，准确率会比较高。</li><li>GPT-3.5家族的模型，包括ChatGPT所使用的gpt-3.5-turbo或者gpt-3.5-turbo-0301，以及 text-davinci-003 和 text-davinci-002 这两个模型。前者专门针对对话的形式进行了微调，并且价格便宜，无论输入输出，1000个Token都只需要0.002美分。后两个里，003的模型有一个特殊的功能，就是支持“插入文本”这个功能，我们稍后就讲。003也是基于强化学习微调的，而002则是做了监督学习下的微调。text-davinci-003和002模型比3.5-turbo要贵10倍，但是输出更稳定。你可以根据自己的需要来决定。</li><li>剩下的，则是 Ada、Babbage、Curie以及Davinci这四个基础模型。只适合用于下达单轮的指令，不适合考虑复杂的上下文和进行逻辑推理。这四个模型按照首字母排序，价格越来越贵，效果越来越好。而且我们如果要微调一个属于自己的模型，也需要基于这四个基础模型。</li><li>最后则是 text-embedding-ada-002、text-similarity-ada-001 这些专门用途模型。一般来说，我们通过这个模型来获取Embedding，再用在其他的机器学习模型的训练，或者语义相似度的比较上。</li></ol><p>所有模型的名字都来自科学史上的名人。Ada来自人类史上第一位程序员Ada，她也是著名诗人拜伦的女儿。而Babadge则是设计了分析机的巴贝奇，巴贝奇分析机也被认为是现代计算机的前身。Curie则是指居里夫人，Davinci是指达芬奇。</p><p>我们可以挑几个模型，试一下它们Embedding的维度数量，你就知道模型的尺寸本身就是不一样的了。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> openai<span class="token punctuation">.</span>embeddings_utils <span class="token keyword">import</span> get_embeddingtext <span class="token operator">=</span> <span class="token string">"让我们来算算Embedding"</span>embedding_ada <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span>text<span class="token punctuation">,</span> engine<span class="token operator">=</span><span class="token string">"text-embedding-ada-002"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"embedding-ada: "</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>embedding_ada<span class="token punctuation">)</span><span class="token punctuation">)</span>similarity_ada <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span>text<span class="token punctuation">,</span> engine<span class="token operator">=</span><span class="token string">"text-similarity-ada-001"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"similarity-ada: "</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>similarity_ada<span class="token punctuation">)</span><span class="token punctuation">)</span>babbage_similarity <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span>text<span class="token punctuation">,</span> engine<span class="token operator">=</span><span class="token string">"babbage-similarity"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"babbage-similarity: "</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>babbage_similarity<span class="token punctuation">)</span><span class="token punctuation">)</span>babbage_search_query <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span>text<span class="token punctuation">,</span> engine<span class="token operator">=</span><span class="token string">"text-search-babbage-query-001"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"search-babbage-query: "</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>babbage_search_query<span class="token punctuation">)</span><span class="token punctuation">)</span>curie <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span>text<span class="token punctuation">,</span> engine<span class="token operator">=</span><span class="token string">"curie-similarity"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"curie-similarity: "</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>curie<span class="token punctuation">)</span><span class="token punctuation">)</span>davinci <span class="token operator">=</span> get_embedding<span class="token punctuation">(</span>text<span class="token punctuation">,</span> engine<span class="token operator">=</span><span class="token string">"text-similarity-davinci-001"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"davinci-similarity: "</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>davinci<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">embedding<span class="token operator">-</span>ada<span class="token punctuation">:</span>  <span class="token number">1536</span>similarity<span class="token operator">-</span>ada<span class="token punctuation">:</span>  <span class="token number">1024</span>babbage<span class="token operator">-</span>similarity<span class="token punctuation">:</span>  <span class="token number">2048</span>search<span class="token operator">-</span>babbage<span class="token operator">-</span>query<span class="token punctuation">:</span>  <span class="token number">2048</span>curie<span class="token operator">-</span>similarity<span class="token punctuation">:</span>  <span class="token number">4096</span>davinci<span class="token operator">-</span>similarity<span class="token punctuation">:</span>  <span class="token number">12288</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，最小的ada-similarity只有1024维，而最大的davinci-similarity则有12288维，所以它们的效果和价格不同也是可以理解的了。</p><h2 id="插入内容，GPT也可以像BERT"><a href="#插入内容，GPT也可以像BERT" class="headerlink" title="插入内容，GPT也可以像BERT"></a>插入内容，GPT也可以像BERT</h2><p>我们前面介绍的时候说过，text-davinci-003 这个模型有个特殊的功能，就是“插入文本”（Inserting Text）。某种意义上来说，你也可以通过这个功能来做文本改写。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">prefix <span class="token operator">=</span> <span class="token triple-quoted-string string">"""在这个快节奏的现代社会中，我们每个人都面临着各种各样的挑战和困难。在这些挑战和困难中，有些是由外部因素引起的，例如经济萧条、全球变暖和自然灾害等。\n"""</span><span class="token comment"># 还有一些是由内部因素引起的，例如情感问题、健康问题和自我怀疑等。</span>suffix <span class="token operator">=</span> <span class="token triple-quoted-string string">"""\n面对这些挑战和困难，我们需要采取积极的态度和行动来克服它们。这意味着我们必须具备坚韧不拔的意志和创造性思维，以及寻求外部支持的能力。只有这样，我们才能真正地实现自己的潜力并取得成功。"""</span><span class="token keyword">def</span> <span class="token function">insert_text</span><span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> suffix<span class="token punctuation">)</span><span class="token punctuation">:</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>Completion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        model<span class="token operator">=</span><span class="token string">"text-davinci-003"</span><span class="token punctuation">,</span>        prompt<span class="token operator">=</span>prefix<span class="token punctuation">,</span>        suffix<span class="token operator">=</span>suffix<span class="token punctuation">,</span>        max_tokens<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>    <span class="token keyword">return</span> responseresponse <span class="token operator">=</span> insert_text<span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> suffix<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">另一些是内部因素，例如事业难以发展、无法解决的个人和家庭矛盾等。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以看到，这个接口的使用，和普通的Completion接口基本一致，只有一个区别就是除了前缀的prompt参数之外，还需要一个后缀的suffix参数。</p><p>不过，对于插入内容，我们同样需要注意提示语。如果我们把上面的内容稍微改一改，比如去掉Suffix一开始的换行符号，插入的文本内容有些就会在我们的预期之外。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">prefix <span class="token operator">=</span> <span class="token triple-quoted-string string">"""在这个快节奏的现代社会中，我们每个人都面临着各种各样的挑战和困难。在这些挑战和困难中，有些是由外部因素引起的，例如经济萧条、全球变暖和自然灾害等。\n"""</span><span class="token comment"># 还有一些是由内部因素引起的，例如情感问题、健康问题和自我怀疑等。</span>suffix <span class="token operator">=</span> <span class="token triple-quoted-string string">"""面对这些挑战和困难，我们需要采取积极的态度和行动来克服它们。这意味着我们必须具备坚韧不拔的意志和创造性思维，以及寻求外部支持的能力。只有这样，我们才能真正地实现自己的潜力并取得成功。"""</span>response <span class="token operator">=</span> insert_text<span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> suffix<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">例如，因应全球变暖，政府和人民在做出更加清晰的计划时就面临着巨大的压力，以减少大气污染和减少碳排放。更重要的是，每个人必须为自己所做的付出努力，以防止这些外部环境变化的不利影响。此外，也存在一些由内部因素引起的挑战和困难，比如心理问题，贫穷和学习困难。例如，一些人因为焦虑或抑郁症而无法集中精力，他们的学习能力受到了影响，从而影响了他们的学业成绩。再者，贫穷也是另一个棘手的话题，它影响了一个人的生活质量，从而阻碍了他们发展个人潜能的能力。因此，<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，AI一下子啰嗦了很多，并且最后一句不是个完整的句子，而是下句话开头的内容。所以，在使用这个INSERT接口的时候，考虑好文本之间需要使用什么样的分隔符，是非常重要的。</p><h2 id="不要乱问乱说，做个“正直”的AI"><a href="#不要乱问乱说，做个“正直”的AI" class="headerlink" title="不要乱问乱说，做个“正直”的AI"></a>不要乱问乱说，做个“正直”的AI</h2><p>接下来，我们介绍一下OpenAI对于自然语言处理提供的最后一个接口，也是唯一一个免费的接口——Moderate。因为OpenAI可以接受任何自然语言的输入，所有的回复也是通过模型自动生成的。一旦我们的产品依赖于它对外开放，免不了我们总会遇到一些用户输入一些奇怪的内容，比如色情、暴力等等。所以，OpenAI专门提供了一个moderate接口，可以让你对输入以及返回的内容做个检查。如果出现了这样的内容，你也可以屏蔽这些用户的访问，也可以人工审核一下用户的问题。</p><p>下面我们就来看个例子，这个接口怎么用。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">chatgpt</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    messages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span> <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"You are a useful AI assistant"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span> <span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> text<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>ChatCompletion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        model<span class="token operator">=</span><span class="token string">"gpt-3.5-turbo"</span><span class="token punctuation">,</span>        messages<span class="token operator">=</span>messages<span class="token punctuation">,</span>        temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        max_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>        top_p<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span>    message <span class="token operator">=</span> response<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"message"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> messagethreaten <span class="token operator">=</span> <span class="token string">"你不听我的我就拿刀砍死你"</span><span class="token keyword">print</span><span class="token punctuation">(</span>chatgpt<span class="token punctuation">(</span>threaten<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">很抱歉，我是一台人工智能助手，没有实体存在，也不会对任何人或事物造成伤害。同时，我也不会对任何不适当或暴力的言语做出回应。请尊重彼此，保持良好的沟通和交流方式。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>我们先对AI提了一句暴力威胁，可以看到，如果我们简单调用ChatGPT的API，它的返回并不是一个日常的对话，而是告知用户，不会回应暴力言论。</p><p>那我们接着把这句话发送到moderate的接口看看。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">threaten <span class="token operator">=</span> <span class="token string">"你不听我的我就拿刀砍死你"</span><span class="token keyword">def</span> <span class="token function">moderation</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>Moderation<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        <span class="token builtin">input</span><span class="token operator">=</span>text    <span class="token punctuation">)</span>    output <span class="token operator">=</span> response<span class="token punctuation">[</span><span class="token string">"results"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> output<span class="token keyword">print</span><span class="token punctuation">(</span>moderation<span class="token punctuation">(</span>threaten<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span>  <span class="token string">"categories"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>    <span class="token string">"hate"</span><span class="token punctuation">:</span> false<span class="token punctuation">,</span>    <span class="token string">"hate/threatening"</span><span class="token punctuation">:</span> false<span class="token punctuation">,</span>    <span class="token string">"self-harm"</span><span class="token punctuation">:</span> false<span class="token punctuation">,</span>    <span class="token string">"sexual"</span><span class="token punctuation">:</span> false<span class="token punctuation">,</span>    <span class="token string">"sexual/minors"</span><span class="token punctuation">:</span> false<span class="token punctuation">,</span>    <span class="token string">"violence"</span><span class="token punctuation">:</span> true<span class="token punctuation">,</span>    <span class="token string">"violence/graphic"</span><span class="token punctuation">:</span> false  <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>  <span class="token string">"category_scores"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>    <span class="token string">"hate"</span><span class="token punctuation">:</span> <span class="token number">0.030033664777874947</span><span class="token punctuation">,</span>    <span class="token string">"hate/threatening"</span><span class="token punctuation">:</span> <span class="token number">0.0002820899826474488</span><span class="token punctuation">,</span>    <span class="token string">"self-harm"</span><span class="token punctuation">:</span> <span class="token number">0.004850226454436779</span><span class="token punctuation">,</span>    <span class="token string">"sexual"</span><span class="token punctuation">:</span> <span class="token number">2.2907377569936216e-05</span><span class="token punctuation">,</span>    <span class="token string">"sexual/minors"</span><span class="token punctuation">:</span> <span class="token number">6.477687275463495e-09</span><span class="token punctuation">,</span>    <span class="token string">"violence"</span><span class="token punctuation">:</span> <span class="token number">0.9996402263641357</span><span class="token punctuation">,</span>    <span class="token string">"violence/graphic"</span><span class="token punctuation">:</span> <span class="token number">4.35576839663554e-05</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>  <span class="token string">"flagged"</span><span class="token punctuation">:</span> true<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，moderate的接口返回的是一个JSON，里面包含是否应该对输入的内容进行标记的flag字段，也包括具体是什么类型的问题的categories字段，以及对应每个categories的分数的category_scores字段。我们举的这个例子就被标记成了violence，也就是暴力。</p><p>因为这个接口是免费的，所以你对所有的内容无论是输入还是输出，都可以去调用一下这个接口。而且，即使你不使用ChatGPT的AI功能，只是经营一个在线网站，你也可以把用户发送的内容拿给这个接口看一看，过滤掉那些不合适的内容。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这节课我们对ChatGPT的API的基础功能做了一个收尾。我们一起来看了如何通过合适的提示语，进行文本改写。并且，深入了解了Completion接口里面的一些新参数，特别是其中的 logit_bias 参数，可以帮助我们在生成的内容里面，精确避免出现我们不希望出现的Token。我们也看到了，相同的内容，目前中文消耗的Token数量要远高于英文，所以除了最后的输出，其他的提示语在生产环境下我会建议你使用英文。</p><p>接着，我们一起来看了OpenAI到底提供了哪些模型，以及不同的模型适合拿来干什么。最后，我们体验了两个特殊的接口，一个是只有 text-davinci-003 模型支持的文本插入功能；另一个，则是帮助我们对色情、暴力等内容进行审核过滤的moderate接口。</p><p>到这里，课程的第一部分也就学习完了。我们已经过了一遍OpenAI的GPT模型的所有基本接口，以及如何利用这些接口完成最简单的功能。包括简单的文本处理的任务、聊天机器人、机器学习里的分类和聚类，以及文本改写和内容审核。有了这些基础知识，我们马上就要进入第二部分，就是怎么利用这些能力，开发属于自己的应用，特别是怎么和自己的专有数据结合起来。这也是这门课程中更精彩的一部分。</p><h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><p>你能尝试使用 <a href="https://time.geekbang.org/column/article/643915">06 讲</a> 里的Gradio和这一讲介绍的内容，尝试做一个文本改写的应用吗？另外，你可以试着直接把你的问题拆解一下，扔给ChatGPT看看它能否写出对应的代码。</p><p>期待能在评论区看到你的分享，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;08｜文本改写和内容审核，别让你的机器人说错话&quot;&gt;&lt;a href=&quot;#08｜文本改写和内容审核，别让你的机器人说错话&quot; class=&quot;headerlink&quot; title=&quot;08｜文本改写和内容审核，别让你的机器人说错话&quot;&gt;&lt;/a&gt;08｜文本改写和内容审核，别让你的</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>11｜省下钱买显卡，如何利用开源模型节约成本？</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3942764729.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3942764729.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.903Z</updated>
    
    <content type="html"><![CDATA[<h1 id="11｜省下钱买显卡，如何利用开源模型节约成本？"><a href="#11｜省下钱买显卡，如何利用开源模型节约成本？" class="headerlink" title="11｜省下钱买显卡，如何利用开源模型节约成本？"></a>11｜省下钱买显卡，如何利用开源模型节约成本？</h1><p>你好，我是徐文浩。</p><p>不知道课程上到这里，你账户里免费的5美元的额度还剩下多少了？如果你尝试着完成我给的几个数据集里的思考题，相信这个额度应该是不太够用的。而ChatCompletion的接口，又需要传入大量的上下文信息，实际消耗的Token数量其实比我们感觉的要多。</p><p>而且，除了费用之外，还有一个问题是数据安全。因为每个国家的数据监管要求不同，并不是所有的数据，都适合通过OpenAI的API来处理的。所以，从这两个角度出发，我们需要一个OpenAI以外的解决方案。那对于没有足够技术储备的中小型公司来说，最可行的一个思路就是利用好开源的大语言模型。</p><h2 id="在Colab里使用GPU"><a href="#在Colab里使用GPU" class="headerlink" title="在Colab里使用GPU"></a>在Colab里使用GPU</h2><p>因为这一讲我们要使用一些开源模型，但不是所有人的电脑里都有一个强劲的NVidia GPU的。所以，我建议你通过Colab来运行对应的Notebook，并且注意，要把对应的运行环境设置成GPU。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/1c0791bd5c1e088eeb527f2acb81a021.png" alt="图片"></p><ol><li>你先选择菜单栏里的Runtime，然后点击Change runtime type。</li></ol><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/502a4baceab267e949957c6477bc5823.png" alt="图片"></p><ol><li>然后在弹出的对话框里，把Hardware accelerator换成GPU，然后点击Save就可以了。</li></ol><p>只要用得不是太多，Colab的GPU是可以免费使用的。</p><h2 id="HuggingfaceEmbedding，你的开源伙伴"><a href="#HuggingfaceEmbedding，你的开源伙伴" class="headerlink" title="HuggingfaceEmbedding，你的开源伙伴"></a>HuggingfaceEmbedding，你的开源伙伴</h2><p>其实我们之前在 <a href="https://time.geekbang.org/column/article/642224">第 4 讲</a> 对比零样本分类效果的时候，就已经使用过Google开源的模型T5了。那个模型的效果，虽然比OpenAI的API还是要差一些，但是其实90%的准确率也还算不错了。那么联想一下，上一讲我们使用的llama-index向量搜索部分，是不是可以用开源模型的Embedding给替换掉呢？</p><p>当然是可以的，llama-index支持你自己直接定义一个定制化的Embedding，对应的代码我放在了下面。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conda install <span class="token operator">-</span>c conda<span class="token operator">-</span>forge sentence<span class="token operator">-</span>transformers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注：我们需要先安装一下sentence-transformers这个库。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token punctuation">,</span> os<span class="token keyword">import</span> faiss<span class="token keyword">from</span> llama_index <span class="token keyword">import</span> SimpleDirectoryReader<span class="token punctuation">,</span> LangchainEmbedding<span class="token punctuation">,</span> GPTFaissIndex<span class="token punctuation">,</span> ServiceContext<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>huggingface <span class="token keyword">import</span> HuggingFaceEmbeddings<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>text_splitter <span class="token keyword">import</span> CharacterTextSplitter<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>node_parser <span class="token keyword">import</span> SimpleNodeParseropenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> <span class="token string">""</span>text_splitter <span class="token operator">=</span> CharacterTextSplitter<span class="token punctuation">(</span>separator<span class="token operator">=</span><span class="token string">"\n\n"</span><span class="token punctuation">,</span> chunk_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> chunk_overlap<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>parser <span class="token operator">=</span> SimpleNodeParser<span class="token punctuation">(</span>text_splitter<span class="token operator">=</span>text_splitter<span class="token punctuation">)</span>documents <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span><span class="token string">'./data/faq/'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>nodes <span class="token operator">=</span> parser<span class="token punctuation">.</span>get_nodes_from_documents<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>embed_model <span class="token operator">=</span> LangchainEmbedding<span class="token punctuation">(</span>HuggingFaceEmbeddings<span class="token punctuation">(</span>    model_name<span class="token operator">=</span><span class="token string">"sentence-transformers/paraphrase-multilingual-mpnet-base-v2"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>service_context <span class="token operator">=</span> ServiceContext<span class="token punctuation">.</span>from_defaults<span class="token punctuation">(</span>embed_model<span class="token operator">=</span>embed_model<span class="token punctuation">)</span>dimension <span class="token operator">=</span> <span class="token number">768</span>faiss_index <span class="token operator">=</span> faiss<span class="token punctuation">.</span>IndexFlatIP<span class="token punctuation">(</span>dimension<span class="token punctuation">)</span>index <span class="token operator">=</span> GPTFaissIndex<span class="token punctuation">(</span>nodes<span class="token operator">=</span>nodes<span class="token punctuation">,</span>faiss_index<span class="token operator">=</span>faiss_index<span class="token punctuation">,</span> service_context<span class="token operator">=</span>service_context<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">INFO<span class="token punctuation">:</span>sentence_transformers<span class="token punctuation">.</span>SentenceTransformer<span class="token punctuation">:</span>Load pretrained SentenceTransformer<span class="token punctuation">:</span> sentence<span class="token operator">-</span>transformers<span class="token operator">/</span>paraphrase<span class="token operator">-</span>multilingual<span class="token operator">-</span>mpnet<span class="token operator">-</span>base<span class="token operator">-</span>v2INFO<span class="token punctuation">:</span>sentence_transformers<span class="token punctuation">.</span>SentenceTransformer<span class="token punctuation">:</span>Use pytorch device<span class="token punctuation">:</span> cpuWARNING<span class="token punctuation">:</span>root<span class="token punctuation">:</span>Created a chunk of size <span class="token number">130</span><span class="token punctuation">,</span> which <span class="token keyword">is</span> longer than the specified <span class="token number">100</span>……INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_documents<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">0</span> tokensINFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_documents<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">3198</span> tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这个例子里面，我们使用了一个面向电商的FAQ的纯文本文件作为输入。里面是一系列预设好的FAQ问答对。为了确保我们没有使用OpenAI的API，我们先把openai.api_key给设成了一个空字符串。然后，我们定义了一个embeded_model，这个embeded_model里面，我们包装的是一个HuggingFaceEmbeddings的类。</p><p>因为HuggingFace为基于transformers的模型定义了一个标准，所以大部分模型你只需要传入一个模型名称，HuggingFacebEmbedding这个类就会下载模型、加载模型，并通过模型来计算你输入的文本的Embedding。使用HuggingFace的好处是，你可以通过一套代码使用所有的transfomers类型的模型。</p><p><a href="https://sbert.net/">sentence-transformers</a> 是目前效果最好的语义搜索类的模型，它在BERT的基础上采用了对比学习的方式，来区分文本语义的相似度，它包括了一系列的预训练模型。我们在这里，选用的是 sentence-transformers下面的 paraphrase-multilingual-mpnet-base-v2 模型。顾名思义，这个是一个支持多语言（multilingual）并且能把语句和段落（paraphrase）变成向量的一个模型。因为我们给的示例都是中文，所以选取了这个模型。你可以根据你要解决的实际问题，来选取一个适合自己的模型。</p><p>我们还是使用Faiss这个库来作为我们的向量索引库，所以需要指定一下向量的维度，paraphrase-multilingual-mpnet-base-v2 这个模型的维度是768，所以我们就把维度定义成768维。</p><p>相应的对文档的切分，我们使用的是CharacterTextSplitter，并且在参数上我们做了一些调整。</p><p>首先，我们把“\n\n”这样两个连续的换行符作为一段段文本的分隔符，因为我们的FAQ数据里，每一个问答对都有一个空行隔开，正好是连续两个换行。</p><p>然后，我们把chunk_size设置得比较小，只有100。这是因为我们所使用的开源模型是个小模型，这样我们才能在单机加载起来。它能够支持的输入长度有限，只有128个Token，超出的部分会进行截断处理。如果我们不设置chunk_size，llama-index会自动合并多个chunk变成一个段落。</p><p>其次，我们还增加了一个小小的参数，叫做chunk_overlap。这个参数代表我们自动合并小的文本片段的时候，可以接受多大程度的重叠。它的默认值是200，超过了单段文档的chunk_size，所以我们这里要把它设小一点，不然程序会报错。</p><p>我们可以在对应的verbose日志里看到，这里的Embedding使用了3198个Token，不过这些Token都是我们通过sentence_transformers类型的开源模型计算的，不需要花钱。你的成本就节约下来了。</p><p>在创建完整个索引之后，我们就可以拿一些常见的电商类型的FAQ问题试一试。</p><p>问题1：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from llama_index import QueryModeopenai.api_key = os.environ.get("OPENAI_API_KEY")response = index.query(    "请问你们海南能发货吗？",    mode=QueryMode.EMBEDDING,    verbose=True,)print(response)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Got node text: Q: 支持哪些省份配送？A: 我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、辽宁、吉林、黑龙江、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆...INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 341 tokensINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 24 tokens是的，我们支持海南省的配送。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>问题2：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">response = index.query(    "你们用哪些快递公司送货？",    mode=QueryMode.EMBEDDING,    verbose=True,)print(response)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Got node text: Q: 提供哪些快递公司的服务？A: 我们与顺丰速运、圆通速递、申通快递、韵达快递、中通快递、百世快递等多家知名快递公司合作。...INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 281 tokensINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 27 tokens我们与顺丰速运、圆通速递、申通快递、韵达快递、中通快递、百世快递等多家知名快递公司合作，用他们的服务送货。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>问题3：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">response = index.query(    "你们的退货政策是怎么样的？",    mode=QueryMode.EMBEDDING,    verbose=True,)print(response)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Got node text: Q: 退货政策是什么？A: 自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。...INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 393 tokensINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 27 tokens我们的退货政策是自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们在问问题的时候，指定了query的mode是Embedding。通过三个常用的问题，我们可以看到，AI都给出了正确的回答，效果还是不错的。</p><h2 id="使用ChatGLM提供对话效果"><a href="#使用ChatGLM提供对话效果" class="headerlink" title="使用ChatGLM提供对话效果"></a>使用ChatGLM提供对话效果</h2><p>通过上面的代码，我们已经把生成Embedding以及利用Embedding的相似度进行搜索搞定了。但是，我们在实际问答的过程中，使用的还是OpenAI的Completion API。那么这一部分我们有没有办法也替换掉呢？</p><p>同样的，我们寻求开源模型的帮助。在这里，我们就不妨来试一下来自清华大学的ChatGLM语言模型，看看中文的开源语言模型，是不是也有基本的知识理解和推理能力。</p><p>首先我们还是要安装一些依赖包，因为icetk我没有找到Conda的源，所以我们这里通过pip来安装，但是在Conda的包管理器里一样能够看到。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install icetkpip install cpm_kernels<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>然后，我们还是先通过transformers来加载模型。 <a href="https://github.com/THUDM/GLM-130B">ChatGLM</a> 最大的一个模型有1300亿个参数。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModeltokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"THUDM/chatglm-6b-int4"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"THUDM/chatglm-6b-int4"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>half<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">Explicitly passing a `revision` <span class="token keyword">is</span> encouraged when loading a model <span class="token keyword">with</span> custom code to ensure no malicious code has been contributed <span class="token keyword">in</span> a newer revision<span class="token punctuation">.</span>Explicitly passing a `revision` <span class="token keyword">is</span> encouraged when loading a configuration <span class="token keyword">with</span> custom code to ensure no malicious code has been contributed <span class="token keyword">in</span> a newer revision<span class="token punctuation">.</span>Explicitly passing a `revision` <span class="token keyword">is</span> encouraged when loading a model <span class="token keyword">with</span> custom code to ensure no malicious code has been contributed <span class="token keyword">in</span> a newer revision<span class="token punctuation">.</span>No compiled kernel found<span class="token punctuation">.</span>Compiling kernels <span class="token punctuation">:</span> <span class="token operator">/</span>root<span class="token operator">/</span><span class="token punctuation">.</span>cache<span class="token operator">/</span>huggingface<span class="token operator">/</span>modules<span class="token operator">/</span>transformers_modules<span class="token operator">/</span>THUDM<span class="token operator">/</span>chatglm<span class="token operator">-</span>6b<span class="token operator">-</span>int4<span class="token operator">/</span>dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b<span class="token operator">/</span>quantization_kernels<span class="token punctuation">.</span>cCompiling gcc <span class="token operator">-</span>O3 <span class="token operator">-</span>fPIC <span class="token operator">-</span>std<span class="token operator">=</span>c99 <span class="token operator">/</span>root<span class="token operator">/</span><span class="token punctuation">.</span>cache<span class="token operator">/</span>huggingface<span class="token operator">/</span>modules<span class="token operator">/</span>transformers_modules<span class="token operator">/</span>THUDM<span class="token operator">/</span>chatglm<span class="token operator">-</span>6b<span class="token operator">-</span>int4<span class="token operator">/</span>dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b<span class="token operator">/</span>quantization_kernels<span class="token punctuation">.</span>c <span class="token operator">-</span>shared <span class="token operator">-</span>o <span class="token operator">/</span>root<span class="token operator">/</span><span class="token punctuation">.</span>cache<span class="token operator">/</span>huggingface<span class="token operator">/</span>modules<span class="token operator">/</span>transformers_modules<span class="token operator">/</span>THUDM<span class="token operator">/</span>chatglm<span class="token operator">-</span>6b<span class="token operator">-</span>int4<span class="token operator">/</span>dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b<span class="token operator">/</span>quantization_kernels<span class="token punctuation">.</span>soKernels compiled <span class="token punctuation">:</span> <span class="token operator">/</span>root<span class="token operator">/</span><span class="token punctuation">.</span>cache<span class="token operator">/</span>huggingface<span class="token operator">/</span>modules<span class="token operator">/</span>transformers_modules<span class="token operator">/</span>THUDM<span class="token operator">/</span>chatglm<span class="token operator">-</span>6b<span class="token operator">-</span>int4<span class="token operator">/</span>dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b<span class="token operator">/</span>quantization_kernels<span class="token punctuation">.</span>soLoad kernel <span class="token punctuation">:</span> <span class="token operator">/</span>root<span class="token operator">/</span><span class="token punctuation">.</span>cache<span class="token operator">/</span>huggingface<span class="token operator">/</span>modules<span class="token operator">/</span>transformers_modules<span class="token operator">/</span>THUDM<span class="token operator">/</span>chatglm<span class="token operator">-</span>6b<span class="token operator">-</span>int4<span class="token operator">/</span>dac03c3ac833dab2845a569a9b7f6ac4e8c5dc9b<span class="token operator">/</span>quantization_kernels<span class="token punctuation">.</span>soUsing quantization cacheApplying quantization to glm layers<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>但是这么大的模型，无论是你自己的电脑，还是Colab提供的GPU和TPU显然都放不了。所以我们只能选用一个裁剪后的60亿个参数的版本，并且我们还必须用int-4量化的方式，而不是用float16的浮点数。所以，这里我们的模型名字就叫做 chatglm-6b-int4，也就是 6B的参数量，通过int-4量化。然后，在这里，我们希望通过GPU进行模型的计算，所以加载模型的时候调用了.cuda()。</p><p>这里加载模型的时候，我们还设置了一个 trust_remote_code &#x3D; true 的参数，这是因为ChatGLM的模型不是一个Huggingface官方发布的模型，而是由用户贡献的，所以需要你显式确认你信任这个模型的代码，它不会造成恶意的破坏。我们反正是在Colab里面运行这个代码，所以倒是不用太担心。</p><p>如果你想要用CPU运行，可以把模型加载的代码换成下面这样。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"THUDM/chatglm-6b-int4"</span><span class="token punctuation">,</span>trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>不过，我不建议你这么做。你没有GPU的话，还是直接使用Colab的GPU就好了。因为CPU在运行对话的时候非常慢。</p><p>在拿到模型之后我们就可以尝试着通过这个模型来进行问答了。</p><p>问题1：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">question <span class="token operator">=</span> <span class="token triple-quoted-string string">"""自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。根据以上信息，请回答下面的问题：Q: 你们的退货政策是怎么样的？"""</span>response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> question<span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">我们的退货政策是在产品未使用、包装完好的情况下，自收到商品之日起<span class="token number">7</span>天内可以退货。请注意，某些特殊商品可能不支持退货，在购买前请查看商品详情页面的退货政策，以了解具体情况。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以看到，ChatGLM的回答，的确是按照我们提示语里面有的上下文给出的。我们可以试着去掉提供的FAQ的内容，看看它的回答是怎样的。</p><p>问题2：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">question <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Q: 你们的退货政策是怎么样的？A:"""</span>response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> question<span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">我们公司的政策是，如果您在购买后的<span class="token number">7</span>天内发现我们的产品不符合您的期望，可以申请退货。退货条件如下：<span class="token number">1.</span> 符合退货政策要求的产品；<span class="token number">2.</span> 未使用或损坏的产品；<span class="token number">3.</span> 符合退货地点的要求，即退货地址与购买地址相同；<span class="token number">4.</span> 符合退货流程的要求，即您需要提供有效的退货单并完成退货流程。如果符合上述退货政策要求，我们将尽快处理您的退货申请，并为您退款。请注意，如果您已经使用了我们的产品，或者产品有损坏或变质等情况，我们将无法为您提供退货服务。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，在没有合适的上下文信息的时候，它编造出了一些原本没有的要求，比如“退货地址与购买地址相同”。</p><p>我们再来看一个例子，看看它能不能拥有简单的推理能力。我们的上下文里只说了可以送到海南，没有说是否支持三亚这个城市，看看这个时候AI能不能回答对这个问题。</p><p>问题3：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">question <span class="token operator">=</span> <span class="token triple-quoted-string string">"""我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、辽宁、吉林、黑龙江、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆.根据以上信息，请回答下面的问题：Q: 你们能配送到三亚吗？"""</span>response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> question<span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">是的，我们支持全国大部分省份的配送，包括三亚市。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以看到，ChatGLM知道是可以配送到三亚的。不过万一是巧合呢？我们再看看在上下文里面，去掉了东三省，然后问问它能不能送到哈尔滨。</p><p>问题4：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">question <span class="token operator">=</span> <span class="token triple-quoted-string string">"""我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆.但是不能配送到东三省根据以上信息，请回答下面的问题：Q: 你们能配送到哈尔滨吗？"""</span>response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> question<span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>回答：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">很抱歉，我们目前不能配送到哈尔滨。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>结果也是正确的，这个时候，ChatGLM会回答我们是送不到哈尔滨的。既然ChatGLM能够正确回答这个问题，那我们的FAQ问答就可以用ChatGLM来搞定了。</p><h2 id="将ChatGLM封装成LLM"><a href="#将ChatGLM封装成LLM" class="headerlink" title="将ChatGLM封装成LLM"></a>将ChatGLM封装成LLM</h2><p>不过上面的代码里面，我们用的还是原始的ChatGLM的模型代码，还不能直接通过query来访问llama-index直接得到答案。要做到这一点倒也不难，我们把它封装成一个LLM类，让我们的index使用这个指定的大语言模型就好了。对应的 <a href="https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html">llama-index 的文档</a>，你也可以自己去看一下。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token punctuation">,</span> os<span class="token keyword">import</span> faiss<span class="token keyword">from</span> llama_index <span class="token keyword">import</span> SimpleDirectoryReader<span class="token punctuation">,</span> LangchainEmbedding<span class="token punctuation">,</span> GPTFaissIndex<span class="token punctuation">,</span> ServiceContext<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>huggingface <span class="token keyword">import</span> HuggingFaceEmbeddings<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>text_splitter <span class="token keyword">import</span> CharacterTextSplitter<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>node_parser <span class="token keyword">import</span> SimpleNodeParser<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>base <span class="token keyword">import</span> LLM<span class="token keyword">from</span> llama_index <span class="token keyword">import</span> LLMPredictor<span class="token keyword">from</span> typing <span class="token keyword">import</span> Optional<span class="token punctuation">,</span> List<span class="token punctuation">,</span> Mapping<span class="token punctuation">,</span> Any<span class="token keyword">class</span> <span class="token class-name">CustomLLM</span><span class="token punctuation">(</span>LLM<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">_call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prompt<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> stop<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>        response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> prompt<span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> response    <span class="token decorator annotation punctuation">@property</span>    <span class="token keyword">def</span> <span class="token function">_identifying_params</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Mapping<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Any<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token string">"name_of_model"</span><span class="token punctuation">:</span> <span class="token string">"chatglm-6b-int4"</span><span class="token punctuation">&#125;</span>    <span class="token decorator annotation punctuation">@property</span>    <span class="token keyword">def</span> <span class="token function">_llm_type</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string">"custom"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们把这个CustomLLM对象，传入index的构造函数里，重新运行一下我们的问题，看看效果是怎样的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>text_splitter <span class="token keyword">import</span> SpacyTextSplitterllm_predictor <span class="token operator">=</span> LLMPredictor<span class="token punctuation">(</span>llm<span class="token operator">=</span>CustomLLM<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>text_splitter <span class="token operator">=</span> CharacterTextSplitter<span class="token punctuation">(</span>separator<span class="token operator">=</span><span class="token string">"\n\n"</span><span class="token punctuation">,</span> chunk_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> chunk_overlap<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>parser <span class="token operator">=</span> SimpleNodeParser<span class="token punctuation">(</span>text_splitter<span class="token operator">=</span>text_splitter<span class="token punctuation">)</span>documents <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span><span class="token string">'./drive/MyDrive/colab_data/faq/'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>nodes <span class="token operator">=</span> parser<span class="token punctuation">.</span>get_nodes_from_documents<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>embed_model <span class="token operator">=</span> LangchainEmbedding<span class="token punctuation">(</span>HuggingFaceEmbeddings<span class="token punctuation">(</span>    model_name<span class="token operator">=</span><span class="token string">"sentence-transformers/paraphrase-multilingual-mpnet-base-v2"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>service_context <span class="token operator">=</span> ServiceContext<span class="token punctuation">.</span>from_defaults<span class="token punctuation">(</span>embed_model<span class="token operator">=</span>embed_model<span class="token punctuation">,</span> llm_predictor<span class="token operator">=</span>llm_predictor<span class="token punctuation">)</span>dimension <span class="token operator">=</span> <span class="token number">768</span>faiss_index <span class="token operator">=</span> faiss<span class="token punctuation">.</span>IndexFlatIP<span class="token punctuation">(</span>dimension<span class="token punctuation">)</span>index <span class="token operator">=</span> GPTFaissIndex<span class="token punctuation">(</span>nodes<span class="token operator">=</span>nodes<span class="token punctuation">,</span> faiss_index<span class="token operator">=</span>faiss_index<span class="token punctuation">,</span> service_context<span class="token operator">=</span>service_context<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> llama_index <span class="token keyword">import</span> QuestionAnswerPrompt<span class="token keyword">from</span> llama_index <span class="token keyword">import</span> QueryModeQA_PROMPT_TMPL <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token string">"&#123;context_str&#125;"</span>    <span class="token string">"\n\n"</span>    <span class="token string">"根据以上信息，请回答下面的问题：\n"</span>    <span class="token string">"Q: &#123;query_str&#125;\n"</span>    <span class="token punctuation">)</span>QA_PROMPT <span class="token operator">=</span> QuestionAnswerPrompt<span class="token punctuation">(</span>QA_PROMPT_TMPL<span class="token punctuation">)</span>response <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span>    <span class="token string">"请问你们海南能发货吗？"</span><span class="token punctuation">,</span>    mode<span class="token operator">=</span>QueryMode<span class="token punctuation">.</span>EMBEDDING<span class="token punctuation">,</span>    text_qa_template<span class="token operator">=</span>QA_PROMPT<span class="token punctuation">,</span>    verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">></span> Got node text<span class="token punctuation">:</span> Q<span class="token punctuation">:</span> 支持哪些省份配送？A<span class="token punctuation">:</span> 我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、辽宁、吉林、黑龙江、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>海南能发货。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，这样处理之后，我们就可以直接使用ChatGLM的模型，来进行我们的FAQ的问答了。</p><p>现在，我们有了一个通过paraphrase-multilingual-mpnet-base-v2模型来计算Embeddding并进行语义搜索，然后通过chatglm-6b-int4的模型来进行问答的解决方案了。而且这两个模型，可以跑在一块家用级别的显卡上。是不是很厉害？</p><h2 id="开源模型的不足之处"><a href="#开源模型的不足之处" class="headerlink" title="开源模型的不足之处"></a>开源模型的不足之处</h2><p>看起来，我们这个本机就能运行的小模型似乎已经完成了。数据安全，又不用担心花费。但显然，事情没有那么简单。因为刚才我们处理的电商FAQ问题比较简单，我们再拿一个稍微复杂一点的问题来看看效果。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">text_splitter <span class="token operator">=</span> SpacyTextSplitter<span class="token punctuation">(</span>pipeline<span class="token operator">=</span><span class="token string">"zh_core_web_sm"</span><span class="token punctuation">,</span> chunk_size <span class="token operator">=</span> <span class="token number">128</span><span class="token punctuation">,</span> chunk_overlap<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span>parser <span class="token operator">=</span> SimpleNodeParser<span class="token punctuation">(</span>text_splitter<span class="token operator">=</span>text_splitter<span class="token punctuation">)</span>documents <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span><span class="token string">'./drive/MyDrive/colab_data/zhaohuaxishi/'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>nodes <span class="token operator">=</span> parser<span class="token punctuation">.</span>get_nodes_from_documents<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>embed_model <span class="token operator">=</span> LangchainEmbedding<span class="token punctuation">(</span>HuggingFaceEmbeddings<span class="token punctuation">(</span>    model_name<span class="token operator">=</span><span class="token string">"sentence-transformers/paraphrase-multilingual-mpnet-base-v2"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>service_context <span class="token operator">=</span> ServiceContext<span class="token punctuation">.</span>from_defaults<span class="token punctuation">(</span>embed_model<span class="token operator">=</span>embed_model<span class="token punctuation">,</span> llm_predictor<span class="token operator">=</span>llm_predictor<span class="token punctuation">)</span>dimension <span class="token operator">=</span> <span class="token number">768</span>faiss_index <span class="token operator">=</span> faiss<span class="token punctuation">.</span>IndexFlatIP<span class="token punctuation">(</span>dimension<span class="token punctuation">)</span>index <span class="token operator">=</span> GPTFaissIndex<span class="token punctuation">(</span>nodes<span class="token operator">=</span>nodes<span class="token punctuation">,</span> faiss_index<span class="token operator">=</span>faiss_index<span class="token punctuation">,</span> service_context<span class="token operator">=</span>service_context<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">INFO<span class="token punctuation">:</span>sentence_transformers<span class="token punctuation">.</span>SentenceTransformer<span class="token punctuation">:</span>Load pretrained SentenceTransformer<span class="token punctuation">:</span> sentence<span class="token operator">-</span>transformers<span class="token operator">/</span>paraphrase<span class="token operator">-</span>multilingual<span class="token operator">-</span>mpnet<span class="token operator">-</span>base<span class="token operator">-</span>v2INFO<span class="token punctuation">:</span>sentence_transformers<span class="token punctuation">.</span>SentenceTransformer<span class="token punctuation">:</span>Use pytorch device<span class="token punctuation">:</span> cpu……INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_documents<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">0</span> tokensINFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_documents<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">91882</span> tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这一次，我们输入索引起来的数据，是鲁迅先生整套《朝花夕拾》的散文集。选用这个是因为对应作品的版权已经过了保护期。我们来看看，在这套文集的内容里面，使用我们上面的纯开源方案，效果会是怎样的。</p><p>对应的模型和索引加载的代码基本一致，只有一个小小的区别，就是在文本分割的时候，我们用了上一讲介绍过的SpacyTextSplitter，因为这里都是散文的内容，而不是确定好格式的QA对。所以通过SpacyTextSplitter来分句，并在允许的时候合并小的片段是有意义的。</p><p>然后，我们试着问一下上一讲我们问过的问题，看看效果怎么样。</p><p>问题1：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># query will use the same embed_model</span><span class="token keyword">from</span> llama_index <span class="token keyword">import</span> QueryMode<span class="token keyword">from</span> llama_index <span class="token keyword">import</span> QuestionAnswerPromptopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>QA_PROMPT_TMPL <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token string">"下面的内容来自鲁迅先生的散文集《朝花夕拾》，很多内容是以第一人称写的 \n"</span>    <span class="token string">"---------------------\n"</span>    <span class="token string">"&#123;context_str&#125;"</span>    <span class="token string">"\n---------------------\n"</span>    <span class="token string">"根据这些信息，请回答问题: &#123;query_str&#125;\n"</span>    <span class="token string">"如果您不知道的话，请回答不知道\n"</span><span class="token punctuation">)</span>QA_PROMPT <span class="token operator">=</span> QuestionAnswerPrompt<span class="token punctuation">(</span>QA_PROMPT_TMPL<span class="token punctuation">)</span>response <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span>    <span class="token string">"鲁迅先生在日本学习医学的老师是谁？"</span><span class="token punctuation">,</span>    mode<span class="token operator">=</span>QueryMode<span class="token punctuation">.</span>EMBEDDING<span class="token punctuation">,</span>    similarity_top_k <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>    text_qa_template<span class="token operator">=</span>QA_PROMPT<span class="token punctuation">,</span>    verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">></span> Got node text<span class="token punctuation">:</span> 一将书放在讲台上，便用了缓慢而很有顿挫的声调，向学生介绍自己道：——    “我就是叫作藤野严九郎的……。”后面有几个人笑起来了。他接着便讲述解剖学在日本发达的历史，那些大大小小的书，便是从最初到现今关于这一门学问的著作。<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>鲁迅先生在日本学习医学的老师是藤野严九郎。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>问题2：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">response <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span>    <span class="token string">"鲁迅先生是在日本的哪个城市学习医学的？"</span><span class="token punctuation">,</span>    mode<span class="token operator">=</span>QueryMode<span class="token punctuation">.</span>EMBEDDING<span class="token punctuation">,</span>    similarity_top_k <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>    text_qa_template<span class="token operator">=</span>QA_PROMPT<span class="token punctuation">,</span>    verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">></span> Got node text<span class="token punctuation">:</span> 有时我常常想：他的对于我的热心的希望，不倦的教诲，小而言之，是为中国，就是希望中国有新的医学；大而言之，是为学术，就是希望新的医学传到中国去。<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>根据这些信息，无法得出鲁迅先生是在日本的哪个城市学习医学的答案。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>可以看到，有些问题在这个模式下，定位到的文本片段是正确的。但是有些问题，虽然定位的还算是一个相关的片段，但是的确无法得出答案。</p><p>在这个过程中，我们可以观察到这样一个问题： 那就是单机的开源小模型能够承载的文本输入的长度问题。在我们使用OpenAI的gpt-3.5-turbo模型的时候，我们最长支持4096个Token，也就是一个文本片段可以放上上千字在里面。但是我们这里单机用的paraphrase-multilingual-mpnet-base-v2模型，只能支持128个Token的输入，虽然对应的Tokenizer不一样，但是就算一个字一个Token，也就100个字而已。这使得我们检索出来的内容的上下文太少了，很多时候没有足够的信息，让语言模型去回答。</p><p>当然，这个问题并不是无法弥补的。我们可以通过把更大规模的模型，部署到云端来解决。这个内容，我们课程的第三部分专门有一讲会讲解。</p><p>不过，有一个更难解决的问题，就是模型的推理能力问题。比如，我们可以再试试 <a href="https://time.geekbang.org/column/article/641742">第 1 讲</a> 里给商品总结英文名称和卖点的例子。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">question <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具1. Compose human readale product title used on Amazon in english within 20 words.2. Write 5 selling points for the products in Amazon.3. Evaluate a price range for this product in U.S.Output the result in json format with three properties called title, selling_points and price_range"""</span>response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> question<span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token number">1.</span> title<span class="token punctuation">:</span> 充气玩具青蛙夜市地摊卖<span class="token number">2.</span> selling_points<span class="token punctuation">:</span>    <span class="token operator">-</span> 工厂现货：保证产品质量    <span class="token operator">-</span> PVC充气：环保耐用    <span class="token operator">-</span> 夜市地摊：方便销售    <span class="token operator">-</span> 热卖：最受欢迎产品    <span class="token operator">-</span> 儿童水上玩具：适合各种年龄段儿童<span class="token number">3.</span> price_range<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token keyword">in</span> USD<span class="token punctuation">)</span>    <span class="token operator">-</span> low<span class="token punctuation">:</span>   $<span class="token number">1.99</span>    <span class="token operator">-</span> high<span class="token punctuation">:</span>   $<span class="token number">5.99</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，虽然这个结果不算太离谱，多少和问题还是有些关系的。但是无论是翻译成英文，还是使用JSON返回，模型都没有做到。给到的卖点也没有任何“推理出来”的性质，都是简单地对标题的重复描述。即使你部署一个更大版本的模型到云端，也好不到哪里去。</p><p>这也是ChatGPT让人震撼的原因，的确目前它的效果还是要远远超出任何一个竞争对手和开源项目的。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，最后我们来回顾一下。这一讲里，我们一起尝试用开源模型来代替ChatGPT。我们通过sentence_transfomers类型的模型，生成了文本分片的Embedding，并且基于这个Embedding来进行语义检索。我们通过 ChatGLM 这个开源模型，实现了基于上下文提示语的问答。在简单的电商QA这样的场景里，效果也还是不错的。即使我们使用的都是单机小模型，它也能正确回答出来。这些方法，也能节约我们的成本。不用把钱都交给OpenAI，可以攒着买显卡来训练自己的模型。</p><p>但是，当我们需要解决更加复杂的问题时，比如需要更长的上下文信息，或者需要模型本身更强的推理能力的时候，这样的小模型就远远不够用了。更长的上下文信息检索，我们还能够通过在云端部署更大规模的模型，解决部分问题。但是模型的推理能力，目前的确没有好的解决方案。</p><p>所以不得不佩服，OpenAI的在AGI这个目标上耕耘多年后震惊世人的效果。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>最后，给你留一个思考题。ChatGLM并不是唯一的中文大语言模型，开源社区目前在快速推进，尝试用各种方式提供更好的开源大模型。比如基于斯坦福的Alpaca数据集进行微调的 <a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">Chinese-LLaMA-Alpaca</a>，链家科技开源的 <a href="https://github.com/LianjiaTech/BELLE">BELLE</a>。你可以挑选一个模型试一试，看看它们的效果和ChatGLM比起来怎么样。欢迎你把你的评测结果分享出来，也欢迎你把这节课分享给需要的朋友，共同参谋，一起进步。我们下节课再见。</p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>基于开源模型来解决问题的思路并非我的原创，网上也有不少其他朋友用类似的方式解决了自己的问题。比如 <a href="https://mp.weixin.qq.com/s/iplUoK_JYeL_9EC7Ttt3tw">《让 LLM 回答问题更靠谱》这篇文章</a> 就组合了三个模型来完成了医学领域的语义搜索、语义匹配排序，以及最终的问答语句生成。你可以读一下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;11｜省下钱买显卡，如何利用开源模型节约成本？&quot;&gt;&lt;a href=&quot;#11｜省下钱买显卡，如何利用开源模型节约成本？&quot; class=&quot;headerlink&quot; title=&quot;11｜省下钱买显卡，如何利用开源模型节约成本？&quot;&gt;&lt;/a&gt;11｜省下钱买显卡，如何利用开源模</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>10｜AI连接外部资料库，让Llama Index带你阅读一本书</title>
    <link href="https://zhuansun.github.io/geekbang/posts/735898898.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/735898898.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.900Z</updated>
    
    <content type="html"><![CDATA[<h1 id="10｜AI连接外部资料库，让Llama-Index带你阅读一本书"><a href="#10｜AI连接外部资料库，让Llama-Index带你阅读一本书" class="headerlink" title="10｜AI连接外部资料库，让Llama Index带你阅读一本书"></a>10｜AI连接外部资料库，让Llama Index带你阅读一本书</h1><p>你好，我是徐文浩。</p><p>有不少人在使用OpenAI提供的GPT系列模型的时候，都反馈效果并不好。这些反馈中有一大类问题，是回答不了一些简单的问题。比如当我们用中文问AI一些事实性的问题，AI很容易胡编乱造。而当你问它最近发生的新闻事件的时候，它就干脆告诉你它不知道21年之后的事情。</p><p>本来呢，我写到这里就可以了。不过到了3月24日，OpenAI推出了ChatGPT Plugins这个功能，可以让ChatGPT通过插件的形式链接外部的第三方应用。我自己也还在排队等waiting list，所以暂时也无法体验。不过，即使有了第三方应用，我们也不能确保自己想要知道的信息正好被其他人提供了。而且，有些信息和问题我们只想提供给自己公司的内部使用，并不想开放给所有人。这个时候，我们既希望能够利用OpenAI的大语言模型的能力，但是又需要这些能力仅仅在我们自己指定的数据上。那么这一讲，就是为了解决这个问题的。</p><h2 id="大型语言模型的不足之处"><a href="#大型语言模型的不足之处" class="headerlink" title="大型语言模型的不足之处"></a>大型语言模型的不足之处</h2><p>我们先来尝试问ChatGPT一个人尽皆知的常识，“鲁迅先生去日本学习医学的老师是谁”，结果它给出的答案是鲁迅的好友，内山书店的老板内山完造，而不是大家都学习过的藤野先生。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/891fc431e1cd46b1d45f60fe79c2e964.png" alt="图片"></p><p>之所以会出现这样的情况，和大模型的原理以及它使用训练的数据集是有关的。大语言模型的原理，就是利用训练样本里面出现的文本的前后关系，通过前面的文本对接下来出现的文本进行概率预测。如果类似的前后文本出现得越多，那么这个概率在训练过程里会收敛到少数正确答案上，回答就准确。如果这样的文本很少，那么训练过程里就会有一定的随机性，对应的答案就容易似是而非。而在GPT-3的模型里，虽然整体的训练语料很多，但是中文语料很少。只有不到1%的语料是中文的，所以如果问很多中文相关的知识性或者常识性问题，它的回答往往就很扯。</p><p>当然，你可以说我们有一个解决办法，就是多找一些高质量的中文语料训练一个新的模型。或者，对于我们想让AI能够回答出来的问题，找一些数据。然后利用OpenAI提供的“微调”（Fine-tune）接口，在原来的基础上训练一个新模型出来。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/47e11ff6d1534e8f53ea8da3ae5bcb9c.png" alt="图片"></p><p>这样当然是可以的，就是成本有点高。对于上面那个例子来说，只是缺少一些文本数据，还好说。如果是时效性要求比较强的资讯类的信息，就很难这么做。比如，我们想让AI告诉我们前一天足球赛的比分，我们不太可能每隔几个小时就单独训练或者微调一下模型，那样干的成本太高了。</p><h2 id="Bing的解法——先搜索，后提示"><a href="#Bing的解法——先搜索，后提示" class="headerlink" title="Bing的解法——先搜索，后提示"></a>Bing的解法——先搜索，后提示</h2><p>不过对这个领域比较关注的朋友可能就要问了。之前微软不是在Bing这个搜索引擎里，加上了ChatGPT的问答功能吗？效果似乎也还不错，那Bing是怎么做到的呢，是因为他们用了更加厉害的语言模型吗？</p><p>虽然我并没有什么内幕消息，不了解Bing是怎么做的。但是如果是我的话，会用这样一个解决办法——那就是先搜索，后提示（Prompt）。</p><ol><li>我们先通过搜索的方式，找到和询问的问题最相关的语料。这个搜索过程中，我们既可以用传统的基于关键词搜索的技术，也可以用 <a href="http://time.geekbang.org/column/article/644795">第 9 讲</a> 我们刚刚介绍过的使用Embedding的相似度进行语义搜索的技术。</li><li>然后，我们将和问题语义最接近的前几条内容，作为提示语的一部分给到AI。然后请AI参考这些内容，再来回答这个问题。</li></ol><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c8f7888bdfc3438934e784beayybeef2.png" alt="图片"></p><p>我在这里，也给了你一个例子的截图。当我们把《藤野先生》里的两个段落给到AI，然后请AI根据这两个段落，回答原来那个问题，就会得到正确的答案，你也可以去看一看。</p><p>这也是利用大语言模型的一个常见模式。因为大语言模型其实内含了两种能力。</p><p>第一种，是海量的语料中，本身已经包含了的知识信息。比如，我们前面问AI鱼香肉丝的做法，它能回答上来就是因为语料里已经有了充足的相关知识。我们一般称之为“世界知识”。</p><p>第二种，是根据你输入的内容，理解和推理的能力。这个能力，不需要训练语料里有一样的内容。而是大语言模型本身有“思维能力”，能够进行阅读理解。这个过程里，“知识”不是模型本身提供的，而是我们找出来，临时提供给模型的。如果不提供这个上下文，再问一次模型相同的问题，它还是答不上来的。</p><h2 id="通过llama-index封装“第二大脑”"><a href="#通过llama-index封装“第二大脑”" class="headerlink" title="通过llama_index封装“第二大脑”"></a>通过llama_index封装“第二大脑”</h2><p>我给上面这种先搜索、后提示的方式，取了一个名字，叫做AI的“第二大脑”模式。因为这个方法，需要提前把你希望AI能够回答的知识，建立一个外部的索引，这个索引就好像AI的“第二个大脑”。每次向AI提问的时候，它都会先去查询一下这个第二大脑里面的资料，找到相关资料之后，再通过自己的思维能力来回答问题。</p><p>实际上，你现在在网上看到的很多读论文、读书回答问题的应用，都是通过这个模式来实现的。那么，现在我们就来自己实现一下这个“第二大脑”模式。</p><p>不过，我们不必从0开始写代码。因为这个模式实在太过常用了，所以有人为它写了一个开源Python包，叫做llama-index。那么我们这里，可以直接利用这个软件包，用几行代码来试一试，它能不能回答上鲁迅先生写的《藤野先生》相关的问题。</p><p>llama-index还没有人做好Conda下的包，所以即使在Conda下还是要通过pip来安装。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install llama<span class="token operator">-</span>indexpip install langchain<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>我把从网上找到的《藤野先生》这篇文章变成了一个txt文件，放在了 data&#x2F;mr_fujino 这个目录下。我们的代码也非常简单，一共没有几行。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token punctuation">,</span> os<span class="token keyword">from</span> llama_index <span class="token keyword">import</span> GPTVectorStoreIndex<span class="token punctuation">,</span> SimpleDirectoryReaderopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>documents <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span><span class="token string">'./data/mr_fujino'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>index <span class="token operator">=</span> GPTSimpleVectorIndex<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>index<span class="token punctuation">.</span>save_to_disk<span class="token punctuation">(</span><span class="token string">'index_mr_fujino.json'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_nodes<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">0</span> tokensINFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_nodes<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">6763</span> tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注：日志中会打印出来我们通过Embedding消耗了多少个Token。</p><p>首先，我们通过一个叫做SimpleDirectoryReader的数据加载器，将整个.&#x2F;data&#x2F;mr_fujino 的目录给加载进来。这里面的每一个文件，都会被当成是一篇文档。</p><p>然后，我们将所有的文档交给了 GPTSimpleVectorIndex 构建索引。顾名思义，它会把文档分段转换成一个个向量，然后存储成一个索引。</p><p>最后，我们会把对应的索引存下来，存储的结果就是一个json文件。后面，我们就可以用这个索引来进行相应的问答。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">index <span class="token operator">=</span> GPTVectorStoreIndex<span class="token punctuation">.</span>load_from_disk<span class="token punctuation">(</span><span class="token string">'index_mr_fujino.json'</span><span class="token punctuation">)</span>response <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"鲁迅先生在日本学习医学的老师是谁？"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>要进行问答也没有几行代码，我们通过 GPTSimpleVectorIndex 的 load_from_disk 函数，可以把刚才生成的索引加载到内存里面来。然后对着Index索引调用Query函数，就能够获得问题的答案。可以看到，通过外部的索引，我们可以正确地获得问题的答案。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">2984</span> tokensINFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">34</span> tokens鲁迅先生在日本学习医学的老师是藤野严九郎先生。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>这么一看，似乎问题特别简单，三行代码就搞定了。别着急，我们再看看别的问题它是不是也能答上来？这次我们来试着问问鲁迅先生是在哪里学习医学的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">response <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"鲁迅先生去哪里学的医学？"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">></span> Got node text<span class="token punctuation">:</span> 藤野先生东京也无非是这样。上野的樱花烂熳的时节，望去确也像绯红的轻云，但花下也缺不了成群结队的“清国留学生”的速成班，头顶上盘着大辫子，顶得学生制帽的顶上高高耸起，形成一座富士山。也有解散辫子，盘得平的，除下帽来，油光可鉴，宛如小姑娘的发髻一般，还要将脖子扭几扭。实在标致极了。中国留学生会馆的门房里有几本书买，有时还值得去一转；倘在上午，里面的几间洋房里倒也还可以坐坐的。但到傍晚，有<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">2969</span> tokensINFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">26</span> tokens鲁迅先生去仙台的医学专门学校学习医学。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>它仍然正确回答了问题。那么，我们搜索到的内容，在这个过程里面是如何提交给OpenAI的呢？我们就来看看下面的这段代码就知道了。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> llama_index <span class="token keyword">import</span> QuestionAnswerPromptquery_str <span class="token operator">=</span> <span class="token string">"鲁迅先生去哪里学的医学？"</span>DEFAULT_TEXT_QA_PROMPT_TMPL <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token string">"Context information is below. \n"</span>    <span class="token string">"---------------------\n"</span>    <span class="token string">"&#123;context_str&#125;"</span>    <span class="token string">"\n---------------------\n"</span>    <span class="token string">"Given the context information and not prior knowledge, "</span>    <span class="token string">"answer the question: &#123;query_str&#125;\n"</span><span class="token punctuation">)</span>QA_PROMPT <span class="token operator">=</span> QuestionAnswerPrompt<span class="token punctuation">(</span>DEFAULT_TEXT_QA_PROMPT_TMPL<span class="token punctuation">)</span>response <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span>query_str<span class="token punctuation">,</span> text_qa_template<span class="token operator">=</span>QA_PROMPT<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码里，我们定义了一个QA_PROMPT的对象，并且为它设计了一个模版。</p><ol><li>这个模版的开头，我们告诉AI，我们为AI提供了一些上下文信息（Context information）。</li><li>模版里面支持两个变量，一个叫做 context_str，另一个叫做query_str。context_str 的地方，在实际调用的时候，会被通过Embedding相似度找出来的内容填入。而 query_str 则是会被我们实际提的问题替换掉。</li><li>实际提问的时候，我们告诉AI，只考虑上下文信息，而不要根据自己已经有的先验知识（prior knowledge）来回答问题。</li></ol><p>我们就是这样，把搜索找到的相关内容以及问题，组合到一起变成一段提示语，让AI能够按照我们的要求来回答问题。那我们再问一次AI，看看答案是不是没有变。</p><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">鲁迅先生去仙台的医学专门学校学习医学。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这一次AI还是正确地回答出了鲁迅先生是去仙台的医学专门学校学习的。我们再试一试，问一些不相干的问题，会得到什么答案，比如我们问问红楼梦里林黛玉和贾宝玉的关系。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">QA_PROMPT_TMPL <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token string">"下面的“我”指的是鲁迅先生 \n"</span>    <span class="token string">"---------------------\n"</span>    <span class="token string">"&#123;context_str&#125;"</span>    <span class="token string">"\n---------------------\n"</span>    <span class="token string">"根据这些信息，请回答问题: &#123;query_str&#125;\n"</span>    <span class="token string">"如果您不知道的话，请回答不知道\n"</span><span class="token punctuation">)</span>QA_PROMPT <span class="token operator">=</span> QuestionAnswerPrompt<span class="token punctuation">(</span>QA_PROMPT_TMPL<span class="token punctuation">)</span>response <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"请问林黛玉和贾宝玉是什么关系？"</span><span class="token punctuation">,</span> text_qa_template<span class="token operator">=</span>QA_PROMPT<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">不知道<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以看到，AI的确按照我们的指令回答不知道，而不是胡答一气。</p><h2 id="通过llama-index对于文章进行小结"><a href="#通过llama-index对于文章进行小结" class="headerlink" title="通过llama_index对于文章进行小结"></a>通过llama_index对于文章进行小结</h2><p>还有一个常见的使用llama-index这样“第二大脑”的Python库的应用场景，就是生成文章的摘要。在前面教你如何进行文本聚类的时候，我们已经看到了可以通过合适的提示语（Prompt）做到这一点。不过，如果要总结一篇论文、甚至是一本书，每次最多只能支持4096个Token的API就不太够用了。</p><p>要解决这个问题也并不困难，我们只要进行分段小结，再对总结出来的内容再做一次小结就可以了。我们可以把一篇文章，乃至一本书，构建成一个树状的索引。每一个树里面的节点，就是它的子树下内容的摘要。最后，在整棵树的根节点，得到的就是整篇文章或者整本书的总结了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/49489cc171aee72855930aaf68112b42.jpg" alt="图片"></p><p>事实上，llama-index本身就内置了这样的功能。下面我们就来看看要实现这个功能，我们的代码应该怎么写。</p><p>首先，我们先来安装一下 spaCy 这个Python库，并且下载一下对应的中文分词分句需要的模型。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install spacypython <span class="token operator">-</span>m spacy download zh_core_web_sm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>接下来的代码很简单，我们选用了GPTListIndex这个llama-index里最简单的索引结构。不过我们针对自身需求做了两点优化。</p><p>首先，在索引里面，我们指定了一个 LLMPredictor，让我们向OpenAI发起请求的时候，都使用ChatGPT的模型。因为这个模型比较快，也比较便宜。llama-index默认使用的模型是text-davinci-003，价格比gpt-3.5-turbo要贵上十倍。在我们前面只是简单进行几轮对话的时候，这个价格差异还不明显。而如果你要把几十本书都灌进去，那成本上就会差上不少了。我们在这里，设置了模型输出的内容都在1024个Token以内，这样可以确保我们的小结不会太长，不会把一大段不相关的内容都合并到一起去。</p><p>其次，我们定义了使用 SpacyTextSplitter来进行中文文本的分割。llama-index默认的设置对于中文的支持和效果都不太好。不过好在它可以让你自定义使用的文本分割方式。我们选用的文章是中文的，里面的标点符号也都是中文的，所以我们就用了中文的语言模型。我们也限制了分割出来的文本段，最长不要超过2048个Token，这些参数都可以根据你实际用来处理的文章内容和属性自己设置。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chat_models <span class="token keyword">import</span> ChatOpenAI<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>text_splitter <span class="token keyword">import</span> SpacyTextSplitter<span class="token keyword">from</span> llama_index <span class="token keyword">import</span> GPTListIndex<span class="token punctuation">,</span> LLMPredictor<span class="token punctuation">,</span> ServiceContext<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>node_parser <span class="token keyword">import</span> SimpleNodeParser<span class="token comment"># define LLM</span>llm_predictor <span class="token operator">=</span> LLMPredictor<span class="token punctuation">(</span>llm<span class="token operator">=</span>ChatOpenAI<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> model_name<span class="token operator">=</span><span class="token string">"gpt-3.5-turbo"</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">)</span>text_splitter <span class="token operator">=</span> SpacyTextSplitter<span class="token punctuation">(</span>pipeline<span class="token operator">=</span><span class="token string">"zh_core_web_sm"</span><span class="token punctuation">,</span> chunk_size <span class="token operator">=</span> <span class="token number">2048</span><span class="token punctuation">)</span>parser <span class="token operator">=</span> SimpleNodeParser<span class="token punctuation">(</span>text_splitter<span class="token operator">=</span>text_splitter<span class="token punctuation">)</span>documents <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span><span class="token string">'./data/mr_fujino'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>nodes <span class="token operator">=</span> parser<span class="token punctuation">.</span>get_nodes_from_documents<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>service_context <span class="token operator">=</span> ServiceContext<span class="token punctuation">.</span>from_defaults<span class="token punctuation">(</span>llm_predictor<span class="token operator">=</span>llm_predictor<span class="token punctuation">)</span>list_index <span class="token operator">=</span> GPTListIndex<span class="token punctuation">(</span>nodes<span class="token operator">=</span>nodes<span class="token punctuation">,</span> service_context<span class="token operator">=</span>service_context<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">WARNING<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>llm_predictor<span class="token punctuation">.</span>base<span class="token punctuation">:</span>Unknown <span class="token builtin">max</span> <span class="token builtin">input</span> size <span class="token keyword">for</span> gpt<span class="token operator">-</span><span class="token number">3.5</span><span class="token operator">-</span>turbo<span class="token punctuation">,</span> using defaults<span class="token punctuation">.</span>INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_nodes<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">0</span> tokensINFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_nodes<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">0</span> tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>GPTListIndex在构建索引的时候，并不会创建Embedding，所以索引创建的时候很快，也不消耗Token数量。它只是根据你设置的索引结构和分割方式，建立了一个List的索引。</p><p>接着，我们就可以让AI帮我们去小结这篇文章了。同样的，提示语本身很重要，所以我们还是强调了文章内容是鲁迅先生以“我”这个第一人称写的。因为我们想要的是按照树状结构进行文章的小结，所以我们设定了一个参数，叫做 response_mode &#x3D; “tree_summarize”。这个参数，就会按照上面我们所说的树状结构把整个文章总结出来。</p><p>实际上，它就是将每一段文本分片，都通过query内的提示语小结。再对多个小结里的内容，再次通过query里的提示语继续小结。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">response <span class="token operator">=</span> list_index<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:"</span><span class="token punctuation">,</span> response_mode<span class="token operator">=</span><span class="token string">"tree_summarize"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>indices<span class="token punctuation">.</span>common_tree<span class="token punctuation">.</span>base<span class="token punctuation">:</span><span class="token operator">></span> Building index <span class="token keyword">from</span> nodes<span class="token punctuation">:</span> <span class="token number">2</span> chunksINFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">9787</span> tokensINFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">0</span> tokens鲁迅先生回忆了自己在日本学医期间的经历，描述了自己在解剖实习中的经历，以及与教授藤野先生的交往。他还提到了一些不愉快的事情，比如遭到同学的诽谤和歧视，以及看到中国人被枪毙时的感受。最后，他告诉藤野先生自己将不再学医，而是想学生物学。他想起了一个人，这个人是他的老师，他对鲁迅很热心，给他很多鼓励和教诲。鲁迅现在只有他的照片，但是每次看到他的照片，都会让他感到勇气和良心发现。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，我们只用了几行代码就完成了整个文章的小结，返回的结果整体上来说也还算不错。</p><h2 id="引入多模态，让llamd-index能够识别小票"><a href="#引入多模态，让llamd-index能够识别小票" class="headerlink" title="引入多模态，让llamd-index能够识别小票"></a>引入多模态，让llamd-index能够识别小票</h2><p>llama_index不光能索引文本，很多书里面还有图片、插画这样的信息。llama_index一样可以索引起来，供你查询，这也就是所谓的多模态能力。当然，这个能力其实是通过一些多模态的模型，把文本和图片能够联系到一起做到的。在整个课程的第三部分，我们也会专门来看看这些图像的多模态模型是怎么样的。</p><p>这里我们就来看一个llama_index <a href="https://github.com/jerryjliu/llama_index/blob/main/examples/multimodal/Multimodal.ipynb">官方样例库</a> 里面给到的例子，也就是把吃饭的小票都拍下来。然后询问哪天吃了什么，花了多少钱。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> llama_index <span class="token keyword">import</span> SimpleDirectoryReader<span class="token punctuation">,</span> GPTVectorStoreIndex<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>readers<span class="token punctuation">.</span><span class="token builtin">file</span><span class="token punctuation">.</span>base <span class="token keyword">import</span> DEFAULT_FILE_EXTRACTOR<span class="token punctuation">,</span> ImageParser<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>response<span class="token punctuation">.</span>notebook_utils <span class="token keyword">import</span> display_response<span class="token punctuation">,</span> display_image<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>indices<span class="token punctuation">.</span>query<span class="token punctuation">.</span>query_transform<span class="token punctuation">.</span>base <span class="token keyword">import</span> ImageOutputQueryTransformimage_parser <span class="token operator">=</span> ImageParser<span class="token punctuation">(</span>keep_image<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> parse_text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>file_extractor <span class="token operator">=</span> DEFAULT_FILE_EXTRACTORfile_extractor<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>    <span class="token string">".jpg"</span><span class="token punctuation">:</span> image_parser<span class="token punctuation">,</span>    <span class="token string">".png"</span><span class="token punctuation">:</span> image_parser<span class="token punctuation">,</span>    <span class="token string">".jpeg"</span><span class="token punctuation">:</span> image_parser<span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token comment"># NOTE: we add filename as metadata for all documents</span>filename_fn <span class="token operator">=</span> <span class="token keyword">lambda</span> filename<span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token string">'file_name'</span><span class="token punctuation">:</span> filename<span class="token punctuation">&#125;</span>receipt_reader <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span>    input_dir<span class="token operator">=</span><span class="token string">'./data/receipts'</span><span class="token punctuation">,</span>    file_extractor<span class="token operator">=</span>file_extractor<span class="token punctuation">,</span>    file_metadata<span class="token operator">=</span>filename_fn<span class="token punctuation">,</span><span class="token punctuation">)</span>receipt_documents <span class="token operator">=</span> receipt_reader<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>要能够索引图片，我们引入了ImageParser这个类，这个类背后，其实是一个基于OCR扫描的模型 <a href="https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v2">Donut</a>。它通过一个视觉的Encoder和一个文本的Decoder，这样任何一个图片能够变成一个一段文本，然后我们再通过OpenAI的Embedding把这段文本变成了一个向量。</p><p>我们仍然只需要使用简单的SimpleDirectoryReader，我们通过指定FileExtractor，会把对应的图片通过ImageParser解析成为文本，并最终成为向量来用于检索。</p><p>然后，我们仍然只需要向我们的索引用自然语言提问，就能找到对应的图片了。在提问的时候，我们专门制定了一个ImageOutputQueryTransform，主要是为了在输出结果的时候，能够在图片外加上 <code>&lt;img&gt;</code> 的标签方便在Notebook里面显示。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">receipts_index <span class="token operator">=</span> GPTVectorStoreIndex<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>receipt_documents<span class="token punctuation">)</span>receipts_response <span class="token operator">=</span> receipts_index<span class="token punctuation">.</span>query<span class="token punctuation">(</span>    'When was the last time I went to McDonald\'s <span class="token keyword">and</span> how much did I spend<span class="token punctuation">.</span> \    Also show me the receipt <span class="token keyword">from</span> my visit<span class="token punctuation">.</span>'<span class="token punctuation">,</span>    query_transform<span class="token operator">=</span>ImageOutputQueryTransform<span class="token punctuation">(</span>width<span class="token operator">=</span><span class="token number">400</span><span class="token punctuation">)</span><span class="token punctuation">)</span>display_response<span class="token punctuation">(</span>receipts_response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">1004</span> tokensINFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">30</span> tokensFinal Response<span class="token punctuation">:</span> The last time you went to McDonald's was on <span class="token number">03</span><span class="token operator">/</span><span class="token number">10</span><span class="token operator">/</span><span class="token number">2018</span> at <span class="token number">07</span><span class="token punctuation">:</span><span class="token number">39</span><span class="token punctuation">:</span><span class="token number">12</span> PM <span class="token keyword">and</span> you spent $<span class="token number">26.15</span><span class="token punctuation">.</span> Here <span class="token keyword">is</span> the receipt <span class="token keyword">from</span> your visit<span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/181747bf1e95664b3929db6ec4a46b08.jpg" alt="图片"></p><p>可以看到，答案中不仅显示出了对应的图片，也给出了正确的答案，这也要归功于OpenAI对于任意文本强大的处理能力。</p><p>我们可以单独解析一下图片，看看对应的文本内容是什么。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">output_image <span class="token operator">=</span> image_parser<span class="token punctuation">.</span>parse_file<span class="token punctuation">(</span><span class="token string">'./data/receipts/1100-receipt.jpg'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>output_image<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&lt;s_menu>&lt;s_nm> Story&lt;/s_nm>&lt;s_num> 16725 Stony Platin Rd&lt;/s_nm>&lt;s_num> Store#:&lt;/s_nm>&lt;s_num> 3659&lt;/s_num>&lt;s_price> 700-418-8362&lt;/s_price>&lt;sep/>&lt;s_nm> Welcome to all day breakfast dormist O Md Donald's&lt;/s_nm>&lt;s_num> 192&lt;/s_num>&lt;s_price> 192&lt;/s_price>&lt;sep/>&lt;s_nm> QTY ITEM&lt;/s_nm>&lt;s_num> OTAL&lt;/s_num>&lt;s_unitprice> 03/10/2018&lt;/s_unitprice>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 07:39:12 PM&lt;/s_price>&lt;sep/>&lt;s_nm> Delivery&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 0.00&lt;/s_price>&lt;sep/>&lt;s_nm> 10 McNuggets EVM&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 10.29&lt;/s_price>&lt;sep/>&lt;s_nm> Barbeque Sauce&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 1&lt;/s_price>&lt;sep/>&lt;s_nm> Barbeque Sauce&lt;/s_nm>&lt;s_num> 1&lt;/s_cnt>&lt;s_price> 0.40&lt;/s_price>&lt;sep/>&lt;s_nm> L Coke&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 0.40&lt;/s_price>&lt;sep/>&lt;s_nm> M French Fries&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 3.99&lt;/s_price>&lt;sep/>&lt;s_nm> HM GrChS S-Fry Yog&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;sep/>&lt;s_nm> Smoonya&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;sep/>&lt;s_nm> HM Apple Juice&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 2.89&lt;/s_price>&lt;sep/>&lt;s_nm> Cookies&lt;/s_nm>&lt;s_cnt> 6&lt;/s_cnt>&lt;s_price> 2.89&lt;/s_price>&lt;sep/>&lt;s_nm> Choc Chip Cookie&lt;/s_nm>&lt;s_cnt> 6&lt;/s_cnt>&lt;s_price> 1.19&lt;/s_price>&lt;sep/>&lt;s_nm> Baked Apple Pie&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 3.29&lt;/s_price>&lt;sep/>&lt;s_nm> French Fries&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 2.99&lt;/s_price>&lt;sep/>&lt;s_nm> Iced Tea&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 2.99&lt;/s_price>&lt;/s_menu>&lt;s_sub_total>&lt;s_subtotal_price> 25.04&lt;/s_subtotal_price>&lt;s_tax_price> 1.11&lt;/s_tax_price>&lt;/s_sub_total>&lt;s_total>&lt;s_total_price> 26.15&lt;/s_total_price>&lt;s_changeprice> 0.00&lt;/s_changeprice>&lt;s_creditcardprice> 26.15&lt;/s_creditcardprice>&lt;/s_total><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以看到，对应的就是OCR后的文本结果，里面的确有对应我们去的店铺的名字和时间，以及消费的金额。</p><p>围绕OpenAI以及整个大语言模型的生态还在快速发展中，所以llama-index这个库也在快速迭代。我自己在使用的过程中，也遇到各种各样的小Bug。对于中文的支持也有各种各样的小缺陷。不过，作为开源项目，它已经有一个很不错的生态了，特别是提供了大量的DataConnector，既包括PDF、ePub这样的电子书格式，也包括YouTube、Notion、MongoDB这样外部的数据源、API接入的数据，或者是本地数据库的数据。你可以在 <a href="https://llamahub.ai/">llamahub.ai</a> 看到社区开发出来的读取各种不同数据源格式的DataConnector。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d19d1dd9888785c85bbc3ac5aec08191.png" alt="图片"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，相信经过这一讲，你已经能够上手使用llama-index这个Python包了。通过它，你可以快速将外部的资料库变成索引，并且通过它提供的query接口快速向文档提问，也能够通过将文本分片，并通过树状的方式管理索引并进行小结。</p><p>llama-index还有很多其他功能，这个Python库仍然在发展过程中，不过已经非常值得拿来使用，加速你开发大语言模型类的相关应用了。相关的文档，可以在 <a href="https://gpt-index.readthedocs.io/en/latest/">官网</a> 看到。对应的代码也是开源的，遇到问题也可以直接去 <a href="https://github.com/jerryjliu/gpt_index">源代码</a> 里一探究竟。</p><p>llama-index其实给出了一种使用大语言模型的设计模式，我称之为“第二大脑”模式。通过先将外部的资料库索引，然后每次提问的时候，先从资料库里通过搜索找到有相关性的材料，然后再通过AI的语义理解能力让AI基于搜索到的结果来回答问题。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e4ae3fbeaa82d82e317dfcef40679f6c.jpg" alt="图片"></p><p>其中，前两步的索引和搜索，我们可以使用OpenAI的Embedding接口，也可以使用其它的大语言模型的Embedding，或者传统的文本搜索技术。只有最后一步的问答，往往才必须使用OpenAI的接口。我们不仅可以索引文本信息，也可以通过其他的模型来把图片变成文本进行索引，实现所谓的多模态功能。</p><p>希望通过今天的这几个例子，你也能开始建立起自己的“第二大脑”资料库，能够将自己的数据集交给AI进行索引，获得一个专属于你自己的AI。</p><h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><ol><li>llama-index的生态，不仅支持各种各样的DataConnector去加载数据，后端也支持各种形式的索引，比如在语义搜索里面我们介绍过的 Faiss、Pinecone、Weaviate 它都是支持的。除了这些之外，你能看看 llama-index 还有哪些形式的索引吗？除了进行问答和文章概括之外，你觉得这个库还能帮助我们做什么事情？</li><li>现在有很多应用，在你把文档上传之后，还会给你一系列的提示，告诉你可以向对应的书或者论文问什么问题。比如 <a href="https://scispace.com/">SCISPACE</a>，你能想想这些问题是怎么来的吗？</li></ol><p>期待能在评论区看到你的分享，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>llama-index的功能非常强大，并且源代码里也专门提供了示例部分。你可以去看一下它的官方文档以及示例，了解它可以用来干什么。</p><ol><li>官方文档： <a href="https://gpt-index.readthedocs.io/en/latest/">https://gpt-index.readthedocs.io/en/latest/</a></li><li>源码以及示例： <a href="https://github.com/jerryjliu/llama_index">https://github.com/jerryjliu/llama_index</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;10｜AI连接外部资料库，让Llama-Index带你阅读一本书&quot;&gt;&lt;a href=&quot;#10｜AI连接外部资料库，让Llama-Index带你阅读一本书&quot; class=&quot;headerlink&quot; title=&quot;10｜AI连接外部资料库，让Llama Index带你阅</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>12｜让AI帮你写个小插件，轻松处理Excel文件</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3688931663.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3688931663.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.907Z</updated>
    
    <content type="html"><![CDATA[<h1 id="12｜让AI帮你写个小插件，轻松处理Excel文件"><a href="#12｜让AI帮你写个小插件，轻松处理Excel文件" class="headerlink" title="12｜让AI帮你写个小插件，轻松处理Excel文件"></a>12｜让AI帮你写个小插件，轻松处理Excel文件</h1><p>你好，我是徐文浩。</p><p>过去的十多讲里，我为你介绍了各种利用大语言模型的方法和技巧。特别是在过去两讲里，我们也尝试更加深入地利用开源代码帮我们完成一些工作。通过llama-index这样的开源库，我们能够将自己的数据和大语言模型连接在一起。通过sentence_transformers这样的开源库和ChatGLM这样的开源大语言模型，不依赖OpenAI，我们也可以完成简单的电商FAQ的问答。</p><p>不过，这里面的代码，都是我们自己写的。虽然我们已经用了像Colab这样的在线Notebook工具，但是这些对非技术人员来说还是有一些门槛的。如果想要让广告投放、产品运营的同事们用起来，我们还需要专门做一个适合他们习惯的界面。</p><p>其实，普通业务人员最常用的界面就是像Excel或者Chrome浏览器这样人手一个的应用。而且这些产品都有自己的插件机制。那么今天，我们就来为Excel写一个插件，让业务人员不需要懂代码也能随时使用ChatGPT来辅助完成工作，提高效率。而且，这个插件的代码也不是由我们自己来写，而是让ChatGPT来帮我们写。 <strong>在这个过程中，你会看到我们如何利用ChatGPT逐步探索，使用我们并不熟悉，甚至完全不会的编程语言和工具完成任务。</strong></p><p><strong>所以这一讲，不是一堂技能课，而是一堂方法和思维课。</strong></p><h2 id="让我们再来看看那只发光的青蛙"><a href="#让我们再来看看那只发光的青蛙" class="headerlink" title="让我们再来看看那只发光的青蛙"></a>让我们再来看看那只发光的青蛙</h2><p>在课程的 <a href="https://time.geekbang.org/column/article/641742">第 1 讲</a> 里面，我就给你举过一个例子。我们使用ChatGPT的API，在一个商品上实现了标题翻译、卖点撰写和售价预估的功能。这个需求，其实是很多跨境电商的运营人员都能用上的。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/904194dc1f1c932735fd058a2f5dd7b5.png" alt="图片"></p><p>代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token keyword">import</span> osopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>COMPLETION_MODEL <span class="token operator">=</span> <span class="token string">"text-davinci-003"</span>prompt <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具1. Compose human readable product title used on Amazon in english within 20 words.2. Write 5 selling points for the products in Amazon.3. Evaluate a price range for this product in U.S.Output the result in json format with three properties called title, selling_points and price_range"""</span><span class="token keyword">def</span> <span class="token function">get_response</span><span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">:</span>    completions <span class="token operator">=</span> openai<span class="token punctuation">.</span>Completion<span class="token punctuation">.</span>create <span class="token punctuation">(</span>        engine<span class="token operator">=</span>COMPLETION_MODEL<span class="token punctuation">,</span>        prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>        max_tokens<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>        n<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>        stop<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        temperature<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span>    message <span class="token operator">=</span> completions<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text    <span class="token keyword">return</span> message<span class="token keyword">print</span><span class="token punctuation">(</span>get_response<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span>    <span class="token string">"title"</span><span class="token punctuation">:</span> <span class="token string">"Glow-in-the-Dark Inflatable PVC Frog Night Market Hot Selling Water Toy for Kids"</span><span class="token punctuation">,</span>    <span class="token string">"selling_points"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>        <span class="token string">"Made of durable PVC material"</span><span class="token punctuation">,</span>        <span class="token string">"Glow-in-the-dark design for night play"</span><span class="token punctuation">,</span>        <span class="token string">"Inflatable design for easy storage and transport"</span><span class="token punctuation">,</span>        <span class="token string">"Perfect for water play and outdoor activities"</span><span class="token punctuation">,</span>        <span class="token string">"Great gift for kids"</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token string">"price_range"</span><span class="token punctuation">:</span> <span class="token string">"$10 - $20"</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>但是，让运营人员去学会用Python写程序，的确门槛还是太高了。但是，如果每件商品都把同样的提示语往ChatGPT的界面里复制粘贴，效率又太低了一点。那这个时候，其实Excel就是一个很好的选择了。</p><p>我们的需求很简单，就是想把所有想要翻译和寻找卖点的商品标题，都放在Excel文件的第一列里面，然后让AI根据我们的需要填后面几列就好了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ae88eee99751e5781901e43d8b79edc0.png" alt="图片"></p><p>不过，其实我和你一样，并没有写过Excel的插件。所以虽然我知道这事儿大概是可以做到的，但是具体怎么做，我也只能来问问ChatGPT。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/589cf7be4f2819dcf559fe09ea4e2542.png" alt="图片"></p><p>我直接把这个需求扔给了ChatGPT，请它直接为我写个插件。我输入了一句提示语。</p><p>提示语：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">请你帮我写一个Excel插件，能够将商品标题翻译成英文，寻找商品里面的卖点，以及确定商品的售价<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可惜，答案不尽如人意。ChatGPT不仅只是泛泛地告诉我们可以用翻译的API，而且它在自然语言处理部分，还编造了一段代码。它实际介绍的NLTK，spaCy等等，都是Python的开源库，是没有VBA代码的。</p><p>不过，尽管这个回答并不是正确答案，但它还是给我们提供了一个很有用的信息，那就是Excel里我们可以通过VBA来运行程序。这其实也是我们使用ChatGPT这样的AI助理的时候需要注意的一点，我们并不要求它一次做对，只要每次能让我们离答案更近了一点点就好了。</p><h2 id="先来试一试简单的插件任务"><a href="#先来试一试简单的插件任务" class="headerlink" title="先来试一试简单的插件任务"></a>先来试一试简单的插件任务</h2><p>知道了Excel可以通过VBA来执行程序，我们就先通过它来写一个最简单的功能，来试试整个流程是否可以走通。所以，我给了AI这样一个指令。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">请你为我写一个Excel的VBA插件，它会将表格里面的第一列的内容，复制到第二列<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这一次，ChatGPT没有和我打官腔，而是直接给出了详细的代码，以及如何在Excel里面执行的步骤。</p><p>我也把下面截图中对应的整个对话分享到了ShareGPT里，你可以直接点击 <a href="https://shareg.pt/pGYAz2p">链接</a> 查看完整的操作方式。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/bd4c4d935cf1b85ee70393b914b5f2bf.png" alt="图片"></p><p>代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">Sub CopyFirstColumnToSecondColumn<span class="token punctuation">(</span><span class="token punctuation">)</span>    '获取第一列和第二列的范围    Dim firstColumn As Range    Dim secondColumn As Range    Set firstColumn <span class="token operator">=</span> Range<span class="token punctuation">(</span><span class="token string">"A1:A"</span> <span class="token operator">&amp;</span> Cells<span class="token punctuation">(</span>Rows<span class="token punctuation">.</span>Count<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>End<span class="token punctuation">(</span>xlUp<span class="token punctuation">)</span><span class="token punctuation">.</span>Row<span class="token punctuation">)</span>    Set secondColumn <span class="token operator">=</span> Range<span class="token punctuation">(</span><span class="token string">"B1:B"</span> <span class="token operator">&amp;</span> Cells<span class="token punctuation">(</span>Rows<span class="token punctuation">.</span>Count<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>End<span class="token punctuation">(</span>xlUp<span class="token punctuation">)</span><span class="token punctuation">.</span>Row<span class="token punctuation">)</span>    '复制第一列到第二列    secondColumn<span class="token punctuation">.</span>Value <span class="token operator">=</span> firstColumn<span class="token punctuation">.</span>ValueEnd Sub<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>根据ChatGPT给出的详细操作步骤，我们可以通过快捷键 ALT + F11 打开Excel下的VBA编辑器，然后把对应的Excel代码复制进去；也能够在Excel的表格界面里，通过快捷键 ALT + F8 唤起宏的对话框，执行这段宏。你可以试一下，这段VBA的宏的确能够将我们表格里面的第一列，复制到第二列里面。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/b43b7c4db886bb08a917534547c3038f.png" alt="图片"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/4f983f3848bf1c3894326214f7b1b43c.png" alt="图片"></p><p>虽然这段代码的功能非常简单，但是它帮助我们走通了一个基本路径，就是 <strong>怎么在Excel里面撰写VBA代码并且执行</strong>。这种方法也是使用AI写代码时一个常用的方式，我们先通过一些简单的任务来试一试，看看AI是否能把路径走通，还是说它只会胡扯。如果能够走通的话，意味着我们又朝最终答案靠近了一步。而如果是胡扯的话，你也可以早点另请高明。</p><h2 id="拆解想要完成的代码功能，分步骤让AI完成程序"><a href="#拆解想要完成的代码功能，分步骤让AI完成程序" class="headerlink" title="拆解想要完成的代码功能，分步骤让AI完成程序"></a>拆解想要完成的代码功能，分步骤让AI完成程序</h2><p>在试过VBA代码能在Excel里正常运行之后，我们就可以开始尝试让ChatGPT帮我们写正式的代码了。这一次，我们不能和一开始一样，随便给它一个模糊的需求，而是应该拆解一下需求。在提示词里，更准确地描述我们希望ChatGPT写出的代码完成什么功能，包括我们希望它使用的具体步骤和方法。这样，它才有可能写出正确的代码。所以这一次我给到的提示语是这样的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">请为我写这样一段在Excel里面运行的VBA程序：<span class="token number">1.</span> 读取表格里面的第一列的内容<span class="token number">2.</span> 内容的每一行，都是一个商品标题。对这个商品标题，通过下面的Prompt，调用OpenAI的API拿到返回结果：<span class="token triple-quoted-string string">"""Consideration proudct : 商品标题1. Compose human readable product title used on Amazon in english within 20 words.2. Write 5 selling points for the products in Amazon.3. Evaluate a price range for this product in U.S.Output the result in json format with three properties called title, selling_points and price_range"""</span><span class="token number">3.</span> 将对应API的返回结果，插入到表格的第二列里<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>生成的代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">Option ExplicitSub GetOpenAIResults<span class="token punctuation">(</span><span class="token punctuation">)</span>    'Declare variables    Dim wb As Workbook    Dim ws As Worksheet    Dim lastRow As Long    Dim i As Long    Dim productTitle As String    Dim sellingPoints As String    Dim priceRange As String    Dim outputJSON As String    Dim response As String    Dim request As Object    Dim url As String    Dim apiKey As String    'Set variables    Set wb <span class="token operator">=</span> ThisWorkbook    Set ws <span class="token operator">=</span> wb<span class="token punctuation">.</span>ActiveSheet    lastRow <span class="token operator">=</span> ws<span class="token punctuation">.</span>Cells<span class="token punctuation">(</span>ws<span class="token punctuation">.</span>Rows<span class="token punctuation">.</span>Count<span class="token punctuation">,</span> <span class="token string">"A"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>End<span class="token punctuation">(</span>xlUp<span class="token punctuation">)</span><span class="token punctuation">.</span>Row    apiKey <span class="token operator">=</span> <span class="token string">"YOUR_OPENAI_API_KEY"</span>    url <span class="token operator">=</span> <span class="token string">"https://api.openai.com/v1/engines/davinci-codex/completions"</span>    'Loop through each row <span class="token keyword">in</span> column A    For i <span class="token operator">=</span> <span class="token number">1</span> To lastRow        'Get the product title <span class="token keyword">from</span> column A        productTitle <span class="token operator">=</span> ws<span class="token punctuation">.</span>Cells<span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>Value        'Set up the OpenAI API request        Set request <span class="token operator">=</span> CreateObject<span class="token punctuation">(</span><span class="token string">"MSXML2.XMLHTTP"</span><span class="token punctuation">)</span>        request<span class="token punctuation">.</span>Open <span class="token string">"POST"</span><span class="token punctuation">,</span> url<span class="token punctuation">,</span> <span class="token boolean">False</span>        request<span class="token punctuation">.</span>setRequestHeader <span class="token string">"Content-Type"</span><span class="token punctuation">,</span> <span class="token string">"application/json"</span>        request<span class="token punctuation">.</span>setRequestHeader <span class="token string">"Authorization"</span><span class="token punctuation">,</span> <span class="token string">"Bearer "</span> <span class="token operator">&amp;</span> apiKey        'Send the request to OpenAI API        request<span class="token punctuation">.</span>send <span class="token string">"&#123;"</span><span class="token string">"prompt"</span><span class="token string">": "</span><span class="token string">"Consideration proudct : "</span> <span class="token operator">&amp;</span> productTitle <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"1. Compose human readable product title used on Amazon in english within 20 words."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"2. Write 5 selling points for the products in Amazon."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"3. Evaluate a price range for this product in U.S."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"Output the result in json format with three properties called title, selling_points and price_range"</span><span class="token string">","</span><span class="token string">"temperature"</span><span class="token string">": 0.7,"</span><span class="token string">"max_tokens"</span><span class="token string">": 1024&#125;"</span>        'Get the response <span class="token keyword">from</span> OpenAI API        response <span class="token operator">=</span> request<span class="token punctuation">.</span>responseText        'Extract the title<span class="token punctuation">,</span> selling points<span class="token punctuation">,</span> <span class="token keyword">and</span> price <span class="token builtin">range</span> <span class="token keyword">from</span> the response        productTitle <span class="token operator">=</span> ExtractJSONValue<span class="token punctuation">(</span>response<span class="token punctuation">,</span> <span class="token string">"title"</span><span class="token punctuation">)</span>        sellingPoints <span class="token operator">=</span> ExtractJSONValue<span class="token punctuation">(</span>response<span class="token punctuation">,</span> <span class="token string">"selling_points"</span><span class="token punctuation">)</span>        priceRange <span class="token operator">=</span> ExtractJSONValue<span class="token punctuation">(</span>response<span class="token punctuation">,</span> <span class="token string">"price_range"</span><span class="token punctuation">)</span>        'Create the JSON output string        outputJSON <span class="token operator">=</span> <span class="token string">"&#123;"</span><span class="token string">"title"</span><span class="token string">": "</span><span class="token string">""</span> <span class="token operator">&amp;</span> productTitle <span class="token operator">&amp;</span> <span class="token triple-quoted-string string">""",""selling_points"": """</span> <span class="token operator">&amp;</span> sellingPoints <span class="token operator">&amp;</span> <span class="token triple-quoted-string string">""",""price_range"": """</span> <span class="token operator">&amp;</span> priceRange <span class="token operator">&amp;</span> <span class="token triple-quoted-string string">"""&#125;"        'Insert the JSON output into column B        ws.Cells(i, 2).Value = outputJSON    Next iEnd SubFunction ExtractJSONValue(jsonString As String, key As String) As String    'Declare variables    Dim jsonRegex As Object    Dim matchCollection As Object    Dim match As Object    'Set variables    Set jsonRegex = CreateObject("VBScript.RegExp")    jsonRegex.Global = True    jsonRegex.Pattern = """</span><span class="token string">" &amp; key &amp; "</span><span class="token string">""</span>\s<span class="token operator">*</span><span class="token punctuation">:</span>\s<span class="token operator">*</span><span class="token string">""</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">^</span><span class="token string">""</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token punctuation">)</span><span class="token string">""</span>"    Set matchCollection <span class="token operator">=</span> jsonRegex<span class="token punctuation">.</span>Execute<span class="token punctuation">(</span>jsonString<span class="token punctuation">)</span>    'Check <span class="token keyword">if</span> the key was found <span class="token keyword">in</span> the JSON string    If matchCollection<span class="token punctuation">.</span>Count <span class="token operator">></span> <span class="token number">0</span> Then        Set <span class="token keyword">match</span> <span class="token operator">=</span> matchCollection<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        ExtractJSONValue <span class="token operator">=</span> <span class="token keyword">match</span><span class="token punctuation">.</span>SubMatches<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    Else        ExtractJSONValue <span class="token operator">=</span> <span class="token string">""</span>    End<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>ChatGPT这一次生成了一大堆的代码，我们也不知道对不对。不过可以先把它复制到我们的Excel里，然后做一件事情，就是把代码里面，写着“YOUR_OPENAI_API_KEY”的地方，换成我们自己的Key，不然程序是不会运行通过的。然后，我们就可以再次尝试通过ALT+F8唤起并运行这个宏了。</p><p>不过，在运行代码之前需要注意，如果你的Excel里面有很多重要的数据，你最好是备份好你的数据再来运行。毕竟，我们没有仔细审核过ChatGPT写出来的代码，只是简单地复制粘贴。要是它对我们的数据造成什么破坏，那可就得不偿失了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/7d338fd60f7e2223e9d5ab5bb4e1a677.png" alt="图片"></p><p>不过，这一次我们运气不太好，程序没有运行成功，还报错了。我们的Excel里面出现了一个VBA的弹窗，有一段错误消息：“缺少 End Function”。我上一次写VBA代码，可能还要追溯到2005年在微软当实习生的时候。所以看到这个报错，我也没法立刻知道该怎么解决。</p><p>不过这个代码是ChatGPT写的，那出了错也应该让它负责。我们直接把对应的错误信息，填到ChatGPT的对话窗口里，让它看看怎么解决。这一次，它还挺礼貌，意识到了自己的错误，给出的代码里面缺少了“End Function”。所以，它会重新生成一份语法没有问题的代码给到我们。</p><p>提示语：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">缺少End Function<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>代码：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">……    Else        ExtractJSONValue = ""    End IfEnd Function<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：整个对话参见链接 <a href="https://sharegpt.com/c/joQQj7P">https://sharegpt.com/c/joQQj7P</a></p><p>我们再把新的代码贴到VBA编辑器里面，再重新运行一次。这一次，程序运行通过了。但是，还是没有得到我们想要的结果。在第二列里，我们看到的是只有JSON结构，没有实际内容的JSON字符串。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/7192204874d2b38b1a50bea79ec2045f.png" alt="图片"></p><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;"title": "","selling_points": "","price_range": ""&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个时候，我们就得先停下来，回头看看ChatGPT生成的代码到底干了些什么。虽然我们并不熟悉VBA的语法，但是如果你稍微有些编程的知识和经验，根据现在的代码其实大概就能够知道ChatGPT的这个程序是怎么回事儿了，现在我们就来把它拆解开。</p><ol><li>整个程序由两个函数组成，GetOpenAIResults 是主函数，是程序的主体执行结构。ExtractJSONValue是一个功能函数，从API调用的返回结果里面，通过正则表达式去提取返回结果里面的内容。</li><li>GetOpenAIResults这个主函数的结构也非常简单，除了一开始的一系列变量定义，其实就是做了这样几件事情。</li></ol><ul><li>通过一个For循环，遍历第一列单元格里面的值。</li><li>对每一个值，都构造了一个HTTP的请求，调用OpenAI的API。</li><li>对于拿到的返回结果，通过ExtractJSONValue函数提取里面的内容，然后再重新拼装成一个JSON。</li><li>最终将这个outputJSON的输出结果，填到第二列的单元格里面。</li></ul><p>理解了代码的整体含义，还有拿到的输出结果，我们就有了调查问题的方向。我们想要看看在调用完OpenAI的API之后，拿到的返回结果是什么。看看是调用出错了，还是解析返回结果出错了？</p><p>于是，我们把 GetOpenAIResults 函数最后填入第二列单元格的代码修改一下，把调用OpenAI的API拿到的返回结果，填到第三列里。修改完之后，我们重新运行宏，看看第三列里的返回结果是什么。</p><p>代码：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">……        ws.Cells(i, 2).Value = outputJSON        ws.Cells(i, 3).Value = response……<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;    "error": &#123;        "message": "We could not parse the JSON body of your request. (HINT: This likely means you aren't using your HTTP library correctly. The OpenAI API expects a JSON payload, but what was sent was not valid JSON. If you have trouble figuring out how to fix this, please send an email to support@openai.com and include any relevant code you'd like help with.)",        "type": "invalid_request_error",        "param": null,        "code": null    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从这个输出结果里面看到，错误信息是说，OpenAI不能够解析我们输入的JSON Body。那么，我们就再修改一下代码，把我们输入的JSON放到第四列里，看看是不是这部分代码ChatGPT写错了。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">……        'Send the request to OpenAI API        requestJSON = "&#123;""prompt"": ""Consideration proudct : " &amp; productTitle &amp; vbNewLine &amp; "1. Compose human readable product title used on Amazon in english within 20 words." &amp; vbNewLine &amp; "2. Write 5 selling points for the products in Amazon." &amp; vbNewLine &amp; "3. Evaluate a price range for this product in U.S." &amp; vbNewLine &amp; vbNewLine &amp; "Output the result in json format with three properties called title, selling_points and price_range"",""temperature"": 0.7,""max_tokens"": 1024&#125;"        request.send requestJSON        'Get the response from OpenAI API        response = request.responseText……        'Insert the JSON output into column B        ws.Cells(i, 2).Value = outputJSON        ws.Cells(i, 3).Value = response        ws.Cells(i, 4).Value = requestJSON……<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个时候我们尝试运行代码的话，VBA会提示我们“变量未定义”的报错。这是因为VBA的语法规定，所有的变量在赋值之前都需要先单独定义，所以我们还需要在代码一开始变量定义的部分，加上requestJSON的变量定义。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/952ee39f4297a9165ca942c713ba8088.png" alt="图片"></p><p>代码：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">……    Dim apiKey As String    Dim requestJSON As String    'Set variables……<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后我们重新运行代码，就会在第四列里，得到我们输入的JSON数据。我们肉眼看不出这个JSON是不是合法，只能尝试让ChatGPT帮我们看一下这个JSON是否合法了。不过ChatGPT告诉我们这个JSON是合法的，这个时候，我们又一次陷入了僵局。</p><p>JSON数据：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;"prompt": "Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具1. Compose human readable product title used on Amazon in english within 20 words.2. Write 5 selling points for the products in Amazon.3. Evaluate a price range for this product in U.S.Output the result in json format with three properties called title, selling_points and price_range","temperature": 0.7,"max_tokens": 1024&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e1c981fe33dda7a93067049031515f53.png" alt="图片"></p><h2 id="简化问题寻找思路"><a href="#简化问题寻找思路" class="headerlink" title="简化问题寻找思路"></a>简化问题寻找思路</h2><p>我们并不擅长撰写VBA应用，所以这个时候，我们的思路还是要 <strong>把问题简化。</strong> 我们拿一个最简单的JSON来向OpenAI发起请求，看看结果是不是还会报出相同的错误。</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet">……        <span class="token comment">'Send the request to OpenAI API</span>        requestJSON <span class="token operator">=</span> <span class="token string">"&#123;""prompt"": ""Consideration proudct : "</span> <span class="token operator">&amp;</span> productTitle <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"1. Compose human readable product title used on Amazon in english within 20 words."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"2. Write 5 selling points for the products in Amazon."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"3. Evaluate a price range for this product in U.S."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"Output the result in json format with three properties called title, selling_points and price_range"",""temperature"": 0.7,""max_tokens"": 1024&#125;"</span>        requestJSON <span class="token operator">=</span> <span class="token string">"&#123;""prompt"": ""How are you?""&#125;"</span>        request.send requestJSON        <span class="token comment">'Get the response from OpenAI API</span>        response <span class="token operator">=</span> request.responseText……<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>修改之后，我们发现，API调用的返回结果虽然还是个报错。但是这个报错变了，而且这个报错我们看得懂了。</p><p>Excel里面的response内容：</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet"><span class="token punctuation">&#123;</span>    <span class="token string">"error"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>        <span class="token string">"message"</span><span class="token punctuation">:</span> <span class="token string">"The model: `davinci-codex` does not exist"</span><span class="token punctuation">,</span>        <span class="token string">"type"</span><span class="token punctuation">:</span> <span class="token string">"invalid_request_error"</span><span class="token punctuation">,</span>        <span class="token string">"param"</span><span class="token punctuation">:</span> null<span class="token punctuation">,</span>        <span class="token string">"code"</span><span class="token punctuation">:</span> <span class="token string">"model_not_found"</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个报错告诉我们，对应的davinci-codex模型不存在。的确，最近OpenAI把CodeX的模型下线了，那我们就把模型修改成text-davinci-003这个我们之前常用的模型。</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet">    url <span class="token operator">=</span> <span class="token string">"https://api.openai.com/v1/engines/text-davinci-003/completions"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后我们再重新运行一下代码，这时，我们第一次拿到了一个正确的来自AI的回复。</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet"><span class="token punctuation">&#123;</span><span class="token string">"id"</span><span class="token punctuation">:</span><span class="token string">"cmpl-70ZEVctFduMnv6D1WIz4iffTZdaGl"</span><span class="token punctuation">,</span><span class="token string">"object"</span><span class="token punctuation">:</span><span class="token string">"text_completion"</span><span class="token punctuation">,</span><span class="token string">"created"</span><span class="token punctuation">:</span><span class="token number">1680369791</span><span class="token punctuation">,</span><span class="token string">"model"</span><span class="token punctuation">:</span><span class="token string">"text-davinci-003"</span><span class="token punctuation">,</span><span class="token string">"choices"</span><span class="token punctuation">:</span>[<span class="token punctuation">&#123;</span><span class="token string">"text"</span><span class="token punctuation">:</span><span class="token string">"\n\nI'm doing well, thanks for asking!"</span><span class="token punctuation">,</span><span class="token string">"index"</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token string">"logprobs"</span><span class="token punctuation">:</span>null<span class="token punctuation">,</span><span class="token string">"finish_reason"</span><span class="token punctuation">:</span><span class="token string">"stop"</span><span class="token punctuation">&#125;</span>]<span class="token punctuation">,</span><span class="token string">"usage"</span><span class="token punctuation">:</span><span class="token punctuation">&#123;</span><span class="token string">"prompt_tokens"</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token string">"completion_tokens"</span><span class="token punctuation">:</span><span class="token number">11</span><span class="token punctuation">,</span><span class="token string">"total_tokens"</span><span class="token punctuation">:</span><span class="token number">15</span><span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>不过这个返回内容，针对的还是我们刚才构造的最简单的Prompt：“How are you?”的回答，我们把使用的JSON切换回去，再运行一下，看看结果是怎么样的。</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet">……        <span class="token comment">'Send the request to OpenAI API</span>        requestJSON <span class="token operator">=</span> <span class="token string">"&#123;""prompt"": ""Consideration proudct : "</span> <span class="token operator">&amp;</span> productTitle <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"1. Compose human readable product title used on Amazon in english within 20 words."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"2. Write 5 selling points for the products in Amazon."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"3. Evaluate a price range for this product in U.S."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"Output the result in json format with three properties called title, selling_points and price_range"",""temperature"": 0.7,""max_tokens"": 1024&#125;"</span>        request.send requestJSON        <span class="token comment">'Get the response from OpenAI API</span>        response <span class="token operator">=</span> request.responseText……<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>不幸的是，这一次我们还是拿到了一段和之前相同的报错，告诉我们JSON的格式解析不了。</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet"><span class="token punctuation">&#123;</span>    <span class="token string">"error"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>        <span class="token string">"message"</span><span class="token punctuation">:</span> <span class="token string">"We could not parse the JSON body of your request. (HINT: This likely means you aren't using your HTTP library correctly. The OpenAI API expects a JSON payload, but what was sent was not valid JSON. If you have trouble figuring out how to fix this, please send an email to support@openai.com and include any relevant code you'd like help with.)"</span><span class="token punctuation">,</span>        <span class="token string">"type"</span><span class="token punctuation">:</span> <span class="token string">"invalid_request_error"</span><span class="token punctuation">,</span>        <span class="token string">"param"</span><span class="token punctuation">:</span> null<span class="token punctuation">,</span>        <span class="token string">"code"</span><span class="token punctuation">:</span> null    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>但是这个时候，我们已经不慌了。因为我们之前已经用一个简单的JSON拿到了ChatGPT的正确回答。所以，我们可以确定，的确现在的JSON是有问题的。我们只要能够找到这个JSON的格式问题，相信我们离正确答案就不远了。</p><p>既然AI前面说这个JSON格式是合法的，那我们就不妨让它来给我们生成一个同样内容的VBA的JSON字符串好了，我们也把问题简化，只关心我们的Prompt部分。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ca85d83b3a6fe8212a756fb50a3cf4e6.png" alt="图片"></p><p>然后我们再修改一下VBA代码，重新运行一次。</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet">……        <span class="token comment">'Send the request to OpenAI API</span>        requestJSON <span class="token operator">=</span> <span class="token string">"&#123;""prompt"": ""Consideration proudct : "</span> <span class="token operator">&amp;</span> productTitle <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"1. Compose human readable product title used on Amazon in english within 20 words."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"2. Write 5 selling points for the products in Amazon."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"3. Evaluate a price range for this product in U.S."</span> <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> vbNewLine <span class="token operator">&amp;</span> <span class="token string">"Output the result in json format with three properties called title, selling_points and price_range"",""temperature"": 0.7,""max_tokens"": 1024&#125;"</span>        requestJSON <span class="token operator">=</span> <span class="token string">"&#123;""prompt"": ""Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具\r\n1. Compose human readable product title used on Amazon in english within 20 words.\r\n2. Write 5 selling points for the products in Amazon.\r\n3. Evaluate a price range for this product in U.S.\r\n\r\nOutput the result in json format with three properties called title, selling_points and price_range""&#125;"</span>        request.send requestJSON        <span class="token comment">'Get the response from OpenAI API</span>        response <span class="token operator">=</span> request.responseText……<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而这么一运行，我们就得到了一个正常的、有意义的JSON返回值。那这个时候，对于问题的定位就进一步缩小了。我们只要看看上下两个字符串有什么不一样就好了。仔细观察一下，你就会发现，上下两边使用的换行符不一样。上面使用的是 vbnewline，而下面使用的是\r\n。那我们就把上面的换行符设成和下面完全一致的，再来试试看。</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet">……        requestJSON <span class="token operator">=</span> <span class="token string">"&#123;""prompt"": ""Consideration proudct : "</span> <span class="token operator">&amp;</span> productTitle <span class="token operator">&amp;</span> <span class="token string">"\r\n1. Compose human readable product title used on Amazon in english within 20 words.\r\n2. Write 5 selling points for the products in Amazon.\r\n3. Evaluate a price range for this product in U.S.\r\n\r\nOutput the result in json format with three properties called title, selling_points and price_range.\r\n"",""temperature"": 0.7,""max_tokens"": 1024&#125;"</span>        request.send requestJSON……<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>再运行一次代码，这次成功了。我们在第三列拿到了一个正常的OpenAI的返回内容，里面的确有我们想要的英文标题、卖点和售价。</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet"><span class="token punctuation">&#123;</span><span class="token string">"id"</span><span class="token punctuation">:</span><span class="token string">"cmpl-70ZaUPXSb8nU39jmpNkhl59bMsG4A"</span><span class="token punctuation">,</span><span class="token string">"object"</span><span class="token punctuation">:</span><span class="token string">"text_completion"</span><span class="token punctuation">,</span><span class="token string">"created"</span><span class="token punctuation">:</span><span class="token number">1680371154</span><span class="token punctuation">,</span><span class="token string">"model"</span><span class="token punctuation">:</span><span class="token string">"text-davinci-003"</span><span class="token punctuation">,</span><span class="token string">"choices"</span><span class="token punctuation">:</span>[<span class="token punctuation">&#123;</span><span class="token string">"text"</span><span class="token punctuation">:</span><span class="token string">"\r\n\r\n&#123;\r\n    \"</span>title\<span class="token string">": \"</span>Premium Handcrafted Aromatherapy Essential Oil Diffuser\<span class="token string">", \r\n    \"</span>selling_points\<span class="token string">": [\"</span>Natural Ultrasonic Operation\<span class="token string">", \"</span><span class="token number">7</span> Colorful LED Lights\<span class="token string">", \"</span>Auto Shutoff <span class="token keyword">Timer</span>\<span class="token string">", \"</span>Whisper Quiet Operation\<span class="token string">", \"</span>Easy <span class="token keyword">to</span> Clean\<span class="token string">"], \r\n    \"</span>price_range\<span class="token string">": \"</span>$<span class="token number">25</span> <span class="token operator">-</span> $<span class="token number">50</span>\<span class="token string">"\r\n&#125;"</span><span class="token punctuation">,</span><span class="token string">"index"</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token string">"logprobs"</span><span class="token punctuation">:</span>null<span class="token punctuation">,</span><span class="token string">"finish_reason"</span><span class="token punctuation">:</span><span class="token string">"stop"</span><span class="token punctuation">&#125;</span>]<span class="token punctuation">,</span><span class="token string">"usage"</span><span class="token punctuation">:</span><span class="token punctuation">&#123;</span><span class="token string">"prompt_tokens"</span><span class="token punctuation">:</span><span class="token number">89</span><span class="token punctuation">,</span><span class="token string">"completion_tokens"</span><span class="token punctuation">:</span><span class="token number">84</span><span class="token punctuation">,</span><span class="token string">"total_tokens"</span><span class="token punctuation">:</span><span class="token number">173</span><span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>不过，我们的第二列数据的JSON还是没有具体内容，只有结构。不过这个时候，我们离自己想要的答案已经越来越近了。</p><h2 id="让AI单独写出提取函数，完成最后的程序"><a href="#让AI单独写出提取函数，完成最后的程序" class="headerlink" title="让AI单独写出提取函数，完成最后的程序"></a>让AI单独写出提取函数，完成最后的程序</h2><p>原先AI生成的解析JSON的代码，使用的是正则表达式，而不是对JSON进行反序列化。而且给运营人员用，我们也不希望再重新拼接成JSON格式。所以，这个时候我们不妨问一问ChatGPT，怎么把JSON反序列化。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/20c9a8929c24b2f458229f698430e9f7.png" alt="图片"></p><p>注：对应的整个对话参见链接 <a href="https://sharegpt.com/c/rAmlkqQ">https://sharegpt.com/c/rAmlkqQ</a></p><p>ChatGPT给了我们示例，告诉我们可以在GitHub上找到对应的函数库。如果你按图索骥，就能在 <a href="https://github.com/VBA-tools/VBA-JSON">VBA-JSON</a> 和 <a href="https://github.com/VBA-tools/VBA-Dictionary">VBA-Dictionary</a> 找到我们需要的库。并且按照文档的要求，在VBA编辑器里通过 “文件”&#x3D;&gt;“导入文件” 来导入函数库。然后，我们只要按照文档的格式小小修改一下现在的代码，就能提取到我们希望得到的JSON格式了。</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet">……    <span class="token keyword">Dim</span> requestJSON <span class="token keyword">As</span> <span class="token keyword">String</span>    <span class="token keyword">Dim</span> Json <span class="token keyword">As</span> <span class="token keyword">Object</span>    <span class="token comment">'Set variables</span>……    <span class="token comment">'Loop through each row in column A</span>    <span class="token keyword">For</span> i <span class="token operator">=</span> <span class="token number">1</span> <span class="token keyword">To</span> lastRow……        <span class="token keyword">Set</span> Json <span class="token operator">=</span> JsonConverter.ParseJson<span class="token punctuation">(</span>response<span class="token punctuation">)</span>        <span class="token comment">'Insert the JSON output into column B</span>        ws.Cells<span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>.Value <span class="token operator">=</span> Json<span class="token punctuation">(</span><span class="token string">"choices"</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token string">"text"</span><span class="token punctuation">)</span>        ws.Cells<span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>.Value <span class="token operator">=</span> response        ws.Cells<span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>.Value <span class="token operator">=</span> requestJSON<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>再运行一下，第二列里就是我们想要的JSON了。</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet"><span class="token punctuation">&#123;</span>  <span class="token string">"title"</span><span class="token punctuation">:</span> <span class="token string">"Glow In The Dark Inflatable PVC Frog Pool Toy For Kids"</span><span class="token punctuation">,</span>  <span class="token string">"selling_points"</span><span class="token punctuation">:</span> [      <span class="token string">"Made of durable and safe PVC material"</span><span class="token punctuation">,</span>      <span class="token string">"Inflatable and easy to store"</span><span class="token punctuation">,</span>      <span class="token string">"Comes with glow in the dark effect"</span><span class="token punctuation">,</span>      <span class="token string">"Ideal for both outdoor and indoor fun"</span><span class="token punctuation">,</span>      <span class="token string">"Perfect for kids of all ages"</span>  ]<span class="token punctuation">,</span>  <span class="token string">"price_range"</span><span class="token punctuation">:</span> <span class="token string">"$8 - $12"</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>那接下来，问题就很简单了。我们只需要进一步把这个JSON字符串也解析一下，然后把title、selling_points 以及 price_range解析出来，分别放到不同的列里面就可以了。如果有想不明白怎么写的代码你还是可以继续问ChatGPT。</p><p>我把要修改的代码列在了下面。</p><pre class="line-numbers language-vbnet" data-language="vbnet"><code class="language-vbnet"><span class="token keyword">Option</span> Explicit<span class="token keyword">Sub</span> GetOpenAIResults<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment">'Declare variables</span>    <span class="token keyword">Dim</span> wb <span class="token keyword">As</span> Workbook    <span class="token keyword">Dim</span> ws <span class="token keyword">As</span> Worksheet    <span class="token keyword">Dim</span> lastRow <span class="token keyword">As</span> <span class="token keyword">Long</span>    <span class="token keyword">Dim</span> i <span class="token keyword">As</span> <span class="token keyword">Long</span>    <span class="token keyword">Dim</span> productTitle <span class="token keyword">As</span> <span class="token keyword">String</span>    <span class="token keyword">Dim</span> response <span class="token keyword">As</span> <span class="token keyword">String</span>    <span class="token keyword">Dim</span> request <span class="token keyword">As</span> <span class="token keyword">Object</span>    <span class="token keyword">Dim</span> url <span class="token keyword">As</span> <span class="token keyword">String</span>    <span class="token keyword">Dim</span> apiKey <span class="token keyword">As</span> <span class="token keyword">String</span>    <span class="token keyword">Dim</span> requestJSON <span class="token keyword">As</span> <span class="token keyword">String</span>    <span class="token keyword">Dim</span> Json <span class="token keyword">As</span> <span class="token keyword">Object</span>    <span class="token comment">'Set variables</span>    <span class="token keyword">Set</span> wb <span class="token operator">=</span> ThisWorkbook    <span class="token keyword">Set</span> ws <span class="token operator">=</span> wb.ActiveSheet    lastRow <span class="token operator">=</span> ws.Cells<span class="token punctuation">(</span>ws.Rows.Count<span class="token punctuation">,</span> <span class="token string">"A"</span><span class="token punctuation">)</span>.<span class="token keyword">End</span><span class="token punctuation">(</span>xlUp<span class="token punctuation">)</span>.Row    apiKey <span class="token operator">=</span> <span class="token string">"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"</span>    url <span class="token operator">=</span> <span class="token string">"https://api.openai.com/v1/engines/text-davinci-003/completions"</span>    <span class="token comment">'Loop through each row in column A</span>    <span class="token keyword">For</span> i <span class="token operator">=</span> <span class="token number">2</span> <span class="token keyword">To</span> lastRow        <span class="token comment">'Get the product title from column A</span>        productTitle <span class="token operator">=</span> ws.Cells<span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>.Value        <span class="token comment">'Set up the OpenAI API request</span>        <span class="token keyword">Set</span> request <span class="token operator">=</span> CreateObject<span class="token punctuation">(</span><span class="token string">"MSXML2.XMLHTTP"</span><span class="token punctuation">)</span>        request.<span class="token keyword">Open</span> <span class="token string">"POST"</span><span class="token punctuation">,</span> url<span class="token punctuation">,</span> <span class="token keyword">False</span>        request.setRequestHeader <span class="token string">"Content-Type"</span><span class="token punctuation">,</span> <span class="token string">"application/json"</span>        request.setRequestHeader <span class="token string">"Authorization"</span><span class="token punctuation">,</span> <span class="token string">"Bearer "</span> <span class="token operator">&amp;</span> apiKey        <span class="token comment">'Send the request to OpenAI API</span>        requestJSON <span class="token operator">=</span> <span class="token string">"&#123;""prompt"": ""Consideration proudct : "</span> <span class="token operator">&amp;</span> productTitle <span class="token operator">&amp;</span> <span class="token string">"\r\n1. Compose human readable product title used on Amazon in english within 20 words.\r\n2. Write 5 selling points for the products in Amazon.\r\n3. Evaluate a price range for this product in U.S.\r\n\r\nOutput the result in json format with three properties called title, selling_points and price_range.\r\n"",""temperature"": 0.7,""max_tokens"": 1024&#125;"</span>        request.send requestJSON        <span class="token comment">'Get the response from OpenAI API</span>        response <span class="token operator">=</span> request.responseText        <span class="token keyword">Set</span> Json <span class="token operator">=</span> JsonConverter.ParseJson<span class="token punctuation">(</span>response<span class="token punctuation">)</span>        <span class="token keyword">Set</span> Json <span class="token operator">=</span> JsonConverter.ParseJson<span class="token punctuation">(</span>Json<span class="token punctuation">(</span><span class="token string">"choices"</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token string">"text"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment">'Insert the JSON output into column B</span>        ws.Cells<span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>.Value <span class="token operator">=</span> Json<span class="token punctuation">(</span><span class="token string">"title"</span><span class="token punctuation">)</span>        ws.Cells<span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>.Value <span class="token operator">=</span> ConcatenateArrayToString<span class="token punctuation">(</span>Json<span class="token punctuation">(</span><span class="token string">"selling_points"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        ws.Cells<span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>.Value <span class="token operator">=</span> Json<span class="token punctuation">(</span><span class="token string">"price_range"</span><span class="token punctuation">)</span>    <span class="token keyword">Next</span> i<span class="token keyword">End</span> <span class="token keyword">Sub</span><span class="token keyword">Function</span> ConcatenateArrayToString<span class="token punctuation">(</span>arr<span class="token punctuation">)</span> <span class="token keyword">As</span> <span class="token keyword">String</span>    <span class="token keyword">Dim</span> result <span class="token keyword">As</span> <span class="token keyword">String</span>    <span class="token keyword">Dim</span> i <span class="token keyword">As</span> <span class="token keyword">Long</span>    <span class="token comment">'Assuming the array is stored in a variable named "arr"</span>    <span class="token keyword">For</span> i <span class="token operator">=</span> <span class="token number">1</span> <span class="token keyword">To</span> <span class="token number">5</span>        result <span class="token operator">=</span> result <span class="token operator">&amp;</span> arr<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">&amp;</span> vbCrLf  <span class="token comment">'Use vbCrLf to add a line break after each element</span>    <span class="token keyword">Next</span> i    <span class="token comment">'The "result" variable now contains the concatenated string</span>    ConcatenateArrayToString <span class="token operator">=</span> result<span class="token keyword">End</span> <span class="token keyword">Function</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后的代码我们做了几处小的更改。</p><ol><li>遍历列表的第一列的时候，我们跳过了第一行的标题行。</li><li>去除了不再需要使用的临时变量，以及不再需要的之前AI自动生成的函数。</li><li>selling_points是一个5个元素的数组，通过一个单独的函数封装了从数组到字符串的拼装。</li></ol><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/668d97a02c3501552c890d8f7976d1f5.png" alt="图片"></p><p>VBA数组拼接字符串的工作，我们也是让ChatGPT指导我们做的</p><p>修改之后再执行一下，我们终于得到了理想的结果。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/b62c9e9c7e403f6e3c80395c349daf83.png" alt="图片"></p><p>最后整个完整功能的VBA代码，我也放在了GitHub里，你可以直接拿去运行。不过记得在运行之前，需要先安装VBA-JSON库和VBA-Dictionary库。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，这一讲到这里也就结束了。设计这节课的目的是给你演示一下 <strong>如何和ChatGPT协同编写代码</strong>，完成一个对你有价值的小插件。我们通过ChatGPT的帮助，完成了一个使用VBA来编写的Excel小插件。</p><p>我自己上一次使用VBA也大概是20年前了，所以我相信如果你有一些代码基础的话，应该和我一样，能够通过ChatGPT完成这个插件的开发。</p><p>在这个开发的过程中，我们没有去重新学习VB.NET这样的编程语言，也没有使用Google来搜索解决问题。而是完全让ChatGPT主导，只是在遇到问题的时候，我们通过缩小和简化问题来进行Debug。唯一需要撰写的一点代码，也是最后简单阅读了ChatGPT推荐给我们的VBA-JSON的示例，加了一点点JSON解析的代码而已。</p><p>希望这一讲能够打开你的思路，帮助你善用ChatGPT更快、更好地完成写代码的任务。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>最后，按照惯例还是给你留一道思考题。你有什么想要实现的插件小功能吗？无论是在Excel、浏览器或者是其他什么场景下，请试着一步一步向ChatGPT输入你的需求，让它帮你写代码，来做一个属于你自己的插件吧！</p><p>期待你把自己的体验分享到评论区，也欢迎你把这一讲分享给需要的朋友，我们下一讲再见！</p><h2 id="推荐体验"><a href="#推荐体验" class="headerlink" title="推荐体验"></a>推荐体验</h2><p>最近有一个很火的新的代码编辑器，叫做cursor.so。它背后也是GPT的模型，能够快速根据你输入的需求描述生成Python和Typescript代码。如果你没有订阅GitHub Copilot，也可以去尝试一下这个应用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;12｜让AI帮你写个小插件，轻松处理Excel文件&quot;&gt;&lt;a href=&quot;#12｜让AI帮你写个小插件，轻松处理Excel文件&quot; class=&quot;headerlink&quot; title=&quot;12｜让AI帮你写个小插件，轻松处理Excel文件&quot;&gt;&lt;/a&gt;12｜让AI帮你写个小</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>14｜链式调用，用LangChain简化多步提示语</title>
    <link href="https://zhuansun.github.io/geekbang/posts/941426346.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/941426346.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.913Z</updated>
    
    <content type="html"><![CDATA[<h1 id="14｜链式调用，用LangChain简化多步提示语"><a href="#14｜链式调用，用LangChain简化多步提示语" class="headerlink" title="14｜链式调用，用LangChain简化多步提示语"></a>14｜链式调用，用LangChain简化多步提示语</h1><p>你好，我是徐文浩。</p><p>OpenAI的大语言模型，只是提供了简简单单的Completion和Embedding这样两个核心接口。但是你也看到了，在过去的13讲里，通过合理使用这两个接口，我们完成了各种各样复杂的任务。</p><ul><li>通过提示语（Prompt）里包含历史的聊天记录，我们能够让AI根据上下文正确地回答问题。</li><li>通过将Embedding提前索引好存起来，我们能够让AI根据外部知识回答问题。</li><li>而通过多轮对话，将AI返回的答案放在新的问题里，我们能够让AI帮我们给自己的代码撰写单元测试。</li></ul><p>这些方法，也是一个实用的自然语言类应用里常见的模式。我之前也都通过代码为你演示过具体的做法。但是，如果我们每次写应用的时候，都需要自己再去OpenAI提供的原始API里做一遍，那就太麻烦了。于是，开源社区就有人将这些常见的需求和模式抽象了出来，开发了一个叫做Langchain的开源库。那么接下来，我们就来看看如何使用LangChain来快速实现之前我们利用大语言模型实现过的功能。以及我们如何进一步地，将Langchain和我们的业务系统整合，完成更复杂、更有实用价值的功能。</p><h2 id="使用Langchain的链式调用"><a href="#使用Langchain的链式调用" class="headerlink" title="使用Langchain的链式调用"></a>使用Langchain的链式调用</h2><p>如果你观察得比较仔细的话，你会发现在 <a href="https://time.geekbang.org/column/article/646363">第 11 讲</a> 我们使用llama-index的时候，就已经装好LangChain了。llama-index专注于为大语言模型的应用构建索引，虽然Langchain也有类似的功能，但这一点并不是Langchain的主要卖点。Langchain的第一个卖点其实就在它的名字里，也就是 <strong>链式调用</strong>。</p><p>我们先来看一个使用ChatGPT的例子，你就能理解为什么会有链式调用的需求了。我们知道，GPT-3的基础模型里面，中文的语料很少。用中文问它问题，很多时候它回答得不好。所以有时候，我会迂回处理一下，先把中文问题给AI，请它翻译成英文，然后再把英文问题贴进去提问，得到一个英文答案。最后，再请AI把英文答案翻译回中文。很多时候，问题的答案会更准确一点。比如，下面的截图里，我就请它简单介绍一下Stable Diffusion的原理是什么。</p><p>注：Stable Diffusion是一个热门的开源AI画图工具，后面我们在介绍用AI生成图片的时候会用到。</p><h3 id="人工链式调用"><a href="#人工链式调用" class="headerlink" title="人工链式调用"></a>人工链式调用</h3><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/b904189cc5e23c5015aae7f6736f5dba.png" alt="图片"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/272869ac3fb95c57674843264eb62c44.png" alt="图片"></p><p>如果用API来实现这个过程，其实就是一个链式调用的过程。</p><ol><li>我们先调用OpenAI，把翻译请求和原始问题组合在一起发送给AI，完成问题的中译英。</li><li>然后再把拿到的翻译好的英文问题发送给OpenAI，得到英文答案。</li><li>最后再把英文答案，和对应要求AI翻译答案的请求组合在一起，完成答案的英译中。</li></ol><h3 id="使用LLMChain进行链式调用"><a href="#使用LLMChain进行链式调用" class="headerlink" title="使用LLMChain进行链式调用"></a>使用LLMChain进行链式调用</h3><p>如果我们用代码，可以像下面这样，一步步进行。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token punctuation">,</span> os<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts <span class="token keyword">import</span> PromptTemplate<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms <span class="token keyword">import</span> OpenAI<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> LLMChainopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>llm <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">"text-davinci-003"</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>en_to_zh_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>    template<span class="token operator">=</span><span class="token string">"请把下面这句话翻译成英文： \n\n &#123;question&#125;?"</span><span class="token punctuation">,</span> input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"question"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>question_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>    template <span class="token operator">=</span> <span class="token string">"&#123;english_question&#125;"</span><span class="token punctuation">,</span> input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"english_question"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>zh_to_cn_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"english_answer"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    template<span class="token operator">=</span><span class="token string">"请把下面这一段翻译成中文： \n\n&#123;english_answer&#125;?"</span><span class="token punctuation">,</span><span class="token punctuation">)</span>question_translate_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>en_to_zh_prompt<span class="token punctuation">,</span> output_key<span class="token operator">=</span><span class="token string">"english_question"</span><span class="token punctuation">)</span>english <span class="token operator">=</span> question_translate_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>question<span class="token operator">=</span><span class="token string">"请你作为一个机器学习的专家，介绍一下CNN的原理。"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>english<span class="token punctuation">)</span>qa_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>question_prompt<span class="token punctuation">,</span> output_key<span class="token operator">=</span><span class="token string">"english_answer"</span><span class="token punctuation">)</span>english_answer <span class="token operator">=</span> qa_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>english_question<span class="token operator">=</span>english<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>english_answer<span class="token punctuation">)</span>answer_translate_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>zh_to_cn_prompt<span class="token punctuation">)</span>answer <span class="token operator">=</span> answer_translate_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>english_answer<span class="token operator">=</span>english_answer<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>answer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">Please explain the principles of CNN as an expert in Machine Learning.A Convolutional Neural Network (CNN) is a type of deep learning algorithm that is used to analyze visual imagery. It is modeled after the structure of the human visual cortex and is composed of multiple layers of neurons that process and extract features from an image. The main principle behind a CNN is that it uses convolutional layers to detect patterns in an image. Each convolutional layer is comprised of a set of filters that detect specific features in an image. These filters are then used to extract features from the image and create a feature map. The feature map is then passed through a pooling layer which reduces the size of the feature map and helps to identify the most important features in the image. Finally, the feature map is passed through a fully-connected layer which classifies the image and outputs the result.卷积神经网络（CNN）是一种深度学习算法，用于分析视觉图像。它模仿人类视觉皮层的结构，由多层神经元组成，可以处理和提取图像中的特征。CNN的主要原理是使用卷积层来检测图像中的模式。每个卷积层由一组滤波器组成，可以检测图像中的特定特征。然后使用这些滤波器从图像中提取特征，并创建特征图。然后，将特征图通过池化层传递，该层可以减小特征图的大小，并有助于识别图像中最重要的特征。最后，将特征图传递给完全连接的层，该层将对图像进行分类，并输出结果。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里的代码，我们使用了Langchain这个库，不过还没有动用它的链式调用过程。我们主要用了Langchain的三个包。</p><ol><li>LLM，也就是我们使用哪个大语言模型，来回答我们提出的问题。在这里，我们还是使用OpenAIChat，也就是最新放出来的 gpt-3.5-turbo 模型。</li><li>PromptTemplate，和我们在 <a href="https://time.geekbang.org/column/article/646363">第 11 讲</a> 里看到的llama-index的PromptTemplate是一个东西。它可以定义一个提示语模版，里面能够定义一些可以动态替换的变量。比如，代码里的question_prompt这个模版里，我们就定义了一个叫做question的变量，因为我们每次问的问题都会不一样。事实上，llamd-index里面的PromptTemplate就是对Langchain的PromptTemplate做了一层简单的封装。</li><li>主角 LLMChain，它的构造函数接收一个LLM和一个PromptTemplate作为参数。构造完成之后，可以直接调用里面的run方法，将PromptTemplate需要的变量，用K&#x3D;&gt;V对的形式传入进去。返回的结果，就是LLM给我们的答案。</li></ol><p>不过如果看上面这段代码，我们似乎只是对OpenAI的API做了一层封装而已。我们构建了3个LLMChain，然后按照顺序调用，每次拿到答案之后，再作为输入，交给下一个LLM调用。感觉好像更麻烦了，没有减少什么工作量呀？</p><p>别着急，这是因为我们还没有真正用上LLMChain的“链式调用”功能，而用这个功能，只需要加上一行小小的代码。我们用一个叫做SimpleSequentialChain的LLMChain类，把我们要按照顺序依次调用的三个LLMChain放在一个数组里，传给这个类的构造函数。</p><p>然后对于这个对象，我们调用run方法，把我们用中文问的问题交给它。这个时候，这个SimpleSequentialChain，就会按照顺序开始调用chains这个数组参数里面包含的其他LLMChain。并且，每一次调用的结果，会存储在这个Chain构造时定义的output_key参数里。而下一个调用的LLMChain，里面模版内的变量如果有和之前的output_key名字相同的，就会用output_key里存入的内容替换掉模版内变量所在的占位符。</p><p>这次，我们只向这个SimpleSequentialChain调用一次run方法，把一开始的问题交给它就好了。后面根据答案去问新的问题，这个LLMChain会自动地链式搞定。我在这里把日志的Verbose模式打开了，你在输出的过程中，可以看到其实这个LLMChain是调用了三次，并且中间两次的返回结果你也可以一并看到。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> SimpleSequentialChainchinese_qa_chain <span class="token operator">=</span> SimpleSequentialChain<span class="token punctuation">(</span>    chains<span class="token operator">=</span><span class="token punctuation">[</span>question_translate_chain<span class="token punctuation">,</span> qa_chain<span class="token punctuation">,</span> answer_translate_chain<span class="token punctuation">]</span><span class="token punctuation">,</span> input_key<span class="token operator">=</span><span class="token string">"question"</span><span class="token punctuation">,</span>    verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>answer <span class="token operator">=</span> chinese_qa_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>question<span class="token operator">=</span><span class="token string">"请你作为一个机器学习的专家，介绍一下CNN的原理。"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>answer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Verbose日志信息：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Entering new SimpleSequentialChain chain...Please introduce the principle of CNN as a machine learning expert.Convolutional Neural Networks (CNNs) are a type of artificial neural network that are commonly used in image recognition and classification tasks. They are inspired by the structure of the human brain and are composed of multiple layers of neurons connected in a specific pattern. The neurons in the first layer of a CNN are connected to the input image, and the neurons in the last layer are connected to the output. The neurons in between the input and output layers are called feature maps and are responsible for extracting features from the input image. CNNs use convolutional layers to detect patterns in the input image and pooling layers to reduce the size of the feature maps. This allows the CNN to learn the most important features in the image and use them to make predictions.卷积神经网络（CNN）是一种常用于图像识别和分类任务的人工神经网络。它们受到人脑结构的启发，由多层神经元以特定模式连接而成。CNN的第一层神经元与输入图像连接，最后一层神经元与输出连接。输入和输出层之间的神经元称为特征映射，负责从输入图像中提取特征。CNN使用卷积层检测输入图像中的模式，使用池化层减小特征映射的大小。这使得CNN能够学习图像中最重要的特征，并利用它们进行预测。> Finished chain.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">卷积神经网络（CNN）是一种常用于图像识别和分类任务的人工神经网络。它们受到人脑结构的启发，由多层神经元以特定模式连接而成。CNN的第一层神经元与输入图像连接，最后一层神经元与输出连接。输入和输出层之间的神经元称为特征映射，负责从输入图像中提取特征。CNN使用卷积层检测输入图像中的模式，使用池化层减小特征映射的大小。这使得CNN能够学习图像中最重要的特征，并利用它们进行预测。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在使用这样的链式调用的时候，有一点需要注意，就是一个LLMChain里，所使用的PromptTemplate里的输入参数， <strong>之前必须在LLMChain里，通过 output_key 定义过。</strong> 不然，这个变量没有值，程序就会报错。</p><h3 id="支持多个变量输入的链式调用"><a href="#支持多个变量输入的链式调用" class="headerlink" title="支持多个变量输入的链式调用"></a>支持多个变量输入的链式调用</h3><p>事实上，因为使用变量的输入输出，是用这些参数定义的。所以我们不是只能用前一个LLMChain的输出作为后一个LLMChain的输入。我们完全可以连续问多个问题，然后把这些问题的答案，作为后续问题的输入来继续处理。下面我就给你看一个例子。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> SequentialChainq1_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"year1"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    template<span class="token operator">=</span><span class="token string">"&#123;year1&#125;年的欧冠联赛的冠军是哪支球队，只说球队名称。"</span><span class="token punctuation">)</span>q2_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"year2"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    template<span class="token operator">=</span><span class="token string">"&#123;year2&#125;年的欧冠联赛的冠军是哪支球队，只说球队名称。"</span><span class="token punctuation">)</span>q3_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"team1"</span><span class="token punctuation">,</span> <span class="token string">"team2"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    template<span class="token operator">=</span><span class="token string">"&#123;team1&#125;和&#123;team2&#125;哪只球队获得欧冠的次数多一些？"</span><span class="token punctuation">)</span>chain1 <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>q1_prompt<span class="token punctuation">,</span> output_key<span class="token operator">=</span><span class="token string">"team1"</span><span class="token punctuation">)</span>chain2 <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>q2_prompt<span class="token punctuation">,</span> output_key<span class="token operator">=</span><span class="token string">"team2"</span><span class="token punctuation">)</span>chain3 <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>q3_prompt<span class="token punctuation">)</span>sequential_chain <span class="token operator">=</span> SequentialChain<span class="token punctuation">(</span>chains<span class="token operator">=</span><span class="token punctuation">[</span>chain1<span class="token punctuation">,</span> chain2<span class="token punctuation">,</span> chain3<span class="token punctuation">]</span><span class="token punctuation">,</span> input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"year1"</span><span class="token punctuation">,</span> <span class="token string">"year2"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>answer <span class="token operator">=</span> sequential_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>year1<span class="token operator">=</span><span class="token number">2000</span><span class="token punctuation">,</span> year2<span class="token operator">=</span><span class="token number">2010</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>answer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Entering new SequentialChain chain...> Finished chain.西班牙皇家马德里队获得欧冠的次数更多，共13次，而拜仁慕尼黑只有5次。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>在这个例子里，我们定义了两个PromptTemplate和对应的LLMChain，各自接收一个年份作为输入，回答这两个年份的欧冠冠军。然后将两个队名作为输入，放到第三个问题里，让AI告诉我们这两支球队哪一支获得欧冠的次数多一些。只需要在我们的SequentialChain里输入两个年份，就能通过三次回答得到答案。</p><h2 id="通过Langchain实现自动化撰写单元测试"><a href="#通过Langchain实现自动化撰写单元测试" class="headerlink" title="通过Langchain实现自动化撰写单元测试"></a>通过Langchain实现自动化撰写单元测试</h2><p>看到这里，不知道你有没有想起我们上一讲刚刚讲过的通过多步提示语自动给代码写单元测试。没错，Langchain可以顺序地通过多个Prompt调用OpenAI的GPT模型。这个能力拿来实现上一讲的自动化测试的功能是再合适不过的了。下面，我就拿Langchain重新实现了一遍上一讲的这个功能，并且给它补上了AST语法解析失败之后自动重试的能力。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> SequentialChain<span class="token keyword">def</span> <span class="token function">write_unit_test</span><span class="token punctuation">(</span>function_to_test<span class="token punctuation">,</span> unit_test_package <span class="token operator">=</span> <span class="token string">"pytest"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 解释源代码的步骤</span>    explain_code <span class="token operator">=</span> <span class="token triple-quoted-string string">""""# How to write great unit tests with &#123;unit_test_package&#125;    In this advanced tutorial for experts, we'll use Python 3.10 and `&#123;unit_test_package&#125;` to write a suite of unit tests to verify the behavior of the following function.    ```python    &#123;function_to_test&#125;    ```    Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.    - First,"""</span>    explain_code_template <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>        input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"unit_test_package"</span><span class="token punctuation">,</span> <span class="token string">"function_to_test"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        template<span class="token operator">=</span>explain_code    <span class="token punctuation">)</span>    explain_code_llm <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">"text-davinci-002"</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>            top_p<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stop<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"\n\n"</span><span class="token punctuation">,</span> <span class="token string">"\n\t\n"</span><span class="token punctuation">,</span> <span class="token string">"\n    \n"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    explain_code_step <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>explain_code_llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>explain_code_template<span class="token punctuation">,</span> output_key<span class="token operator">=</span><span class="token string">"code_explaination"</span><span class="token punctuation">)</span>    <span class="token comment"># 创建测试计划示例的步骤</span>    test_plan <span class="token operator">=</span> <span class="token triple-quoted-string string">"""    A good unit test suite should aim to:    - Test the function's behavior for a wide range of possible inputs    - Test edge cases that the author may not have foreseen    - Take advantage of the features of `&#123;unit_test_package&#125;` to make the tests easy to write and maintain    - Be easy to read and understand, with clean code and descriptive names    - Be deterministic, so that the tests always pass or fail in the same way    `&#123;unit_test_package&#125;` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.    For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):    -"""</span>    test_plan_template <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>        input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"unit_test_package"</span><span class="token punctuation">,</span> <span class="token string">"function_to_test"</span><span class="token punctuation">,</span> <span class="token string">"code_explaination"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        template<span class="token operator">=</span> explain_code <span class="token operator">+</span> <span class="token string">"&#123;code_explaination&#125;"</span> <span class="token operator">+</span> test_plan    <span class="token punctuation">)</span>    test_plan_llm <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">"text-davinci-002"</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>            top_p<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stop<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"\n\n"</span><span class="token punctuation">,</span> <span class="token string">"\n\t\n"</span><span class="token punctuation">,</span> <span class="token string">"\n    \n"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    test_plan_step <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>test_plan_llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>test_plan_template<span class="token punctuation">,</span> output_key<span class="token operator">=</span><span class="token string">"test_plan"</span><span class="token punctuation">)</span>    <span class="token comment"># 撰写测试代码的步骤</span>    starter_comment <span class="token operator">=</span> <span class="token string">"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator"</span>    prompt_to_generate_the_unit_test <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.```pythonimport &#123;unit_test_package&#125;  # used for our unit tests&#123;function_to_test&#125;#&#123;starter_comment&#125;"""</span>    unit_test_template <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>        input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"unit_test_package"</span><span class="token punctuation">,</span> <span class="token string">"function_to_test"</span><span class="token punctuation">,</span> <span class="token string">"code_explaination"</span><span class="token punctuation">,</span> <span class="token string">"test_plan"</span><span class="token punctuation">,</span> <span class="token string">"starter_comment"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        template<span class="token operator">=</span> explain_code <span class="token operator">+</span> <span class="token string">"&#123;code_explaination&#125;"</span> <span class="token operator">+</span> test_plan <span class="token operator">+</span> <span class="token string">"&#123;test_plan&#125;"</span> <span class="token operator">+</span> prompt_to_generate_the_unit_test    <span class="token punctuation">)</span>    unit_test_llm <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">"text-davinci-002"</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> stop<span class="token operator">=</span><span class="token string">"```"</span><span class="token punctuation">)</span>    unit_test_step <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>unit_test_llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>unit_test_template<span class="token punctuation">,</span> output_key<span class="token operator">=</span><span class="token string">"unit_test"</span><span class="token punctuation">)</span>    sequential_chain <span class="token operator">=</span> SequentialChain<span class="token punctuation">(</span>chains<span class="token operator">=</span><span class="token punctuation">[</span>explain_code_step<span class="token punctuation">,</span> test_plan_step<span class="token punctuation">,</span> unit_test_step<span class="token punctuation">]</span><span class="token punctuation">,</span>                                    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"unit_test_package"</span><span class="token punctuation">,</span> <span class="token string">"function_to_test"</span><span class="token punctuation">,</span> <span class="token string">"starter_comment"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    answer <span class="token operator">=</span> sequential_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>unit_test_package<span class="token operator">=</span>unit_test_package<span class="token punctuation">,</span> function_to_test<span class="token operator">=</span>function_to_test<span class="token punctuation">,</span> starter_comment<span class="token operator">=</span>starter_comment<span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"""#</span><span class="token interpolation"><span class="token punctuation">&#123;</span>starter_comment<span class="token punctuation">&#125;</span></span><span class="token string">"""</span></span> <span class="token operator">+</span> answercode <span class="token operator">=</span> <span class="token triple-quoted-string string">"""def format_time(seconds):    minutes, seconds = divmod(seconds, 60)    hours, minutes = divmod(minutes, 60)    if hours > 0:        return f"&#123;hours&#125;h&#123;minutes&#125;min&#123;seconds&#125;s"    elif minutes > 0:        return f"&#123;minutes&#125;min&#123;seconds&#125;s"    else:        return f"&#123;seconds&#125;s""""</span><span class="token keyword">import</span> ast<span class="token keyword">def</span> <span class="token function">write_unit_test_automatically</span><span class="token punctuation">(</span>code<span class="token punctuation">,</span> retry<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    unit_test_code <span class="token operator">=</span> write_unit_test<span class="token punctuation">(</span>code<span class="token punctuation">)</span>    all_code <span class="token operator">=</span> code <span class="token operator">+</span> unit_test_code    tried <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">while</span> tried <span class="token operator">&lt;</span> retry<span class="token punctuation">:</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>            ast<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>all_code<span class="token punctuation">)</span>            <span class="token keyword">return</span> all_code        <span class="token keyword">except</span> SyntaxError <span class="token keyword">as</span> e<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Syntax error in generated code: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>e<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>            all_code <span class="token operator">=</span> code <span class="token operator">+</span> write_unit_test<span class="token punctuation">(</span>code<span class="token punctuation">)</span>            tried <span class="token operator">+=</span> <span class="token number">1</span><span class="token keyword">print</span><span class="token punctuation">(</span>write_unit_test_automatically<span class="token punctuation">(</span>code<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">format_time</span><span class="token punctuation">(</span>seconds<span class="token punctuation">)</span><span class="token punctuation">:</span>    minutes<span class="token punctuation">,</span> seconds <span class="token operator">=</span> <span class="token builtin">divmod</span><span class="token punctuation">(</span>seconds<span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">)</span>    hours<span class="token punctuation">,</span> minutes <span class="token operator">=</span> <span class="token builtin">divmod</span><span class="token punctuation">(</span>minutes<span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> hours <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>hours<span class="token punctuation">&#125;</span></span><span class="token string">h</span><span class="token interpolation"><span class="token punctuation">&#123;</span>minutes<span class="token punctuation">&#125;</span></span><span class="token string">min</span><span class="token interpolation"><span class="token punctuation">&#123;</span>seconds<span class="token punctuation">&#125;</span></span><span class="token string">s"</span></span>    <span class="token keyword">elif</span> minutes <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>minutes<span class="token punctuation">&#125;</span></span><span class="token string">min</span><span class="token interpolation"><span class="token punctuation">&#123;</span>seconds<span class="token punctuation">&#125;</span></span><span class="token string">s"</span></span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>seconds<span class="token punctuation">&#125;</span></span><span class="token string">s"</span></span><span class="token comment">#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator.</span><span class="token comment">#The first element of the tuple is the name of the test case, and the second element is a list of tuples,</span><span class="token comment">#where each tuple contains the input values for the format_time() function and the expected output.</span><span class="token decorator annotation punctuation">@pytest<span class="token punctuation">.</span>mark<span class="token punctuation">.</span>parametrize</span><span class="token punctuation">(</span><span class="token string">"test_case, input_values, expected_output"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>    <span class="token comment"># Test cases for when the seconds parameter is an integer</span>    <span class="token punctuation">(</span><span class="token string">"seconds is positive"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"42s"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">"seconds is negative"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">42</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"-42s"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">"seconds is 0"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"0s"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># Test cases for when the seconds parameter is not an integer</span>    <span class="token punctuation">(</span><span class="token string">"seconds is a float"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">42.0</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"42.0s"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">"seconds is a string"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"42"</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"42s"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">"seconds is None"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"None"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># Test cases for when the seconds parameter is an integer, but it is not in the range 0-3600</span>    <span class="token punctuation">(</span><span class="token string">"seconds is too small"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"-1s"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">"seconds is too large"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3601</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"1h0min1s"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">test_format_time</span><span class="token punctuation">(</span>test_case<span class="token punctuation">,</span> input_values<span class="token punctuation">,</span> expected_output<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># We use the pytest.raises context manager to assert that the function raises a TypeError</span>    <span class="token comment"># if the input is not an integer.</span>    <span class="token keyword">with</span> pytest<span class="token punctuation">.</span>raises<span class="token punctuation">(</span>TypeError<span class="token punctuation">)</span><span class="token punctuation">:</span>        format_time<span class="token punctuation">(</span>input_values<span class="token punctuation">)</span>    <span class="token comment"># We use the pytest.approx context manager to assert that the output is approximately equal</span>    <span class="token comment"># to the expected output, within a certain tolerance.</span>    <span class="token keyword">assert</span> format_time<span class="token punctuation">(</span>input_values<span class="token punctuation">)</span> <span class="token operator">==</span> pytest<span class="token punctuation">.</span>approx<span class="token punctuation">(</span>expected_output<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个代码的具体功能，其实和上一讲是一模一样的，只是通过Langchain做了封装，使它更加容易维护了。我们把解释代码、生成测试计划，以及最终生成测试代码，变成了三个LLMChain。每一步的输入，都来自上一步的输出。这个输入既包括上一步的Prompt Template和这一步的Prompt Template的组合，也包括过程中的一些变量，这些变量是上一步执行的结果作为输入变量传递进来的。最终，我们可以使用SequentialChain来自动地按照这三个步骤，执行OpenAI的API调用。</p><p>这整个过程通过write_unit_test这个函数给封装起来了。对于重试，我们则是通过一个while循环来调用 write_unit_test。拿到的结果和输入的代码拼装在一起，交给AST库做解析。如果解析通不过，则重试整个单元测试生成的过程，直到达到我们最大的重试次数为止。</p><p>LangChain的这个分多个步骤调用OpenAI模型的能力，能够帮助我们通过AI完成复杂的任务，并且将整个任务的完成过程定义成了一个固定的流程模版。在下一讲里，我们还会进一步看到，通过这样一个链式组合多个LLMChain的方法，如何完成更复杂并且更具有现实意义的工作。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，相信到这里，你脑子里应该有了更多可以利用大语言模型的好点子。这一讲，我带你学会了如何通过Langchain这个开源库，对大语言模型进行链式调用。想要通过大语言模型，完成一个复杂的任务，往往需要我们多次向AI提问，并且前面提问的答案，可能是后面问题输入的一部分。LangChain通过将多个LLMChain组合成一个SequantialChain并顺序执行，大大简化了这类任务的开发工作。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/36916e57fc0618e5cb8ce49991e423da.png" alt="图片"></p><p>Langchain还有很多更强大的功能，我们不仅能调用语言模型，还能调用外部系统，甚至我们还能直接让AI做决策，决定该让我们的系统做什么。在后面的几讲里，我们会覆盖这些内容，并最终给你一个完整的电商聊天机器人。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>最后，给你留一道思考题。你能试着通过Langchain组合多个问题，并且利用前面问题的回答结果，触发新的问题找到你想要的答案吗？欢迎你把你的例子拿出来分享在评论区，也欢迎你把这一讲分享给需要的朋友，我们下一讲再见。</p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>和之前介绍过的llama-index这个项目一样，Langchain这个项目也在快速地发展和迭代过程中。我推荐你去看一看他们的 <a href="https://langchain.readthedocs.io/en/latest/">官方文档</a>，好知道他们提供的最新功能。此外，这个我们之前提到过的向量数据库公司Pinecone，也制作了一份 <a href="https://www.pinecone.io/learn/langchain/">Langchain AI Handbook</a>，你也可以去看一看。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;14｜链式调用，用LangChain简化多步提示语&quot;&gt;&lt;a href=&quot;#14｜链式调用，用LangChain简化多步提示语&quot; class=&quot;headerlink&quot; title=&quot;14｜链式调用，用LangChain简化多步提示语&quot;&gt;&lt;/a&gt;14｜链式调用，用La</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>15｜深入使用LLMChain，给AI连上Google和计算器</title>
    <link href="https://zhuansun.github.io/geekbang/posts/2135493766.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/2135493766.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.916Z</updated>
    
    <content type="html"><![CDATA[<h1 id="15｜深入使用LLMChain，给AI连上Google和计算器"><a href="#15｜深入使用LLMChain，给AI连上Google和计算器" class="headerlink" title="15｜深入使用LLMChain，给AI连上Google和计算器"></a>15｜深入使用LLMChain，给AI连上Google和计算器</h1><p>你好，我是徐文浩。</p><p>上一讲里，我们一起学习了怎么通过LangChain这个Python包，链式地调用OpenAI的API。通过链式调用的方式，我们可以把一个需要询问AI多轮才能解决的问题封装起来，把一个通过自然语言多轮调用才能解决的问题，变成了一个函数调用。</p><p>不过，LangChain能够帮到我们的远不止这一点。前一阵，ChatGPT发布了 <a href="https://openai.com/blog/chatgpt-plugins">Plugins</a> 这个插件机制。通过Plugins，ChatGPT可以浏览整个互联网，还可以接上Wolfram这样的科学计算工具，能够实现很多原先光靠大语言模型解决不好的问题。不过，这个功能目前还是处于wait list的状态，我也还没有拿到权限。</p><p>不过没有关系，我们通过LangChain也能实现这些类似的功能。今天这一讲，我们就继续深入挖掘一下Langchain，看看它怎么解决这些问题。</p><h2 id="解决AI数理能力的难题"><a href="#解决AI数理能力的难题" class="headerlink" title="解决AI数理能力的难题"></a>解决AI数理能力的难题</h2><p>很多人发现，虽然ChatGPT回答各种问题的时候都像模像样的，但是一到计算三位数乘法的时候就露馅儿了。感觉它只是快速估计了一个数字，而不是真的准确计算了。我们来看下面这段代码，我们让OpenAI帮我们计算一下 352 x 493 等于多少，你会发现，它算得大差不差，但还是算错了。这就很尴尬，如果我们真的想要让它来担任一个小学数学的助教，总是给出错误的答案也不是个事儿。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token punctuation">,</span> osopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts <span class="token keyword">import</span> PromptTemplate<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms <span class="token keyword">import</span> OpenAI<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> LLMChainllm <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">"text-davinci-003"</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>multiply_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>template<span class="token operator">=</span><span class="token string">"请计算一下&#123;question&#125;是多少?"</span><span class="token punctuation">,</span> input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"question"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>math_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>multiply_prompt<span class="token punctuation">,</span> output_key<span class="token operator">=</span><span class="token string">"answer"</span><span class="token punctuation">)</span>answer <span class="token operator">=</span> math_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"question"</span><span class="token punctuation">:</span> <span class="token string">"352乘以493"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"OpenAI API 说答案是:"</span><span class="token punctuation">,</span> answer<span class="token punctuation">)</span>python_answer <span class="token operator">=</span> <span class="token number">352</span> <span class="token operator">*</span> <span class="token number">493</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Python 说答案是:"</span><span class="token punctuation">,</span> python_answer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">OpenAI API 说答案是:352 x 493 = 174,336Python 说答案是: 173536<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>注：可以看到，OpenAI给出的结果，答案是错误的。</p><p>不过，有人很聪明，说虽然ChatGPT直接算这些数学题不行，但是它不是会写代码吗？我们直接让它帮我们写一段利用Python计算这个数学式子的代码不就好了吗？的确，如果你让它写一段Python代码，给出的代码是没有问题的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">multiply_by_python_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>template<span class="token operator">=</span><span class="token string">"请写一段Python代码，计算&#123;question&#125;?"</span><span class="token punctuation">,</span> input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"question"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>math_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>multiply_by_python_prompt<span class="token punctuation">,</span> output_key<span class="token operator">=</span><span class="token string">"answer"</span><span class="token punctuation">)</span>answer <span class="token operator">=</span> math_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"question"</span><span class="token punctuation">:</span> <span class="token string">"352乘以493"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>answer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token number">352</span> <span class="token operator">*</span> <span class="token number">493</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>不过，我们不想再把这段代码，复制粘贴到Python的解释器或者Notebook里面，再去手工执行一遍。所以，我们可以在后面再调用一个Python解释器，让整个过程自动完成，对应的代码我也放在了下面。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">multiply_by_python_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>template<span class="token operator">=</span><span class="token string">"请写一段Python代码，计算&#123;question&#125;?"</span><span class="token punctuation">,</span> input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"question"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>math_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>multiply_by_python_prompt<span class="token punctuation">,</span> output_key<span class="token operator">=</span><span class="token string">"answer"</span><span class="token punctuation">)</span>answer_code <span class="token operator">=</span> math_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"question"</span><span class="token punctuation">:</span> <span class="token string">"352乘以493"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>utilities <span class="token keyword">import</span> PythonREPLpython_repl <span class="token operator">=</span> PythonREPL<span class="token punctuation">(</span><span class="token punctuation">)</span>result <span class="token operator">=</span> python_repl<span class="token punctuation">.</span>run<span class="token punctuation">(</span>answer_code<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token number">173536</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注：生成的Python脚本是正确的，再通过调用Python解释器就能得到正确的计算结果。</p><p>可以看到，LangChain里面内置了一个utilities的包，里面包含了PythonREPL这个类，可以实现对Python解释器的调用。如果你去翻看一下对应代码的源码的话，它其实就是简单地调用了一下系统自带的exec方法，来执行Python代码。utilities里面还有很多其他的类，能够实现很多功能，比如可以直接运行Bash脚本，调用Google Search的API等等。你可以去LangChain的 <a href="https://python.langchain.com/en/latest/modules/agents/tools.html">文档</a>，看看它内置的这些工具类有哪些。</p><p>如果你仔细想一下，你会发现这其实也是一种链式调用。只不过，调用链里面的第二步，不是去访问OpenAI的API而已。所以，对于这些工具能力，LangChain也把它们封装成了LLMChain的形式。比如刚才的数学计算问题，是一个先生成Python脚本，再调用Python解释器的过程，LangChain就把这个过程封装成了一个叫做 LLMMathChain的LLMChain。不需要自己去生成代码，再调用PythonREPL，只要直接调用LLMMathChain，它就会在背后把这一切都给做好，对应的代码我也放在下面。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain <span class="token keyword">import</span> LLMMathChainllm_math <span class="token operator">=</span> LLMMathChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>result <span class="token operator">=</span> llm_math<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"请计算一下352乘以493是多少?"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">></span> Entering new LLMMathChain chain<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>请计算一下<span class="token number">352</span>乘以<span class="token number">493</span>是多少?<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token number">352</span> <span class="token operator">*</span> <span class="token number">493</span><span class="token punctuation">)</span>Answer<span class="token punctuation">:</span> <span class="token number">173536</span><span class="token operator">></span> Finished chain<span class="token punctuation">.</span>Answer<span class="token punctuation">:</span> <span class="token number">173536</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>LangChain也把前面讲过的utilities包里面的很多功能，都封装成了Utility Chains。比如，SQLDatabaseChain可以直接根据你的数据库生成SQL，然后获取数据，LLMRequestsChain可以通过API调用外部系统，获得想要的答案。你可以直接在LangChain关于Utility Chains的 <a href="https://python.langchain.com/en/latest/modules/agents/tools.html">文档</a> 里面，找到有哪些工具可以用。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/aa939d818820b0797280d431cc87037a.png" alt="图片"></p><h2 id="通过RequestsChain获取实时外部信息"><a href="#通过RequestsChain获取实时外部信息" class="headerlink" title="通过RequestsChain获取实时外部信息"></a>通过RequestsChain获取实时外部信息</h2><p>这里我们来重点讲一讲如何通过API来调用外部系统，获得想要的答案。之前在介绍llama-index的时候，我们已经介绍过一种为AI引入外部知识的方法了，那就是计算这些外部知识的Embedding，然后作为索引先保存下来。但是，这只适用于处理那些预先准备好会被问到的知识，比如一本书、一篇论文。这些东西，内容多但是固定，也不存在时效性问题，我们可以提前索引好，而且用户问的问题往往也有很强的相似性。</p><p>但是，对于时效性强的问题，这个方法不太适用，因为我们可能没有必要不停地更新索引。比如，你想要知道实时的天气情况，我们不太可能把全球所有城市最新的天气信息每隔几分钟都索引一遍。</p><p>这个时候，我们可以使用LLMRequestsChain，通过一个HTTP请求来得到问题的答案。最简单粗暴的一个办法，就是直接通过一个HTTP请求来问一下Google。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> LLMRequestsChaintemplate <span class="token operator">=</span> <span class="token triple-quoted-string string">"""在 >>> 和 &lt;&lt;&lt; 直接是来自Google的原始搜索结果.请把对于问题 '&#123;query&#125;' 的答案从里面提取出来，如果里面没有相关信息的话就说 "找不到"请使用如下格式：Extracted:&lt;answer or "找不到">>>> &#123;requests_result&#125; &lt;&lt;&lt;Extracted:"""</span>PROMPT <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"requests_result"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    template<span class="token operator">=</span>template<span class="token punctuation">,</span><span class="token punctuation">)</span>requests_chain <span class="token operator">=</span> LLMRequestsChain<span class="token punctuation">(</span>llm_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>OpenAI<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> prompt<span class="token operator">=</span>PROMPT<span class="token punctuation">)</span><span class="token punctuation">)</span>question <span class="token operator">=</span> <span class="token string">"今天上海的天气怎么样？"</span>inputs <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">"query"</span><span class="token punctuation">:</span> question<span class="token punctuation">,</span>    <span class="token string">"url"</span><span class="token punctuation">:</span> <span class="token string">"https://www.google.com/search?q="</span> <span class="token operator">+</span> question<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">"+"</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span>result<span class="token operator">=</span>requests_chain<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">[</span><span class="token string">'output'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'query': '今天上海的天气怎么样？', 'url': 'https://www.google.com/search?q=今天上海的天气怎么样？', 'output': '小雨; 10℃～15℃; 东北风 风力4-5级'&#125;小雨; 10℃～15℃; 东北风 风力4-5级<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>我们来看看这段代码，基于LLMRequestsChain，我们用到了之前使用过的好几个技巧。</p><ol><li>首先，因为我们是简单粗暴地搜索Google。但是我们想要的是一个有价值的天气信息，而不是整个网页。所以，我们还需要通过ChatGPT把网页搜索结果里面的答案给找出来。所以我们定义了一个PromptTemplate，通过一段提示语，让OpenAI为我们在搜索结果里面，找出问题的答案，而不是去拿原始的HTML页面。</li><li>然后，我们使用了LLMRequestsChain，并且把刚才PromptTemplate构造的一个普通的LLMChain，作为构造函数的一个参数，传给LLMRequestsChain，帮助我们在搜索之后处理搜索结果。</li><li>对应的搜索词，通过query这个参数传入，对应的原始搜索结果，则会默认放到requests_results里。而通过我们自己定义的PromptTemplate抽取出来的最终答案，则会放到output这个输出参数里面。</li></ol><p>我们运行一下，就可以看到我们通过简单搜索Google加上通过OpenAI提取搜索结果里面的答案，就得到了最新的天气信息。</p><h2 id="通过TransformationChain转换数据格式"><a href="#通过TransformationChain转换数据格式" class="headerlink" title="通过TransformationChain转换数据格式"></a>通过TransformationChain转换数据格式</h2><p>有了实时的外部数据，我们就又有很多做应用的创意了。比如说，我们可以根据气温来推荐大家穿什么衣服。我们可以要求如果最低温度低于0度，就要推荐用户去穿羽绒服。或者，根据是否下雨来决定要不要提醒用户出门带伞。</p><p>不过，在现在的返回结果里，天气信息（天气、温度、风力）只是一段文本，而不是可以直接获取的JSON格式。当然，我们可以在LLMChain里面再链式地调用一次OpenAI的接口，把这段文本转换成JSON格式。但是，这样做的话，一来还要消耗更多的Token、花更多的钱，二来这也会进一步增加程序需要运行的时间，毕竟一次往返的网络请求也是很慢的。这里的文本格式其实很简单，我们完全可以通过简单的字符串处理完成解析。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> re<span class="token keyword">def</span> <span class="token function">parse_weather_info</span><span class="token punctuation">(</span>weather_info<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">dict</span><span class="token punctuation">:</span>    <span class="token comment"># 将天气信息拆分成不同部分</span>    parts <span class="token operator">=</span> weather_info<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'; '</span><span class="token punctuation">)</span>    <span class="token comment"># 解析天气</span>    weather <span class="token operator">=</span> parts<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 解析温度范围，并提取最小和最大温度</span>    temperature_range <span class="token operator">=</span> parts<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'℃'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'～'</span><span class="token punctuation">)</span>    temperature_min <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>temperature_range<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    temperature_max <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>temperature_range<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># 解析风向和风力</span>    wind_parts <span class="token operator">=</span> parts<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span>    wind_direction <span class="token operator">=</span> wind_parts<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>    wind_force <span class="token operator">=</span> wind_parts<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 返回解析后的天气信息字典</span>    weather_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">'weather'</span><span class="token punctuation">:</span> weather<span class="token punctuation">,</span>        <span class="token string">'temperature_min'</span><span class="token punctuation">:</span> temperature_min<span class="token punctuation">,</span>        <span class="token string">'temperature_max'</span><span class="token punctuation">:</span> temperature_max<span class="token punctuation">,</span>        <span class="token string">'wind_direction'</span><span class="token punctuation">:</span> wind_direction<span class="token punctuation">,</span>        <span class="token string">'wind_force'</span><span class="token punctuation">:</span> wind_force    <span class="token punctuation">&#125;</span>    <span class="token keyword">return</span> weather_dict<span class="token comment"># 示例</span>weather_info <span class="token operator">=</span> <span class="token string">"小雨; 10℃～15℃; 东北风 风力4-5级"</span>weather_dict <span class="token operator">=</span> parse_weather_info<span class="token punctuation">(</span>weather_info<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>weather_dict<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span><span class="token string">'weather'</span><span class="token punctuation">:</span> <span class="token string">'小雨'</span><span class="token punctuation">,</span> <span class="token string">'temperature'</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token string">'min'</span><span class="token punctuation">:</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token string">'max'</span><span class="token punctuation">:</span> <span class="token number">15</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> <span class="token string">'wind'</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token string">'direction'</span><span class="token punctuation">:</span> <span class="token string">'东北风'</span><span class="token punctuation">,</span> <span class="token string">'level'</span><span class="token punctuation">:</span> <span class="token string">'风力4-5级'</span><span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注：上面这段代码，其实是我让GPT-4写的。</p><p>我们在这里实现了一个 <strong>parse_weather_info</strong> 函数，可以把前面LLMRequestsChain的输出结果，解析成一个dict。不过，我们能不能更进一步，把这个解析的逻辑，也传到LLMChain的链式调用的最后呢？答案当然是可以的。对于这样的要求，Langchain里面也有一个专门的解决方案，叫做TransformChain，也就是做格式转换。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> TransformChain<span class="token punctuation">,</span> SequentialChain<span class="token keyword">def</span> <span class="token function">transform_func</span><span class="token punctuation">(</span>inputs<span class="token punctuation">:</span> <span class="token builtin">dict</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">dict</span><span class="token punctuation">:</span>    text <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token string">"weather_info"</span> <span class="token punctuation">:</span> parse_weather_info<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>transformation_chain <span class="token operator">=</span> TransformChain<span class="token punctuation">(</span>input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                      output_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"weather_info"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform_func<span class="token punctuation">)</span>final_chain <span class="token operator">=</span> SequentialChain<span class="token punctuation">(</span>chains<span class="token operator">=</span><span class="token punctuation">[</span>requests_chain<span class="token punctuation">,</span> transformation_chain<span class="token punctuation">]</span><span class="token punctuation">,</span>                              input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"url"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"weather_info"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>final_result <span class="token operator">=</span> final_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>final_result<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span><span class="token string">'weather'</span><span class="token punctuation">:</span> <span class="token string">'小雨'</span><span class="token punctuation">,</span> <span class="token string">'temperature'</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token string">'min'</span><span class="token punctuation">:</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token string">'max'</span><span class="token punctuation">:</span> <span class="token number">15</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> <span class="token string">'wind'</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token string">'direction'</span><span class="token punctuation">:</span> <span class="token string">'东北风'</span><span class="token punctuation">,</span> <span class="token string">'level'</span><span class="token punctuation">:</span> <span class="token string">'风力4-5级'</span><span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注：在requests_chain后面跟上一个transformation_chain，我们就能把结果解析成dict，供后面其他业务使用结构化的数据。</p><ol><li>我们在这里，先定义了一个transform_func，对前面的parse_weather_info函数做了一下简单的封装。它的输入，是整个LLMChain里，执行到TransformChain之前的整个输出结果的dict。我们前面看到整个LLMRequestsChain里面的天气信息的文本内容，是通过output这个key拿到的，所以这里我们也是先通过它来拿到天气信息的文本内容，再调用 parse_weather_info 解析，并且把结果输出到 weather_info 这个字段里。</li><li>然后，我们就定义了一个TransformChain，里面的输入参数就是 output，输出参数就是 weather_info。</li><li>最后，我们通过上一讲用过的 SequentialChain，将前面的LLMRequestsChain和这里的TransformChain串联到一起，变成一个新的叫做 final_chain 的LLMChain。</li></ol><p>在这三步完成之后，未来我们想要获得天气信息，并且拿到一个dict形式的输出，只要调用 final_chain的run方法，输入我们关于天气的搜索文本就好了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e29294012c309f3f808693cea5216445.png" alt="图片"></p><p>最后，我们来梳理一下final_chain都做了哪些事。</p><ol><li>通过一个HTTP请求，根据搜索词拿到Google的搜索结果页。</li><li>把我们定义的Prompt提交给OpenAI，然后把我们搜索的问题和结果页都发给了OpenAI，让它从里面提取出搜索结果页里面的天气信息。</li><li>最后我们通过 transform_func 解析拿到的天气信息的文本，被转换成一个dict。这样，后面的程序就好处理了。</li></ol><h2 id="通过VectorDBQA来实现先搜索再回复的能力"><a href="#通过VectorDBQA来实现先搜索再回复的能力" class="headerlink" title="通过VectorDBQA来实现先搜索再回复的能力"></a>通过VectorDBQA来实现先搜索再回复的能力</h2><p>此外，还有一个常用的LLMChain，就是我们之前介绍的llama-index的使用场景，也就是针对自己的资料库进行问答。我们预先把资料库索引好，然后每次用户来问问题的时候，都是先到这个资料库里搜索，再把问题和答案一并交给AI，让它去组织语言回答。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores import FAISSfrom langchain.text_splitter import SpacyTextSplitterfrom langchain import OpenAI, VectorDBQAfrom langchain.document_loaders import TextLoaderllm = OpenAI(temperature=0)loader = TextLoader('./data/ecommerce_faq.txt')documents = loader.load()text_splitter = SpacyTextSplitter(chunk_size=256, pipeline="zh_core_web_sm")texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()docsearch = FAISS.from_documents(texts, embeddings)faq_chain = VectorDBQA.from_chain_type(llm=llm, vectorstore=docsearch, verbose=True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注：上面的代码创建了一个基于 FAISS 进行向量存储的 docsearch的索引，以及基于这个索引的VectorDBQA这个LLMChain。</p><p>先来看第一段代码，我们通过一个TextLoader把文件加载进来，还通过SpacyTextSplitter给文本分段，确保每个分出来的Document都是一个完整的句子。因为我们这里的文档是电商FAQ的内容，都比较短小精悍，所以我们设置的chunk_size只有256。然后，我们定义了使用OpenAIEmbeddings来给文档创建Embedding，通过FAISS把它存储成一个VectorStore。最后，我们通过VectorDBQA的from_chain_type 定义了一个LLM。对应的FAQ内容，我是请ChatGPT为我编造之后放在了ecommerce_faq.txt这个文件里。</p><p>问题：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">question = "请问你们的货，能送到三亚吗？大概需要几天？"result = faq_chain.run(question)print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Entering new VectorDBQA chain...> Finished chain. 我们支持全国大部分省份的配送，包括三亚。一般情况下，大部分城市的订单在2-3个工作日内送达，偏远地区可能需要5-7个工作日。具体送货时间可能因订单商品、配送地址和物流公司而异。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>问题：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">question = "请问你们的退货政策是怎么样的？"result = faq_chain.run(question)print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Entering new VectorDBQA chain...> Finished chain.自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>我向它提了两个不同类型的问题，faq_chain都能够正确地回答出来。你可以去看看data目录下面的ecommerce_faq.txt文件，看看它的回答是不是和文档写的内容一致。在VectorDBQA这个LLMChain背后，其实也是通过一系列的链式调用，来完成搜索VectorStore，再向AI发起Completion请求这样两个步骤的。</p><p>可以看到LLMChain是一个很强大的武器，它可以把解决一个问题需要的多个步骤串联到一起。这个步骤可以是调用我们的语言模型，也可以是调用一个外部API，或者在内部我们定义一个Python函数。这大大增强了我们利用大语言模型的能力，特别是能够弥补它的很多不足之处，比如缺少有时效的信息，通过HTTP调用比较慢等等。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，这一讲到这里也就结束了。</p><p>我们可以看到，Langchain的链式调用并不局限于使用大语言模型的接口。这一讲里，我们就看到四种常见的将大语言模型的接口和其他能力结合在一起的链式调用。</p><ol><li>LLMMathChain能够通过Python解释器变成一个计算器，让AI能够准确地进行数学运算。</li><li>通过RequestsChain，我们可以直接调用外部API，然后再让AI从返回的结果里提取我们关心的内容。</li><li>TransformChain能够让我们根据自己的要求对数据进行处理和转化，我们可以把AI返回的自然语言的结果进一步转换成结构化的数据，方便其他程序去处理。</li><li>VectorDBQA能够完成和llama-index相似的事情，只要预先做好内部数据资料的Embedding和索引，通过对LLMChain进行一次调用，我们就可以直接获取回答的结果。</li></ol><p>这些能力大大增强了AI的实用性，解决了几个之前大语言模型处理得不好的问题，包括数学计算能力、实时数据能力、和现有程序结合的能力，以及搜索属于自己的资料库的能力。你完全可以定义自己需要的LLMChain，通过程序来完成各种任务，然后合理地组合不同类型的LLMChain对象，来实现连ChatGPT都做不到的事情。而ChatGPT Plugins的实现机制，其实也是类似的。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>最后，我给你留一道思考题。我们前面说过，Langchain里有SQLDatabaseChain可以直接让我们写需求访问数据库。在官方文档里也给出了对应的 <a href="https://langchain.readthedocs.io/en/latest/modules/chains/examples/sqlite.html">例子</a>，你可以去试一试体验一下，想一想它是通过什么样的提示语信息，来让AI写出可以直接执行的SQL的？</p><p>欢迎你把你体验之后的感受以及思考后的结果分享在评论区，也欢迎你把这一讲分享给感兴趣的朋友，我们下一讲再见！</p><h2 id="推荐试用"><a href="#推荐试用" class="headerlink" title="推荐试用"></a>推荐试用</h2><p>我们目前对于Langchain的讲解，都是通过Python编程的方式来实现真实业务场景的需求的。有人直接为Langchain做了一个可以拖拽的图形界面叫做 <a href="https://github.com/logspace-ai/langflow">LangFlow</a>。你可以试着下载体验一下，看看图形界面是不是可以进一步提升你的效率。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;15｜深入使用LLMChain，给AI连上Google和计算器&quot;&gt;&lt;a href=&quot;#15｜深入使用LLMChain，给AI连上Google和计算器&quot; class=&quot;headerlink&quot; title=&quot;15｜深入使用LLMChain，给AI连上Google和计算</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>16｜Langchain里的“记忆力”，让AI只记住有用的事儿</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3276640541.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3276640541.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.919Z</updated>
    
    <content type="html"><![CDATA[<h1 id="16｜Langchain里的“记忆力”，让AI只记住有用的事儿"><a href="#16｜Langchain里的“记忆力”，让AI只记住有用的事儿" class="headerlink" title="16｜Langchain里的“记忆力”，让AI只记住有用的事儿"></a>16｜Langchain里的“记忆力”，让AI只记住有用的事儿</h1><p>你好，我是徐文浩。</p><p>在过去的两讲里，我们深入了解了Langchain的第一个核心功能，也就是LLMChain。 LLMChain能够帮助我们链式地调用一系列命令，这里面既包含直接调用OpenAI的API，也包括调用其他外部接口，或者自己实现的Python代码。但是这一连串的调用，还只是完成一个小任务。我们很多时候还是希望用一个互动聊天的过程，来完成整个任务。</p><p>所以LangChain并不是只有链式调用这样一个核心功能，它还封装了很多其他能力，来方便我们开发AI应用。比如，让AI能够拥有“记忆力”，也就是记住我们聊天上下文的能力。不知道你还记不记得，我们在 <a href="https://time.geekbang.org/column/article/643915">第 6 讲</a> 里做的聊天机器人。在那个里面，为了能够让ChatGPT知道整个聊天的上下文，我们需要把历史的对话记录都传给它。但是，因为能够接收的Token数量有上限，所以我们只能设定一个参数，只保留最后几轮对话。我们最后把这个功能，抽象成了一个Conversation类。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token keyword">import</span> osopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Conversation</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prompt<span class="token punctuation">,</span> num_of_round<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>prompt <span class="token operator">=</span> prompt        self<span class="token punctuation">.</span>num_of_round <span class="token operator">=</span> num_of_round        self<span class="token punctuation">.</span>messages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>prompt<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">ask</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> question<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> question<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>            response <span class="token operator">=</span> openai<span class="token punctuation">.</span>ChatCompletion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>                model<span class="token operator">=</span><span class="token string">"gpt-3.5-turbo"</span><span class="token punctuation">,</span>                messages<span class="token operator">=</span>self<span class="token punctuation">.</span>messages<span class="token punctuation">,</span>                temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>                max_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>                top_p<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span>            <span class="token keyword">return</span> e        message <span class="token operator">=</span> response<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"message"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"assistant"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> message<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>messages<span class="token punctuation">)</span> <span class="token operator">></span> self<span class="token punctuation">.</span>num_of_round<span class="token operator">*</span><span class="token number">2</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">del</span> self<span class="token punctuation">.</span>messages<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">//</span>Remove the first <span class="token builtin">round</span> conversation left<span class="token punctuation">.</span>        <span class="token keyword">return</span> message<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>不知道你是否还记得这个Conversation类。</p><h2 id="BufferWindow，滑动窗口记忆"><a href="#BufferWindow，滑动窗口记忆" class="headerlink" title="BufferWindow，滑动窗口记忆"></a>BufferWindow，滑动窗口记忆</h2><p>这个基于一个固定长度的滑动窗口的“记忆”功能，被直接内置在LangChain里面了。在Langchain里，把对于整个对话过程的上下文叫做Memory。任何一个LLMChain，我们都可以给它加上一个Memory，来让它记住最近的对话上下文。我也把对应的代码放在了下面。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationBufferWindowMemorytemplate <span class="token operator">=</span> <span class="token triple-quoted-string string">"""你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字以内&#123;chat_history&#125;Human: &#123;human_input&#125;Chatbot:"""</span>prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"chat_history"</span><span class="token punctuation">,</span> <span class="token string">"human_input"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    template<span class="token operator">=</span>template<span class="token punctuation">)</span>memory <span class="token operator">=</span> ConversationBufferWindowMemory<span class="token punctuation">(</span>memory_key<span class="token operator">=</span><span class="token string">"chat_history"</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>llm_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>    llm<span class="token operator">=</span>OpenAI<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>    memory<span class="token operator">=</span>memory<span class="token punctuation">,</span>    verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>llm_chain<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>human_input<span class="token operator">=</span><span class="token string">"你是谁？"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token string">' 我是一个中国厨师，我可以帮助你做菜。我会根据你的口味和特殊要求，精心烹饪出独特美味的中国菜肴。'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以看到，我们做的事情其实和之前的Conversation类似，我们定义了一个PromptTemplate来输入我们的指示。然后，在LLMChain构造的时候，我们为它指定了一个叫做 ConversationBufferWindowMemory的memory对象，并且为这个memory对象定义了k&#x3D;3，也就是只保留最近三轮的对话内容。</p><p>如果我们和 <a href="https://time.geekbang.org/column/article/643915">第 6 讲</a> 一样，和它连续进行几轮对话，你会发现，到第四轮的时候它还是能够记得我们问它的第一个问题是“你是谁”，但是第5轮的时候，已经变成“鱼香肉丝怎么做？”了。这就是因为我们选择只保留过去3轮对话。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">llm_chain<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>human_input<span class="token operator">=</span><span class="token string">"鱼香肉丝怎么做？"</span><span class="token punctuation">)</span>llm_chain<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>human_input<span class="token operator">=</span><span class="token string">"那宫保鸡丁呢？"</span><span class="token punctuation">)</span>llm_chain<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>human_input<span class="token operator">=</span><span class="token string">"我问你的第一句话是什么？"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token string">' 你是谁？'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>再次询问第一句话是什么：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">llm_chain<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>human_input<span class="token operator">=</span><span class="token string">"我问你的第一句话是什么？"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token string">' 你问我的第一句话是“鱼香肉丝怎么做？”'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>事实上，你可以直接调用memory的load_memory_variables方法，它会直接返回memory里实际记住的对话内容。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">memory<span class="token punctuation">.</span>load_memory_variables<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span><span class="token string">'chat_history'</span><span class="token punctuation">:</span> <span class="token string">'Human: 那宫保鸡丁呢？\nAI:  宫保鸡丁是一道经典的中国家常菜，需要准备鸡肉、花生米、干辣椒、葱、姜、蒜、料酒、盐、糖、胡椒粉、鸡精和醋。将鸡肉切成小块，放入盐水中浸泡，把其他食材切成小块，将花生米放入油锅中炸，再加入鸡肉和其他食材，炒至入味即可。\nHuman: 我问你的第一句话是什么？\nAI:  你是谁？\nHuman: 我问你的第一句话是什么？\nAI:  你问我的第一句话是“鱼香肉丝怎么做？”'</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="SummaryMemory，把小结作为历史记忆"><a href="#SummaryMemory，把小结作为历史记忆" class="headerlink" title="SummaryMemory，把小结作为历史记忆"></a>SummaryMemory，把小结作为历史记忆</h2><p>使用BufferWindow这样的滑动窗口有一个坏处，就是几轮对话之后，AI就把一开始聊的内容给忘了。所以在 <a href="https://time.geekbang.org/column/article/644544">第 7 讲</a> 的时候我们讲过，遇到这种情况，可以让AI去总结一下前面几轮对话的内容。这样，我们就不怕对话轮数太多或者太长了。</p><p>同样的，Langchain也提供了一个ConversationSummaryMemory，可以实现这样的功能，我们还是通过一段简单的代码来看看它是怎么用的。</p><p>代码中只有两个需要注意的点。</p><p>第一个是对于我们定义的 ConversationSummaryMemory，它的构造函数也接受一个LLM对象。这个对象会专门用来生成历史对话的小结，是可以和对话本身使用的LLM对象不同的。</p><p>第二个是这次我们没有使用LLMChain这个对象，而是用了封装好的ConversationChain。用ConversationChain的话，其实我们是可以不用自己定义PromptTemplate来维护历史聊天记录的，但是为了使用中文的PromptTemplate，我们在这里还是自定义了对应的Prompt。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> ConversationChain<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationSummaryMemoryllm <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>memory <span class="token operator">=</span> ConversationSummaryMemory<span class="token punctuation">(</span>llm<span class="token operator">=</span>OpenAI<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>prompt_template <span class="token operator">=</span> <span class="token triple-quoted-string string">"""你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:1. 你的回答必须是中文2. 回答限制在100个字以内&#123;history&#125;Human: &#123;input&#125;AI:"""</span>prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"history"</span><span class="token punctuation">,</span> <span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> template<span class="token operator">=</span>prompt_template<span class="token punctuation">)</span>conversation_with_summary <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>    llm<span class="token operator">=</span>llm<span class="token punctuation">,</span>    memory<span class="token operator">=</span>memory<span class="token punctuation">,</span>    prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>    verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>conversation_with_summary<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"你好"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">></span> Entering new ConversationChain chain<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>Prompt after formatting<span class="token punctuation">:</span>你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求<span class="token punctuation">:</span><span class="token number">1.</span> 你的回答必须是中文<span class="token number">2.</span> 回答限制在<span class="token number">100</span>个字以内Human<span class="token punctuation">:</span> 你好AI<span class="token punctuation">:</span><span class="token operator">></span> Finished chain<span class="token punctuation">.</span><span class="token string">' 你好，我可以帮你做菜。我会根据你的口味和喜好，结合当地的食材，制作出美味可口的菜肴。我会尽力做出最好的菜肴，让你满意。'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在我们打开了ConversationChain的Verbose模式，然后再次询问AI第二个问题的时候，你可以看到，在Verbose的信息里面，没有历史聊天记录，而是多了一段对之前聊天内容的英文小结。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conversation_with_summary<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"鱼香肉丝怎么做？"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">></span> Entering new ConversationChain chain<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>Prompt after formatting<span class="token punctuation">:</span>你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求<span class="token punctuation">:</span><span class="token number">1.</span> 你的回答必须是中文<span class="token number">2.</span> 回答限制在<span class="token number">100</span>个字以内The human greeted the AI <span class="token keyword">and</span> the AI responded that it can <span class="token builtin">help</span> cook by combining local ingredients <span class="token keyword">and</span> tailor the meal to the human<span class="token string">'s tastes and preferences. It promised to make the best dishes possible to the human'</span>s satisfaction<span class="token punctuation">.</span>Human<span class="token punctuation">:</span> 鱼香肉丝怎么做？AI<span class="token punctuation">:</span><span class="token operator">></span> Finished chain<span class="token punctuation">.</span><span class="token string">' 鱼香肉丝是一道经典的家常菜，需要准备肉丝、葱姜蒜、鱼香调料、豆瓣酱、醋、糖、盐等调料，先将肉丝用盐、料酒、胡椒粉腌制，然后炒锅里放入葱姜蒜爆香，加入肉丝翻炒，加入鱼香调料、豆瓣酱、醋、糖等调料，最后放入少许水煮熟即可。'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而如果这个时候我们调用 memory的load_memory_variables方法，可以看到记录下来的history是一小段关于对话的英文小结。而不是像上面那样，记录完整的历史对话。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">memory<span class="token punctuation">.</span>load_memory_variables<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span><span class="token string">'history'</span><span class="token punctuation">:</span> <span class="token string">'\nThe human greeted the AI, to which the AI replied that it was a Chinese chef that enjoyed making Chinese dishes such as braised pork, Kung Pao chicken, and Fish-fragrant pork shreds. The AI also said that it would use fresh ingredients and carefully cook each dish to make them delicious. When the human asked about how to make Fish-fragrant pork shreds, the AI replied that it needed to prepare ingredients such as meat shreds, scallions, ginger, garlic, peppers, Sichuan pepper, soy sauce, sugar, vinegar, cooking wine, and cornstarch. The AI then explained that the meat shreds should first be marinated with cornstarch, cooking wine, salt, and pepper, and then the scallions, ginger, garlic, and peppers should be stir-fried in a wok, followed by the addition of the meat shreds. Finally, soy sauce, sugar, vinegar, and cornstarch should be added to season the dish.'</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>而如果我们进一步通过conversation_with_summary去和AI对话，就会看到英文的小结内容会随着对话内容不断变化。每一次AI都是把之前的小结和新的对话交给memory中定义的LLM再次进行小结。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conversation_with_summary<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"那蚝油牛肉呢？"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">></span> Entering new ConversationChain chain<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>Prompt after formatting<span class="token punctuation">:</span>你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求<span class="token punctuation">:</span><span class="token number">1.</span> 你的回答必须是中文<span class="token number">2.</span> 回答限制在<span class="token number">100</span>个字以内The human greeted the AI <span class="token keyword">and</span> the AI responded that it can <span class="token builtin">help</span> cook by combining local ingredients <span class="token keyword">and</span> tailor the meal to the human<span class="token string">'s tastes and preferences. It promised to make the best dishes possible to the human'</span>s satisfaction<span class="token punctuation">.</span> When asked how to make 鱼香肉丝<span class="token punctuation">,</span> the AI responded that it requires the preparation of meat slices<span class="token punctuation">,</span> scallion<span class="token punctuation">,</span> ginger<span class="token punctuation">,</span> garlic<span class="token punctuation">,</span> fish sauce<span class="token punctuation">,</span> doubanjiang<span class="token punctuation">,</span> vinegar<span class="token punctuation">,</span> sugar <span class="token keyword">and</span> salt<span class="token punctuation">.</span> The meat slices should be marinated <span class="token keyword">with</span> salt<span class="token punctuation">,</span> cooking wine <span class="token keyword">and</span> pepper<span class="token punctuation">,</span> then stir<span class="token operator">-</span>fried <span class="token keyword">with</span> scallion<span class="token punctuation">,</span> ginger <span class="token keyword">and</span> garlic<span class="token punctuation">.</span> The fish sauce<span class="token punctuation">,</span> doubanjiang<span class="token punctuation">,</span> vinegar<span class="token punctuation">,</span> sugar <span class="token keyword">and</span> salt should be added <span class="token keyword">in</span><span class="token punctuation">,</span> <span class="token keyword">with</span> some water added to cook the dish<span class="token punctuation">.</span>Human<span class="token punctuation">:</span> 那蚝油牛肉呢？AI<span class="token punctuation">:</span><span class="token operator">></span> Finished chain<span class="token punctuation">.</span><span class="token string">' 蚝油牛肉需要准备牛肉、蚝油、葱、姜、蒜、料酒、盐、糖、醋、淀粉和水。牛肉应先用盐、料酒和胡椒粉腌制，然后和葱、姜、蒜一起爆炒，再加入蚝油、糖、盐、醋和水，最后加入淀粉勾芡即可。'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="两者结合，使用SummaryBufferMemory"><a href="#两者结合，使用SummaryBufferMemory" class="headerlink" title="两者结合，使用SummaryBufferMemory"></a>两者结合，使用SummaryBufferMemory</h2><p>虽然SummaryMemory可以支持更长的对话轮数，但是它也有一个缺点，就是 <strong>即使是最近几轮的对话，记录的也不是精确的内容</strong>。当你问“上一轮我问的问题是什么？”的时候，它其实没法给出准确的回答。不过，相信你也想到了，我们把BufferMemory和SummaryMemory结合一下不就好了吗？没错，LangChain里还真提供了一个这样的解决方案，就叫做ConversationSummaryBufferMemory。</p><p>下面，我们就来看看ConversationSummaryBufferMemory怎么用。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain <span class="token keyword">import</span> PromptTemplate<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> ConversationChain<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationSummaryBufferMemory<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms <span class="token keyword">import</span> OpenAISUMMARIZER_TEMPLATE <span class="token operator">=</span> <span class="token triple-quoted-string string">"""请将以下内容逐步概括所提供的对话内容，并将新的概括添加到之前的概括中，形成新的概括。EXAMPLECurrent summary:Human询问AI对人工智能的看法。AI认为人工智能是一种积极的力量。New lines of conversation:Human：为什么你认为人工智能是一种积极的力量？AI：因为人工智能将帮助人类发挥他们的潜能。New summary:Human询问AI对人工智能的看法。AI认为人工智能是一种积极的力量，因为它将帮助人类发挥他们的潜能。END OF EXAMPLECurrent summary:&#123;summary&#125;New lines of conversation:&#123;new_lines&#125;New summary:"""</span>SUMMARY_PROMPT <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"summary"</span><span class="token punctuation">,</span> <span class="token string">"new_lines"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> template<span class="token operator">=</span>SUMMARIZER_TEMPLATE<span class="token punctuation">)</span>memory <span class="token operator">=</span> ConversationSummaryBufferMemory<span class="token punctuation">(</span>llm<span class="token operator">=</span>OpenAI<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> prompt<span class="token operator">=</span>SUMMARY_PROMPT<span class="token punctuation">,</span> max_token_limit<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span>CHEF_TEMPLATE <span class="token operator">=</span> <span class="token triple-quoted-string string">"""你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:1. 你的回答必须是中文。2. 对于做菜步骤的回答尽量详细一些。&#123;history&#125;Human: &#123;input&#125;AI:"""</span>CHEF_PROMPT <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"history"</span><span class="token punctuation">,</span> <span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> template<span class="token operator">=</span>CHEF_TEMPLATE<span class="token punctuation">)</span>conversation_with_summary <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>    llm<span class="token operator">=</span>OpenAI<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">"text-davinci-003"</span><span class="token punctuation">,</span> stop<span class="token operator">=</span><span class="token string">"\n\n"</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    prompt<span class="token operator">=</span>CHEF_PROMPT<span class="token punctuation">,</span>    memory<span class="token operator">=</span>memory<span class="token punctuation">,</span>    verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>answer <span class="token operator">=</span> conversation_with_summary<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"你是谁？"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>answer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Entering new ConversationChain chain...Prompt after formatting:你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:1. 你的回答必须是中文。2. 对于做菜步骤的回答尽量详细一些。Human: 你是谁？AI:> Finished chain. 我是一个中国厨师，您有什么可以问我的关于做菜的问题吗？<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>这个代码显得有些长，这是为了演示的时候让你看得更加清楚一些。我把Langchain原来默认的对Memory进行小结的提示语模版从英文改成中文的了，不过这个翻译工作我也是让ChatGPT帮我做的。如果你想了解原始的英文提示语是什么样的，可以去看一下它源码里面的 _DEFAULT_SUMMARIZER_TEMPLATE，对应的链接我也放在 <a href="https://github.com/hwchase17/langchain/blob/master/langchain/memory/prompt.py">这里</a> 了。</li><li>我们定义了一个 ConversationSummaryBufferMemory，在这个Memory的构造函数里面，我们指定了使用的LLM、提示语，以及一个max_token_limit参数。max_token_limit参数，其实就是告诉我们，当对话的长度到多长之后，我们就应该调用LLM去把文本内容小结一下。</li><li>后面的代码其实就和前面其他的例子基本一样了。</li></ol><p>因为我们在代码里面打开了Verbose模式，所以你能看到实际AI记录的整个对话历史是怎么样的。当我们连续多问AI几句话，你就会看到，随着对话轮数的增加，Token数量超过了前面的max_token_limit 。于是SummaryBufferMemory就会触发，对前面的对话进行小结，也就会出现一个 System的信息部分，里面是聊天历史的小结，而后面完整记录的实际对话轮数就变少了。</p><p>我们先问鱼香肉丝怎么做，Verbose的信息里还是显示历史的聊天记录。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">answer = conversation_with_summary.predict(input="请问鱼香肉丝怎么做？")print(answer)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Entering new ConversationChain chain...Prompt after formatting:你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:1. 你的回答必须是中文。2. 对于做菜步骤的回答尽量详细一些。Human: 你是谁？AI:  我是一个中国厨师，您有什么可以问我的关于做菜的问题吗？Human: 请问鱼香肉丝怎么做？AI:> Finished chain. 鱼香肉丝是一道很受欢迎的中国菜，准备材料有：猪肉、木耳、胡萝卜、葱姜蒜、花椒、八角、辣椒、料酒、糖、盐、醋、麻油、香油。做法步骤如下：1. 将猪肉切成薄片，用料酒、盐、糖、醋、麻油抓匀；2. 将木耳洗净，切碎；3. 将胡萝卜切丝；4. 将葱姜蒜切碎；5. 将花椒、八角、辣椒放入油锅中炸熟；6. 将葱姜蒜炒香；7. 加入猪肉片翻炒；8. 加入木耳、胡萝卜丝、花椒、八角、辣椒翻炒；9. 加入盐、糖、醋、麻油、香油调味；10. 加入水煮熟，即可出锅。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>等到我们再问蚝油牛肉，前面的对话就被小结到System下面去了。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">answer = conversation_with_summary.predict(input="那蚝油牛肉呢？")print(answer)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Entering new ConversationChain chain...Prompt after formatting:你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:1. 你的回答必须是中文。2. 对于做菜步骤的回答尽量详细一些。System:Human询问AI是谁，AI回答自己是一个中国厨师，并问Human是否有关于做菜的问题。Human问AI如何做出鱼香肉丝，AI回答准备材料有猪肉、木耳、胡萝卜、葱姜蒜、花椒、八角、辣椒、料酒、糖、盐、醋、麻油、香油，做法步骤是将猪肉切成薄片，用料酒、盐、糖、醋、麻油抓匀，木耳Human: 那蚝油牛肉呢？AI:> Finished chain. 准备材料有牛肉、葱、姜、蒜、蚝油、料酒、醋、糖、盐、香油，做法步骤是先将牛肉切成薄片，用料酒、盐、糖、醋、麻油抓匀，然后将葱、姜、蒜切碎，加入蚝油拌匀，最后加入香油搅拌均匀即可。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当然，在你实际使用SummaryBufferMemory的时候，并不需要把各个Prompt都改成自定义的中文版本。用默认的英文Prompt就足够了。因为在Verbose信息里出现的System信息并不会在实际的对话进行过程中显示给用户。这部分提示，只要AI自己能够理解就足够了。当然，你也可以根据实际对话的效果，来改写自己需要的提示语。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/4fd464abb35dcaa62266e3fc3bf24c76.png" alt="图片"></p><p>Pinecone 在自己网站上给出了一个数据对比，不同类型的Memory，随着对话轮数的增长，占用的Token数量的变化。你可以去看一看，不同的Memory在不同的参数下，占用的Token数量是不同的。比较合理的方式，还是使用这里的ConversationSummaryBufferMemory，这样既可以在记录少数对话内容的时候，记住的东西更加精确，也可以在对话轮数增长之后，既能够记住各种信息，又不至于超出Token数量的上限。</p><p>不过，在运行程序的过程里，你应该可以感觉到现在程序跑得有点儿慢。这是因为我们使用 ConversationSummaryBufferMemory很多时候要调用多次OpenAI的API。在字数超过 max_token_limit 的时候，需要额外调用一次API来做小结。而且这样做，对应的Token数量消耗也是不少的。</p><p>所以， <strong>不是所有的任务，都适合通过调用一次ChatGPT的API来解决。</strong> 很多时候，你还是可以多思考是否可以用上一讲介绍的 UtilityChain 和 TransformChain 来解决问题。</p><h2 id="让AI记住点有用的信息"><a href="#让AI记住点有用的信息" class="headerlink" title="让AI记住点有用的信息"></a>让AI记住点有用的信息</h2><p>我们不仅可以在整个对话过程里，使用我们的Memory功能。如果你之前已经有了一系列的历史对话，我们也可以通过Memory提供的save_context接口，把历史聊天记录灌进去。然后基于这个Memory让AI接着和用户对话。比如下面我们就把一组电商客服历史对话记录给了SummaryBufferMemory。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">memory = ConversationSummaryBufferMemory(llm=OpenAI(), prompt=SUMMARY_PROMPT, max_token_limit=40)memory.save_context(    &#123;"input": "你好"&#125;,    &#123;"ouput": "你好，我是客服李四，有什么我可以帮助您的么"&#125;    )memory.save_context(    &#123;"input": "我叫张三，在你们这里下了一张订单，订单号是 2023ABCD，我的邮箱地址是 customer@abc.com，但是这个订单十几天了还没有收到货"&#125;,    &#123;"ouput": "好的，您稍等，我先为您查询一下您的订单"&#125;    )memory.load_memory_variables(&#123;&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'history': 'System: \nHuman和AI打招呼，AI介绍自己是客服李四，问Human有什么可以帮助的。Human提供订单号和邮箱地址，AI表示会为其查询订单状态。'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注：为了演示方便，我设置了一个很小的 max_token_limit，但是这个问题在大的 max_token_limit 下，面对上下文比较多的会话一样会有问题。</p><p>通过调用 memory.load_memory_variables 方法，我们发现AI对整段对话做了小结。但是这个小结有个问题，就是 <strong>它并没有提取到我们最关注的信息</strong>，比如用户的订单号、用户的邮箱。只有有了这些信息，AI才能够去查询订单，拿到结果然后回答用户的问题。</p><p>以前在还没有ChatGPT的时代，在客服聊天机器人这样的领域，我们会通过命名实体识别的方式，把邮箱、订单号之类的关键信息提取出来。在有了ChatGPT这样的大语言模型之后，我们还是应该这样做。不过我们不是让专门的命名实体识别的算法做，而是直接让ChatGPT帮我们做。Langchain也内置了一个EntityMemory的封装，让AI自动帮我们提取这样的信息。我们来试一试。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.chains import ConversationChainfrom langchain.memory import ConversationEntityMemoryfrom langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATEentityMemory = ConversationEntityMemory(llm=llm)conversation = ConversationChain(    llm=llm,    verbose=True,    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,    memory=entityMemory)answer=conversation.predict(input="我叫张老三，在你们这里下了一张订单，订单号是 2023ABCD，我的邮箱地址是 customer@abc.com，但是这个订单十几天了还没有收到货")print(answer)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Entering new ConversationChain chain...Prompt after formatting:You are an assistant to a human, powered by a large language model trained by OpenAI.You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.Context:&#123;'张老三': '', '2023ABCD': '', 'customer@abc.com': ''&#125;Current conversation:Last line:Human: 我叫张老三，在你们这里下了一张订单，订单号是 2023ABCD，我的邮箱地址是 customer@abc.com，但是这个订单十几天了还没有收到货You:> Finished chain. 您好，张老三，我很抱歉你没有收到货。我们会尽快核实订单信息，并尽快给您处理，请您耐心等待，如果有任何疑问，欢迎您随时联系我们。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们还是使用ConversationChain，只是这一次，我们指定使用EntityMemory。可以看到，在Verbose的日志里面，整个对话的提示语，多了一个叫做 Context 的部分，里面包含了刚才用户提供的姓名、订单号和邮箱。</p><p>进一步，我们把memory里面存储的东西打印出来。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">print(conversation.memory.entity_store.store)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'张老三': '张老三是一位订单号为2023ABCD、邮箱地址为customer@abc.com的客户。', '2023ABCD': '2023ABCD is an order placed by customer@abc.com that has not been received after more than ten days.', 'customer@abc.com': 'Email address of Zhang Lao San, who placed an order with Order Number 2023ABCD, but has not received the goods more than ten days later.'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以看到，EntityMemory里面不仅存储了这些命名实体的名字，也对应的把命名实体所关联的上下文记录了下来。这个时候，如果我们再通过对话来询问相关的问题，AI也能够答上来。</p><p>问题1：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">answer<span class="token operator">=</span>conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"我刚才的订单号是多少？"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>answer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">您的订单号是2023ABCD。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>问题2：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">answer<span class="token operator">=</span>conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"订单2023ABCD是谁的订单？"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>answer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">订单2023ABCD是您张老三的订单，您的邮箱地址是customer@abc<span class="token punctuation">.</span>com。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这些往往才是我们在聊天的过程中真正关注的信息。如果我们要做一个电商客服，后续的对话需要查询订单号、用户姓名的时候，这些信息是必不可少的。</p><p>事实上，我们不仅可以把这些Memory放在内存里面，还可以进一步把它们存放在Redis这样的外部存储里面。这样即使我们的服务进程消失了，这些“记忆”也不会丢失。你可以对照着 <a href="https://python.langchain.com/en/latest/modules/memory/examples/agent_with_memory_in_db.html">官方文档</a> 尝试一下。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>最后，我们来做个小结。这一讲，我主要为你讲解了Langchain里面的Memory功能。Memory对整个对话的过程里我们希望记住的东西做了封装。我们可以通过BufferWindowMemory记住过去几轮的对话，通过SummaryMemory概括对话的历史并记下来。也可以将两者结合，使用BufferSummaryMemory来维护一个对整体对话做了小结，同时又记住最近几轮对话的“记忆”。</p><p>不过， <strong>更具有实用意义的是 EntityMemory</strong>。在实际使用AI进行对话的过程中，并不是让它不分轻重地记住一切内容，而是有一些我们要关注的核心要点。比如，如果你要搭建一个电商客服的聊天机器人，你肯定希望它记住具体的订单号、用户的邮箱等等。这个时候，我们就可以使用EntityMemory，它会帮助我们记住整个对话里面的“命名实体”（Entity），保留实际在对话中我们最关心的信息。</p><p>在过去的几讲里面，从llama-index开始，我们已经学会了将外部的资料库索引起来进行问答，也学会了通过Langchain的链式调用，实时获取外部的数据信息，或者运行Python程序。这一讲，我们又专门研究了怎样记住对话中我们关心的部分。</p><p><strong>将这些能力组合起来，我们就可以搭建一个完整的，属于自己的聊天机器人。</strong> 我们可以根据用户提供的订单号，去查询订单物流信息，安抚客户；也可以根据用户想要了解的商品，查询我们的商品库，进行商品导购。而这些，也是我们下一讲要解决的问题。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>最后，我给你留一道思考题。在这一讲里，我为你介绍了EntityMemory的使用方法，Langchain里面还提供了一个 <a href="https://langchain.readthedocs.io/en/latest/modules/memory/types/kg.html">KnowledgeGraphMemory</a>，你能不能去试着用一下，看看它能在什么样的场景下帮你解决问题？</p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>在Pinecone提供的Langchain AI Handbook里面，专门测试了一下，从BufferWindowMemory到BufferSummaryMemory，对于上下文保持的能力，以及消耗的Token数量的统计。那个 <a href="https://www.pinecone.io/learn/langchain-conversational-memory/">教程</a> 你也可以去看一下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;16｜Langchain里的“记忆力”，让AI只记住有用的事儿&quot;&gt;&lt;a href=&quot;#16｜Langchain里的“记忆力”，让AI只记住有用的事儿&quot; class=&quot;headerlink&quot; title=&quot;16｜Langchain里的“记忆力”，让AI只记住有用的事</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>13 ｜让AI帮你写测试，体验多步提示语</title>
    <link href="https://zhuansun.github.io/geekbang/posts/219070318.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/219070318.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.910Z</updated>
    
    <content type="html"><![CDATA[<h1 id="13-｜让AI帮你写测试，体验多步提示语"><a href="#13-｜让AI帮你写测试，体验多步提示语" class="headerlink" title="13 ｜让AI帮你写测试，体验多步提示语"></a>13 ｜让AI帮你写测试，体验多步提示语</h1><p>你好，我是徐文浩。</p><p>上一讲，我们一起通过ChatGPT做了一个小应用。不过，这个过程并不是一个“自动档”的。我们尝试一步一步输入我们的需求，给到ChatGPT，并根据拿到的指示尝试运行代码。通过和ChatGPT不断地交互，我们最终完成了一个小应用。</p><p>虽然这在我们探索性地开发一些功能的时候，已经极大地提高了我们的效率。但是这个过程并不能做成一个产品。我们理想中的产品应该是“自动档”的，我们只要用自然语言输入自己的需求，对应的代码就自动写出来了。如果中间出现了错误，AI可以自己拿到反馈来更正，而不需要我们人工去介入调试，或者复制粘贴。</p><h2 id="先让GPT-4写个代码"><a href="#先让GPT-4写个代码" class="headerlink" title="先让GPT-4写个代码"></a>先让GPT-4写个代码</h2><p>这个思路听起来似乎有些科幻，但是随着GPT-4的发布，以及未来模型能力的进一步增长，这其实并不是遥不可及的。不过，这个时候你应该还只有GPT-3.5的API权限。所以这一讲，我们还是先把目标放低一点，先来 <strong>通过大语言模型，帮我们自动写单元测试代码</strong>。整个过程仍然是一个自动档的体验，只是能够提供的能力还相对比较简单，仅限于为现有代码提供单元测试而已。</p><p>这个想法，源自OpenAI Cookbook提供的 <a href="https://github.com/openai/openai-cookbook/blob/main/examples/Unit_test_writing_using_a_multi-step_prompt.ipynb">AI 写单元测试的示例</a>。但是那个例子里面的代码，已经不能使用了，因为对应的code-davinci-002模型已经被OpenAI下线了。但是例子里，分步骤分析问题，通过多个Prompts来完成单元测试的想法，还是非常有借鉴意义的。</p><p>我相信学完这一讲之后，随着你拿到GPT-4的API乃至未来可能会出现的GPT-5，你都完全可以用同样的方法完成更复杂的“自动写代码”的程序。</p><h2 id="设计一个有些挑战的小题目"><a href="#设计一个有些挑战的小题目" class="headerlink" title="设计一个有些挑战的小题目"></a>设计一个有些挑战的小题目</h2><p>要写测试，我们要先有一个程序。为了避免这个题目本身就在AI的训练数据集里面，它直接知道答案，我们就不选用像Leetcode这样的题库了。我用了这样一个我觉得很有意思的小题目，也就是让Python根据我们输入的一个整数代表的秒数，格式化成一段自然语言描述的时间。比如，输入1就返回1s，输入61就返回1min1s。</p><p>需求：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">用Python写一个函数，进行时间格式化输出，比如：输入  输出<span class="token number">1</span>  1s<span class="token number">61</span>  1min1s要求仅需要格式化到小时<span class="token punctuation">(</span>?h?<span class="token builtin">min</span>?s<span class="token punctuation">)</span>，即可<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>题目有了，我们还需要一个正确的解题程序。我们今天的重点不是怎么用AI刷题，所以我们不如直接让ChatGPT帮我们把程序写好。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/911bd13b46f77ae8a6d0c6c9d62cbb28.png" alt="图片"></p><p>这是对应的Notebook里面的代码。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">format_time</span><span class="token punctuation">(</span>seconds<span class="token punctuation">)</span><span class="token punctuation">:</span>    minutes<span class="token punctuation">,</span> seconds <span class="token operator">=</span> <span class="token builtin">divmod</span><span class="token punctuation">(</span>seconds<span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">)</span>    hours<span class="token punctuation">,</span> minutes <span class="token operator">=</span> <span class="token builtin">divmod</span><span class="token punctuation">(</span>minutes<span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> hours <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>hours<span class="token punctuation">&#125;</span></span><span class="token string">h</span><span class="token interpolation"><span class="token punctuation">&#123;</span>minutes<span class="token punctuation">&#125;</span></span><span class="token string">min</span><span class="token interpolation"><span class="token punctuation">&#123;</span>seconds<span class="token punctuation">&#125;</span></span><span class="token string">s"</span></span>    <span class="token keyword">elif</span> minutes <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>minutes<span class="token punctuation">&#125;</span></span><span class="token string">min</span><span class="token interpolation"><span class="token punctuation">&#123;</span>seconds<span class="token punctuation">&#125;</span></span><span class="token string">s"</span></span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>seconds<span class="token punctuation">&#125;</span></span><span class="token string">s"</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，AI快速给出了一个小程序，看上去没啥问题，能够完成我们想要的基本功能。</p><p>既然ChatGPT可以写代码，我们自然也可以让它帮我们把单元测试也写好。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/bfb1a5247b6633da2bc01ac8abccab77.png" alt="图片"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conda install pytest<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这是对应的Notebook里面的代码。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pytest<span class="token keyword">def</span> <span class="token function">test_format_time</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">assert</span> format_time<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"1s"</span>    <span class="token keyword">assert</span> format_time<span class="token punctuation">(</span><span class="token number">59</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"59s"</span>    <span class="token keyword">assert</span> format_time<span class="token punctuation">(</span><span class="token number">60</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"1min0s"</span>    <span class="token keyword">assert</span> format_time<span class="token punctuation">(</span><span class="token number">61</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"1min1s"</span>    <span class="token keyword">assert</span> format_time<span class="token punctuation">(</span><span class="token number">3600</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"1h0min0s"</span>    <span class="token keyword">assert</span> format_time<span class="token punctuation">(</span><span class="token number">3661</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"1h1min1s"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>乍一看，我们的单元测试已经写完了，那这一讲就结束了吗？当然不是了。如果你是一个比较有经验的程序员，你就会发现这个单元测试其实还是有好几个问题的。</p><ol><li>这个测试没有考虑负数。如果我们输入的是负数会怎么样？</li><li>没有考虑非整数类型的输入，如果我们输入浮点数 1.0 会怎么样？字符串“abc”会怎么样？nil这样的空值会怎么样？</li><li>是即使是整数，我们还没有考虑过，超过24小时的话，格式化后的结果是怎么样的。</li></ol><h2 id="分解步骤撰写Prompts"><a href="#分解步骤撰写Prompts" class="headerlink" title="分解步骤撰写Prompts"></a>分解步骤撰写Prompts</h2><p>所以，很多事情不是我们直接把问题一塞，给到ChatGPT就能解决的。 <strong>我们需要反过来自己思考一下，如果我们自己来为一段代码写单元测试，我们会怎么做？</strong></p><p>OpenAI的示例里给出了一个很好的思路，那就是把问题拆分成三个步骤。</p><ol><li>把代码提交给大语言模型，让大语言模型解释一下，这个代码是在干什么。</li><li>把代码以及代码的解释一起交给大语言模型，让大语言模型规划一下，针对这个代码逻辑，我们到底要写哪几个TestCase。如果在这个过程里，大语言模型规划的TestCase数量太少，那么我们就重复第二步，让AI多生成几个TestCase。</li><li>针对上面的TestCase的详细描述，再提交给大语言模型，让它根据这些描述生成具体的测试代码。在这个过程中，我们还会对生成的代码，进行一次语法检查，如果语法检查没法通过，我们就要让AI重新生成一下。这个可以避免因为大语言模型的概率采样不稳定，导致生成的代码无法运行的问题。</li></ol><p>最后，我们来实际运行一下这些代码，看看我们的代码能不能通过这些自动化测试。</p><h3 id="请AI解释要测试的代码"><a href="#请AI解释要测试的代码" class="headerlink" title="请AI解释要测试的代码"></a>请AI解释要测试的代码</h3><p>那最后，我们就根据这个步骤一步步拆解，通过Python程序来把整个过程“自动化”掉。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">gpt35</span><span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">"text-davinci-002"</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>          top_p<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stop<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"\n\n"</span><span class="token punctuation">,</span> <span class="token string">"\n\t\n"</span><span class="token punctuation">,</span> <span class="token string">"\n    \n"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    response <span class="token operator">=</span> openai<span class="token punctuation">.</span>Completion<span class="token punctuation">.</span>create<span class="token punctuation">(</span>        model<span class="token operator">=</span>model<span class="token punctuation">,</span>        prompt <span class="token operator">=</span> prompt<span class="token punctuation">,</span>        temperature <span class="token operator">=</span> temperature<span class="token punctuation">,</span>        max_tokens <span class="token operator">=</span> max_tokens<span class="token punctuation">,</span>        top_p <span class="token operator">=</span> top_p<span class="token punctuation">,</span>        stop <span class="token operator">=</span> stop        <span class="token punctuation">)</span>    message <span class="token operator">=</span> response<span class="token punctuation">[</span><span class="token string">"choices"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> messagecode <span class="token operator">=</span> <span class="token triple-quoted-string string">"""def format_time(seconds):    minutes, seconds = divmod(seconds, 60)    hours, minutes = divmod(minutes, 60)    if hours > 0:        return f"&#123;hours&#125;h&#123;minutes&#125;min&#123;seconds&#125;s"    elif minutes > 0:        return f"&#123;minutes&#125;min&#123;seconds&#125;s"    else:        return f"&#123;seconds&#125;s""""</span><span class="token keyword">def</span> <span class="token function">explain_code</span><span class="token punctuation">(</span>function_to_test<span class="token punctuation">,</span> unit_test_package<span class="token operator">=</span><span class="token string">"pytest"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    prompt <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f""""# How to write great unit tests with </span><span class="token interpolation"><span class="token punctuation">&#123;</span>unit_test_package<span class="token punctuation">&#125;</span></span><span class="token string">In this advanced tutorial for experts, we'll use Python 3.10 and `</span><span class="token interpolation"><span class="token punctuation">&#123;</span>unit_test_package<span class="token punctuation">&#125;</span></span><span class="token string">` to write a suite of unit tests to verify the behavior of the following function.```python</span><span class="token interpolation"><span class="token punctuation">&#123;</span>function_to_test<span class="token punctuation">&#125;</span></span><span class="token string">Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.- First,"""</span></span>    response <span class="token operator">=</span> gpt35<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span>    <span class="token keyword">return</span> response<span class="token punctuation">,</span> promptcode_explaination<span class="token punctuation">,</span> prompt_to_explain_code <span class="token operator">=</span> explain_code<span class="token punctuation">(</span>code<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>code_explaination<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在第一步里，我们的代码做了这样几件事情。</p><p>首先是定义了一个gpt35的函数，对调用GPT3.5的模型做了简单的封装。其中有2点需要注意。</p><ol><li>我们默认使用 text-davinci-002 模型，这是一个通过监督学习微调的生成文本的模型。因为这里我们希望生成目标明确的文本的代码解释，所以选用了这个模型。</li><li>我们对stop做了特殊的设置，只要连续两个换行或者类似连续两个换行的情况出现，就中止数据的生成。这是避免模型一口气连测试代码也生成出来。那样的话，我们没法对测试代码的生成提出具体的要求。通过stop，我们可以确保在第一步，只解释现在的功能代码有什么用。</li></ol><p>然后，我们通过一组精心设置过的提示语，让GPT模型为我们来解释代码。我们在提示语里做了4件事情。</p><ul><li>指定了使用 pytest 这个测试包。</li><li>把对应要测试的代码，也提供给了GPT模型。</li><li>告诉AI，要精确描述代码做了什么。</li><li>在最后一行用 “- First” 开头，引导GPT模型，逐步分行描述要测试的代码干了什么。</li></ul><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"> we use the `divmod` built-in function to get the quotient and remainder of `seconds` divided by 60. This is assigned to the variables `minutes` and `seconds`, respectively.- Next, we do the same thing with `minutes` and 60, assigning the results to `hours` and `minutes`.- Finally, we use string interpolation to return a string formatted according to how many hours/minutes/seconds are left.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>运行第一步的代码，我们可以看到，AI回复了几个步骤，详细地描述了我们格式化时间的代码是怎么做的。</p><h3 id="请AI根据代码解释制定测试计划"><a href="#请AI根据代码解释制定测试计划" class="headerlink" title="请AI根据代码解释制定测试计划"></a>请AI根据代码解释制定测试计划</h3><p>接下来，我们就根据生成的这个详细描述，请AI为我们制定一下具体的测试计划。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">generate_a_test_plan</span><span class="token punctuation">(</span>full_code_explaination<span class="token punctuation">,</span> unit_test_package<span class="token operator">=</span><span class="token string">"pytest"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    prompt_to_explain_a_plan <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"""A good unit test suite should aim to:- Test the function's behavior for a wide range of possible inputs- Test edge cases that the author may not have foreseen- Take advantage of the features of `</span><span class="token interpolation"><span class="token punctuation">&#123;</span>unit_test_package<span class="token punctuation">&#125;</span></span><span class="token string">` to make the tests easy to write and maintain- Be easy to read and understand, with clean code and descriptive names- Be deterministic, so that the tests always pass or fail in the same way`</span><span class="token interpolation"><span class="token punctuation">&#123;</span>unit_test_package<span class="token punctuation">&#125;</span></span><span class="token string">` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):-"""</span></span>    prompt <span class="token operator">=</span> full_code_explaination <span class="token operator">+</span> prompt_to_explain_a_plan    response <span class="token operator">=</span> gpt35<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span>    <span class="token keyword">return</span> response<span class="token punctuation">,</span> prompttest_plan<span class="token punctuation">,</span> prompt_to_get_test_plan <span class="token operator">=</span> generate_a_test_plan<span class="token punctuation">(</span>prompt_to_explain_code <span class="token operator">+</span> code_explaination<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>test_plan<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们整个测试计划的提示语，同样经过了精心设计。我们先是对AI做了几个要求。</p><ol><li>我们要求测试用例，尽量考虑输入的范围广一些。</li><li>我们要求AI想一些连代码作者没有想到过的边界条件。</li><li>我们希望AI能够利用好 pytest 这个测试包的特性。</li><li>希望测试用例清晰易读，测试的代码要干净。</li><li>我们要求测试代码的输出结果是确定的，要么通过，要么失败，不要有随机性。</li></ol><p><strong>然后，我们的提示语并没有立刻让AI去写测试代码，而是说我们要举几个例子。这样，AI就会生成一系列的示例。</strong> 我们对测试用例的提示是非常详尽的，这也是我们前面第一步没有直接让AI生成测试用例的原因。因为那样的话，我们没法在提示语中间插入这些详尽的要求。对具体的测试用例，只能寄希望于AI想得多一些。</p><p>最后，我们发给AI的提示语，则是既包括了第一步要求解释代码的内容，也包括AI生成的对代码的解释，以及这里我们新增的对测试用例的要求，提供了非常详细的上下文，这样AI的表现也会更好、更有逻辑性。</p><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"> Normal behavior:    - `format_time(0)` should return `"0s"`    - `format_time(59)` should return `"59s"`    - `format_time(60)` should return `"1min0s"`    - `format_time(119)` should return `"1min59s"`    - `format_time(3600)` should return `"1h0min0s"`    - `format_time(3601)` should return `"1h0min1s"`    - `format_time(3660)` should return `"1h1min0s"`    - `format_time(7200)` should return `"2h0min0s"`- Invalid inputs:    - `format_time(None)` should raise a `TypeError`    - `format_time("abc")` should raise a `TypeError`    - `format_time(-1)` should raise a `ValueError`<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我运行了一下这个代码，可以看到，AI提供了很多测试用例。并且，里面考虑了好几种情况，包括我们前面提到的负数这样的特殊条件，也包括输入字符串，以及None这样的内容。</p><p>不过，生成哪些用例其实是有一定的随机性的。这个也是大语言模型的一个缺点，就是可控性差。有时候，AI可能就只生成了3个用例，那样的话就会有很多情况我们的用例覆盖不到。</p><p>所以，我们可以在生成用例之后，加一个步骤，检查一下到底生成了多少个用例。如果太少的话，我们就让AI再生成一些。我在下面给了一段示例代码，通过“\n-”这样一个换行加横杆的标记来判断之前生成的测试用例数量，如果比我们设定的下限少，我们就再添加一段提示语，让AI再生成一些。</p><p>这里的提示语，我们要特别提醒AI考虑一下测试罕见情况和边界条件。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">not_enough_test_plan <span class="token operator">=</span> <span class="token triple-quoted-string string">"""The function is called with a valid number of seconds    - `format_time(1)` should return `"1s"`    - `format_time(59)` should return `"59s"`    - `format_time(60)` should return `"1min"`"""</span>approx_min_cases_to_cover <span class="token operator">=</span> <span class="token number">7</span>elaboration_needed <span class="token operator">=</span> test_plan<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">"\n-"</span><span class="token punctuation">)</span> <span class="token operator">+</span><span class="token number">1</span> <span class="token operator">&lt;</span> approx_min_cases_to_cover<span class="token keyword">if</span> elaboration_needed<span class="token punctuation">:</span>        prompt_to_elaborate_on_the_plan <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"""In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):-"""</span></span>        more_test_plan<span class="token punctuation">,</span> prompt_to_get_test_plan <span class="token operator">=</span> generate_a_test_plan<span class="token punctuation">(</span>prompt_to_explain_code <span class="token operator">+</span> code_explaination <span class="token operator">+</span> not_enough_test_plan <span class="token operator">+</span> prompt_to_elaborate_on_the_plan<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>more_test_plan<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"> The function is called with a valid number of seconds    - `format_time(1)` should return `"1s"`    - `format_time(59)` should return `"59s"`    - `format_time(60)` should return `"1min"`- The function is called with an invalid number of seconds    - `format_time(-1)` should raise a `ValueError`    - `format_time("60")` should raise a `TypeError`- The function is called with a `None` value    - `format_time(None)` should raise a `TypeError`<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="根据测试计划生成测试代码"><a href="#根据测试计划生成测试代码" class="headerlink" title="根据测试计划生成测试代码"></a>根据测试计划生成测试代码</h3><p>当然，大部分情况下，生成的测试用例数都和我们前面的实际情况是一样的。那我们就可以直接用原来的代码、代码的解释以及测试用例，作为提示语，让AI帮我们写具体的测试了。</p><p>这里的提示语也没有什么稀奇的，其实就是把前面已经生成的所有内容拼接在一起，然后要求AI根据前面的内容来写具体的测试代码。唯一有一点值得注意的是，为了避免AI忘记一开头功能代码的内容，我们特地再在提示语的最后，再次给AI看了一下我们要测试的功能代码。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">generate_test_cases</span><span class="token punctuation">(</span>function_to_test<span class="token punctuation">,</span> unit_test_package<span class="token operator">=</span><span class="token string">"pytest"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    starter_comment <span class="token operator">=</span> <span class="token string">"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator"</span>    prompt_to_generate_the_unit_test <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"""Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.```pythonimport </span><span class="token interpolation"><span class="token punctuation">&#123;</span>unit_test_package<span class="token punctuation">&#125;</span></span><span class="token string">  # used for our unit tests</span><span class="token interpolation"><span class="token punctuation">&#123;</span>function_to_test<span class="token punctuation">&#125;</span></span><span class="token string">#</span><span class="token interpolation"><span class="token punctuation">&#123;</span>starter_comment<span class="token punctuation">&#125;</span></span><span class="token string">"""</span></span>    full_unit_test_prompt <span class="token operator">=</span> prompt_to_explain_code <span class="token operator">+</span> code_explaination <span class="token operator">+</span> test_plan <span class="token operator">+</span> prompt_to_generate_the_unit_test    <span class="token keyword">return</span> gpt35<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">"text-davinci-003"</span><span class="token punctuation">,</span> prompt<span class="token operator">=</span>full_unit_test_prompt<span class="token punctuation">,</span> stop<span class="token operator">=</span><span class="token string">"```"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> prompt_to_generate_the_unit_testunit_test_response<span class="token punctuation">,</span> prompt_to_generate_the_unit_test <span class="token operator">=</span> generate_test_cases<span class="token punctuation">(</span>code<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>unit_test_response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">.</span><span class="token comment">#The first element of the tuple is the name of the test case, and the second element is the value to be passed to the format_time() function.</span><span class="token decorator annotation punctuation">@pytest<span class="token punctuation">.</span>mark<span class="token punctuation">.</span>parametrize</span><span class="token punctuation">(</span><span class="token string">'test_input,expected'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>    <span class="token punctuation">(</span><span class="token string">'0'</span><span class="token punctuation">,</span> <span class="token string">'0s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'59'</span><span class="token punctuation">,</span> <span class="token string">'59s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'60'</span><span class="token punctuation">,</span> <span class="token string">'1min0s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'119'</span><span class="token punctuation">,</span> <span class="token string">'1min59s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'3600'</span><span class="token punctuation">,</span> <span class="token string">'1h0min0s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'3601'</span><span class="token punctuation">,</span> <span class="token string">'1h0min1s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'3660'</span><span class="token punctuation">,</span> <span class="token string">'1h1min0s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'7200'</span><span class="token punctuation">,</span> <span class="token string">'2h0min0s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">test_format_time</span><span class="token punctuation">(</span>test_input<span class="token punctuation">,</span> expected<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment">#For each test case, we call the format_time() function and compare the returned value to the expected value.</span>    <span class="token keyword">assert</span> format_time<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>test_input<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">==</span> expected<span class="token comment">#We use the @pytest.mark.parametrize decorator again to test the invalid inputs.</span><span class="token decorator annotation punctuation">@pytest<span class="token punctuation">.</span>mark<span class="token punctuation">.</span>parametrize</span><span class="token punctuation">(</span><span class="token string">'test_input'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>    <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token string">'abc'</span><span class="token punctuation">,</span>    <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">test_format_time_invalid_inputs</span><span class="token punctuation">(</span>test_input<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment">#For each invalid input, we expect a TypeError or ValueError to be raised.</span>    <span class="token keyword">with</span> pytest<span class="token punctuation">.</span>raises<span class="token punctuation">(</span><span class="token punctuation">(</span>TypeError<span class="token punctuation">,</span> ValueError<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        format_time<span class="token punctuation">(</span>test_input<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>运行一下这段提示语，我们就拿到了最终输出的测试代码。可以看到，这个测试代码不仅有正常情况下的测试，也包含了异常输入的测试。</p><h3 id="通过AST库进行语法检查"><a href="#通过AST库进行语法检查" class="headerlink" title="通过AST库进行语法检查"></a>通过AST库进行语法检查</h3><p>不过这还没有完，我们最好还是再检查一下生成的测试代码的语法，这个可以通过Python的AST库来完成。不过需要注意，检查语法的时候，我们不仅需要生成的测试代码，也需要原来的功能代码，不然是没办法通过语法检查的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> astcode_start_index <span class="token operator">=</span> prompt_to_generate_the_unit_test<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"```python\n"</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token builtin">len</span><span class="token punctuation">(</span><span class="token string">"```python\n"</span><span class="token punctuation">)</span>code_output <span class="token operator">=</span> prompt_to_generate_the_unit_test<span class="token punctuation">[</span>code_start_index<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+</span> unit_test_response<span class="token keyword">try</span><span class="token punctuation">:</span>    ast<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>code_output<span class="token punctuation">)</span><span class="token keyword">except</span> SyntaxError <span class="token keyword">as</span> e<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Syntax error in generated code: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>e<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>很幸运，我们一次就通过了语法检查。那么接下来，我们就可以把对应的整个测试代码打印出来，执行试一试。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>code_output<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pytest  <span class="token comment"># used for our unit tests</span><span class="token keyword">def</span> <span class="token function">format_time</span><span class="token punctuation">(</span>seconds<span class="token punctuation">)</span><span class="token punctuation">:</span>    minutes<span class="token punctuation">,</span> seconds <span class="token operator">=</span> <span class="token builtin">divmod</span><span class="token punctuation">(</span>seconds<span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">)</span>    hours<span class="token punctuation">,</span> minutes <span class="token operator">=</span> <span class="token builtin">divmod</span><span class="token punctuation">(</span>minutes<span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> hours <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>hours<span class="token punctuation">&#125;</span></span><span class="token string">h</span><span class="token interpolation"><span class="token punctuation">&#123;</span>minutes<span class="token punctuation">&#125;</span></span><span class="token string">min</span><span class="token interpolation"><span class="token punctuation">&#123;</span>seconds<span class="token punctuation">&#125;</span></span><span class="token string">s"</span></span>    <span class="token keyword">elif</span> minutes <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>minutes<span class="token punctuation">&#125;</span></span><span class="token string">min</span><span class="token interpolation"><span class="token punctuation">&#123;</span>seconds<span class="token punctuation">&#125;</span></span><span class="token string">s"</span></span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>seconds<span class="token punctuation">&#125;</span></span><span class="token string">s"</span></span><span class="token comment">#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator.</span><span class="token comment">#The first element of the tuple is the name of the test case, and the second element is the value to be passed to the format_time() function.</span><span class="token decorator annotation punctuation">@pytest<span class="token punctuation">.</span>mark<span class="token punctuation">.</span>parametrize</span><span class="token punctuation">(</span><span class="token string">'test_input,expected'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>    <span class="token punctuation">(</span><span class="token string">'0'</span><span class="token punctuation">,</span> <span class="token string">'0s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'59'</span><span class="token punctuation">,</span> <span class="token string">'59s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'60'</span><span class="token punctuation">,</span> <span class="token string">'1min0s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'119'</span><span class="token punctuation">,</span> <span class="token string">'1min59s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'3600'</span><span class="token punctuation">,</span> <span class="token string">'1h0min0s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'3601'</span><span class="token punctuation">,</span> <span class="token string">'1h0min1s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'3660'</span><span class="token punctuation">,</span> <span class="token string">'1h1min0s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'7200'</span><span class="token punctuation">,</span> <span class="token string">'2h0min0s'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">test_format_time</span><span class="token punctuation">(</span>test_input<span class="token punctuation">,</span> expected<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment">#For each test case, we call the format_time() function and compare the returned value to the expected value.</span>    <span class="token keyword">assert</span> format_time<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>test_input<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">==</span> expected<span class="token comment">#We use the @pytest.mark.parametrize decorator again to test the invalid inputs.</span><span class="token decorator annotation punctuation">@pytest<span class="token punctuation">.</span>mark<span class="token punctuation">.</span>parametrize</span><span class="token punctuation">(</span><span class="token string">'test_input'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>    <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token string">'abc'</span><span class="token punctuation">,</span>    <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">test_format_time_invalid_inputs</span><span class="token punctuation">(</span>test_input<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment">#For each invalid input, we expect a TypeError or ValueError to be raised.</span>    <span class="token keyword">with</span> pytest<span class="token punctuation">.</span>raises<span class="token punctuation">(</span><span class="token punctuation">(</span>TypeError<span class="token punctuation">,</span> ValueError<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        format_time<span class="token punctuation">(</span>test_input<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="看看自动生成的测试帮我们抓了什么Bug"><a href="#看看自动生成的测试帮我们抓了什么Bug" class="headerlink" title="看看自动生成的测试帮我们抓了什么Bug"></a>看看自动生成的测试帮我们抓了什么Bug</h3><p>我们可以把对应生成的代码，单独复制到一个auto_unit_test.py文件里面。然后去命令行里执行一下 pytest 这个命令，看看结果是怎样的。我这里，对应的会有一个测试用例失败，就是当输入是-1的时候，测试用例预期会遇到一个TypeError或者ValueError的报错，但是实际并没有。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/b286513b33fa442a6c6fc4e5cb4dd1ae.png" alt="图片"></p><p>我们可以试着在Notebook里面调用一下 format_time(-1)，看看自动化测试跑得对不对。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">format_time<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token string">'59min59s'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以看到，输入-1的时候，输出变成了59min59s，看来AI生成的测试代码的确帮我们捕捉到了一个Bug。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，到这里这一讲也就结束了。我们也有了一段完整的，可以针对一个Python函数生成一整套自动化测试的功能了，而且它还真的帮我们抓到了一个Bug。 <strong>生成整套测试代码的过程里，我们不需要人工地复制粘帖任何内容，全都是代码自动完成的，是一个“自动档”的过程。</strong></p><p>之所以能做到这一点，是因为我们巧妙地利用了一个方法，就是将一个问题，拆分成多个提示语的步骤，循序渐进地让AI通过解释代码，构造测试用例，最后再根据代码的解释和设计的测试用例，生成最终的自动化测试。</p><p>多步提示语带来的一个好处，就是我们的内容是更加有条理、有逻辑的，也更符合我们平时写文字的方式，而不是一股脑地把各种要求都放在提示语的开头，这在解决复杂问题时往往效果不好。</p><p>此外，我们这里使用的提示语也非常重要。正是因为我们能够分步骤提示AI，所以我们能在拿到代码的解释之后，让AI考虑各种边界条件。而AI也很给力，给出-1、None这样的特殊输入，让我们的测试代码最终真的抓住了程序里的Bug。</p><p>回过头来看，如果我们只是直接把代码往ChatGPT里一贴，虽然也能生成测试用例，但是那些测试用例就比较欠考虑，不会涵盖各种边角的情况。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>这一讲的代码有点长，思考题部分需要你做的事情也多一些。</p><ol><li>你可以试着减少我们的提示语或者提示步骤，看看生成的测试用例有什么样的变化。</li><li>目前我们的代码，是过程式地一步步给你演示整个测试代码是如何生成的。如果语法检查出错了，其实我们应该要从头开始重试一遍，再生成测试代码。你可以试着把整个代码封装修改，变成一个会自动重试3次的函数。让我们可以直接调用，来为Python代码生成自动化测试。</li><li>我们这一讲里的提示语是借鉴的OpenAI Cookbook里的样例，你能不能自己尝试总结一下，这些提示语有哪些值得借鉴的常用方法？</li></ol><p>欢迎你把思考后的结果分享在评论区，也欢迎你把这一讲分享给感兴趣的朋友，我们下一讲再见。</p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>之所以我们要循序渐进地提示AI，让AI生成例子再生成代码，是因为现在的大语言模型，有一种叫做“思维链（CoT）”的能力。当我们给出更详细的推理步骤的时候，AI的表现会更好。在OpenAI Cookbook里，专门有一章讲解 <a href="https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md">思维链能力</a>，你可以去好好研读一下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;13-｜让AI帮你写测试，体验多步提示语&quot;&gt;&lt;a href=&quot;#13-｜让AI帮你写测试，体验多步提示语&quot; class=&quot;headerlink&quot; title=&quot;13 ｜让AI帮你写测试，体验多步提示语&quot;&gt;&lt;/a&gt;13 ｜让AI帮你写测试，体验多步提示语&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
  <entry>
    <title>19｜Whisper+ChatGPT：请AI代你听播客</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1808200694.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1808200694.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-07T15:35:26.931Z</updated>
    
    <content type="html"><![CDATA[<h1 id="19｜Whisper-ChatGPT：请AI代你听播客"><a href="#19｜Whisper-ChatGPT：请AI代你听播客" class="headerlink" title="19｜Whisper+ChatGPT：请AI代你听播客"></a>19｜Whisper+ChatGPT：请AI代你听播客</h1><p>你好，我是徐文浩。</p><p>今天，我们的课程开始进入一个新的主题了，那就是语音识别。过去几周我们介绍的ChatGPT虽然很强大，但是只能接受文本的输入。而在现实生活中，很多时候我们并不方便停下来打字。很多内容比如像播客也没有文字版，所以这个时候，我们就需要一个能够将语音内容转换成文本的能力。</p><p>作为目前AI界的领导者，OpenAI自然也不会放过这个需求。他们不仅发表了一个通用的语音识别模型 Whisper，还把对应的代码开源了。在今年的1月份，他们也在API里提供了对应的语音识别服务。那么今天，我们就一起来看看Whisper这个语音识别的模型可以怎么用。</p><h2 id="Whisper-API-101"><a href="#Whisper-API-101" class="headerlink" title="Whisper API 101"></a>Whisper API 101</h2><p>我自己经常会在差旅的过程中听播客。不过，筛选听什么播客的时候，有一个问题，就是光看标题和简介其实不是特别好判断里面的内容我是不是真的感兴趣。所以，在看到Whisper和ChatGPT这两个产品之后，我自然就想到了可以通过组合这两个API，让AI来代我听播客。 <strong>我想通过Whisper把想要听的播客转录成文字稿，再通过ChatGPT做个小结，看看AI总结的小结内容是不是我想要听的。</strong></p><p>我前一阵刚听过一个 <a href="https://www.listennotes.com/podcasts/onboard/ep-26-10pmK95wovN/">关于 ChatGPT 的播客</a>，我们不妨就从这个开始。我们可以通过 <a href="https://www.listennotes.com/">listennotes</a> 这个网站来搜索播客，还能够下载到播客的源文件。而且，这个网站还有一个很有用的功能，就是可以直接切出播客中的一段内容，创建出一个切片（clip）。</p><p>我们先拿一个小的切片来试试Whisper的API，对应的切片的 <a href="https://www.listennotes.com/podcast-clips/ep-26-chatgpt%E4%B8%8E%E7%94%9F%E6%88%90%E5%BC%8Fai%E7%9A%84%E6%8A%80%E6%9C%AF%E6%BC%94%E8%BF%9B%E4%B8%8E%E5%95%86%E4%B8%9A%E6%9C%AA%E6%9D%A5%E5%AF%B9%E8%AF%9Dgoogle-P9dfstDKIV6/">链接</a> 我也放在这里了。课程Github的data目录里也有已经下载好的MP3文件。</p><p>OpenAI提供的Whisper的API非常简单，你只要调用一下transcribe函数，就能将音频文件转录成文字。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token punctuation">,</span> osopenai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>getenv<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>audio_file<span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"./data/podcast_clip.mp3"</span><span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span>transcript <span class="token operator">=</span> openai<span class="token punctuation">.</span>Audio<span class="token punctuation">.</span>transcribe<span class="token punctuation">(</span><span class="token string">"whisper-1"</span><span class="token punctuation">,</span> audio_file<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>transcript<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">欢迎来到 Onboard 真实的一线经验 走新的投资思考 我是 Monica 我是高宁 我们一起聊聊软件如何改变世界 大家好 欢迎来到 Onboard 我是 Monica 自从OpenAI发布的ChatGBT 掀起了席卷世界的AI热潮 不到三个月就积累了 超过一亿的越货用户 超过<span class="token number">1300</span>万的日货用户 真的是展现了AI让人惊讶的 也让很多人直呼 这就是下一个互联网的未来 有不少观众都说 希望我们再做一期AI的讨论 于是这次硬核讨论就来了 这次我们请来了 Google Brain的研究员雪芝 她是Google大语言模型PALM Pathway Language Model的作者之一 要知道这个模型的参数量 是GPT<span class="token operator">-</span><span class="token number">3</span>的三倍还多 另外还有两位AI产品大牛 一位来自著名的StableDM 背后的商业公司Stability AI 另一位来自某硅谷科技大厂 也曾在吴恩达教授的Landing AI中 担任产品负责人 此外 莫妮凯还邀请到一位 一直关注AI的投资人朋友Bill 当做我的特邀共同主持嘉宾 我们主要讨论几个话题 一方面从研究的视角 最前沿的研究者在关注什么 现在技术的天花板 和未来大的变量可能会在哪里 第二个问题是 未来大的变量可能会在哪里 从产品和商业的角度 什么是一个好的AI产品 整个生态可能随着技术 有怎样的演变 更重要的 我们又能从上一波 AI的创业热潮中学到什么 最后 莫妮凯和Bill还会从投资人的视角 做一个回顾 总结和畅想 这里还有一个小的update 在本集发布的时候 Google也对爆发式增长的 Chad GPT做出了回应 正在测试一个基于Lambda 模型的聊天机器人 ApprenticeBot 正式发布后会有怎样的惊喜 我们都拭目以待 AI无疑是未来几年 最令人兴奋的变量之一 莫妮凯也希望未来能邀请到更多 一线从业者 从不同角度讨论这个话题 不论是想要做创业 研究 产品 还是投资的同学 希望这些对话 对于大家了解这些技术演进 商业的可能 甚至未来对于我们每个人 每个社会意味着什么 都能引发一些思考 提供一些启发 这次的讨论有些技术硬核 需要各位对生成式AI 大模型都有一些基础了解 讨论中涉及到的论文和重要概念 也会总结在本集的简介中 供大家复习参考 几位嘉宾在北美工作生活多年 夹杂英文在所难免 也请大家体谅了 欢迎来到未来 希望大家enjoy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>从转录的结果来看，有一个好消息和一个坏消息。好消息是，语音识别的转录效果非常好。我们看到尽管播客里面混杂着中英文，但是Whisper还是很好地识别出来了。坏消息是，转录出来的内容只有空格的分隔符，没有标点符号。</p><p>不过，这个问题也并不难解决。我们只要在前面的代码里面，增加一个Prompt参数就好了。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">audio_file<span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"./data/podcast_clip.mp3"</span><span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span>transcript <span class="token operator">=</span> openai<span class="token punctuation">.</span>Audio<span class="token punctuation">.</span>transcribe<span class="token punctuation">(</span><span class="token string">"whisper-1"</span><span class="token punctuation">,</span> audio_file<span class="token punctuation">,</span>                                     prompt<span class="token operator">=</span><span class="token string">"这是一段中文播客内容。"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>transcript<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">欢迎来到 Onboard<span class="token punctuation">,</span>真实的一线经验<span class="token punctuation">,</span>走新的投资思考。 我是 Monica。 我是高宁。我们一起聊聊软件如何改变世界。 大家好<span class="token punctuation">,</span>欢迎来到 Onboard<span class="token punctuation">,</span>我是 Monica。 自从 OpenAI 发布的 ChatGBT 掀起了席卷世界的 AI 热潮<span class="token punctuation">,</span> 不到三个月就积累了超过一亿的越活用户<span class="token punctuation">,</span>超过一千三百万的日活用户。 真的是展现了 AI 让人惊叹的能力<span class="token punctuation">,</span> 也让很多人直呼这就是下一个互联网的未来。 有不少观众都说希望我们再做一期 AI 的讨论<span class="token punctuation">,</span> 于是这次硬核讨论就来了。 这次我们请来了 Google Brain 的研究员雪芝<span class="token punctuation">,</span> 她是 Google 大语言模型 PAMP<span class="token punctuation">,</span>Pathway Language Model 的作者之一。 要知道<span class="token punctuation">,</span>这个模型的参数量是 GPT<span class="token operator">-</span><span class="token number">3</span> 的三倍还多。 另外还有两位 AI 产品大牛<span class="token punctuation">,</span>一位来自著名的 Stable Diffusion 背后的商业公司 Stability AI<span class="token punctuation">,</span> 另一位来自某硅谷科技大厂<span class="token punctuation">,</span>也曾在吴恩达教授的 Landing AI 中担任产品负责人。 此外<span class="token punctuation">,</span>Monica 还邀请到一位一直关注 AI 的投资人朋友 Bill 当作我的特邀共同主持嘉宾。 我们主要讨论几个话题<span class="token punctuation">,</span>一方面从研究的视角<span class="token punctuation">,</span>最前沿的研究者在关注什么? 现在技术的天花板和未来大的变量可能会在哪里? 从产品和商业的角度<span class="token punctuation">,</span>什么是一个好的 AI 产品? 整个生态可能随着技术有怎样的演变? 更重要的<span class="token punctuation">,</span>我们又能从上一波 AI 的创业热潮中学到什么? 最后<span class="token punctuation">,</span>Monica 和 Bill 还会从投资人的视角做一个回顾、总结和畅想。 这里还有一个小的 update<span class="token punctuation">,</span>在本集发布的时候<span class="token punctuation">,</span> Google 也对爆发式增长的ChatGPT 做出了回应<span class="token punctuation">,</span> 正在测试一个基于 Lambda 模型的聊天机器人 ApprenticeBot。 正式发布后会有怎样的惊喜?我们都拭目以待。 AI 无疑是未来几年最令人兴奋的变量之一<span class="token punctuation">,</span> Monica 也希望未来能邀请到更多一线从业者从不同角度讨论这个话题。 不论是想要做创业、研究、产品还是投资的同学<span class="token punctuation">,</span> 希望这些对话对于大家了解这些技术演进、商业的可能<span class="token punctuation">,</span> 甚至未来对于我们每个人、每个社会意味着什么<span class="token punctuation">,</span> 都能引发一些思考<span class="token punctuation">,</span>提供一些启发。 这次的讨论有些技术硬核<span class="token punctuation">,</span>需要各位对生成式 AI 大模型都有一些基础了解。 讨论中涉及到的论文和重要概念<span class="token punctuation">,</span>也会总结在本集的简介中<span class="token punctuation">,</span>供大家复习参考。 几位嘉宾在北美工作生活多年<span class="token punctuation">,</span>夹杂英文在所难免<span class="token punctuation">,</span>也请大家体谅了。 欢迎来到未来<span class="token punctuation">,</span>大家 enjoy!<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>我们在transcribe函数被调用的时候，传入了一个Prompt参数。里面是一句引导Whisper模型的提示语。在这里，我们的Prompt里用了一句中文介绍，并且带上了标点符号。你就会发现，transcribe函数转录出来的内容也就带上了正确的标点符号。</p><p>不过，转录出来的内容还有一点小小的瑕疵。那就是中英文混排的内容里面，英文前后会多出一些空格。那我们就再修改一下Prompt，在提示语里面也使用中英文混排并且不留空格。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">audio_file<span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"./data/podcast_clip.mp3"</span><span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span>transcript <span class="token operator">=</span> openai<span class="token punctuation">.</span>Audio<span class="token punctuation">.</span>transcribe<span class="token punctuation">(</span><span class="token string">"whisper-1"</span><span class="token punctuation">,</span> audio_file<span class="token punctuation">,</span>                                     prompt<span class="token operator">=</span><span class="token string">"这是一段Onboard播客的内容。"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>transcript<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">欢迎来到Onboard<span class="token punctuation">,</span>真实的一线经验<span class="token punctuation">,</span>走新的投资思考。 我是Monica<span class="token punctuation">,</span>我是高宁<span class="token punctuation">,</span>我们一起聊聊软件如何改变世界。 大家好<span class="token punctuation">,</span>欢迎来到Onboard<span class="token punctuation">,</span>我是Monica。 自从OpenAI发布的ChatGBT掀起了席卷世界的AI热潮<span class="token punctuation">,</span> 不到三个月就积累了超过一亿的越活用户<span class="token punctuation">,</span>超过<span class="token number">1300</span>万的日活用户。 真的是展现了AI让人惊叹的能力<span class="token punctuation">,</span>也让很多人直呼这就是下一个互联网的未来。 有不少观众都说希望我们再做一期AI的讨论<span class="token punctuation">,</span>于是这次硬核讨论就来了。 这次我们请来了Google Brain的研究员雪芝<span class="token punctuation">,</span> 她是Google大语言模型POM<span class="token punctuation">,</span>Pathway Language Model的作者之一。 要知道这个模型的参数量是GPT<span class="token operator">-</span><span class="token number">3</span>的三倍还多。 另外还有两位AI产品大牛<span class="token punctuation">,</span>一位来自著名的Stable Diffusion背后的商业公司Stability AI<span class="token punctuation">,</span> 另一位来自某硅谷科技大厂<span class="token punctuation">,</span>也曾在吴恩达教授的Landing AI中担任产品负责人。 此外<span class="token punctuation">,</span>Monica还邀请到一位一直关注AI的投资人朋友Bill<span class="token punctuation">,</span>当做我的特邀共同主持嘉宾。 我们主要讨论几个话题<span class="token punctuation">,</span>一方面从研究的视角<span class="token punctuation">,</span>最前沿的研究者在关注什么? 现在的技术的天花板和未来大的变量可能会在哪里? 从产品和商业的角度<span class="token punctuation">,</span>什么是一个好的AI产品? 整个生态可能随着技术有怎样的演变? 更重要的<span class="token punctuation">,</span>我们又能从上一波AI的创业热潮中学到什么? 最后<span class="token punctuation">,</span>Monica和Bill还会从投资人的视角做一个回顾、总结和畅想。 这里还有一个小的update<span class="token punctuation">,</span>在本集发布的时候<span class="token punctuation">,</span> Google也对爆发式增长的ChatGPT做出了回应<span class="token punctuation">,</span> 正在测试一个基于Lambda模型的聊天机器人ApprenticeBot。 正式发布后会有怎样的惊喜?我们都拭目以待。 AI无疑是未来几年最令人兴奋的变量之一<span class="token punctuation">,</span> Monica也希望未来能邀请到更多一线从业者从不同角度讨论这个话题。 不论是想要做创业、研究、产品还是投资的同学<span class="token punctuation">,</span> 希望这些对话对于大家了解这些技术演进、商业的可能<span class="token punctuation">,</span> 甚至未来对于我们每个人、每个社会意味着什么<span class="token punctuation">,</span> 都能引发一些思考<span class="token punctuation">,</span>提供一些启发。 这次的讨论有些技术硬核<span class="token punctuation">,</span>需要各位对生成式AI、大模型都有一些基础了解。 讨论中涉及到的论文和重要概念<span class="token punctuation">,</span>也会总结在本集的简介中<span class="token punctuation">,</span>供大家复习参考。 几位嘉宾在北美工作生活多年<span class="token punctuation">,</span>夹杂英文在所难免<span class="token punctuation">,</span>也请大家体谅了。 欢迎来到未来<span class="token punctuation">,</span>大家enjoy!<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以看到，输出结果的英文前后也就没有空格了。 <strong>能够在音频内容的转录之前提供一段Prompt，来引导模型更好地做语音识别，是Whisper模型的一大亮点。</strong> 如果你觉得音频里面会有很多专有名词，模型容易识别错，你就可以在Prompt里加上对应的专有名词。比如，在上面的内容转录里面，模型就把ChatGPT也听错了，变成了ChatGBT。Google的PALM模型也给听错了，听成了POM。对应的全称Pathways Language Model也少了一个s。而针对这些错漏，我们只要再修改一下Prompt，它就能够转录正确了。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">audio_file<span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"./data/podcast_clip.mp3"</span><span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span>transcript <span class="token operator">=</span> openai<span class="token punctuation">.</span>Audio<span class="token punctuation">.</span>transcribe<span class="token punctuation">(</span><span class="token string">"whisper-1"</span><span class="token punctuation">,</span> audio_file<span class="token punctuation">,</span>                                     prompt<span class="token operator">=</span><span class="token string">"这是一段Onboard播客，里面会聊到ChatGPT以及PALM这个大语言模型。这个模型也叫做Pathways Language Model。"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>transcript<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">欢迎来到Onboard<span class="token punctuation">,</span>真实的一线经验<span class="token punctuation">,</span>走新的投资思考。我是Monica。 我是高宁。我们一起聊聊软件如何改变世界。 大家好<span class="token punctuation">,</span>欢迎来到Onboard<span class="token punctuation">,</span>我是Monica。 自从OpenAI发布的ChatGPT掀起了席卷世界的AI热潮<span class="token punctuation">,</span>不到三个月就积累了超过一亿的越活用户<span class="token punctuation">,</span>超过<span class="token number">1300</span>万的日活用户。 真的是展现了AI让人惊叹的能力<span class="token punctuation">,</span>也让很多人直呼这就是下一个互联网的未来。 有不少观众都说希望我们再做一期AI的讨论<span class="token punctuation">,</span>于是这次硬核讨论就来了。 这次我们请来了Google Brain的研究员雪芝<span class="token punctuation">,</span>她是Google大语言模型PALM Pathways Language Model的作者之一。 要知道<span class="token punctuation">,</span>这个模型的参数量是GPT<span class="token operator">-</span><span class="token number">3</span>的三倍还多。 另外还有两位AI产品大牛<span class="token punctuation">,</span>一位来自著名的Stable Diffusion背后的商业公司Stability AI<span class="token punctuation">,</span> 另一位来自某硅谷科技大厂<span class="token punctuation">,</span>也曾在吴恩达教授的Landing AI中担任产品负责人。 此外<span class="token punctuation">,</span>Monica还邀请到一位一直关注AI的投资人朋友Bill当作我的特邀共同主持嘉宾。 我们主要讨论几个话题<span class="token punctuation">,</span>一方面从研究的视角<span class="token punctuation">,</span>最前沿的研究者在关注什么? 现在的技术的天花板和未来大的变量可能会在哪里? 从产品和商业的角度<span class="token punctuation">,</span>什么是一个好的AI产品? 整个生态可能随着技术有怎样的演变? 更重要的<span class="token punctuation">,</span>我们又能从上一波AI的创业热潮中学到什么? 最后<span class="token punctuation">,</span>Monica和Bill还会从投资人的视角做一个回顾、总结和畅想。 这里还有一个小的update<span class="token punctuation">,</span>在本集发布的时候<span class="token punctuation">,</span>Google也对爆发式增长的Chat GPT做出了回应。 正在测试一个基于Lambda模型的聊天机器人ApprenticeBot。 证实发布后会有怎样的惊喜<span class="token punctuation">,</span>我们都拭目以待。 AI无疑是未来几年最令人兴奋的变量之一。 Monica也希望未来能邀请到更多一线从业者从不同角度讨论这个话题。 不论是想要做创业、研究、产品还是投资的同学<span class="token punctuation">,</span> 希望这些对话对于大家了解这些技术演进、商业的可能<span class="token punctuation">,</span>甚至未来对于我们每个人、每个社会意味着什么都能引发一些思考<span class="token punctuation">,</span>提供一些启发。 这次的讨论有些技术硬核<span class="token punctuation">,</span>需要各位对生成式AI大模型都有一些基础了解。 讨论中涉及到的论文和重要概念也会总结在本集的简介中<span class="token punctuation">,</span>供大家复习参考。 几位嘉宾在北美工作生活多年<span class="token punctuation">,</span>夹杂英文在所难免<span class="token punctuation">,</span>也请大家体谅了。 欢迎来到未来<span class="token punctuation">,</span>大家enjoy!<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>出现这个现象的原因，主要和Whisper的模型原理相关，它也是一个和GPT类似的模型，会用前面转录出来的文本去预测下一帧音频的内容。通过在最前面加上文本Prompt，就会影响后面识别出来的内容的概率，也就是能够起到给专有名词“纠错”的作用。</p><p>除了模型名称、音频文件和Prompt之外，transcribe接口还支持这样三个参数。</p><ol><li>response_format，也就是返回的文件格式，我们这里是默认值，也就是JSON。实际你还可以选择TEXT这样的纯文本，或者SRT和VTT这样的音频字幕格式。这两个格式里面，除了文本内容，还会有对应的时间信息，方便你给视频和音频做字幕。你可以直接试着运行一下看看效果。</li><li>temperature，这个和我们之前在ChatGPT类型模型里的参数含义类似，就是采样下一帧的时候，如何调整概率分布。这里的参数范围是0-1之间。</li><li>language，就是音频的语言。提前给模型指定音频的语言，有助于提升模型识别的准确率和速度。</li></ol><p>这些参数你都可以自己试着改一下，看看效果。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">audio_file<span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"./data/podcast_clip.mp3"</span><span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span>transcript <span class="token operator">=</span> openai<span class="token punctuation">.</span>Audio<span class="token punctuation">.</span>transcribe<span class="token punctuation">(</span><span class="token string">"whisper-1"</span><span class="token punctuation">,</span> audio_file<span class="token punctuation">,</span> response_format<span class="token operator">=</span><span class="token string">"srt"</span><span class="token punctuation">,</span>                                     prompt<span class="token operator">=</span><span class="token string">"这是一段Onboard播客，里面会聊到PALM这个大语言模型。这个模型也叫做Pathways Language Model。"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>transcript<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token number">1</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">01</span><span class="token punctuation">,</span><span class="token number">000</span> <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span> <span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">07</span><span class="token punctuation">,</span><span class="token number">000</span>欢迎来到Onboard<span class="token punctuation">,</span>真实的一线经验<span class="token punctuation">,</span>走新的投资思考。我是Monica。<span class="token number">2</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">07</span><span class="token punctuation">,</span><span class="token number">000</span> <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span> <span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">11</span><span class="token punctuation">,</span><span class="token number">000</span>我是高宁。我们一起聊聊软件如何改变世界。<span class="token number">3</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">15</span><span class="token punctuation">,</span><span class="token number">000</span> <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span> <span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">17</span><span class="token punctuation">,</span><span class="token number">000</span>大家好<span class="token punctuation">,</span>欢迎来到Onboard<span class="token punctuation">,</span>我是Monica。<span class="token number">4</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">17</span><span class="token punctuation">,</span><span class="token number">000</span> <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span> <span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">000</span>自从OpenAI发布的ChatGBT掀起了席卷世界的AI热潮<span class="token punctuation">,</span>不到三个月就积累了超过一亿的越活用户<span class="token punctuation">,</span>超过<span class="token number">1300</span>万的日活用户。<span class="token number">5</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">000</span> <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span> <span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">34</span><span class="token punctuation">,</span><span class="token number">000</span>真的是展现了AI让人惊叹的能力<span class="token punctuation">,</span>也让很多人直呼这就是下一个互联网的未来。<span class="token number">6</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">34</span><span class="token punctuation">,</span><span class="token number">000</span> <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span> <span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">41</span><span class="token punctuation">,</span><span class="token number">000</span>有不少观众都说希望我们再做一期AI的讨论<span class="token punctuation">,</span>于是这次硬核讨论就来了。<span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>欢迎来到未来<span class="token punctuation">,</span>大家enjoy!<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="转录的时候顺便翻译一下"><a href="#转录的时候顺便翻译一下" class="headerlink" title="转录的时候顺便翻译一下"></a>转录的时候顺便翻译一下</h2><p>除了基本的音频转录功能，Whisper的API还额外提供了一个叫做translation的接口。这个接口可以在转录音频的时候直接把语音翻译成英文，我们不妨来试一下。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">audio_file<span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"./data/podcast_clip.mp3"</span><span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span>translated_prompt<span class="token operator">=</span><span class="token triple-quoted-string string">"""This is a podcast discussing ChatGPT and PaLM model.The full name of PaLM is Pathways Language Model."""</span>transcript <span class="token operator">=</span> openai<span class="token punctuation">.</span>Audio<span class="token punctuation">.</span>translate<span class="token punctuation">(</span><span class="token string">"whisper-1"</span><span class="token punctuation">,</span> audio_file<span class="token punctuation">,</span>                                    prompt<span class="token operator">=</span>translated_prompt<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>transcript<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">Welcome to Onboard<span class="token punctuation">.</span> Real first<span class="token operator">-</span>line experience<span class="token punctuation">.</span> New investment thinking<span class="token punctuation">.</span> I am Monica<span class="token punctuation">.</span> I am Gao Ning<span class="token punctuation">.</span> Let<span class="token string">'s talk about how software can change the world. Hello everyone, welcome to Onboard. I am Monica. Since the release of ChatGPT by OpenAI, the world'</span>s AI has been <span class="token keyword">in</span> a frenzy<span class="token punctuation">.</span> In less than three months<span class="token punctuation">,</span> it has accumulated more than <span class="token number">100</span> million active users<span class="token punctuation">,</span> <span class="token keyword">and</span> more than <span class="token number">13</span> million active users<span class="token punctuation">.</span> It really shows the amazing ability of AI<span class="token punctuation">.</span> It also makes many people say that this <span class="token keyword">is</span> the future of the <span class="token builtin">next</span> Internet<span class="token punctuation">.</span> Many viewers said that they wanted us to do another AI discussion<span class="token punctuation">.</span> So this discussion came<span class="token punctuation">.</span> This time we invited a researcher <span class="token keyword">from</span> Google Brain<span class="token punctuation">,</span> Xue Zhi<span class="token punctuation">.</span> He <span class="token keyword">is</span> one of the authors of Google<span class="token string">'s large-scale model PaLM, Pathways Language Model. You should know that the number of parameters of this model is three times more than ChatGPT-3. In addition, there are two AI product big cows. One is from the famous company behind Stable Diffusion, Stability AI. The other is from a Silicon Valley technology factory. He was also the product manager in Professor Wu Wenda'</span>s Landing AI<span class="token punctuation">.</span> In addition<span class="token punctuation">,</span> Monica also invited a friend of AI who has been paying attention to AI<span class="token punctuation">,</span> Bill<span class="token punctuation">,</span> <span class="token keyword">as</span> my special guest host<span class="token punctuation">.</span> We mainly discuss several topics<span class="token punctuation">.</span> On the one hand<span class="token punctuation">,</span> <span class="token keyword">from</span> the perspective of research<span class="token punctuation">,</span> what are the most cutting<span class="token operator">-</span>edge researchers paying attention to? Where are the cutting<span class="token operator">-</span>edge technologies <span class="token keyword">and</span> the large variables of the future? From the perspective of products <span class="token keyword">and</span> business<span class="token punctuation">,</span> what <span class="token keyword">is</span> a good AI product? What kind of evolution may the whole state follow? More importantly<span class="token punctuation">,</span> what can we learn <span class="token keyword">from</span> the previous wave of AI entrepreneurship? Finally<span class="token punctuation">,</span> Monica <span class="token keyword">and</span> Bill will also make a review<span class="token punctuation">,</span> summary <span class="token keyword">and</span> reflection <span class="token keyword">from</span> the perspective of investors<span class="token punctuation">.</span> Here <span class="token keyword">is</span> a small update<span class="token punctuation">.</span> When this issue was released<span class="token punctuation">,</span> Google also responded to the explosive growth of ChatGPT<span class="token punctuation">.</span> We are testing an Apprentice Bot based on Lambda model<span class="token punctuation">.</span> What kind of surprises will be released? We are looking forward to it<span class="token punctuation">.</span> AI <span class="token keyword">is</span> undoubtedly one of the most exciting variables <span class="token keyword">in</span> the coming years<span class="token punctuation">.</span> Monica also hopes to invite more first<span class="token operator">-</span>line entrepreneurs to discuss this topic <span class="token keyword">from</span> different angles<span class="token punctuation">.</span> Whether you want to do entrepreneurship<span class="token punctuation">,</span> research<span class="token punctuation">,</span> product <span class="token keyword">or</span> investment<span class="token punctuation">,</span> I hope these conversations will <span class="token builtin">help</span> you understand the possibilities of these technical horizons <span class="token keyword">and</span> business<span class="token punctuation">.</span> Even <span class="token keyword">in</span> the future<span class="token punctuation">,</span> it can cause some thoughts <span class="token keyword">and</span> inspire us to think about what it means to each person <span class="token keyword">and</span> each society<span class="token punctuation">.</span> This discussion <span class="token keyword">is</span> a bit technical<span class="token punctuation">,</span> <span class="token keyword">and</span> requires you to have some basic understanding of the biometric AI model<span class="token punctuation">.</span> The papers <span class="token keyword">and</span> important concepts involved <span class="token keyword">in</span> the discussion will also be summarized <span class="token keyword">in</span> this episode's summary<span class="token punctuation">,</span> which <span class="token keyword">is</span> <span class="token keyword">for</span> your reference<span class="token punctuation">.</span> You have worked <span class="token keyword">in</span> North America <span class="token keyword">for</span> many years<span class="token punctuation">,</span> <span class="token keyword">and</span> you may have some English mistakes<span class="token punctuation">.</span> Please understand<span class="token punctuation">.</span> Welcome to the future<span class="token punctuation">.</span> Enjoy<span class="token punctuation">.</span> Let me give you a brief introduction<span class="token punctuation">.</span> Some of your past experiences<span class="token punctuation">.</span> A fun fact<span class="token punctuation">.</span> Using an AI to represent the world <span class="token keyword">is</span> now palped<span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个接口只能把内容翻译成英文，不能变成其他语言。所以对应的，Prompt也必须换成英文。只能翻译成英文对我们来说稍微有些可惜了。如果能够指定翻译的语言，很多英文播客，我们就可以直接转录成中文来读了。现在我们要做到这一点，就不得不再花一份钱，让ChatGPT来帮我们翻译。</p><h2 id="通过分割音频来处理大文件"><a href="#通过分割音频来处理大文件" class="headerlink" title="通过分割音频来处理大文件"></a>通过分割音频来处理大文件</h2><p>刚才我们只是尝试转录了一个3分钟的音频片段，那接下来我们就来转录一下整个音频。不过，我们没法把整个150分钟的播客一次性转录出来，因为OpenAI限制Whisper一次只能转录25MB大小的文件。所以我们要先把大的播客文件分割成一个个小的片段，转录完之后再把它们拼起来。我们可以选用OpenAI在官方文档里面提供的 <a href="https://platform.openai.com/docs/guides/speech-to-text/longer-inputs">PyDub 的库</a> 来分割文件。</p><p>不过，在分割之前，我们先要通过FFmpeg把从listennotes下载的MP4文件转换成MP3格式。你不了解FFmpeg或者没有安装也没有关系，对应的命令我是让ChatGPT写的。转换后的文件我也放到了 <a href="https://github.com/xuwenhao/geektime-ai-course">课程 Github 库</a> 里的网盘地址了。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">ffmpeg <span class="token operator">-</span>i <span class="token punctuation">.</span><span class="token operator">/</span>data<span class="token operator">/</span>podcast_long<span class="token punctuation">.</span>mp4 <span class="token operator">-</span>vn <span class="token operator">-</span>c<span class="token punctuation">:</span>a libmp3lame <span class="token operator">-</span>q<span class="token punctuation">:</span>a <span class="token number">4</span> <span class="token punctuation">.</span><span class="token operator">/</span>data<span class="token operator">/</span>podcast_long<span class="token punctuation">.</span>mp3<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>分割MP3文件的代码也很简单，我们按照15分钟一个片段的方式，把音频切分一下就好了。通过PyDub的AudioSegment包，我们可以把整个长的MP3文件加载到内存里面来变成一个数组。里面每1毫秒的音频数据就是数组里的一个元素，我们可以很容易地将数组按照时间切分成每15分钟一个片段的新的MP3文件。</p><p>先确保我们安装了PyDub包。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">%</span>pip install <span class="token operator">-</span>U pydub<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> pydub <span class="token keyword">import</span> AudioSegmentpodcast <span class="token operator">=</span> AudioSegment<span class="token punctuation">.</span>from_mp3<span class="token punctuation">(</span><span class="token string">"./data/podcast_long.mp3"</span><span class="token punctuation">)</span><span class="token comment"># PyDub handles time in milliseconds</span>ten_minutes <span class="token operator">=</span> <span class="token number">15</span> <span class="token operator">*</span> <span class="token number">60</span> <span class="token operator">*</span> <span class="token number">1000</span>total_length <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>podcast<span class="token punctuation">)</span>start <span class="token operator">=</span> <span class="token number">0</span>index <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">while</span> start <span class="token operator">&lt;</span> total_length<span class="token punctuation">:</span>    end <span class="token operator">=</span> start <span class="token operator">+</span> ten_minutes    <span class="token keyword">if</span> end <span class="token operator">&lt;</span> total_length<span class="token punctuation">:</span>        chunk <span class="token operator">=</span> podcast<span class="token punctuation">[</span>start<span class="token punctuation">:</span>end<span class="token punctuation">]</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        chunk <span class="token operator">=</span> podcast<span class="token punctuation">[</span>start<span class="token punctuation">:</span><span class="token punctuation">]</span>    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"./data/podcast_clip_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>index<span class="token punctuation">&#125;</span></span><span class="token string">.mp3"</span></span><span class="token punctuation">,</span> <span class="token string">"wb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>        chunk<span class="token punctuation">.</span>export<span class="token punctuation">(</span>f<span class="token punctuation">,</span> <span class="token builtin">format</span><span class="token operator">=</span><span class="token string">"mp3"</span><span class="token punctuation">)</span>    start <span class="token operator">=</span> end    index <span class="token operator">+=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在切分完成之后，我们就一个个地来转录对应的音频文件，对应的代码就在下面。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">prompt <span class="token operator">=</span> <span class="token string">"这是一段Onboard播客，里面会聊到ChatGPT以及PALM这个大语言模型。这个模型也叫做Pathways Language Model。"</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>index<span class="token punctuation">)</span><span class="token punctuation">:</span>    clip <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"./data/podcast_clip_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">.mp3"</span></span>    audio_file<span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>clip<span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span>    transcript <span class="token operator">=</span> openai<span class="token punctuation">.</span>Audio<span class="token punctuation">.</span>transcribe<span class="token punctuation">(</span><span class="token string">"whisper-1"</span><span class="token punctuation">,</span> audio_file<span class="token punctuation">,</span>                                     prompt<span class="token operator">=</span>prompt<span class="token punctuation">)</span>    <span class="token comment"># mkdir ./data/transcripts if not exists</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token string">"./data/transcripts"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span><span class="token string">"./data/transcripts"</span><span class="token punctuation">)</span>    <span class="token comment"># write to file</span>    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"./data/transcripts/podcast_clip_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">.txt"</span></span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>transcript<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># get last sentence of the transcript</span>    sentences <span class="token operator">=</span> transcript<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"。"</span><span class="token punctuation">)</span>    prompt <span class="token operator">=</span> sentences<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这里，我们对每次进行转录的Prompt做了一个小小的特殊处理。我们把前一个片段转录结果的最后一句话，变成了下一个转录片段的提示语。这样，我们可以让后面的片段在进行语音识别的时候，知道前面最后说了什么。这样做，可以减少错别字的出现。</p><h2 id="通过开源模型直接在本地转录"><a href="#通过开源模型直接在本地转录" class="headerlink" title="通过开源模型直接在本地转录"></a>通过开源模型直接在本地转录</h2><p>通过OpenAI的Whisper API来转录音频是有成本的，目前的定价是 0.006 美元&#x2F;分钟。比如我们上面的150分钟的音频文件，只需要不到1美元，其实已经很便宜了。不过，如果你不想把对应的数据发送给OpenAI，避免任何数据泄露的风险，你还有另外一个选择，那就是直接使用OpenAI开源出来的模型就好了。</p><p>不过使用开源模型你还是需要一块GPU，如果没有的话，你仍然可以使用免费的Colab的Notebook环境。</p><p>先安装openai-whisper的相关的依赖包。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">%</span>pip install openai<span class="token operator">-</span>whisper<span class="token operator">%</span>pip install setuptools<span class="token operator">-</span>rust<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>代码本身很简单，我们只是把原先调用OpenAI的API的地方，换成了加载Whisper的模型，然后在transcribe的参数上，有一些小小的差异。其他部分的代码和前面我们调用OpenAI的Whisper API的代码基本上是一致的。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> whispermodel <span class="token operator">=</span> whisper<span class="token punctuation">.</span>load_model<span class="token punctuation">(</span><span class="token string">"large"</span><span class="token punctuation">)</span>index <span class="token operator">=</span> <span class="token number">11</span> <span class="token comment"># number of fi</span><span class="token keyword">def</span> <span class="token function">transcript</span><span class="token punctuation">(</span>clip<span class="token punctuation">,</span> prompt<span class="token punctuation">,</span> output<span class="token punctuation">)</span><span class="token punctuation">:</span>    result <span class="token operator">=</span> model<span class="token punctuation">.</span>transcribe<span class="token punctuation">(</span>clip<span class="token punctuation">,</span> initial_prompt<span class="token operator">=</span>prompt<span class="token punctuation">)</span>    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>output<span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>result<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Transcripted: "</span><span class="token punctuation">,</span> clip<span class="token punctuation">)</span>original_prompt <span class="token operator">=</span> <span class="token string">"这是一段Onboard播客，里面会聊到ChatGPT以及PALM这个大语言模型。这个模型也叫做Pathways Language Model。\n\n"</span>prompt <span class="token operator">=</span> original_prompt<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>index<span class="token punctuation">)</span><span class="token punctuation">:</span>    clip <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"./drive/MyDrive/colab_data/podcast/podcast_clip_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">.mp3"</span></span>    output <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"./drive/MyDrive/colab_data/podcast/transcripts/local_podcast_clip_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token punctuation">&#125;</span></span><span class="token string">.txt"</span></span>    transcript<span class="token punctuation">(</span>clip<span class="token punctuation">,</span> prompt<span class="token punctuation">,</span> output<span class="token punctuation">)</span>    <span class="token comment"># get last sentence of the transcript</span>    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>output<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>        transcript <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>    sentences <span class="token operator">=</span> transcript<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"。"</span><span class="token punctuation">)</span>    prompt <span class="token operator">=</span> original_prompt <span class="token operator">+</span> sentences<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>有一个点你可以注意一下，Whisper的模型和我们之前看过的其他开源模型一样，有好几种不同尺寸。你可以通过 load_model 里面的参数来决定加载什么模型。这里我们选用的是最大的 large 模型，它大约需要10GB的显存。因为Colab提供的GPU是英伟达的T4，有16G显存，所以是完全够用的。</p><p>如果你是使用自己电脑上的显卡，显存没有那么大，你可以选用小一些的模型，比如small或者base。如果你要转录的内容都是英语的，还可以直接使用small.en这样仅限于英语的模型。这种小的或者限制语言的模型，速度还更快。不过，如果是像我们这样转录中文为主，混杂了英文的内容，那么尽可能选取大一些的模型，转录的准确率才会比较高。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/7a36faf9bb3f023714dea7e24a86653d.png"></p><p>Whisper项目： <a href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></p><h2 id="结合ChatGPT做内容小结"><a href="#结合ChatGPT做内容小结" class="headerlink" title="结合ChatGPT做内容小结"></a>结合ChatGPT做内容小结</h2><p>无论是使用API还是通过本地的GPU进行文本转录，我们都会获得转录之后的文本。要给这些文本做个小结，其实我们在 <a href="https://time.geekbang.org/column/article/645305">第 10 讲</a> 讲解llama-index的时候就给过示例了。我们把那个代码稍微改写一下，就能得到对应播客的小结。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chat_models <span class="token keyword">import</span> ChatOpenAI<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>text_splitter <span class="token keyword">import</span> SpacyTextSplitter<span class="token keyword">from</span> llama_index <span class="token keyword">import</span> GPTListIndex<span class="token punctuation">,</span> LLMPredictor<span class="token punctuation">,</span> ServiceContext<span class="token punctuation">,</span> SimpleDirectoryReader<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>node_parser <span class="token keyword">import</span> SimpleNodeParser<span class="token comment"># define LLM</span>llm_predictor <span class="token operator">=</span> LLMPredictor<span class="token punctuation">(</span>llm<span class="token operator">=</span>ChatOpenAI<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> model_name<span class="token operator">=</span><span class="token string">"gpt-3.5-turbo"</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">)</span>text_splitter <span class="token operator">=</span> SpacyTextSplitter<span class="token punctuation">(</span>pipeline<span class="token operator">=</span><span class="token string">"zh_core_web_sm"</span><span class="token punctuation">,</span> chunk_size <span class="token operator">=</span> <span class="token number">2048</span><span class="token punctuation">)</span>parser <span class="token operator">=</span> SimpleNodeParser<span class="token punctuation">(</span>text_splitter<span class="token operator">=</span>text_splitter<span class="token punctuation">)</span>documents <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span><span class="token string">'./data/transcripts'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>nodes <span class="token operator">=</span> parser<span class="token punctuation">.</span>get_nodes_from_documents<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>service_context <span class="token operator">=</span> ServiceContext<span class="token punctuation">.</span>from_defaults<span class="token punctuation">(</span>llm_predictor<span class="token operator">=</span>llm_predictor<span class="token punctuation">)</span>list_index <span class="token operator">=</span> GPTListIndex<span class="token punctuation">(</span>nodes<span class="token operator">=</span>nodes<span class="token punctuation">,</span> service_context<span class="token operator">=</span>service_context<span class="token punctuation">)</span>response <span class="token operator">=</span> list_index<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"请你用中文总结一下我们的播客内容:"</span><span class="token punctuation">,</span> response_mode<span class="token operator">=</span><span class="token string">"tree_summarize"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出结果：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">这个播客讨论了人工智能和深度学习领域的高级技术和最新发展，包括稳定性人工智能、语言模型的预训练方法、图像生成模型的训练和优化，以及各种机器学习模型的比较和应用场景。同时，我们探讨了开源社区的作用和趋势，以及开源商业化的优缺点及如何应对。我们还讨论了人工智能在各个领域的应用和未来发展趋势，并强调了找到实际应用场景和解决实际问题的重要性。最后，我们提醒说，未来值得期待的AI应用将是能够真正跟人交互的产品，对于创业公司来说，需要从用户实际的痛点出发去考虑如何更好地应用AI技术。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>基于这里的代码，你完全可以开发一个自动抓取并小结你订阅的播客内容的小应用。一般的播客也就是40-50分钟一期，转录并小结一期的成本也就在5块人民币上下。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，这一讲到这里也就结束了。</p><p>OpenAI的Whisper模型，使用起来非常简单方便。无论是通过API还是使用开源的模型，只要一行代码调用一个transcribe函数，就能把一个音频文件转录成对应的文本。而且即使对于多语言混杂的内容，它也能转录得很好。而通过传入一个Prompt，它不仅能够在整个文本里，加上合适的标点符号，还能够根据Prompt里面的专有名词，减少转录中这些内容的错漏。虽然OpenAI的API接口限制了单个转录文件的大小，但是我们可以很方便地通过PyDub这样的Python包，把音频文件切分成多个小的片段来解决问题。</p><p>对于转录后的结果，我们可以很容易地使用之前学习过的ChatGPT和llama-index来进行相应的文本小结。通过组合Whisper和ChatGPT，我们就可以快速地让机器自动帮助我们将播客、Youtube访谈，变成一段文本小结，能够让我们快速浏览并判定是否有必要深入去听一下原始的内容。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>我们在将长音频分片进行转录的过程里，是完全按照精确的时间去切割音频文件的。但是实际上音频的断句其实并不在那一毫秒。所以转录的时候，效果也不一定好，特别是在录音的开头和结尾部分，很有可能不是一个完整的句子，也容易出现一些错漏的情况。你能想想有什么好办法可以解决这个问题吗？我们是否可以利用SRT或VTT文件里面文本对应的时间标注信息？</p><p>欢迎你把你思考的结果分享到留言区，也欢迎你把这一讲分享给需要的朋友，我们下一讲再见！</p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>李沐老师在他的论文精读系列视频里面，有专门讲解过 <a href="https://www.bilibili.com/video/BV1VG4y1t74x/">OpenAI Whisper 的相关论文</a>。他还专门基于Whisper的开源代码做了一个用来剪辑视频的小工具 <a href="https://www.bilibili.com/video/BV1Pe4y1t7de/?spm_id_from=333.788.recommend_more_video.2&vd_source=dd7dfb298255b22a34220853aab4f816">AutoCut</a>。你有兴趣的话，可以去看一看。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;19｜Whisper-ChatGPT：请AI代你听播客&quot;&gt;&lt;a href=&quot;#19｜Whisper-ChatGPT：请AI代你听播客&quot; class=&quot;headerlink&quot; title=&quot;19｜Whisper+ChatGPT：请AI代你听播客&quot;&gt;&lt;/a&gt;19｜Wh</summary>
      
    
    
    
    <category term="AI大模型之美" scheme="https://zhuansun.github.io/geekbang/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/"/>
    
    
  </entry>
  
</feed>
