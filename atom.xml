<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>geekbang</title>
  
  
  <link href="https://zhuansun.github.io/geekbang/atom.xml" rel="self"/>
  
  <link href="https://zhuansun.github.io/geekbang/"/>
  <updated>2023-12-06T03:49:11.557Z</updated>
  <id>https://zhuansun.github.io/geekbang/</id>
  
  <author>
    <name>码农张三</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>加餐丨拜占庭将军问题：如何基于签名消息实现作战计划的一致性？</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1924459455.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1924459455.html</id>
    <published>2023-10-23T00:00:00.000Z</published>
    <updated>2023-12-06T03:49:11.557Z</updated>
    
    <content type="html"><![CDATA[<p>你好，我是韩健。</p><p>现在，课程更新了一大半，我也一直关注着留言区的问题，我发现很多同学还是对一些知识有一些误区，再三考虑之后，决定利用今天这节课，先解决留言区提到的一个比较多的问题：如何基于签名消息实现作战计划的一致性？</p><p><strong>除此之外，在论文学习中，很多同学遇到的共性问题比较多（比如 ZAB 协议的细节，后面我会补充几讲），在这里，我十分感谢你提出了这样宝贵的意见，不同的声音会帮助我不断优化课程。</strong></p><p>所以，在课程结束之后，我会再从头梳理一遍，按照关注点通过更多的加餐不断优化内容，把相关的理论和算法的内容展开，帮你彻底吃透相关的内容。</p><p>说回咱们的拜占庭将军问题。在01 讲中，为了不啰嗦，让你举一反三地学习，我对签名消息型拜占庭问题之解，没有详细展开，而是聚焦在最核心的点“签名约束了叛徒的作恶行为”，但从留言来看，很多同学在理解签名和如何实现作战一致性上，还是遇到了问题。比如不理解如何实现作战计划的一致性。</p><p>另外，考虑到签名消息是一些常用的拜占庭容错算法（比如 PBFT）的实现基础，很重要，所以这节课我会对签名消息型拜占庭问题之解进行补充。在今天的内容中，除了具体讲解如何基于签名消息实现作战计划的一致性之外，我还会说一说什么是签名消息。希望在帮你掌握签名消息型拜占庭问题之解的同时，还帮你吃透相关的基础知识。</p><p>在这里，我想强调一下，为了更好地理解这一讲的内容，我建议你先回顾一下 01 讲，加深印象。当然，在学完 01 讲之后，相信你已经明白了，签名消息拜占庭问题之解，之所以能够容忍任意数量的叛徒，关键就在于通过消息的签名，约束了叛徒的作恶行为，也就是说，任何篡改和伪造忠将的消息的行为，都会被发现。</p><p>既然签名消息这么重要，那么什么是签名消息呢？</p><h2 id="什么是签名消息？"><a href="#什么是签名消息？" class="headerlink" title="什么是签名消息？"></a>什么是签名消息？</h2><p>签名消息指的就是带有数字签名的消息，你可以这么理解“数字签名”：类似在纸质合同上进行签名来确认合同内容和证明身份。</p><p>在这里我想说的是，数字签名既可以证实内容的完整性，又可以确认内容的来源，实现不可抵赖性（Non-Repudiation）。既然签名消息优点那么多，<strong>那么如何实现签名消息呢？</strong></p><p>你应该还记得密码学的学术 CP（Bob 和 Alice）吧（不记得的话也没关系，你把他们当作 2 个人就可以了），今天 Bob 要给 Alice 发送一个消息，告诉她，“我已经到北京了”，但是 Bob 希望这个消息能被 Alice 完整地接收到，内容不能被篡改或者伪造，我们一起帮 Bob 和 Alice 想想办法，看看如何实现这个消息。</p><p>首先，为了避免密钥泄露，我们推荐 Bob 和 Alice 使用非对称加密算法（比如 RSA）。也就是说，加密和解密使用不同的秘钥，在这里，Bob 持有需要安全保管的私钥，Alice 持有公开的公钥。</p><p>然后，Bob 用哈希算法（比如 MD5）对消息进行摘要，然后用私钥对摘要进行加密，生成数字签名（Signature），就像下图的样子：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223005903.png" alt="image-20230927223005903"></p><p>接着，Bob 将加密摘要和消息一起发送给 Alice：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223015748.png" alt="image-20230927223015748"></p><p>接下来，当 Alice 接收到消息和加密摘要（Signature）后，她会用自己的公钥对加密摘要（Signature）进行解密，并对消息内容进行摘要（Degist-2），然后将新获取的摘要（Degist-2）和解密后的摘要（Degist-1）进行对比，如果 2 个摘要（Digest-1 和 Digest-2）一致，就说明消息是来自 Bob 的，并且是完整的，就像下图的样子：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223025858.png" alt="image-20230927223025858"></p><p>你看，通过这种方法，Bob 的消息就能被 Alice 完整接收到了，任何篡改和伪造 Bob 消息的行为，都会因为摘要不一致，而被发现。<strong>而这个消息就是签名消息。</strong></p><p>现在，你应该理解了什么是签名消息了吧？另外，关于在留言区提到的“为什么签名消息能约束叛将们的作恶行为？”，在这里，我再补充下，通过上面的 Bob 和 Alice 的故事，我们可以看到，在数字签名的约束下，叛将们是无法篡改和伪造忠将的消息的，因为任何篡改和伪造消息的行为都会被发现，也就是作恶的行为被约束了。也就是说，叛将这时能做“小”恶（比如，不响应消息，或者叛将们相互串通发送指定的消息）但他们无法篡改或伪造忠将的消息了。</p><p>既然数字签名约束了叛将们的作恶行为，那么苏秦怎么做才能实现作战的一致性的呢？也就是忠将们执行一致的作战计划。</p><h2 id="如何实现作战计划的一致性？"><a href="#如何实现作战计划的一致性？" class="headerlink" title="如何实现作战计划的一致性？"></a>如何实现作战计划的一致性？</h2><p>之前我已经提到了，苏秦可以通过签名消息的方式，不仅能在不增加将军人数的情况下，解决二忠一叛的难题，还能实现无论叛将数多少，忠诚的将军们始终能达成一致的作战计划。</p><p>为了方便你理解，我以二忠二叛（更复杂的叛徒作恶模型，因为叛徒们可以相互勾结串通）为例具体演示一下，是怎样实现作战计划的一致性的：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223038471.png" alt="image-20230927223038471"></p><p>需要你注意的是，4 位将军约定了一些流程来发送作战信息、执行作战指令。</p><p><strong>第一轮：</strong></p><p>先发送作战指令的将军，作为指挥官，其他的将军作为副官。</p><p>指挥官将他的签名的作战指令发送给每位副官。</p><p>每位副官，将从指挥官处收到的新的作战指令（也就与之前收的作战指令不同），按照顺序（比如按照首字母字典排序）放到一个盒子里。</p><p><strong>第二轮：</strong></p><p>除了第一轮的指挥官外，剩余的 3 位将军将分别作为指挥官，在上一轮收到的作战指令上，加上自己的签名，并转发给其他将军。</p><p><strong>第三轮：</strong></p><p>除了第一、二轮的指挥官外，剩余的 2 位将军将分别作为指挥官，在上一轮收到的作战指令上，加上自己的签名，并转发给其他将军。</p><p>最后，各位将军按照约定，比如使用盒子里最中间的那个指令来执行作战指令。（假设盒子中的指令为 A、B、C，那中间的指令也就是第 n &#x2F;2 个命令。其中，n 为盒子里的指令数，指令从 0 开始编号，也就是 B）。</p><p>为了帮你直观地理解，如何基于签名消息实现忠将们作战计划的一致性，我来演示一下作战信息协商过程。<strong>而且我会分别以忠将和叛将先发送作战信息为例来演示，</strong>这样可以完整地演示叛将对作战计划干扰破坏的可能性。</p><p>那么忠诚的将军先发送作战信息的情况是什么呢？</p><p>为了演示方便，假设苏秦先发起带有签名的作战信息，作战指令是“进攻”。那么在第一轮作战信息协商中，苏秦向齐、楚、燕发送作战指令“进攻”。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223052727.png" alt="image-20230927223052727"></p><p>在第二轮作战信息协商中，齐、楚、燕分别作为指挥官，向另外 2 位发送作战信息“进攻”。可是楚、燕已经叛变了，<strong>但在签名的约束下，他们无法篡改和伪造忠将的消息，</strong>为了达到干扰作战计划的目的，他们俩一个选择发送消息，一个默不作声，不配合。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223102672.png" alt="image-20230927223102672"></p><p>在第三轮作战信息协商中，齐、楚分别作为指挥官，将接收到的作战信息，附加上自己的签名，并转发给另外一位（这时的叛徒燕，还是默不作声，不配合）。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223111367.png" alt="image-20230927223111367"></p><p>最终，齐收到的作战信息都是“进攻”（它收到了苏秦和楚的），按照“执行盒子最中间的指令”的约定，齐会和苏秦一起执行作战指令“进攻”，实现忠将们作战计划的一致性。</p><p>那么如果是叛徒楚先发送作战信息，干扰作战计划，结果会有所不同吗？我们来具体看一看。在第一轮作战信息协商中，楚向苏秦发送作战指令“进攻”，向齐、燕发送作战指令“撤退”。（当然还有其他的情况，这里只是选择了其中一种，其他的情况，你可以都推导着试试，看看结果是不是一样？）</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223119526.png" alt="image-20230927223119526"></p><p>然后，在第二轮作战信息协商中，苏秦、齐、燕分别作为指挥官，将接收到的作战信息，附加上自己的签名，并转发给另外两位。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223127998.png" alt="image-20230927223127998"></p><p><strong>为了达到干扰作战计划的目的，叛徒楚和燕相互勾结了。</strong>比如，燕拿到了楚的私钥，也就是燕可以伪造楚的签名，这个时候，燕为了干扰作战计划，给苏秦发送作战指令“进攻”，给齐发送作战指令却是“撤退”。</p><p>接着，在第三轮作战信息协商中，苏秦、齐、燕分别作为指挥官，将接收到的作战信息，附加上自己的签名，并转发给另外一位。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927223136827.png" alt="image-20230927223136827"></p><p>最终，苏秦和齐收到的作战信息都是“撤退、进攻”，按照“执行盒子最中间的指令”的约定，苏秦、齐和燕一起执行作战指令“撤退”，实现了作战计划的一致性。也就是说，无论叛将楚和燕如何捣乱，苏秦和齐都能执行一致的作战计划，保证作战的胜利。</p><p>另外在这里，我想补充一点，签名消息的拜占庭问题之解，也是需要进行 m+1 轮（其中 m 为叛将数，所以你看，只有楚、燕是叛变的，那么就进行了三轮协商）。你也可以从另外一个角度理解：n 位将军，能容忍 (n - 2) 位叛将（只有一位忠将没有意义，因为此时不需要达成共识了）。<strong>关于这个公式，你只需要记住就好了，推导过程你可以参考论文。</strong></p><p>最后，我想说的是，签名消息型拜占庭问题之解，解决的是忠将们如何就作战计划达成共识的问题，也就只要忠将们执行了一致的作战计划就可以了。但它不关心这个共识是什么，比如，在适合进攻的时候，忠将们可能执行的作战计划是撤退。也就是，这个算法比较理论化。</p><p>关于理论化这一点，有的同学会想知道它如何去用，在我看来呢，这个算法解决的是共识的问题，没有与实际场景结合，是很难在实际场景中落地的。在实际场景中，你可以考虑后来的改进过后的拜占庭容错算法，比如 PBFT 算法。</p><h2 id="内容小结"><a href="#内容小结" class="headerlink" title="内容小结"></a>内容小结</h2><p>本节课我主要带你了解了什么签名消息，以及忠将们如何通过签名消息实现作战的一致性，我希望你明确这样几个重点：</p><ol><li>数字签名是基于非对称加密算法（比如 RSA、DSA、DH）实现的，它能防止消息的内容被篡改和消息被伪造。</li><li>签名消息约束了叛徒的作恶行为，比如，叛徒可以不响应，可以相互勾结串通，但叛徒无法篡改和伪造忠将的消息。</li><li>需要你注意的是，签名消息拜占庭问题之解，虽然实现了忠将们作战计划的一致性，但它不关心达成共识的结果是什么。</li></ol><p>最后，我想说的是，签名消息、拜占庭将军问题的签名消息之解是非常经典的基础知识，影响和启发了后来的众多拜占庭容错算法（比如 PBFT），理解了本讲的内容后，你能更好地理解其他的拜占庭容错算法，以及它们如何改进的？为什么要这么改进？比如，在 PBFT 中，基于性能的考虑，大部分场景的消息采用消息认证码（MAC），只有在视图变更（View Change）等少数场景中采用了数字签名。</p><h2 id="课堂思考"><a href="#课堂思考" class="headerlink" title="课堂思考"></a>课堂思考</h2><p>我演示了在“二忠二叛”情况下，忠将们如何实现作战计划的一致性，那么你不妨推演下，在“二忠一叛”情况下，忠将们如何实现作战计划的一致性呢？欢迎在留言区分享你的看法，与我一同讨论。</p><p>最后，感谢你的阅读，如果这篇文章让你有所收获，也欢迎你将它分享给更多的朋友。</p><hr><iframe width="100%"  frameborder=1 height=1000px src="https://time.geekbang.org/comment/nice/215640" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;你好，我是韩健。&lt;/p&gt;
&lt;p&gt;现在，课程更新了一大半，我也一直关注着留言区的问题，我发现很多同学还是对一些知识有一些误区，再三考虑之后，决定利用今天这节课，先解决留言区提到的一个比较多的问题：如何基于签名消息实现作战计划的一致性？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;除此之外，</summary>
      
    
    
    
    <category term="分布式协议与算法实战" scheme="https://zhuansun.github.io/geekbang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE%E4%B8%8E%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98/"/>
    
    
  </entry>
  
  <entry>
    <title>结束语丨静下心来，享受技术的乐趣</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1734230882.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1734230882.html</id>
    <published>2023-10-22T00:00:00.000Z</published>
    <updated>2023-12-06T03:49:11.560Z</updated>
    
    <content type="html"><![CDATA[<p>你好，我是韩健。</p><p>一晃几个月的时间就过去了，这段日子里，我们一起在课程里沟通交流，与我而言，这是一段很特别的经历。我看到很多同学凌晨还在学习、留言，留言区里经常会看到熟悉的身影，比如约书亚、唔多志、每天晒白牙、小晏子，很感谢你们一直保持着学习的热情。</p><p>就要说再见了，借今天这个机会，我想跟你唠点儿心里话。我问自己，如果只说一句话会是啥？想来想去，我觉得就是它了：<strong>静下心来，享受技术的乐趣。</strong>其实这与我之前的经历有关，我想你也能从我的经历中，看到你自己的影子。</p><p>我们都有这样的感觉，无论任何事情，如果想把它做好，其实都不容易。我记得自己在开发 InfluxDB 系统期间，为了确保进度不失控，常常睡在公司，加班加点；在写稿期间，为了交付更高质量的课程，我总是会有很多想法，偶尔会通宵写稿，核对每句话、每个细节；再比如，为了解答 kernel_distribution 同学的一个关于外部 PPT 的问题，我通过 Google 找到相关代码的出处，然后反复推敲，在凌晨 4 点准备了一个答案。</p><p>当然，技术的学习就更加不容易了，不是读几遍材料、调调代码就可以了，而是需要我们设计检测模型，来验证自己是否准确地理解了技术。我曾见过一些团队，做技术决策的依据是不成立的，设计和开发的系统，尽管迭代多版，也始终稳定不下来。在我看来，这些团队最大的问题，就是对技术的理解不准、不够。</p><p>在我看来，我们需要调整下心态，也就是静下心来，全身心地投入，去体会技术的乐趣，“Hack it and enjoy it!”。然后学习和工作中的小成就，又会不断地给我们正反馈，激励我们，最终可以行云流水般地把事情越做越好。</p><p>具体到我们课程的主题，也就是分布式技术，该怎么继续精进呢？我们都知道，分布式技术属于新技术，仍在快速发展（比如 Raft 在 2013 年才提出），没有体系化的学习材料，而且知识碎片，学习起来尤为不易。今天我想再补充几点个人看法。</p><p><strong>首先是“杨不悔”。</strong>也就是我们要“衣带渐宽终不悔，为伊消得人憔悴”。想想你在大学的时候，是不是很执着呢？学习分布式技术，也需要这么个劲头儿。</p><p><strong>其次是“张无忌”。</strong>也就是我们要“不唯书不唯上只唯实”。理论是为了解决问题的，而不是为了“正确”，理论也是在实战中不断发展的，所以在日常学习和使用技术时，我们要注意妥协，没有十全十美的技术，我们需要根据场景特点，权衡折中使用技术，并且实战也会进一步加深我们对技术的理解。</p><p><strong>最后是“师夷长技以制夷”。</strong>也就是我们要科学上网，多阅读英文资料。</p><p>另外，有些同学可能刚刚接触分布式系统和分布式技术，我对你的建议是“单点突破，再全面开花”。比如，你可以反复研究 20 讲的分布式 KV 系统，然后研究 Raft 算法，最后再去研究其他分布式算法，循序渐进地学习。</p><p>为了帮助你更好地学习，掌握“渔”的技巧。在这里，我推荐一些适合入门和深究的学习材料（当然材料不能太多，太多了，相当于没推荐）。</p><p>迭戈·安加罗（Diego Ongaro）的博士论文：安加罗的博士论文，对 Raft 算法做了很详细的描述，我建议你反复读，结合源码（比如 Hashicorp Raft）读，直到读懂每一句话。</p><p>《Paxos Made Live》：这是 Google 团队的 Paxos 实践总结，我建议你从工程实践的角度去阅读，多想想如果是你，你会怎么做。</p><p>《Eventually Consistent》：了解下沃纳·威格尔（亚马逊 CTO）对一致性的理解和定义。</p><p>说到这里，我还想强调一点，希望能在后续的工作和学习中帮到你。那就是，“技术要具有成本优势”。什么意思呢？</p><p>基于开源软件，我们很容易“堆砌”一套业务需要的功能。基于大型互联网后台（比如 QQ）的架构理念，我们能支撑极其海量的服务和流量。也就是说，实现功能或支撑海量流量，相关的软件和理念，都已经很成熟，不是挑战了，但功能背后的成本问题突出。</p><p>而成本就是钱，功能背后的成本问题是需要重视和解决的，比如，自研 KV 存储相比 Redis 降低了数量级倍数的成本。另外，分布式技术本身就是适用于规模业务的，而且随着业务规模的增加，成本的痛点会更加突出。我希望你能注意到这点，在根据实际场景设计系统架构时，如果需要的话，也将成本作为一个权衡点考虑进去。</p><p>为什么要考虑这些？<strong>因为我真心希望你是分布式系统的架构师、开发者，而不仅仅是开源软件的使用者。</strong></p><p>好了，专栏到此就告一段落了。但专栏的结束，也是另一种开始。我会花时间处理还没来得及回复的留言，也会针对一些同学的共性问题策划答疑或者加餐（这是一个承诺，也请你监督）。总的来说，我会继续帮你吃透算法原理，让你掌握分布式系统的开发实战能力。当然，你可以随时在遇到问题时，在留言区留言，我们一起交流讨论。</p><p>在文章结尾，我为你准备了一份调查问卷，题目不多，希望你能抽出两三分钟填写一下。我非常希望听听你对这个专栏的意见和建议，期待你的反馈！</p><p>最后，我想用一段话结束今天的分享，学习技术的路上你可能会遇到对无法准确理解某技术原理的问题，但你不要觉得孤单，因为这是一个正常的情况，大家都会遇到。如果你觉得某技术的原理，理解起来很吃力，你不妨先把这个技术使用起来，然后多想想，如果是你，你会怎么设计，接着你可以带着自己的猜测去研究技术背后的原理。</p><p>希望你能在繁忙的工作中，保持一颗极客的初心，享受技术的乐趣！</p><iframe width="100%"  frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/218938" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;你好，我是韩健。&lt;/p&gt;
&lt;p&gt;一晃几个月的时间就过去了，这段日子里，我们一起在课程里沟通交流，与我而言，这是一段很特别的经历。我看到很多同学凌晨还在学习、留言，留言区里经常会看到熟悉的身影，比如约书亚、唔多志、每天晒白牙、小晏子，很感谢你们一直保持着学习的热情。&lt;/p&gt;
</summary>
      
    
    
    
    <category term="分布式协议与算法实战" scheme="https://zhuansun.github.io/geekbang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE%E4%B8%8E%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98/"/>
    
    
  </entry>
  
  <entry>
    <title>20丨基于Raft的分布式KV系统开发实战（二）：如何实现代码？</title>
    <link href="https://zhuansun.github.io/geekbang/posts/280707866.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/280707866.html</id>
    <published>2023-10-21T00:00:00.000Z</published>
    <updated>2023-12-06T03:49:11.554Z</updated>
    
    <content type="html"><![CDATA[<p>你好，我是韩健。</p><p>学完上一讲后，相信你已经了解了分布式 KV 系统的架构设计，同时应该也很好奇，架构背后的细节代码是怎么实现的呢？</p><p>别着急，今天这节课，我会带你弄明白这个问题。我会具体讲解分布式 KV 系统核心功能点的实现细节。比如，如何实现读操作对应的 3 种一致性模型。而我希望你能在课下反复运行程序，多阅读源码，掌握所有的细节实现。</p><p>话不多说，我们开始今天的学习。</p><p>在上一讲中，咱们将系统划分为三大功能块（接入协议、KV 操作、分布式集群），那么今天我会按顺序具体说一说每块功能的实现，帮助你掌握架构背后的细节代码。首先，先来了解一下，如何实现接入协议。</p><h2 id="如何实现接入协议？"><a href="#如何实现接入协议？" class="headerlink" title="如何实现接入协议？"></a>如何实现接入协议？</h2><p>在 19 讲提到，我们选择了 HTTP 协议作为通讯协议，并设计了”&#x2F;key”和”&#x2F;join”2 个 HTTP RESTful API，分别用于支持 KV 操作和增加节点的操作，那么，它们是如何实现的呢？</p><p>接入协议的核心实现，就是下面的样子。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222415760.png" alt="image-20230927222415760"></p><p>我带你走一遍这三个步骤，便于你加深印象。</p><ul><li>在 ServeHTTP() 中，会根据 URL 路径设置相关的路由信息。比如，会在 handlerKeyRequest() 中处理 URL 路径前缀为”&#x2F;key”的请求，会在 handleJoin() 中处理 URL 路径为”&#x2F;join”的请求。</li><li>在 handleKeyRequest() 中，处理来自客户端的 KV 操作请求，也就是基于 HTTP POST 请求的赋值操作、基于 HTTP GET 请求的查询操作、基于 HTTP DELETE 请求的删除操作。</li><li>在 handleJoin() 中，处理增加节点的请求，最终调用 raft.AddVoter() 函数，将新节点加入到集群中。</li></ul><p>在这里，需要你注意的是，在根据 URL 设置相关路由信息时，你需要考虑是路径前缀匹配（比如 strings.HasPrefix(r.URL.Path, “&#x2F;key”)），还是完整匹配（比如 r.URL.Path &#x3D;&#x3D; “&#x2F;join”），避免在实际运行时，路径匹配出错。比如，如果对”&#x2F;key”做完整匹配（比如 r.URL.Path &#x3D;&#x3D; “&#x2F;key”），那么下面的查询操作会因为路径匹配出错，无法找到路由信息，而执行失败。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XGET</span> raft-cluster-host01:8091/key/foo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>另外，还需要你注意的是，只有领导者节点才能执行 raft.AddVoter() 函数，也就是说，handleJoin() 函数，只能在领导者节点上执行。</p><p>说完接入协议后，接下来咱们来分析一下第二块功能的实现，也就是，如何实现 KV 操作。</p><h2 id="如何实现-KV-操作？"><a href="#如何实现-KV-操作？" class="headerlink" title="如何实现 KV 操作？"></a>如何实现 KV 操作？</h2><p>上一节课，我提到这个分布式 KV 系统会实现赋值、查询、删除 3 类操作，那具体怎么实现呢？你应该知道，赋值操作是基于 HTTP POST 请求来实现的，就像下面的样子。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XPOST</span> http://raft-cluster-host01:8091/key <span class="token parameter variable">-d</span> <span class="token string">'&#123;"foo": "bar"&#125;'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>也就是说，我们是通过 HTTP POST 请求，实现了赋值操作。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222512463.png" alt="image-20230927222512463"></p><p>同样的，我们走一遍这个过程，加深一下印象。</p><p>当接收到 KV 操作的请求时，系统将调用 handleKeyRequest() 进行处理。</p><p>在 handleKeyRequest() 函数中，检测到 HTTP 请求类型为 POST 请求时，确认了这是一个赋值操作，将执行 store.Set() 函数。</p><p>在 Set() 函数中，将创建指令，并通过 raft.Apply() 函数将指令提交给 Raft。最终指令将被应用到状态机。</p><p>当 Raft 将指令应用到状态机后，最终将执行 applySet() 函数，创建相应的 key 和值到内存中。</p><p>在这里，我想补充一下，FSM 结构复用了 Store 结构体，并实现了 fsm.Apply()、fsm.Snapshot()、fsm.Restore()3 个函数。最终应用到状态机的数据，以 map[string]string 的形式，存放在 Store.m 中。</p><p>那查询操作是怎么实现的呢？它是基于 HTTP GET 请求来实现的。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XGET</span> http://raft-cluster-host01:8091/key/foo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>也就是说，我们是通过 HTTP GET 请求实现了查询操作。在这里我想强调一下，相比需要将指令应用到状态机的赋值操作，查询操作要简单多了，因为系统只需要查询内存中的数据就可以了，不涉及状态机。具体的代码流程如图所示。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222534481.png" alt="image-20230927222534481"></p><p>我们走一遍这个过程，加深一下印象。</p><p>当接收到 KV 操作的请求时，系统将调用 handleKeyRequest() 进行处理。</p><p>在 handleKeyRequest() 函数中，检测到 HTTP 请求类型为 GET 请求时，确认了这是一个赋值操作，将执行 store.Get() 函数。</p><p>Get() 函数在内存中查询指定 key 对应的值。</p><p>而最后一个删除操作，是基于 HTTP DELETE 请求来实现的。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XDELETE</span> http://raft-cluster-host01:8091/key/foo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>也就是说，我们是通过 HTTP DELETE 请求，实现了删除操作。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222554461.png" alt="image-20230927222554461"></p><p>同样的，我们走一遍这个过程。</p><p>1、当接收到 KV 操作的请求时，系统将调用 handleKeyRequest() 进行处理。</p><p>2、在 handleKeyRequest() 函数中，检测到 HTTP 请求类型为 DELETE 请求时，确认了这是一个删除操作，将执行 store.Delete() 函数。</p><p>3、在 Delete() 函数中，将创建指令，并通过 raft.Apply() 函数，将指令提交给 Raft。最终指令将被应用到状态机。</p><p>4、当前 Raft 将指令应用到状态机后，最终执行 applyDelete() 函数，删除 key 和值。</p><p>学习这部分内容的时候，有一些同学可能会遇到，不知道如何判断指定的操作是否需要在领导者节点上执行的问题，我给的建议是这样的。</p><ul><li>需要向 Raft 状态机中提交指令的操作，是必须要在领导者节点上执行的，也就是所谓的写请求，比如赋值操作和删除操作。</li><li>需要读取最新数据的查询操作（比如客户端设置查询操作的读一致性级别为 consistent），是必须在领导者节点上执行的。</li></ul><p>说完了如何实现 KV 操作后，来看一下最后一块功能，如何实现分布式集群。</p><h2 id="如何实现分布式集群？"><a href="#如何实现分布式集群？" class="headerlink" title="如何实现分布式集群？"></a>如何实现分布式集群？</h2><h3 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h3><p>实现一个 Raft 集群，首先我们要做的就是创建集群，创建 Raft 集群，主要分为两步。首先，第一个节点通过 Bootstrap 的方式启动，并作为领导者节点。启动命令就像下面的样子。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token variable">$GOPATH</span>/bin/raftdb <span class="token parameter variable">-id</span> node01  <span class="token parameter variable">-haddr</span> raft-cluster-host01:8091 <span class="token parameter variable">-raddr</span> raft-cluster-host01:8089 ~/.raftdb<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这时将在 Store.Open() 函数中，调用 BootstrapCluster() 函数将节点启动起来。</p><p>接着，其他节点会通过 -join 参数指定领导者节点的地址信息，并向领导者节点发送，包含当前节点配置信息的增加节点请求。启动命令就像下面的样子。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token variable">$GOPATH</span>/bin/raftdb <span class="token parameter variable">-id</span> node02 <span class="token parameter variable">-haddr</span> raft-cluster-host02:8091 <span class="token parameter variable">-raddr</span> raft-cluster-host02:8089 <span class="token parameter variable">-join</span> raft-cluster-host01:8091 ~/.raftdb<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>当领导者节点接收到来自其他节点的增加节点请求后，将调用 handleJoin() 函数进行处理，并最终调用 raft.AddVoter() 函数，将新节点加入到集群中。</p><p>在这里，需要你注意的是，只有在向集群中添加新节点时，才需要使用 -join 参数。当节点加入集群后，就可以像下面这样，正常启动进程就可以了。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token variable">$GOPATH</span>/bin/raftdb <span class="token parameter variable">-id</span> node02 <span class="token parameter variable">-haddr</span> raft-cluster-host02:8091 <span class="token parameter variable">-raddr</span> raft-cluster-host02:8089  ~/.raftdb<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>集群运行起来后，因为领导者是可能会变的，那么如何实现写操作，来保证写请求都在领导者节点上执行呢？</p><h3 id="写操作"><a href="#写操作" class="headerlink" title="写操作"></a>写操作</h3><p>在 19 讲中，我们选择了方法 2 来实现写操作。也就是，当跟随者接收到写请求后，将拒绝处理该请求，并将领导者的地址信息转发给客户端。后续客户端就可以直接访问领导者（为了演示方便，我们以赋值操作为例）。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222653086.png" alt="image-20230927222653086"></p><p>我们来看一下具体的内容。</p><p>调用 Set() 函数执行赋值操作。</p><p>如果执行 Set() 函数成功，将执行步骤 3；如果执行 Set() 函数出错，且提示出错的原因是当前节点不是领导者，那这就说明了当前节点不是领导者，不能执行写操作，将执行步骤 4；如果执行 Set() 函数出错，且提示出错的原因不是因为当前节点不是领导者，将执行步骤 5。</p><p>赋值操作执行成功，正常返回。</p><p>节点将构造包含领导者地址信息的重定向响应，并返回给客户端。然后客户端直接访问领导者节点执行赋值操作。</p><p>系统运行出错，返回错误信息给客户端。</p><p>需要你注意的是，赋值操作和删除操作属于写操作，必须在领导者节点上执行。而查询操作，只是查询内存中的数据，不涉及指令提交，可以在任何节点上执行。</p><p>而为了更好的利用 curl 客户端的 HTTP 重定向功能，我实现了 HTTP 307 重定向，这样，你在执行赋值操作时，就不需要关心访问节点是否是领导者节点了。比如，你可以使用下面的命令，访问节点 2（也就是 raft-cluster-host02，192.168.0.20）执行赋值操作。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XPOST</span> raft-cluster-host02:8091/key <span class="token parameter variable">-d</span> <span class="token string">'&#123;"foo": "bar"&#125;'</span> <span class="token parameter variable">-L</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如果当前节点（也就是节点 2）不是领导者，它将返回包含领导者地址信息的 HTTP 307 重定向响应给 curl。这时，curl 根据响应信息，重新发起赋值操作请求，并直接访问领导者节点（也就是节点 1，192.168.0.10）。具体的过程，就像下面的 Wireshark 截图。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222712159.png" alt="image-20230927222712159"></p><p>相比写请求必须在领导者节点上执行，虽然查询操作属于读操作，可以在任何节点上执行，但是如何实现却更加复杂，因为读操作的实现关乎着一致性的实现。那么，具体怎么实现呢？</p><h3 id="读操作"><a href="#读操作" class="headerlink" title="读操作"></a>读操作</h3><p>我想说的是，我们可以实现 3 种一致性模型（也就是 stale、default、consistent），这样，用户就可以根据场景特点，按需选择相应的一致性级别，是不是很灵活呢？</p><p>具体的读操作的代码实现，就像下面的样子。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202310/image-20230927222723662.png" alt="image-20230927222723662"></p><p>我们走一遍这个过程。</p><p>1、当接收到 HTTP GET 的查询请求时，系统会先调用 level() 函数，来获取当前请求的读一致性级别。</p><p>2、调用 Get() 函数，查询指定 key 和读一致性级别对应的数据。</p><p>3、如果执行 Get() 函数成功，将执行步骤 4；如果执行 Get() 函数出错，且提示出错的原因是当前节点不是领导者节点，那么这就说明了，在当前节点上执行查询操作不满足读一致性级别，必须要到领导者节点上执行查询操作，将执行步骤 5；如果执行 Get() 函数出错，且提示出错的原因不是因为当前节点不是领导者，将执行步骤 6。</p><p>4、查询操作执行成功，返回查询到的值给客户端。</p><p>5、节点将构造，包含领导者地址信息的重定向响应，并返回给客户端。然后客户端直接访问领导者节点查询数据。</p><p>6、系统运行出错，返回错误信息给客户端。</p><p>在这里，为了更好地利用 curl 客户端的 HTTP 重定向功能，我同样实现了 HTTP 307 重定向（具体原理，前面已经介绍了，这里就不啰嗦了）。比如，你可以使用下面的命令，来实现一致性级别为 consistent 的查询操作，不需要关心访问节点（raft-cluster-host02）是否是领导者节点。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-XGET</span> raft-cluster-host02:8091/key/foo?level<span class="token operator">=</span>consistent  <span class="token parameter variable">-L</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><iframe width="100%"  frameborder=1 height=1000px src="https://time.geekbang.org/comment/nice/218093" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;你好，我是韩健。&lt;/p&gt;
&lt;p&gt;学完上一讲后，相信你已经了解了分布式 KV 系统的架构设计，同时应该也很好奇，架构背后的细节代码是怎么实现的呢？&lt;/p&gt;
&lt;p&gt;别着急，今天这节课，我会带你弄明白这个问题。我会具体讲解分布式 KV 系统核心功能点的实现细节。比如，如何实现读操</summary>
      
    
    
    
    <category term="分布式协议与算法实战" scheme="https://zhuansun.github.io/geekbang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE%E4%B8%8E%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98/"/>
    
    
  </entry>
  
  <entry>
    <title>01｜LangChain系统安装和快速入门</title>
    <link href="https://zhuansun.github.io/geekbang/posts/343780227.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/343780227.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.415Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01｜LangChain系统安装和快速入门"><a href="#01｜LangChain系统安装和快速入门" class="headerlink" title="01｜LangChain系统安装和快速入门"></a>01｜LangChain系统安装和快速入门</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>在我们开始正式的学习之前，先做一些基本知识储备。虽然大语言模型的使用非常简单，但是如果我们通过API来进行应用开发，那么还是有些基础知识应该先了解了解，比如什么是大模型，怎么安装LangChain，OpenAI的API有哪些类型，以及常用的开源大模型从哪里下载等等。</p><h2 id="什么是大语言模型"><a href="#什么是大语言模型" class="headerlink" title="什么是大语言模型"></a>什么是大语言模型</h2><p>大语言模型是一种人工智能模型，通常使用深度学习技术，比如神经网络，来理解和生成人类语言。这些模型的“大”在于它们的参数数量非常多，可以达到数十亿甚至更多，这使得它们能够理解和生成高度复杂的语言模式。</p><p>你可以 <strong>将大语言模型想象成一个巨大的预测机器，其训练过程主要基于“猜词”</strong>：给定一段文本的开头，它的任务就是预测下一个词是什么。模型会根据大量的训练数据（例如在互联网上爬取的文本），试图理解词语和词组在语言中的用法和含义，以及它们如何组合形成意义。它会通过不断地学习和调整参数，使得自己的预测越来越准确。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/5730e6debb8c1a0876f79814c0fb78e5.png"></p><p>比如我们给模型一个句子：“今天的天气真”，模型可能会预测出“好”作为下一个词，因为在它看过的大量训练数据中，“今天的天气真好”是一个常见的句子。这种预测并不只基于词语的统计关系，还包括对上下文的理解，甚至有时能体现出对世界常识的认知，比如它会理解到，人们通常会在天气好的时候进行户外活动。因此也就能够继续生成或者说推理出相关的内容。</p><p>但是，大语言模型并不完全理解语言，它们没有人类的情感、意识或理解力。它们只是通过复杂的数学函数学习到的语言模式，一个概率模型来做预测，所以有时候它们会犯错误，或者生成不合理甚至偏离主题的内容。</p><p>咱们当然还是主说LangChain。 <strong>LangChain 是一个全方位的、基于大语言模型这种预测能力的应用开发工具</strong>，它的灵活性和模块化特性使得处理语言模型变得极其简便。不论你在何时何地，都能利用它流畅地调用语言模型，并基于语言模型的“预测”或者说“推理”能力开发新的应用。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/6259a17134fd5a080fc3d9856a08050c.png"></p><p>LangChain 的预构建链功能，就像乐高积木一样，无论你是新手还是经验丰富的开发者，都可以选择适合自己的部分快速构建项目。对于希望进行更深入工作的开发者，LangChain 提供的模块化组件则允许你根据自己的需求定制和创建应用中的功能链条。</p><p>LangChain支持Python和JavaScript两个开发版本，我们这个教程中全部使用Python版本进行讲解。</p><h2 id="安装LangChain"><a href="#安装LangChain" class="headerlink" title="安装LangChain"></a>安装LangChain</h2><p>LangChain的基本安装特别简单。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">pip install langchain<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这是安装 LangChain 的最低要求。这里我要提醒你一点，LangChain 要与各种模型、数据存储库集成，比如说最重要的OpenAI的API接口，比如说开源大模型库HuggingFace Hub，再比如说对各种向量数据库的支持。默认情况下，是没有同时安装所需的依赖项。</p><p>也就是说，当你 <code>pip install langchain</code> 之后，可能还需要 <code>pip install openai</code>、 <code>pip install chroma</code>（一种向量数据库）……</p><p>用下面两种方法，我们就可以在安装 LangChain 的方法时，引入大多数的依赖项。</p><p>安装LangChain时包括常用的开源LLM（大语言模型） 库：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">pip install langchain[llms]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装完成之后，还需要更新到 LangChain 的最新版本，这样才能使用较新的工具。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">pip install --upgrade langchain<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如果你想从源代码安装，可以克隆存储库并运行：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">pip install -e<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>我个人觉得非常好的学习渠道也在这儿分享给你。</p><p>LangChain 的 <a href="https://github.com/langchain-ai/langchain">GitHub</a> 社区非常活跃，你可以在这里找到大量的教程和最佳实践，也可以和其他开发者分享自己的经验和观点。</p><p>LangChain也提供了详尽的 <a href="https://python.langchain.com/docs/get_started">API 文档</a>，这是你在遇到问题时的重要参考。不过呢，我觉得因为 LangChain太新了，有时你可能会发现文档中有一些错误。在这种情况下，你可以考虑更新你的版本，或者在官方平台上提交一个问题反馈。</p><p>当我遇到问题，我通常会在LangChain的GitHub开一个Issue，很快就可以得到解答。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ff014c517a428cc970e26a34b700c2a8.png"></p><p>跟着LangChain其快速的更新步伐，你就能在这个领域取得显著的进步。</p><h2 id="OpenAI-API"><a href="#OpenAI-API" class="headerlink" title="OpenAI API"></a>OpenAI API</h2><p>下面我想说一说OpenAI的API。</p><p>关于ChatGPT和GPT-4，我想就没有必要赘言了，网上已经有太多资料了。但是要继续咱们的LangChain实战课，你需要对OpenAI的API有进一步的了解。因为， <strong>LangChain本质上就是对各种大模型提供的API的套壳，是为了方便我们使用这些API，搭建起来的一些框架、模块和接口。</strong></p><p>因此，要了解LangChain的底层逻辑，需要了解大模型的API的基本设计思路。而目前接口最完备的、同时也是最强大的大语言模型，当然是OpenAI提供的GPT家族模型。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/413abcbb7c08bd0a2655a15368b980e4.png"></p><p>当然，要使用OpenAI API，你需要先用科学的方法进行注册，并得到一个API Key。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/205151183f71bdd86c25c01f99e248bf.png"></p><p>有了OpenAI的账号和Key，你就可以在面板中看到各种信息，比如模型的费用、使用情况等。下面的图片显示了各种模型的访问数量限制信息。其中，TPM和RPM分别代表tokens-per-minute、requests-per-minute。也就是说，对于GPT-4，你通过API最多每分钟调用200次、传输40000个字节。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/66e055f3c48b4bc3e11ffe0e85a5c7f3.png"></p><p>这里，我们需要重点说明的两类模型，就是图中的Chat Model和Text Model。这两类Model，是大语言模型的代表。当然，OpenAI还提供Image、Audio和其它类型的模型，目前它们不是LangChain所支持的重点，模型数量也比较少。</p><ul><li><strong>Chat Model，聊天模型</strong>，用于产生人类和AI之间的对话，代表模型当然是gpt-3.5-turbo（也就是ChatGPT）和GPT-4。当然，OpenAI还提供其它的版本，gpt-3.5-turbo-0613代表ChatGPT在2023年6月13号的一个快照，而gpt-3.5-turbo-16k则代表这个模型可以接收16K长度的Token，而不是通常的4K。（注意了，gpt-3.5-turbo-16k并未开放给我们使用，而且你传输的字节越多，花钱也越多）</li><li><strong>Text Model，文本模型</strong>，在ChatGPT出来之前，大家都使用这种模型的API来调用GPT-3，文本模型的代表作是text-davinci-003（基于GPT3）。而在这个模型家族中，也有专门训练出来做文本嵌入的text-embedding-ada-002，也有专门做相似度比较的模型，如text-similarity-curie-001。</li></ul><p>上面这两种模型，提供的功能类似，都是接收对话输入（input，也叫prompt），返回回答文本（output，也叫response）。但是，它们的调用方式和要求的输入格式是有区别的，这个我们等下还会进一步说明。</p><p>下面我们用简单的代码段说明上述两种模型的调用方式。先看比较原始的Text模型（GPT3.5之前的版本）。</p><h3 id="调用Text模型"><a href="#调用Text模型" class="headerlink" title="调用Text模型"></a>调用Text模型</h3><p>第1步，先注册好你的API Key。</p><p>第2步，用 <code>pip install openai</code> 命令来安装OpenAI库。</p><p>第3步，导入 OpenAI API Key。</p><p>导入API Key有多种方式，其中之一是通过下面的代码：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">import osos.environ["OPENAI_API_KEY"] = '你的Open API Key'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>OpenAI库就会查看名为OPENAI_API_KEY的环境变量，并使用它的值作为API密钥。</p><p>也可以像下面这样先导入OpenAI库，然后指定api_key的值。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">import openaiopenai.api_key = '你的Open API Key'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>当然，这种把Key直接放在代码里面的方法最不可取，因为你一不小心共享了代码，密钥就被别人看到了，他就可以使用你的GPT-4资源！所以，建议你给自己的OpenAI账户设个上限，比如每月10美元啥的。</p><p>所以更好的方法是在操作系统中定义环境变量，比如在Linux系统的命令行中使用：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">export OPENAI_API_KEY='你的Open API Key'<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>或者，你也可以考虑把环境变量保存在.env文件中，使用python-dotenv库从文件中读取它，这样也可以降低API密钥暴露在代码中的风险。</p><p>第4步，导入OpenAI库。（如果你在上一步导入OpenAI API Key时并没有导入OpenAI库）</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">import openai<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>第5步，调用Text模型，并返回结果。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">response = openai.Completion.create(  model="text-davinci-003",  temperature=0.5,  max_tokens=100,  prompt="请给我的花店起个名")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在使用OpenAI的文本生成模型时，你可以通过一些参数来控制输出的内容和样式。这里我总结为了一些常见的参数。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/34aaeaff93368c3c3596c12523c1ccc3.jpg"></p><p>第6步，打印输出大模型返回的文字。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">print(response.choices[0].text.strip())<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>当你调用OpenAI的Completion.create方法时，它会返回一个响应对象，该对象包含了模型生成的输出和其他一些信息。这个响应对象是一个字典结构，包含了多个字段。</p><p>在使用Text模型（如text-davinci-003）的情况下，响应对象的主要字段包括：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/4cb717e0258971c7e92dace9c4d8f2ce.jpg"></p><p>choices字段是一个列表，因为在某些情况下，你可以要求模型生成多个可能的输出。每个选择都是一个字典，其中包含以下字段：</p><ul><li>text：模型生成的文本。</li><li>finish_reason：模型停止生成的原因，可能的值包括 stop（遇到了停止标记）、length（达到了最大长度）或 temperature（根据设定的温度参数决定停止）。</li></ul><p>所以， <code>response.choices[0].text.strip()</code> 这行代码的含义是：从响应中获取第一个（如果在调用大模型时，没有指定n参数，那么就只有唯一的一个响应）选择，然后获取该选择的文本，并移除其前后的空白字符。这通常是你想要的模型的输出。</p><p>至此，任务完成，模型的输出如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">心动花庄、芳华花楼、轩辕花舍、簇烂花街、满园春色<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>不错。下面，让我们再来调用Chat模型（GPT-3.5和GPT-4）。</p><h3 id="调用Chat模型"><a href="#调用Chat模型" class="headerlink" title="调用Chat模型"></a>调用Chat模型</h3><p>整体流程上，Chat模型和Text模型的调用是完全一样的，只是输入（prompt）和输出（response）的数据格式有所不同。</p><p>示例代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">response = openai.ChatCompletion.create(  model="gpt-4",  messages=[        &#123;"role": "system", "content": "You are a creative AI."&#125;,        &#123;"role": "user", "content": "请给我的花店起个名"&#125;,    ],  temperature=0.8,  max_tokens=60)print(response['choices'][0]['message']['content'])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码中，除去刚才已经介绍过的temperature、max_tokens等参数之外，有两个专属于Chat模型的概念，一个是消息，一个是角色！</p><p>先说 <strong>消息</strong>，消息就是传入模型的提示。此处的messages参数是一个列表，包含了多个消息。每个消息都有一个role（可以是system、user或assistant）和content（消息的内容）。系统消息设定了对话的背景（你是一个很棒的智能助手），然后用户消息提出了具体请求（请给我的花店起个名）。模型的任务是基于这些消息来生成回复。</p><p>再说 <strong>角色</strong>，在OpenAI的Chat模型中，system、user和assistant都是消息的角色。每一种角色都有不同的含义和作用。</p><ul><li>system：系统消息主要用于设定对话的背景或上下文。这可以帮助模型理解它在对话中的角色和任务。例如，你可以通过系统消息来设定一个场景，让模型知道它是在扮演一个医生、律师或者一个知识丰富的AI助手。系统消息通常在对话开始时给出。</li><li>user：用户消息是从用户或人类角色发出的。它们通常包含了用户想要模型回答或完成的请求。用户消息可以是一个问题、一段话，或者任何其他用户希望模型响应的内容。</li><li>assistant：助手消息是模型的回复。例如，在你使用API发送多轮对话中新的对话请求时，可以通过助手消息提供先前对话的上下文。然而，请注意在对话的最后一条消息应始终为用户消息，因为模型总是要回应最后这条用户消息。</li></ul><p>在使用Chat模型生成内容后，返回的 <strong>响应</strong>，也就是response会包含一个或多个choices，每个choices都包含一个message。每个message也都包含一个role和content。role可以是system、user或assistant，表示该消息的发送者，content则包含了消息的实际内容。</p><p>一个典型的response对象可能如下所示：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123; 'id': 'chatcmpl-2nZI6v1cW9E3Jg4w2Xtoql0M3XHfH', 'object': 'chat.completion', 'created': 1677649420, 'model': 'gpt-4', 'usage': &#123;'prompt_tokens': 56, 'completion_tokens': 31, 'total_tokens': 87&#125;, 'choices': [   &#123;    'message': &#123;      'role': 'assistant',      'content': '你的花店可以叫做"花香四溢"。'     &#125;,    'finish_reason': 'stop',    'index': 0   &#125;  ]&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以下是各个字段的含义：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/934aaf3e187de074348198e0b0d307bd.jpg"></p><p>这就是response的基本结构，其实它和Text模型返回的响应结构也是很相似，只是choices字段中的Text换成了Message。你可以通过解析这个对象来获取你需要的信息。例如，要获取模型的回复，可使用 response[‘choices’][0][‘message’][‘content’]。</p><h3 id="Chat模型-vs-Text模型"><a href="#Chat模型-vs-Text模型" class="headerlink" title="Chat模型 vs Text模型"></a>Chat模型 vs Text模型</h3><p>Chat模型和Text模型都有各自的优点，其适用性取决于具体的应用场景。</p><p>相较于Text模型，Chat模型的设计更适合处理对话或者多轮次交互的情况。这是因为它可以接受一个消息列表作为输入，而不仅仅是一个字符串。这个消息列表可以包含system、user和assistant的历史信息，从而在处理交互式对话时提供更多的上下文信息。</p><p>这种设计的主要优点包括：</p><ol><li>对话历史的管理：通过使用Chat模型，你可以更方便地管理对话的历史，并在需要时向模型提供这些历史信息。例如，你可以将过去的用户输入和模型的回复都包含在消息列表中，这样模型在生成新的回复时就可以考虑到这些历史信息。</li><li>角色模拟：通过system角色，你可以设定对话的背景，给模型提供额外的指导信息，从而更好地控制输出的结果。当然在Text模型中，你在提示中也可以为AI设定角色，作为输入的一部分。</li></ol><p>然而，对于简单的单轮文本生成任务，使用Text模型可能会更简单、更直接。例如，如果你只需要模型根据一个简单的提示生成一段文本，那么Text模型可能更适合。从上面的结果看，Chat模型给我们输出的文本更完善，是一句完整的话，而Text模型输出的是几个名字。这是因为ChatGPT经过了对齐（基于人类反馈的强化学习），输出的答案更像是真实聊天场景。</p><p>好了，我们对OpenAI的API调用，理解到这个程度就可以了。毕竟我们主要是通过LangChain这个高级封装的框架来访问Open AI。</p><h2 id="通过LangChain调用Text和Chat模型"><a href="#通过LangChain调用Text和Chat模型" class="headerlink" title="通过LangChain调用Text和Chat模型"></a>通过LangChain调用Text和Chat模型</h2><p>最后，让我们来使用LangChain来调用OpenAI的Text和Chat模型，完成了这两个任务，我们今天的课程就可以结束了！</p><h3 id="调用Text模型-1"><a href="#调用Text模型-1" class="headerlink" title="调用Text模型"></a>调用Text模型</h3><p>代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">import osos.environ["OPENAI_API_KEY"] = '你的Open API Key'from langchain.llms import OpenAIllm = OpenAI(    model="text-davinci-003",    temperature=0.8,    max_tokens=60,)response = llm.predict("请给我的花店起个名")print(response)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">花之缘、芳华花店、花语心意、花风旖旎、芳草世界、芳色年华<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这只是一个对OpenAI API的简单封装：先导入LangChain的OpenAI类，创建一个LLM（大语言模型）对象，指定使用的模型和一些生成参数。使用创建的LLM对象和消息列表调用OpenAI类的__call__方法，进行文本生成。生成的结果被存储在response变量中。没有什么需要特别解释之处。</p><h3 id="调用Chat模型-1"><a href="#调用Chat模型-1" class="headerlink" title="调用Chat模型"></a>调用Chat模型</h3><p>代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">import osos.environ["OPENAI_API_KEY"] = '你的Open API Key'from langchain.chat_models import ChatOpenAIchat = ChatOpenAI(model="gpt-4",                    temperature=0.8,                    max_tokens=60)from langchain.schema import (    HumanMessage,    SystemMessage)messages = [    SystemMessage(content="你是一个很棒的智能助手"),    HumanMessage(content="请给我的花店起个名")]response = chat(messages)print(response)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码也不难理解，主要是通过导入LangChain的ChatOpenAI类，创建一个Chat模型对象，指定使用的模型和一些生成参数。然后从LangChain的schema模块中导入LangChain的SystemMessage和HumanMessage类，创建一个消息列表。消息列表中包含了一个系统消息和一个人类消息。你已经知道系统消息通常用来设置一些上下文或者指导AI的行为，人类消息则是要求AI回应的内容。之后，使用创建的chat对象和消息列表调用ChatOpenAI类的__call__方法，进行文本生成。生成的结果被存储在response变量中。</p><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">content='当然可以，叫做"花语秘境"怎么样？'additional_kwargs=&#123;&#125; example=False<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>从响应内容“ <strong>当然可以，叫做‘花语秘境’怎么样？</strong>”不难看出，GPT-4的创造力真的是胜过GPT-3，她给了我们这么有意境的一个店名，比我自己起的“易速鲜花”好多了。</p><p>另外，无论是langchain.llms中的OpenAI（Text模型），还是langchain.chat_models中的ChatOpenAI中的ChatOpenAI（Chat模型），其返回的结果response变量的结构，都比直接调用OpenAI API来得简单一些。这是因为，LangChain已经对大语言模型的output进行了解析，只保留了响应中最重要的文字部分。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>好了，今天课程的内容不少，我希望你理解OpenAI从Text模型到Chat模型的进化，以及什么时候你会选用Chat模型，什么时候会选用Text模型。另外就是这两种模型的最基本调用流程，掌握了这些内容，我们就可以继续后面的学习。</p><p>另外，大语言模型可不是OpenAI一家独大，知名的大模型开源社群HugginFace网站上面提供了很多开源模型供你尝试使用。就在我写这节课的时候，Meta的Llama-2最受热捧，而且通义千问（Qwen）则刚刚开源。这些趋势，你点击下面的图片就看得到。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/05f5c16f3d908b4b0a16958e28842e0e.png"></p><p>两点提醒，一是这个领域进展太快，当你学这门课程的时候，流行的开源模型肯定变成别的了；二是这些新的开源模型，LangChain还不一定提供很好的接口，因此通过LangChain来使用最新的开源模型可能不容易。</p><p>不过LangChain作为最流行的LLM框架，新的开源模型被封装进来是迟早的事情。而且，LangChain的框架也已经定型，各个组件的设计都基本固定了。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>最后给你留几个有难度的思考题，有些题目你可能现在没有答案，但是我希望你带着这些问题去继续学习后续课程。</p><ol><li><p>从今天的两个例子看起来，使用LangChain并不比直接调用OpenAI API来得省事？而且也仍然需要OpenAI API才能调用GPT家族模型。那么LangChain的核心价值在哪里？至少从这两个示例中没看出来。针对这个问题，你仔细思考思考。</p><p><strong>提示</strong>：这个问题没有标准答案，仁者见仁智者见智，等学完了课程，我们可以再回过头来回答一次。</p></li><li><p>LangChain支持的可绝不只有OpenAI模型，那么你能否试一试HuggingFace开源社区中的其它模型，看看能不能用。</p><p><strong>提示</strong>：你要选择Text-Generation、Text-Text Generation和Question-Answer这一类的文本生成式模型。</p></li></ol><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain import HuggingFaceHubllm = HuggingFaceHub(model_id="bigscience/bloom-1b7")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li><p>上面我提到了生成式模型，那么，大语言模型除了文本生成式模型，还有哪些类别的模型？比如说有名的Bert模型，是不是文本生成式的模型？</p><p><strong>提示</strong>：如果你没有太多NLP基础知识，建议你可以看一下我的专栏《 <a href="https://time.geekbang.org/column/intro/100085501">零基础实战机器学习</a>》和公开课《 <a href="https://time.geekbang.org/opencourse/videointro/100541201">ChatGPT和预训练模型实战课</a>》。</p></li></ol><p>期待在留言区看到你的思考，如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h1 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h1><ol><li>LangChain官方文档（ <a href="https://python.langchain.com/docs/get_started/introduction.html">Python版</a>）（ <a href="https://js.langchain.com/docs/get_started/introduction.html">JavaScript版</a>），这是你学习专栏的过程中，有任何疑惑都可以随时去探索的知识大本营。我个人觉得，目前LangChain的文档还不够体系化，有些杂乱，讲解也不大清楚。但是，这是官方文档，会维护得越来越好。</li><li><a href="https://platform.openai.com/docs/introduction">OpenAI API 官方文档</a>，深入学习OpenAI API的地方。</li><li><a href="https://huggingface.co/">HuggingFace 官方网站</a>，玩开源大模型的好地方。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01｜LangChain系统安装和快速入门&quot;&gt;&lt;a href=&quot;#01｜LangChain系统安装和快速入门&quot; class=&quot;headerlink&quot; title=&quot;01｜LangChain系统安装和快速入门&quot;&gt;&lt;/a&gt;01｜LangChain系统安装和快速入门&lt;/</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>02｜用LangChain快速构建基于“易速鲜花”本地知识库的智能问答系统</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1285040136.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1285040136.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.418Z</updated>
    
    <content type="html"><![CDATA[<h1 id="02｜用LangChain快速构建基于“易速鲜花”本地知识库的智能问答系统"><a href="#02｜用LangChain快速构建基于“易速鲜花”本地知识库的智能问答系统" class="headerlink" title="02｜用LangChain快速构建基于“易速鲜花”本地知识库的智能问答系统"></a>02｜用LangChain快速构建基于“易速鲜花”本地知识库的智能问答系统</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>在深入讲解LangChain的每一个具体组件之前，我想带着你从头完成一个很实用、很有意义的实战项目。目的就是让你直观感受一下LangChain作为一个基于大语言模型的应用开发框架，功能到底有多么强大。好的，现在就开始！</p><h2 id="项目及实现框架"><a href="#项目及实现框架" class="headerlink" title="项目及实现框架"></a>项目及实现框架</h2><p>我们先来整体了解一下这个项目。</p><p><strong>项目名称</strong>：“易速鲜花”内部员工知识库问答系统。</p><p><strong>项目介绍</strong>：“易速鲜花”作为一个大型在线鲜花销售平台，有自己的业务流程和规范，也拥有针对员工的SOP手册。新员工入职培训时，会分享相关的信息。但是，这些信息分散于内部网和HR部门目录各处，有时不便查询；有时因为文档过于冗长，员工无法第一时间找到想要的内容；有时公司政策已更新，但是员工手头的文档还是旧版内容。</p><p>基于上述需求，我们将开发一套基于各种内部知识手册的 “Doc-QA” 系统。这个系统将充分利用LangChain框架，处理从员工手册中产生的各种问题。这个问答系统能够理解员工的问题，并基于最新的员工手册，给出精准的答案。</p><p><strong>开发框架</strong>：下面这张图片描述了通过LangChain框架实现一个知识库文档系统的整体框架。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c66995f1bf8575fb8fyye6293200eabf.jpg"></p><p>整个框架分为这样三个部分。</p><ul><li>数据源（Data Sources）：数据可以有很多种，包括PDF在内的非结构化的数据（Unstructured Data）、SQL在内的结构化的数据（Structured Data），以及Python、Java之类的代码（Code）。在这个示例中，我们聚焦于对非结构化数据的处理。</li><li>大模型应用（Application，即LLM App）：以大模型为逻辑引擎，生成我们所需要的回答。</li><li>用例（Use-Cases）：大模型生成的回答可以构建出QA&#x2F;聊天机器人等系统。</li></ul><p><strong>核心实现机制：</strong> 这个项目的核心实现机制是下图所示的数据处理管道（Pipeline）。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/73a46eecd42038961db9067e75de3387.jpg"></p><p>在这个管道的每一步中，LangChain都为我们提供了相关工具，让你轻松实现基于文档的问答功能。</p><p>具体流程分为下面5步。</p><ol><li>Loading：文档加载器把Documents <strong>加载</strong> 为以LangChain能够读取的形式。</li><li>Splitting：文本分割器把Documents <strong>切分</strong> 为指定大小的分割，我把它们称为“文档块”或者“文档片”。</li><li>Storage：将上一步中分割好的“文档块”以“嵌入”（Embedding）的形式 <strong>存储</strong> 到向量数据库（Vector DB）中，形成一个个的“嵌入片”。</li><li>Retrieval：应用程序从存储中 <strong>检索</strong> 分割后的文档（例如通过比较余弦相似度，找到与输入问题类似的嵌入片）。</li><li>Output：把问题和相似的嵌入片传递给语言模型（LLM），使用包含问题和检索到的分割的提示 <strong>生成答案</strong>。</li></ol><p>上面5个环节的介绍都非常简单，有些概念（如嵌入、向量存储）是第一次出现，理解起来需要一些背景知识，别着急，我们接下来具体讲解这5步。</p><h2 id="数据的准备和载入"><a href="#数据的准备和载入" class="headerlink" title="数据的准备和载入"></a>数据的准备和载入</h2><p>“易速鲜花”的内部资料包括 pdf、word 和 txt 格式的各种文件，我已经放在 <a href="https://github.com/huangjia2019/langchain/tree/main/02_%E6%96%87%E6%A1%A3QA%E7%B3%BB%E7%BB%9F">这里</a> 供你下载。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/b69956a706112266df404eee953459ff.jpg"></p><p>其中一个文档的示例如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/931a55af4f0a3842a640d95c2c4bf224.jpg"></p><p>我们首先用LangChain中的document_loaders来加载各种格式的文本文件。（这些文件我把它放在OneFlower这个目录中了，如果你创建自己的文件夹，就要调整一下代码中的目录。）</p><p>在这一步中，我们从 pdf、word 和 txt 文件中加载文本，然后将这些文本存储在一个列表中。（注意：可能需要安装PyPDF、Docx2txt等库）</p><p>代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">import osos.environ["OPENAI_API_KEY"] = '你的Open AI API Key'# 1.Load 导入Document Loadersfrom langchain.document_loaders import PyPDFLoaderfrom langchain.document_loaders import Docx2txtLoaderfrom langchain.document_loaders import TextLoader# 加载Documentsbase_dir = '.\OneFlower' # 文档的存放目录documents = []for file in os.listdir(base_dir):    # 构建完整的文件路径    file_path = os.path.join(base_dir, file)    if file.endswith('.pdf'):        loader = PyPDFLoader(file_path)        documents.extend(loader.load())    elif file.endswith('.docx'):        loader = Docx2txtLoader(file_path)        documents.extend(loader.load())    elif file.endswith('.txt'):        loader = TextLoader(file_path)        documents.extend(loader.load())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里我们首先导入了OpenAI的API Key。因为后面我们需要利用Open AI的两种不同模型做以下两件事：</p><ul><li>用OpenAI的Embedding模型为文档做嵌入。</li><li>调用OpenAI的GPT模型来生成问答系统中的回答。</li></ul><p>当然了，LangChain所支持的大模型绝不仅仅是Open AI而已，你完全可以遵循这个框架，把Embedding模型和负责生成回答的语言模型都替换为其他的开源模型。</p><p>在运行上面的程序时，除了要导入正确的Open AI Key之外，还要注意的是工具包的安装。使用LangChain时，根据具体的任务，往往需要各种不同的工具包（比如上面的代码需要PyPDF和Docx2txt工具）。它们安装起来都非常简单，如果程序报错缺少某个包，只要通过 <code>pip install</code> 安装相关包即可。</p><h2 id="文本的分割"><a href="#文本的分割" class="headerlink" title="文本的分割"></a>文本的分割</h2><p>接下来需要将加载的文本分割成更小的块，以便进行嵌入和向量存储。这个步骤中，我们使用 LangChain中的RecursiveCharacterTextSplitter 来分割文本。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 2.Split 将Documents切分成块以便后续进行嵌入和向量存储from langchain.text_splitter import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)chunked_documents = text_splitter.split_documents(documents)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>现在，我们的文档被切成了一个个200字符左右的文档块。这一步，是为把它们存储进下面的向量数据库做准备。</p><h2 id="向量数据库存储"><a href="#向量数据库存储" class="headerlink" title="向量数据库存储"></a>向量数据库存储</h2><p>紧接着，我们将这些分割后的文本转换成嵌入的形式，并将其存储在一个向量数据库中。在这个例子中，我们使用了 OpenAIEmbeddings 来生成嵌入，然后使用 Qdrant 这个向量数据库来存储嵌入（这里需要pip install qdrant-client）。</p><p>如果文本的“嵌入”这个概念对你来说有些陌生的话，你可以看一下下面的说明。</p><blockquote><p>词嵌入（Word Embedding）是自然语言处理和机器学习中的一个概念，它将文字或词语转换为一系列数字，通常是一个向量。简单地说，词嵌入就是一个为每个词分配的数字列表。这些数字不是随机的，而是捕获了这个词的含义和它在文本中的上下文。因此，语义上相似或相关的词在这个数字空间中会比较接近。</p><p>举个例子，通过某种词嵌入技术，我们可能会得到：</p><p>“国王” -&gt; [1.2, 0.5, 3.1, …]</p><p>“皇帝” -&gt; [1.3, 0.6, 2.9, …]</p><p>“苹果” -&gt; [0.9, -1.2, 0.3, …]</p><p>从这些向量中，我们可以看到“国王”和“皇帝”这两个词的向量在某种程度上是相似的，而与“苹果”这个词相比，它们的向量则相差很大，因为这两个概念在语义上是不同的。</p><p>词嵌入的优点是，它提供了一种将文本数据转化为计算机可以理解和处理的形式，同时保留了词语之间的语义关系。这在许多自然语言处理任务中都是非常有用的，比如文本分类、机器翻译和情感分析等。</p></blockquote><p>你也可以对照下面的讲解学习一下向量数据库这个概念，它最近因为大语言模型的流行变得非常火爆。</p><blockquote><p>向量数据库，也称为矢量数据库或者向量搜索引擎，是一种专门用于存储和搜索向量形式的数据的数据库。在众多的机器学习和人工智能应用中，尤其是自然语言处理和图像识别这类涉及大量非结构化数据的领域，将数据转化为高维度的向量是常见的处理方式。这些向量可能拥有数百甚至数千个维度，是对复杂的非结构化数据如文本、图像的一种数学表述，从而使这些数据能被机器理解和处理。然而，传统的关系型数据库在存储和查询如此高维度和复杂性的向量数据时，往往面临着效率和性能的问题。因此，向量数据库被设计出来以解决这一问题，它具备高效存储和处理高维向量数据的能力，从而更好地支持涉及非结构化数据处理的人工智能应用。</p></blockquote><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e3c7e244b15f9527a4eb811e550a8f16.png"></p><p>向量数据库有很多种，比如Pinecone、Chroma和Qdrant，有些是收费的，有些则是开源的。</p><p>LangChain中支持很多向量数据库，这里我们选择的是开源向量数据库Qdrant。（注意，需要安装qdrant-client）</p><p>具体实现代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 3.Store 将分割嵌入并存储在矢量数据库Qdrant中from langchain.vectorstores import Qdrantfrom langchain.embeddings import OpenAIEmbeddingsvectorstore = Qdrant.from_documents(    documents=chunked_documents, # 以分块的文档    embedding=OpenAIEmbeddings(), # 用OpenAI的Embedding Model做嵌入    location=":memory:",  # in-memory 存储    collection_name="my_documents",) # 指定collection_name<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>目前，易速鲜花的所有内部文档，都以“文档块嵌入片”的格式被存储在向量数据库里面了。那么，我们只需要查询这个向量数据库，就可以找到大体上相关的信息了。</p><h2 id="相关信息的获取"><a href="#相关信息的获取" class="headerlink" title="相关信息的获取"></a>相关信息的获取</h2><p>当内部文档存储到向量数据库之后，我们需要根据问题和任务来提取最相关的信息。此时，信息提取的基本方式就是把问题也转换为向量，然后去和向量数据库中的各个向量进行比较，提取最接近的信息。</p><p>向量之间的比较通常基于向量的距离或者相似度。在高维空间中，常用的向量距离或相似度计算方法有欧氏距离和余弦相似度。</p><ul><li><strong>欧氏距离</strong>：这是最直接的距离度量方式，就像在二维平面上测量两点之间的直线距离那样。在高维空间中，两个向量的欧氏距离就是各个对应维度差的平方和的平方根。</li><li><strong>余弦相似度</strong>：在很多情况下，我们更关心向量的方向而不是它的大小。例如在文本处理中，一个词的向量可能会因为文本长度的不同，而在大小上有很大的差距，但方向更能反映其语义。余弦相似度就是度量向量之间方向的相似性，它的值范围在-1到1之间，值越接近1，表示两个向量的方向越相似。</li></ul><p>这两种方法都被广泛应用于各种机器学习和人工智能任务中，选择哪一种方法取决于具体的应用场景。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/32db77431433da86d9f818037752bd7a.png"></p><p>当然这里你肯定会问了，那么到底什么时候选择欧式距离，什么时候选择余弦相似度呢？</p><p>简单来说，关心数量等大小差异时用欧氏距离，关心文本等语义差异时用余弦相似度。</p><p>具体来说，欧氏距离度量的是绝对距离，它能很好地反映出向量的绝对差异。当我们关心数据的绝对大小，例如在物品推荐系统中，用户的购买量可能反映他们的偏好强度，此时可以考虑使用欧氏距离。同样，在数据集中各个向量的大小相似，且数据分布大致均匀时，使用欧氏距离也比较适合。</p><p>余弦相似度度量的是方向的相似性，它更关心的是两个向量的角度差异，而不是它们的大小差异。在处理文本数据或者其他高维稀疏数据的时候，余弦相似度特别有用。比如在信息检索和文本分类等任务中，文本数据往往被表示为高维的词向量，词向量的方向更能反映其语义相似性，此时可以使用余弦相似度。</p><p>在这里，我们正在处理的是文本数据，目标是建立一个问答系统，需要从语义上理解和比较问题可能的答案。因此，我建议使用余弦相似度作为度量标准。通过比较问题和答案向量在语义空间中的方向，可以找到与提出的问题最匹配的答案。</p><p>在这一步的代码部分，我们会创建一个聊天模型。然后需要创建一个 RetrievalQA 链，它是一个检索式问答模型，用于生成问题的答案。</p><p>在RetrievalQA 链中有下面两大重要组成部分。</p><ul><li>LLM是大模型，负责回答问题。</li><li>retriever（vectorstore.as_retriever()）负责根据问题检索相关的文档，找到具体的“嵌入片”。这些“嵌入片”对应的“文档块”就会作为知识信息，和问题一起传递进入大模型。本地文档中检索而得的知识很重要，因为 <strong>从互联网信息中训练而来的大模型不可能拥有“易速鲜花”作为一个私营企业的内部知识</strong>。</li></ul><p>具体代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 4. Retrieval 准备模型和Retrieval链import logging # 导入Logging工具from langchain.chat_models import ChatOpenAI # ChatOpenAI模型from langchain.retrievers.multi_query import MultiQueryRetriever # MultiQueryRetriever工具from langchain.chains import RetrievalQA # RetrievalQA链# 设置Logginglogging.basicConfig()logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)# 实例化一个大模型工具 - OpenAI的GPT-3.5llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)# 实例化一个MultiQueryRetrieverretriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)# 实例化一个RetrievalQA链qa_chain = RetrievalQA.from_chain_type(llm,retriever=retriever_from_llm)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>现在我们已经为后续的步骤做好了准备，下一步就是接收来自系统用户的具体问题，并根据问题检索信息，生成回答。</p><h2 id="生成回答并展示"><a href="#生成回答并展示" class="headerlink" title="生成回答并展示"></a>生成回答并展示</h2><p>这一步是问答系统应用的主要UI交互部分，这里会创建一个 Flask 应用（需要安装Flask包）来接收用户的问题，并生成相应的答案，最后通过 index.html 对答案进行渲染和呈现。</p><p>在这个步骤中，我们使用了之前创建的 RetrievalQA 链来获取相关的文档和生成答案。然后，将这些信息返回给用户，显示在网页上。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 5. Output 问答系统的UI实现from flask import Flask, request, render_templateapp = Flask(__name__) # Flask APP@app.route('/', methods=['GET', 'POST'])def home():    if request.method == 'POST':        # 接收用户输入作为问题        question = request.form.get('question')        # RetrievalQA链 - 读入问题，生成答案        result = qa_chain(&#123;"query": question&#125;)        # 把大模型的回答结果返回网页进行渲染        return render_template('index.html', result=result)    return render_template('index.html')if __name__ == "__main__":    app.run(host='0.0.0.0',debug=True,port=5000)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>相关HTML网页的关键代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&lt;body>    &lt;div class="container">        &lt;div class="header">            &lt;h1>易速鲜花内部问答系统&lt;/h1>            &lt;img src="&#123;&#123; url_for('static', filename='flower.png') &#125;&#125;" alt="flower logo" width="200">        &lt;/div>        &lt;form method="POST">            &lt;label for="question">Enter your question:&lt;/label>&lt;br>            &lt;input type="text" id="question" name="question">&lt;br>            &lt;input type="submit" value="Submit">        &lt;/form>        &#123;% if result is defined %&#125;            &lt;h2>Answer&lt;/h2>            &lt;p>&#123;&#123; result.result &#125;&#125;&lt;/p>        &#123;% endif %&#125;    &lt;/div>&lt;/body><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个项目的目录结构如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/2110cd73ddb8677f9b188d41c589c73e.png"></p><p>运行程序之后，我们跑起一个网页 <a href="http://127.0.0.1:5000/">http://127.0.0.1:5000/</a>。与网页进行互动时，可以发现，问答系统完美生成了专属于异速鲜花内部资料的回答。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/46b5b08c5f022f2c4c5975436b3e2d17.png"></p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>来回顾一下上面的流程。正如下图所示，我们先把本地知识切片后做Embedding，存储到向量数据库中，然后把用户的输入和从向量数据库中检索到的本地知识传递给大模型，最终生成所想要的回答。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/249c631211275e40f3e72d05dda976af.jpg"></p><p>怎么样，你是不是觉得整个流程特别简单易懂？</p><p>对了，LangChain+LLM的配置就是使原本复杂的东西变得特别简单，特别易于操作。而这个任务，在大模型和LangChain出现之前，要实现起来可不是这么简单的。</p><p>如果这个示例让你了解到了LangChain的威力，那么这节课的目标也就完成了。除了流程图的回顾，我也为你准备了一个详细版的脑图，你可以对照着复习。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/78a4b0435639b4db8c4e024d830a2ac2.jpg"></p><p>在后面几节课中，我们即将对LangChain的模型、链、内存、代理等组件进行详细拆解，我会带着你实现更多任务，开发出更奇妙的应用。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>请你用自己的话简述一下这个基于文档的QA（问答）系统的实现流程？</li><li>LangChain支持很多种向量数据库，你能否用另一种常用的向量数据库Chroma来实现这个任务？</li><li>LangChain支持很多种大语言模型，你能否用HuggingFace网站提供的开源模型 <a href="https://huggingface.co/google/flan-t5-xl">google&#x2F;flan-t5-x1</a> 代替GPT-3.5完成这个任务？</li></ol><p>题目较多，可以选择性尝试，期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>LangChain官方文档对 <a href="https://docs.langchain.com/docs/use-cases/qa-docs">Document QA 系统</a> 设计及实现的详细说明</li><li>HuggingFace官网上的 <a href="https://huggingface.co/tasks/document-question-answering">文档问答</a> 资源</li><li>论文 <a href="https://arxiv.org/abs/2010.10439">开放式表格与文本问题回答</a>，Chen, W., Chang, M.-W., Schlinger, E., Wang, W., &amp; Cohen, W. W. (2021). Open Question Answering over Tables and Text. ICLR 2021.</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;02｜用LangChain快速构建基于“易速鲜花”本地知识库的智能问答系统&quot;&gt;&lt;a href=&quot;#02｜用LangChain快速构建基于“易速鲜花”本地知识库的智能问答系统&quot; class=&quot;headerlink&quot; title=&quot;02｜用LangChain快速构建基</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>03｜模型I/O：输入提示、调用模型、解析输出</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3884962345.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3884962345.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.422Z</updated>
    
    <content type="html"><![CDATA[<h1 id="03｜模型I-O：输入提示、调用模型、解析输出"><a href="#03｜模型I-O：输入提示、调用模型、解析输出" class="headerlink" title="03｜模型I&#x2F;O：输入提示、调用模型、解析输出"></a>03｜模型I&#x2F;O：输入提示、调用模型、解析输出</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>从这节课开始，我们将对LangChain中的六大核心组件一一进行详细的剖析。</p><p>模型，位于LangChain框架的最底层，它是基于语言模型构建的应用的 <strong>核心元素</strong>，因为所谓LangChain应用开发，就是以LangChain作为框架，通过API调用大模型来解决具体问题的过程。</p><p>可以说，整个LangChain框架的逻辑都是由LLM这个发动机来驱动的。没有模型，LangChain这个框架也就失去了它存在的意义。那么这节课我们就详细讲讲模型，最后你会收获一个能够自动生成鲜花文案的应用程序。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/76619cf2f73ef200dd57cd16c0d55ec4.png"></p><h2 id="Model-I-O"><a href="#Model-I-O" class="headerlink" title="Model I&#x2F;O"></a>Model I&#x2F;O</h2><p>我们可以把对模型的使用过程拆解成三块，分别是 <strong>输入提示</strong>（对应图中的Format）、 <strong>调用模型</strong>（对应图中的Predict）和 <strong>输出解析</strong>（对应图中的Parse）。这三块形成了一个整体，因此在LangChain中这个过程被统称为 <strong>Model I&#x2F;O</strong>（Input&#x2F;Output）。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ac67214287154dcfbbf12d81086c8023.png"></p><p>在模型 I&#x2F;O的每个环节，LangChain都为咱们提供了模板和工具，快捷地形成调用各种语言模型的接口。</p><ol><li><strong>提示模板</strong>：使用模型的第一个环节是把提示信息输入到模型中，你可以创建LangChain模板，根据实际需求动态选择不同的输入，针对特定的任务和应用调整输入。</li><li><strong>语言模型</strong>：LangChain允许你通过通用接口来调用语言模型。这意味着无论你要使用的是哪种语言模型，都可以通过同一种方式进行调用，这样就提高了灵活性和便利性。</li><li><strong>输出解析</strong>：LangChain还提供了从模型输出中提取信息的功能。通过输出解析器，你可以精确地从模型的输出中获取需要的信息，而不需要处理冗余或不相关的数据，更重要的是还可以把大模型给回的非结构化文本，转换成程序可以处理的结构化数据。</li></ol><p>下面我们用示例的方式来深挖一下这三个环节。先来看看LangChain中提示模板的构建。</p><h2 id="提示模板"><a href="#提示模板" class="headerlink" title="提示模板"></a>提示模板</h2><p>语言模型是个无穷无尽的宝藏，人类的知识和智慧，好像都封装在了这个“魔盒”里面了。但是，怎样才能解锁其中的奥秘，那可就是仁者见仁智者见智了。所以，现在“提示工程”这个词特别流行，所谓Prompt Engineering，就是专门研究对大语言模型的提示构建。</p><p>我的观点是，使用大模型的场景千差万别，因此肯定不存在那么一两个神奇的模板，能够骗过所有模型，让它总能给你最想要的回答。然而，好的提示（其实也就是好的问题或指示啦），肯定能够让你在调用语言模型的时候事半功倍。</p><p>那其中的具体原则，不外乎吴恩达老师在他的 <a href="https://learn.deeplearning.ai/login?redirect_course=chatgpt-prompt-eng">提示工程课程</a> 中所说的：</p><ol><li>给予模型清晰明确的指示</li><li>让模型慢慢地思考</li></ol><p>说起来很简单，对吧？是的，道理总是简单，但是如何具体实践这些原则，又是个大问题。让我从创建一个简单的LangChain提示模板开始。</p><p>这里，我们希望为销售的每一种鲜花生成一段简介文案，那么每当你的员工或者顾客想了解某种鲜花时，调用该模板就会生成适合的文字。</p><p>这个提示模板的生成方式如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入LangChain中的提示模板from langchain import PromptTemplate# 创建原始模板template = """您是一位专业的鲜花店文案撰写员。\n对于售价为 &#123;price&#125; 元的 &#123;flower_name&#125; ，您能提供一个吸引人的简短描述吗？"""# 根据原始模板创建LangChain提示模板prompt = PromptTemplate.from_template(template)# 打印LangChain提示模板的内容print(prompt)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>提示模板的具体内容如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">input_variables=['flower_name', 'price']output_parser=None partial_variables=&#123;&#125;template='/\n您是一位专业的鲜花店文案撰写员。\n对于售价为 &#123;price&#125; 元的 &#123;flower_name&#125; ，您能提供一个吸引人的简短描述吗？\n'template_format='f-string'validate_template=True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这里，所谓“模板”就是一段描述某种鲜花的文本格式，它是一个 f-string，其中有两个变量 {flower_name} 和 {price} 表示花的名称和价格，这两个值是模板里面的占位符，在实际使用模板生成提示时会被具体的值替换。</p><p>代码中的from_template是一个类方法，它允许我们直接从一个字符串模板中创建一个PromptTemplate对象。打印出这个PromptTemplate对象，你可以看到这个对象中的信息包括输入的变量（在这个例子中就是 <code>flower_name</code> 和 <code>price</code>）、输出解析器（这个例子中没有指定）、模板的格式（这个例子中为 <code>&#39;f-string&#39;</code>）、是否验证模板（这个例子中设置为 <code>True</code>）。</p><p>因此PromptTemplate的from_template方法就是将一个原始的模板字符串转化为一个更丰富、更方便操作的PromptTemplate对象，这个对象就是LangChain中的提示模板。LangChain 提供了多个类和函数，也 <strong>为各种应用场景设计了很多内置模板，使构建和使用提示变得容易</strong>。我们下节课还会对提示工程的基本原理和LangChain中的各种提示模板做更深入的讲解。</p><p>下面，我们将会使用这个刚刚构建好的提示模板来生成提示，并把提示输入到大语言模型中。</p><h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a><strong>语言模型</strong></h2><p>LangChain中支持的模型有三大类。</p><ol><li>大语言模型（LLM） ，也叫Text Model，这些模型将文本字符串作为输入，并返回文本字符串作为输出。Open AI的text-davinci-003、Facebook的LLaMA、ANTHROPIC的Claude，都是典型的LLM。</li><li>聊天模型（Chat Model），主要代表Open AI的ChatGPT系列模型。这些模型通常由语言模型支持，但它们的 API 更加结构化。具体来说，这些模型将聊天消息列表作为输入，并返回聊天消息。</li><li>文本嵌入模型（Embedding Model），这些模型将文本作为输入并返回浮点数列表，也就是Embedding。而文本嵌入模型如OpenAI的text-embedding-ada-002，我们之前已经见过了。文本嵌入模型负责把文档存入向量数据库，和我们这里探讨的提示工程关系不大。</li></ol><p>然后，我们将调用语言模型，让模型帮我们写文案，并且返回文案的结果。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 设置OpenAI API Keyimport osos.environ["OPENAI_API_KEY"] = '你的Open AI API Key'# 导入LangChain中的OpenAI模型接口from langchain import OpenAI# 创建模型实例model = OpenAI(model_name='text-davinci-003')# 输入提示input = prompt.format(flower_name=["玫瑰"], price='50')# 得到模型的输出output = model(input)# 打印输出内容print(output)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>input = prompt.format(flower_name=[&quot;玫瑰&quot;], price=&#39;50&#39;)</code> 这行代码的作用是将模板实例化，此时将 <code>&#123;flower_name&#125;</code> 替换为 <code>&quot;玫瑰&quot;</code>， <code>&#123;price&#125;</code> 替换为 <code>&#39;50&#39;</code>，形成了具体的提示：“您是一位专业的鲜花店文案撰写员。对于售价为 50 元的玫瑰，您能提供一个吸引人的简短描述吗？”</p><p>接收到这个输入，调用模型之后，得到的输出如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">让你心动！50元就可以拥有这支充满浪漫气息的玫瑰花束，让TA感受你的真心爱意。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>复用提示模板，我们可以同时生成多个鲜花的文案。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入LangChain中的提示模板from langchain import PromptTemplate# 创建原始模板template = """您是一位专业的鲜花店文案撰写员。\n对于售价为 &#123;price&#125; 元的 &#123;flower_name&#125; ，您能提供一个吸引人的简短描述吗？"""# 根据原始模板创建LangChain提示模板prompt = PromptTemplate.from_template(template)# 打印LangChain提示模板的内容print(prompt)# 设置OpenAI API Keyimport osos.environ["OPENAI_API_KEY"] = '你的Open AI API Key'# 导入LangChain中的OpenAI模型接口from langchain import OpenAI# 创建模型实例model = OpenAI(model_name='text-davinci-003')# 多种花的列表flowers = ["玫瑰", "百合", "康乃馨"]prices = ["50", "30", "20"]# 生成多种花的文案for flower, price in zip(flowers, prices):    # 使用提示模板生成输入    input_prompt = prompt.format(flower_name=flower, price=price)    # 得到模型的输出    output = model(input_prompt)    # 打印输出内容    print(output)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>模型的输出如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">这支玫瑰，深邃的红色，传递着浓浓的深情与浪漫，令人回味无穷！百合：美丽的花朵，多彩的爱恋！30元让你拥有它！康乃馨—20元，象征爱的祝福，送给你最真挚的祝福。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>你也许会问我，在这个过程中，使用LangChain的意义究竟何在呢？我直接调用Open AI的API，不是完全可以实现相同的功能吗？</p><p>的确如此，让我们来看看直接使用Open AI API来完成上述功能的代码。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">import openai # 导入OpenAIopenai.api_key = 'Your-OpenAI-API-Key' # API Keyprompt_text = "您是一位专业的鲜花店文案撰写员。对于售价为&#123;&#125;元的&#123;&#125;，您能提供一个吸引人的简短描述吗？" # 设置提示flowers = ["玫瑰", "百合", "康乃馨"]prices = ["50", "30", "20"]# 循环调用Text模型的Completion方法，生成文案for flower, price in zip(flowers, prices):    prompt = prompt_text.format(price, flower)    response = openai.Completion.create(        engine="text-davinci-003",        prompt=prompt,        max_tokens=100    )    print(response.choices[0].text.strip()) # 输出文案<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面的代码是直接使用Open AI和带有 {} 占位符的提示语，同时生成了三种鲜花的文案。看起来也是相当简洁。</p><p>不过，如果你深入思考一下，你就会发现LangChain的优势所在。 <strong>我们只需要定义一次模板，就可以用它来生成各种不同的提示。</strong> 对比单纯使用 f-string 来格式化文本，这种方法更加简洁，也更容易维护。而LangChain在提示模板中，还整合了output_parser、template_format 以及是否需要validate_template等功能。</p><p>更重要的是，使用LangChain提示模板，我们还可以很方便地把程序切换到不同的模型，而不需要修改任何提示相关的代码。</p><p>下面，我们用完全相同的提示模板来生成提示，并发送给HuggingFaceHub中的开源模型来创建文案。（注意：需要注册HUGGINGFACEHUB_API_TOKEN）</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c8c0d84349ebd2d1d82a2836383164ec.png"></p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入LangChain中的提示模板from langchain import PromptTemplate# 创建原始模板template = """You are a flower shop assitiant。\nFor &#123;price&#125; of &#123;flower_name&#125; ，can you write something for me？"""# 根据原始模板创建LangChain提示模板prompt = PromptTemplate.from_template(template)# 打印LangChain提示模板的内容print(prompt)import osos.environ['HUGGINGFACEHUB_API_TOKEN'] = '你的HuggingFace API Token'# 导入LangChain中的OpenAI模型接口from langchain import HuggingFaceHub# 创建模型实例model= HuggingFaceHub(repo_id="google/flan-t5-large")# 输入提示input = prompt.format(flower_name=["rose"], price='50')# 得到模型的输出output = model(input)# 打印输出内容print(output)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">i love you<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>真是一分钱一分货，当我使用较早期的开源模型T5，得到了很粗糙的文案 “i love you”（哦，还要注意T5还没有支持中文的能力，我把提示文字换成英文句子，结构其实都没变）。</p><p>当然，这里我想要向你传递的信息是：你可以重用模板，重用程序结构，通过LangChain框架调用任何模型。如果你熟悉机器学习的训练流程的话，这LangChain是不是让你联想到PyTorch和TensorFlow这样的框架—— <strong>模型可以自由选择、自主训练，而调用模型的框架往往是有章法、而且可复用的</strong>。</p><p>因此，使用LangChain和提示模板的好处是：</p><ol><li>代码的可读性：使用模板的话，提示文本更易于阅读和理解，特别是对于复杂的提示或多变量的情况。</li><li>可复用性：模板可以在多个地方被复用，让你的代码更简洁，不需要在每个需要生成提示的地方重新构造提示字符串。</li><li>维护：如果你在后续需要修改提示，使用模板的话，只需要修改模板就可以了，而不需要在代码中查找所有使用到该提示的地方进行修改。</li><li>变量处理：如果你的提示中涉及到多个变量，模板可以自动处理变量的插入，不需要手动拼接字符串。</li><li>参数化：模板可以根据不同的参数生成不同的提示，这对于个性化生成文本非常有用。</li></ol><p>那我们就接着介绍模型 I&#x2F;O的最后一步，输出解析。</p><h2 id="输出解析"><a href="#输出解析" class="headerlink" title="输出解析"></a><strong>输出解析</strong></h2><p>LangChain提供的解析模型输出的功能，使你能够更容易地从模型输出中获取结构化的信息，这将大大加快基于语言模型进行应用开发的效率。</p><p>为什么这么说呢？请你思考一下刚才的例子，你只是让模型生成了一个文案。这段文字是一段字符串，正是你所需要的。但是，在开发具体应用的过程中，很明显 <strong>我们不仅仅需要文字，更多情况下我们需要的是程序能够直接处理的、结构化的数据</strong>。</p><p>比如说，在这个文案中，如果你希望模型返回两个字段：</p><ul><li>description：鲜花的说明文本</li><li>reason：解释一下为何要这样写上面的文案</li></ul><p>那么，模型可能返回的一种结果是：</p><p><strong>A</strong>：“文案是：让你心动！50元就可以拥有这支充满浪漫气息的玫瑰花束，让TA感受你的真心爱意。为什么这样说呢？因为爱情是无价的，50元对应热恋中的情侣也会觉得值得。”</p><p>上面的回答并不是我们在处理数据时所需要的，我们需要的是一个类似于下面的Python字典。</p><p><strong>B</strong>：{description: “让你心动！50元就可以拥有这支充满浪漫气息的玫瑰花束，让TA感受你的真心爱意。” ; reason: “因为爱情是无价的，50元对应热恋中的情侣也会觉得值得。”}</p><p>那么从A的笼统言语，到B这种结构清晰的数据结构，如何自动实现？这就需要LangChain中的输出解析器上场了。</p><p>下面，我们就通过LangChain的输出解析器来重构程序，让模型有能力生成结构化的回应，同时对其进行解析，直接将解析好的数据存入CSV文档。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 通过LangChain调用模型from langchain import PromptTemplate, OpenAI# 导入OpenAI Keyimport osos.environ["OPENAI_API_KEY"] = '你的OpenAI API Key'# 创建原始提示模板prompt_template = """您是一位专业的鲜花店文案撰写员。对于售价为 &#123;price&#125; 元的 &#123;flower_name&#125; ，您能提供一个吸引人的简短描述吗？&#123;format_instructions&#125;"""# 创建模型实例model = OpenAI(model_name='text-davinci-003')# 导入结构化输出解析器和ResponseSchemafrom langchain.output_parsers import StructuredOutputParser, ResponseSchema# 定义我们想要接收的响应模式response_schemas = [    ResponseSchema(name="description", description="鲜花的描述文案"),    ResponseSchema(name="reason", description="问什么要这样写这个文案")]# 创建输出解析器output_parser = StructuredOutputParser.from_response_schemas(response_schemas)# 获取格式指示format_instructions = output_parser.get_format_instructions()# 根据原始模板创建提示，同时在提示中加入输出解析器的说明prompt = PromptTemplate.from_template(prompt_template,                partial_variables=&#123;"format_instructions": format_instructions&#125;)# 数据准备flowers = ["玫瑰", "百合", "康乃馨"]prices = ["50", "30", "20"]# 创建一个空的DataFrame用于存储结果import pandas as pddf = pd.DataFrame(columns=["flower", "price", "description", "reason"]) # 先声明列名for flower, price in zip(flowers, prices):    # 根据提示准备模型的输入    input = prompt.format(flower_name=flower, price=price)    # 获取模型的输出    output = model(input)    # 解析模型的输出（这是一个字典结构）    parsed_output = output_parser.parse(output)    # 在解析后的输出中添加“flower”和“price”    parsed_output['flower'] = flower    parsed_output['price'] = price    # 将解析后的输出添加到DataFrame中    df.loc[len(df)] = parsed_output# 打印字典print(df.to_dict(orient='records'))# 保存DataFrame到CSV文件df.to_csv("flowers_with_descriptions.csv", index=False)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">[&#123;'flower': '玫瑰', 'price': '50', 'description': 'Luxuriate in the beauty of this 50 yuan rose, with its deep red petals and delicate aroma.', 'reason': 'This description emphasizes the elegance and beauty of the rose, which will be sure to draw attention.'&#125;,&#123;'flower': '百合', 'price': '30', 'description': '30元的百合，象征着坚定的爱情，带给你的是温暖而持久的情感！', 'reason': '百合是象征爱情的花，写出这样的描述能让顾客更容易感受到百合所带来的爱意。'&#125;,&#123;'flower': '康乃馨', 'price': '20', 'description': 'This beautiful carnation is the perfect way to show your love and appreciation. Its vibrant pink color is sure to brighten up any room!', 'reason': 'The description is short, clear and appealing, emphasizing the beauty and color of the carnation while also invoking a sense of love and appreciation.'&#125;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这段代码中，首先定义输出结构，我们希望模型生成的答案包含两部分：鲜花的描述文案（description）和撰写这个文案的原因（reason）。所以我们定义了一个名为response_schemas的列表，其中包含两个ResponseSchema对象，分别对应这两部分的输出。</p><p>根据这个列表，我通过StructuredOutputParser.from_response_schemas方法创建了一个输出解析器。</p><p>然后，我们通过输出解析器对象的get_format_instructions()方法获取输出的格式说明（format_instructions），再根据原始的字符串模板和输出解析器格式说明创建新的提示模板（这个模板就整合了输出解析结构信息）。再通过新的模板生成模型的输入，得到模型的输出。此时模型的输出结构将尽最大可能遵循我们的指示，以便于输出解析器进行解析。</p><p>对于每一个鲜花和价格组合，我们都用 output_parser.parse(output) 把模型输出的文案解析成之前定义好的数据格式，也就是一个Python字典，这个字典中包含了description 和 reason 这两个字段的值。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">parsed_output&#123;'description': 'This 50-yuan rose is... feelings.', 'reason': 'The description is s...y emotion.'&#125;len(): 2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>最后，把所有信息整合到一个pandas DataFrame对象中（需要安装Pandas库）。这个DataFrame对象中包含了flower、price、description 和 reason 这四个字段的值。其中，description 和 reason 是由 output_parser 从模型的输出中解析出来的，flower 和 price 是我们自己添加的。</p><p>我们可以打印出DataFrame的内容，也方便地在程序中处理它，比如保存为下面的CSV文件。因为此时数据不再是模糊的、无结构的文本，而是结构清晰的有格式的数据。 <strong>输出解析器在这个过程中的功劳很大</strong>。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/3264157dc13f229641d87dcb34dafbf2.png"></p><p>到这里，我们今天的任务也就顺利完成了。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>这样，你就从头到尾利用大模型开发出来了一个能够自动生成鲜花文案的应用程序！怎么样，是不是感觉和我们平时所做的基于SQL和数据库表以及固定业务逻辑的应用开发很不一样？</p><p>你看，每一次运行都有不同的结果，而我们完全不知道大模型下一次会给我们带来怎样的新东西。因此，基于大模型构建的应用可以说充满了创造力。</p><p>总结一下使用LangChain框架的好处，你会发现它有这样几个优势。</p><ol><li>模板管理：在大型项目中，可能会有许多不同的提示模板，使用 LangChain 可以帮助你更好地管理这些模板，保持代码的清晰和整洁。</li><li>变量提取和检查：LangChain 可以自动提取模板中的变量并进行检查，确保你没有忘记填充任何变量。</li><li>模型切换：如果你想尝试使用不同的模型，只需要更改模型的名称就可以了，无需修改代码。</li><li>输出解析：LangChain的提示模板可以嵌入对输出格式的定义，以便在后续处理过程中比较方便地处理已经被格式化了的输出。</li></ol><p>在下节课中，我们将继续深入探索LangChain中的提示模板，看一看如何通过高质量的提示工程让模型创造出更为精准、更高质量的输出。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>请你用自己的理解，简述LangChain调用大语言模型来做应用开发的优势。</li><li>在上面的示例中，format_instructions，也就是输出格式是怎样用output_parser构建出来的，又是怎样传递到提示模板中的？</li><li>加入了partial_variables，也就是输出解析器指定的format_instructions之后的提示，为什么能够让模型生成结构化的输出？你可以打印出这个提示，一探究竟。</li><li>使用输出解析器后，调用模型时有没有可能仍然得不到所希望的输出？也就是说，模型有没有可能仍然返回格式不够完美的输出？</li></ol><p>题目较多，可以选择性思考，期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>吴恩达老师的 <a href="https://learn.deeplearning.ai/login?redirect_course=chatgpt-prompt-eng">提示工程课程</a>，吴老师也有LangChain的简单介绍课程呦！网上也有这些课程的中文翻译版！</li><li>LangChain官方文档中，关于模型I&#x2F;O的资料 <a href="https://python.langchain.com/docs/modules/model_io/">在此</a>。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;03｜模型I-O：输入提示、调用模型、解析输出&quot;&gt;&lt;a href=&quot;#03｜模型I-O：输入提示、调用模型、解析输出&quot; class=&quot;headerlink&quot; title=&quot;03｜模型I&amp;#x2F;O：输入提示、调用模型、解析输出&quot;&gt;&lt;/a&gt;03｜模型I&amp;#x2F;</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>04｜提示工程（上）：用少样本FewShotTemplate和ExampleSelector创建应景文案</title>
    <link href="https://zhuansun.github.io/geekbang/posts/2015469645.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/2015469645.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.426Z</updated>
    
    <content type="html"><![CDATA[<h1 id="04｜提示工程（上）：用少样本FewShotTemplate和ExampleSelector创建应景文案"><a href="#04｜提示工程（上）：用少样本FewShotTemplate和ExampleSelector创建应景文案" class="headerlink" title="04｜提示工程（上）：用少样本FewShotTemplate和ExampleSelector创建应景文案"></a>04｜提示工程（上）：用少样本FewShotTemplate和ExampleSelector创建应景文案</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>上节课我给你留了一个思考题： <strong>在提示模板的构建过程中加入了partial_variables，也就是输出解析器指定的format_instructions之后，为什么能够让模型生成结构化的输出？</strong></p><p>当你用print语句打印出最终传递给大模型的提示时，一切就变得非常明了。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">您是一位专业的鲜花店文案撰写员。对于售价为 50 元的 玫瑰 ，您能提供一个吸引人的简短描述吗？The output should be a markdown code snippet formatted in the following schema, including the leading and trailing "```json" and "```":```json&#123;        "description": string  // 鲜花的描述文案        "reason": string  // 问什么要这样写这个文案&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>秘密在于，LangChain的输出解析器偷偷的在提示中加了一段话，也就是 {format_instructions} 中的内容。这段由LangChain自动添加的文字，就清楚地指示着我们希望得到什么样的回答以及回答的具体格式。提示指出，模型需要根据一个schema来格式化输出文本，这个schema从 ```json 开始，到 ``` 结束。</p><p>这就是在告诉模型，你就follow这个schema（schema，可以理解为对数据结构的描述）的格式，就行啦！</p><p>这就是一个很棒、很典型的 <strong>提示工程</strong>。有了这样清晰的提示，智能程度比较高的模型（比如GPT3.5及以上版本），肯定能够输出可以被解析的数据结构，如JSON格式的数据。</p><p>那么这节课我就带着你进一步深究，如何利用LangChain中的提示模板，做好提示工程。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/3b5584552720f22ac10e1ab1430f61fe.jpg"></p><p>上节课我说过，针对大模型的提示工程该如何做，吴恩达老师在他的 <a href="https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/">ChatGPT Prompt Engineering for Developers</a> 公开课中，给出了两个大的原则：第一条原则是写出清晰而具体的指示，第二条原则是给模型思考的时间。</p><p>无独有偶，在Open AI的官方文档 <a href="https://platform.openai.com/docs/guides/gpt-best-practices/gpt-best-practices">GPT 最佳实践</a> 中，也给出了和上面这两大原则一脉相承的6大策略。分别是：</p><ol><li>写清晰的指示</li><li>给模型提供参考（也就是示例）</li><li>将复杂任务拆分成子任务</li><li>给GPT时间思考</li><li>使用外部工具</li><li>反复迭代问题</li></ol><p>怎么样，这些原则和策略是不是都是大白话？这些原则其实不仅能够指导大语言模型，也完全能够指导你的思维过程，让你处理问题时的思路更为清晰。所以说，大模型的思维过程和我们人类的思维过程，还是蛮相通的。</p><h2 id="提示的结构"><a href="#提示的结构" class="headerlink" title="提示的结构"></a>提示的结构</h2><p>当然了，从大原则到实践，还是有一些具体工作需要说明，下面我们先看一个实用的提示框架。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/b77a15cd83b66bba55032d711bcf3c16.png" alt="图片"></p><p>在这个提示框架中：</p><ul><li><strong>指令</strong>（Instuction）告诉模型这个任务大概要做什么、怎么做，比如如何使用提供的外部信息、如何处理查询以及如何构造输出。这通常是一个提示模板中比较固定的部分。一个常见用例是告诉模型“你是一个有用的XX助手”，这会让他更认真地对待自己的角色。</li><li><strong>上下文</strong>（Context）则充当模型的额外知识来源。这些信息可以手动插入到提示中，通过矢量数据库检索得来，或通过其他方式（如调用API、计算器等工具）拉入。一个常见的用例时是把从向量数据库查询到的知识作为上下文传递给模型。</li><li><strong>提示输入</strong>（Prompt Input）通常就是具体的问题或者需要大模型做的具体事情，这个部分和“指令”部分其实也可以合二为一。但是拆分出来成为一个独立的组件，就更加结构化，便于复用模板。这通常是作为变量，在调用模型之前传递给提示模板，以形成具体的提示。</li><li><strong>输出指示器</strong>（Output Indicator）标记​​要生成的文本的开始。这就像我们小时候的数学考卷，先写一个“解”，就代表你要开始答题了。如果生成 Python 代码，可以使用 “import” 向模型表明它必须开始编写 Python 代码（因为大多数 Python 脚本以import开头）。这部分在我们和ChatGPT对话时往往是可有可无的，当然LangChain中的代理在构建提示模板时，经常性的会用一个“Thought：”（思考）作为引导词，指示模型开始输出自己的推理（Reasoning）。</li></ul><p>下面，就让我们看看如何使用 LangChain中的各种提示模板做提示工程，将更优质的提示输入大模型。</p><h2 id="LangChain-提示模板的类型"><a href="#LangChain-提示模板的类型" class="headerlink" title="LangChain 提示模板的类型"></a>LangChain 提示模板的类型</h2><p>LangChain中提供String（StringPromptTemplate）和Chat（BaseChatPromptTemplate）两种基本类型的模板，并基于它们构建了不同类型的提示模板：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/feefbb0a166f53f14f647b88e1025cyy.jpg"></p><p>这些模板的导入方式如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.prompts.prompt import PromptTemplatefrom langchain.prompts import FewShotPromptTemplatefrom langchain.prompts.pipeline import PipelinePromptTemplatefrom langchain.prompts import ChatPromptTemplatefrom langchain.prompts import (    ChatMessagePromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我发现有时候不指定 .prompts，直接从LangChain包也能导入模板。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain import PromptTemplate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>下面我们通过示例来介绍上面这些模版，前两个我们简单了解就好，其中最典型的FewShotPromptTemplate会重点讲。至于PipelinePrompt和自定义模板，使用起来比较简单，请你参考LangChain文档自己学习。</p><h2 id="使用-PromptTemplate"><a href="#使用-PromptTemplate" class="headerlink" title="使用 PromptTemplate"></a>使用 PromptTemplate</h2><p>下面通过示例简单说明一下PromptTemplate的使用。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain import PromptTemplatetemplate = """\你是业务咨询顾问。你给一个销售&#123;product&#125;的电商公司，起一个好的名字？"""prompt = PromptTemplate.from_template(template)print(prompt.format(product="鲜花"))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">你是业务咨询顾问。你给一个销售鲜花的电商公司，起一个好的名字？<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个程序的主要功能是生成适用于不同场景的提示，对用户定义的一种产品或服务提供公司命名建议。</p><p>在这里， <code>&quot;你是业务咨询顾问。你给一个销售&#123;product&#125;的电商公司，起一个好的名字？&quot;</code> 就是原始提示模板，其中 {product} 是占位符。</p><p>然后通过PromptTemplate的from_template方法，我们创建了一个提示模板对象，并通过prompt.format方法将模板中的 {product} 替换为 <code>&quot;鲜花&quot;</code>。</p><p>这样，就得到了一句具体的提示： _你是业务咨询顾问。你给一个销售鲜花的电商公司，起一个好的名字？_——这就要求大语言模型，要有的放矢。</p><p>在上面这个过程中，LangChain中的模板的一个方便之处是from_template方法可以从传入的字符串中自动提取变量名称（如product），而无需刻意指定。 <strong>上面程序中的product自动成为了format方法中的一个参数</strong>。</p><p>当然，也可以通过提示模板类的构造函数，在创建模板时手工指定input_variables，示例如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">prompt = PromptTemplate(    input_variables=["product", "market"],    template="你是业务咨询顾问。对于一个面向&#123;market&#125;市场的，专注于销售&#123;product&#125;的公司，你会推荐哪个名字？")print(prompt.format(product="鲜花", market="高端"))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">你是业务咨询顾问。对于一个面向高端市场的，专注于销售鲜花的公司，你会推荐哪个名字？<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>上面的方式直接生成了提示模板，并没有通过from_template方法从字符串模板中创建提示模板。二者效果是一样的。</p><h2 id="使用-ChatPromptTemplate"><a href="#使用-ChatPromptTemplate" class="headerlink" title="使用 ChatPromptTemplate"></a>使用 ChatPromptTemplate</h2><p>对于OpenAI推出的ChatGPT这一类的聊天模型，LangChain也提供了一系列的模板，这些模板的不同之处是它们有对应的角色。</p><p>下面代码展示了OpenAI的Chat Model中的各种消息角色。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">import openaiopenai.ChatCompletion.create(  model="gpt-3.5-turbo",  messages=[        &#123;"role": "system", "content": "You are a helpful assistant."&#125;,        &#123;"role": "user", "content": "Who won the world series in 2020?"&#125;,        &#123;"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."&#125;,        &#123;"role": "user", "content": "Where was it played?"&#125;    ])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>OpenAI对传输到gpt-3.5-turbo和GPT-4的messsage格式说明如下：</p><blockquote><p>消息必须是消息对象的数组，其中每个对象都有一个角色（系统、用户或助理）和内容。对话可以短至一条消息，也可以来回多次。</p><p>通常，对话首先由系统消息格式化，然后是交替的用户消息和助理消息。</p><p>系统消息有助于设置助手的行为。例如，你可以修改助手的个性或提供有关其在整个对话过程中应如何表现的具体说明。但请注意，系统消息是可选的，并且没有系统消息的模型的行为可能类似于使用通用消息，例如“你是一个有用的助手”。</p><p>用户消息提供助理响应的请求或评论。</p><p>助理消息存储以前的助理响应，但也可以由你编写以给出所需行为的示例。</p></blockquote><p>LangChain的ChatPromptTemplate这一系列的模板，就是 <strong>跟着这一系列角色而设计的</strong>。</p><p>下面，我给出一个示例。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入聊天消息类模板from langchain.prompts import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    HumanMessagePromptTemplate,)# 模板的构建template="你是一位专业顾问，负责为专注于&#123;product&#125;的公司起名。"system_message_prompt = SystemMessagePromptTemplate.from_template(template)human_template="公司主打产品是&#123;product_detail&#125;。"human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])# 格式化提示消息生成提示prompt = prompt_template.format_prompt(product="鲜花装饰", product_detail="创新的鲜花设计。").to_messages()# 下面调用模型，把提示传入模型，生成结果import osos.environ["OPENAI_API_KEY"] = '你的OpenAI Key'from langchain.chat_models import ChatOpenAIchat = ChatOpenAI()result = chat(prompt)print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">content='1. 花语创意\n2. 花韵设计\n3. 花艺创新\n4. 花漾装饰\n5. 花语装点\n6. 花翩翩\n7. 花语之美\n8. 花馥馥\n9. 花语时尚\n10. 花之魅力'additional_kwargs=&#123;&#125;example=False<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>好吧，尽管模型成功地完成了任务，但是感觉没有咱“易速鲜花”响亮！</p><p>讲完上面两种简单易用的提示模板，下面开始介绍今天的重点内容，FewShotPromptTemplate。FewShot，也就是少样本这一概念，是提示工程中非常重要的部分，对应着OpenAI提示工程指南中的第2条——给模型提供参考（也就是示例）。</p><h2 id="FewShot的思想起源"><a href="#FewShot的思想起源" class="headerlink" title="FewShot的思想起源"></a>FewShot的思想起源</h2><p>讲解概念之前，我先分享个事儿哈，帮助你理解。</p><p>今天我下楼跑步时，一个老爷爷教孙子学骑车，小孩总掌握不了平衡，蹬一两下就下车。</p><ul><li>爷爷说：“宝贝，你得有毅力！”</li><li>孙子说：“爷爷，什么是毅力？”</li><li>爷爷说：“你看这个叔叔，绕着楼跑了10多圈了，这就是毅力，你也得至少蹬个10几趟才能骑起来。”</li></ul><p>这老爷爷就是给孙子做了一个One-Shot学习。如果他的孙子第一次听说却上来就明白什么是毅力，那就神了，这就叫Zero-Shot，表明这孩子的语言天赋不是一般的高，从知识积累和当前语境中就能够推知新词的涵义。有时候我们把Zero-Shot翻译为“顿悟”，聪明的大模型，某些情况下也是能够做到的。</p><p>Few-Shot（少样本）、One-Shot（单样本）和与之对应的 Zero-Shot（零样本）的概念都起源于机器学习。如何让机器学习模型在极少量甚至没有示例的情况下学习到新的概念或类别，对于许多现实世界的问题是非常有价值的，因为我们往往无法获取到大量的标签化数据。</p><p>这几个重要概念并非在某一篇特定的论文中首次提出，而是在机器学习和深度学习的研究中逐渐形成和发展的。</p><ul><li>对于Few-Shot Learning，一个重要的参考文献是2016年Vinyals, O.的论文《小样本学习的匹配网络》。</li><li>这篇论文提出了一种新的学习模型——匹配网络（Matching Networks），专门针对单样本学习（One-Shot Learning）问题设计， <strong>而</strong> <strong>One-Shot Learning</strong> <strong>可以看作是一种最常见的</strong> <strong>Few-Shot</strong> <strong>学习的情况。</strong></li><li>对于Zero-Shot Learning，一个代表性的参考文献是Palatucci, M.在2009年提出的《基于语义输出编码的零样本学习（Zero-Shot Learning with semantic output codes）》，这篇论文提出了零次学习（Zero-Shot Learning）的概念，其中的学习系统可以根据类的语义描述来识别之前未见过的类。</li></ul><p>在提示工程（Prompt Engineering）中，Few-Shot 和 Zero-Shot 学习的概念也被广泛应用。</p><ul><li>在Few-Shot学习设置中，模型会被给予几个示例，以帮助模型理解任务，并生成正确的响应。</li><li>在Zero-Shot学习设置中，模型只根据任务的描述生成响应，不需要任何示例。</li></ul><p>而OpenAI在介绍GPT-3模型的重要论文《Language models are Few-Shot learners（语言模型是少样本学习者）》中，更是直接指出：GPT-3模型，作为一个大型的自我监督学习模型，通过提升模型规模，实现了出色的Few-Shot学习性能。</p><p>这篇论文为大语言模型可以进行Few-Shot学习提供了扎实的理论基础。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/481yy45346cc28ec48269c752c3647bc.png"></p><p>下图就是OpenAI的GPT-3论文给出的GPT-3在翻译任务中，通过FewShot提示完成翻译的例子。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/357e9ca0ce2b4699a24e3fe512c047ca.png"></p><p>以上，就是ZeroShot、OneShot、FewShot这些重要概念的起源。</p><h2 id="使用-FewShotPromptTemplate"><a href="#使用-FewShotPromptTemplate" class="headerlink" title="使用 FewShotPromptTemplate"></a>使用 FewShotPromptTemplate</h2><p>下面，就让我们来通过LangChain中的FewShotPromptTemplate构建出最合适的鲜花文案。</p><p><strong>1. 创建示例样本</strong></p><p>首先，创建一些示例，作为提示的样本。其中每个示例都是一个字典，其中键是输入变量，值是这些输入变量的值。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 1. 创建一些示例samples = [  &#123;    "flower_type": "玫瑰",    "occasion": "爱情",    "ad_copy": "玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。"  &#125;,  &#123;    "flower_type": "康乃馨",    "occasion": "母亲节",    "ad_copy": "康乃馨代表着母爱的纯洁与伟大，是母亲节赠送给母亲的完美礼物。"  &#125;,  &#123;    "flower_type": "百合",    "occasion": "庆祝",    "ad_copy": "百合象征着纯洁与高雅，是你庆祝特殊时刻的理想选择。"  &#125;,  &#123;    "flower_type": "向日葵",    "occasion": "鼓励",    "ad_copy": "向日葵象征着坚韧和乐观，是你鼓励亲朋好友的最好方式。"  &#125;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>samples这个列表，它包含了四个字典，每个字典代表了一种花的类型、适合的场合，以及对应的广告文案。 这些示例样本，就是构建FewShotPrompt时，作为例子传递给模型的参考信息。</p><p><strong>2. 创建提示模板</strong></p><p>配置一个提示模板，将一个示例格式化为字符串。这个格式化程序应该是一个PromptTemplate对象。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 2. 创建一个提示模板from langchain.prompts.prompt import PromptTemplatetemplate="鲜花类型: &#123;flower_type&#125;\n场合: &#123;occasion&#125;\n文案: &#123;ad_copy&#125;"prompt_sample = PromptTemplate(input_variables=["flower_type", "occasion", "ad_copy"],                               template=template)print(prompt_sample.format(**samples[0]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>提示模板的输出如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">鲜花类型: 玫瑰场合: 爱情文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>在这个步骤中，我们创建了一个PromptTemplate对象。这个对象是根据指定的输入变量和模板字符串来生成提示的。在这里，输入变量包括 <code>&quot;flower_type&quot;</code>、 <code>&quot;occasion&quot;</code>、 <code>&quot;ad_copy&quot;</code>，模板是一个字符串，其中包含了用大括号包围的变量名，它们会被对应的变量值替换。</p><p>到这里，我们就把字典中的示例格式转换成了提示模板，可以形成一个个具体可用的LangChain提示。比如我用samples[0]中的数据替换了模板中的变量，生成了一个完整的提示。</p><p><strong>3. 创建 FewShotPromptTemplate 对象</strong></p><p>然后，通过使用上一步骤中创建的prompt_sample，以及samples列表中的所有示例， 创建一个FewShotPromptTemplate对象，生成更复杂的提示。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 3. 创建一个FewShotPromptTemplate对象from langchain.prompts.few_shot import FewShotPromptTemplateprompt = FewShotPromptTemplate(    examples=samples,    example_prompt=prompt_sample,    suffix="鲜花类型: &#123;flower_type&#125;\n场合: &#123;occasion&#125;",    input_variables=["flower_type", "occasion"])print(prompt.format(flower_type="野玫瑰", occasion="爱情"))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">鲜花类型: 玫瑰场合: 爱情文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。鲜花类型: 康乃馨场合: 母亲节文案: 康乃馨代表着母爱的纯洁与伟大，是母亲节赠送给母亲的完美礼物。鲜花类型: 百合场合: 庆祝文案: 百合象征着纯洁与高雅，是你庆祝特殊时刻的理想选择。鲜花类型: 向日葵场合: 鼓励文案: 向日葵象征着坚韧和乐观，是你鼓励亲朋好友的最好方式。鲜花类型: 野玫瑰场合: 爱情<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，FewShotPromptTemplate是一个更复杂的提示模板，它包含了多个示例和一个提示。这种模板可以使用多个示例来指导模型生成对应的输出。目前我们创建一个新提示，其中包含了根据指定的花的类型“野玫瑰”和场合“爱情”。</p><p><strong>4. 调用大模型创建新文案</strong></p><p>最后，把这个对象输出给大模型，就可以根据提示，得到我们所需要的文案了！</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 4. 把提示传递给大模型import osos.environ["OPENAI_API_KEY"] = '你的Open AI Key'from langchain.llms import OpenAImodel = OpenAI(model_name='text-davinci-003')result = model(prompt.format(flower_type="野玫瑰", occasion="爱情"))print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">文案: 野玫瑰代表着爱情的坚贞，是你向心爱的人表达爱意的最佳礼物。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>好！模型成功地模仿了我们的示例，写出了新文案，从结构到语气都蛮相似的。</p><h2 id="使用示例选择器"><a href="#使用示例选择器" class="headerlink" title="使用示例选择器"></a>使用示例选择器</h2><p>如果我们的示例很多，那么一次性把所有示例发送给模型是不现实而且低效的。另外，每次都包含太多的Token也会浪费流量（OpenAI是按照Token数来收取费用）。</p><p>LangChain给我们提供了示例选择器，来选择最合适的样本。（注意，因为示例选择器使用向量相似度比较的功能，此处需要安装向量数据库，这里我使用的是开源的Chroma，你也可以选择之前用过的Qdrant。）</p><p>下面，就是使用示例选择器的示例代码。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 5. 使用示例选择器from langchain.prompts.example_selector import SemanticSimilarityExampleSelectorfrom langchain.vectorstores import Chromafrom langchain.embeddings import OpenAIEmbeddings# 初始化示例选择器example_selector = SemanticSimilarityExampleSelector.from_examples(    samples,    OpenAIEmbeddings(),    Chroma,    k=1)# 创建一个使用示例选择器的FewShotPromptTemplate对象prompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=prompt_sample,    suffix="鲜花类型: &#123;flower_type&#125;\n场合: &#123;occasion&#125;",    input_variables=["flower_type", "occasion"])print(prompt.format(flower_type="红玫瑰", occasion="爱情"))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">鲜花类型: 玫瑰场合: 爱情文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。鲜花类型: 红玫瑰场合: 爱情<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这个步骤中，它首先创建了一个SemanticSimilarityExampleSelector对象，这个对象可以根据语义相似性选择最相关的示例。然后，它创建了一个新的FewShotPromptTemplate对象，这个对象使用了上一步创建的选择器来选择最相关的示例生成提示。</p><p>然后，我们又用这个模板生成了一个新的提示，因为我们的提示中需要创建的是红玫瑰的文案，所以，示例选择器example_selector会根据语义的相似度（余弦相似度）找到最相似的示例，也就是“玫瑰”，并用这个示例构建了FewShot模板。</p><p>这样，我们就避免了把过多的无关模板传递给大模型，以节省Token的用量。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>好的，到这里，今天这一讲就结束了。我们介绍了提示工程的原理，几种提示模板的用法，以及最重要的FewShot的思路。其实说白了，就是给模型一些示例做参考，模型才能明白你要什么。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f46817a7ed56c6fef64a6aeee4c1yy0d.png"></p><p>总的来说，提供示例对于解决某些任务至关重要，通常情况下，FewShot的方式能够显著提高模型回答的质量。不过，当少样本提示的效果不佳时，这可能表示模型在任务上的学习不足。在这种情况下，我们建议对模型进行微调或尝试更高级的提示技术。</p><p>下一节课，我们将在探讨输出解析的同时，讲解另一种备受关注的提示技术，被称为“思维链提示”（Chain of Thought，简称CoT）。这种技术因其独特的应用方式和潜在的实用价值而引人注目。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>如果你观察LangChain中的prompt.py中的PromptTemplate的实现代码，你会发现除了我们使用过的input_variables、template等初始化参数之外，还有template_format、validate_template等参数。举例来说，template_format可以指定除了f-string之外，其它格式的模板，比如jinja2。请你查看LangChain文档，并尝试使用这些参数。</li></ol><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">template_format: str = "f-string""""The format of the prompt template. Options are: 'f-string', 'jinja2'."""validate_template: bool = True"""Whether or not to try validating the template."""<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li><p>请你尝试使用PipelinePromptTemplate和自定义Template。</p></li><li><p>请你构想一个关于鲜花店运营场景中客户服务对话的少样本学习任务。在这个任务中，模型需要根据提供的示例，学习如何解答客户的各种问题，包括询问花的价格、推荐鲜花、了解鲜花的保养方法等。最好是用ChatModel完成这个任务。</p></li></ol><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.chat_models import ChatOpenAIfrom langchain import PromptTemplatefrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>题目较多，可以选择性思考，期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>论文： Open AI的GPT-3模型： <a href="https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">大模型是少样本学习者</a>， Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … &amp; Agarwal, S. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</li><li>论文： <a href="https://arxiv.org/abs/1606.04080">单样本学习的匹配网络</a>，Vinyals, O., Blundell, C., Lillicrap, T., &amp; Wierstra, D. (2016). Matching networks for one shot learning. In Advances in neural information processing systems (pp. 3630-3638).</li><li>论文： <a href="https://www.cs.toronto.edu/~hinton/absps/palatucci.pdf">用语义输出编码做零样本学习</a>，Palatucci, M., Pomerleau, D., Hinton, G. E., &amp; Mitchell, T. M. (2009). Zero-shot learning with semantic output codes. In Advances in neural information processing systems (pp. 1410-1418).</li><li>论文： <a href="https://doi.org/10.48550/arXiv.2202.12837">对示例角色的重新思考：是什么使得上下文学习有效？</a> Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., &amp; Zettlemoyer, L. (2022). Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022).</li><li>论文： <a href="https://arxiv.org/pdf/2109.01652.pdf">微调后的语言模型是零样本学习者</a>，Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., &amp; Le, Q. V. (2022). Finetuned Language Models Are Zero-Shot Learners. Proceedings of the International Conference on Learning Representations (ICLR 2022).</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;04｜提示工程（上）：用少样本FewShotTemplate和ExampleSelector创建应景文案&quot;&gt;&lt;a href=&quot;#04｜提示工程（上）：用少样本FewShotTemplate和ExampleSelector创建应景文案&quot; class=&quot;headerl</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>05｜提示工程（下）：用思维链和思维树提升模型思考质量</title>
    <link href="https://zhuansun.github.io/geekbang/posts/327332409.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/327332409.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.429Z</updated>
    
    <content type="html"><![CDATA[<h1 id="05｜提示工程（下）：用思维链和思维树提升模型思考质量"><a href="#05｜提示工程（下）：用思维链和思维树提升模型思考质量" class="headerlink" title="05｜提示工程（下）：用思维链和思维树提升模型思考质量"></a>05｜提示工程（下）：用思维链和思维树提升模型思考质量</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>我在 <a href="https://time.geekbang.org/column/article/700699">第4课</a> 的结尾时说了，你可以尝试用思维链也就是CoT（Chain of Thought）的概念来引导模型的推理，让模型生成更详实、更完备的文案，今天我们就一起看一看CoT的使用。</p><h2 id="什么是-Chain-of-Thought"><a href="#什么是-Chain-of-Thought" class="headerlink" title="什么是 Chain of Thought"></a>什么是 Chain of Thought</h2><p>CoT这个概念来源于学术界，是谷歌大脑的Jason Wei等人于2022年在论文《 <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>（自我一致性提升了语言模型中的思维链推理能力）》中提出来的概念。它提出，如果生成一系列的中间推理步骤，就能够显著提高大型语言模型进行复杂推理的能力。</p><h3 id="Few-Shot-CoT"><a href="#Few-Shot-CoT" class="headerlink" title="Few-Shot CoT"></a>Few-Shot CoT</h3><p>Few-Shot CoT 简单的在提示中提供了一些链式思考示例（Chain-of-Thought Prompting），足够大的语言模型的推理能力就能够被增强。简单说，就是给出一两个示例，然后在示例中写清楚推导的过程。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f27cec109dff8947d85507b34ce240a0.png"></p><p>论文中给出了一个大模型通过思维链做数学题的示例。图左和图右，大模型都读入了OneShot示例，但是图左只给出了答案，而图右则在OneShot示例中给出了解题的具体思路。结果，只给出了答案的模型推理错误，而给出解题思路后，同一个模型生成了正确的答案。</p><p>在三种大型语言模型的实验中，CoT在一系列的算术、常识和符号推理任务中都提高了性能。在GSM8K数学问题基准测试中，通过CoT指导后，大模型的表现可以达到当时最先进的准确性。</p><p>CoT从概念上非常容易理解，从应用上非常容易操作。虽然简单，但这种思想可以给我们的开发过程带来很多启发。</p><p>比如，假设我们正在开发一个AI花店助手，它的任务是帮助用户选择他们想要的花，并生成一个销售列表。在这个过程中，我们可以使用CoT来引导AI的推理过程。</p><p>👉 整体指导：你需要跟着下面的步骤一步步的推理。</p><ol><li><p>问题理解：首先，AI需要理解用户的需求。例如，用户可能会说：“今天要参加朋友的生日Party，想送束花祝福她。”我们可以给AI一个提示模板，里面包含示例：“ <em><strong>遇到XX问题，我先看自己有</strong></em> <em><strong>没有</strong></em> _<strong>相关知识，有的话，就提供答案；没有，就调用工具搜索，有了知识后再试图解决。</strong>_”—— 这就是给了AI一个思维链的示例。</p></li><li><p>信息搜索：接下来，AI需要搜索相关信息。例如，它可能需要查找哪些花最适合生日派对。</p></li><li><p>决策制定：基于收集到的信息，AI需要制定一个决策。我们可以通过思维链让他详细思考决策的流程，先做什么后做什么。例如，我们可以给它一个示例：“ _<strong>遇到生日派对送花的情况，我先考虑用户的需求，然后查看鲜花的库存，最后决定推荐一些玫瑰和百合，因为这些花通常适合生日派对。</strong>_”—— 那么有了生日派对这个场景做示例，大模型就能把类似的思维流程运用到其它场景。</p></li><li><p>生成销售列表：最后，AI使用OutputParser生成一个销售列表，包括推荐的花和价格。</p></li></ol><p>在这个过程中，整体上，思维链引导AI从理解问题，到搜索信息，再到制定决策，最后生成销售列表。这种方法不仅使AI的推理过程更加清晰，也使得生成的销售列表更加符合用户的需求。具体到每一个步骤，也可以通过思维链来设计更为详细的提示模板，来引导模型每一步的思考都遵循清晰准确的逻辑。</p><p>其实LangChain的核心组件Agent的本质就是进行好的提示工程，并大量地使用预置的FewShot和CoT模板。这个在之后的课程学习中我们会理解得越来越透彻。</p><h3 id="Zero-Shot-CoT"><a href="#Zero-Shot-CoT" class="headerlink" title="Zero-Shot CoT"></a>Zero-Shot CoT</h3><p>下面的这两个CoT提示模板的例子，来自于Google Research和东京大学的论文《 <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf">大语言模型是零样本推理者</a>》。</p><p>图中的（d）示例非常非常有意思，在Zero-Shot CoT中，你只要简单地告诉模型“ <strong>让我们一步步的思考（Let’s think step by step）</strong>”，模型就能够给出更好的答案！</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/09a48be47b3e0e9ec0ae7ebd483d868b.png"></p><p>哈哈哈，这样太神奇了吧，这不由得让我联想起最简单的提示工程，角色设定——模型回答之前，先告诉它“你是一个很有经验的XX专家”，模型应该就会在开始胡说八道之前三思。</p><p>简单总结一下：Few-Shot CoT，指的就是在带有示例的提示过程中，加入思考的步骤，从而引导模型给出更好的结果。而Zero-Shot CoT，就是直接告诉模型要一步一步地思考，慢慢地推理。</p><h2 id="Chain-of-Thought-实战"><a href="#Chain-of-Thought-实战" class="headerlink" title="Chain of Thought 实战"></a>Chain of Thought 实战</h2><p>现在，就让我带着你完成一次Chain of Thought的LangChain应用开发实战。</p><p><strong>项目需求</strong>：在这个示例中，你正在开发一个AI运营助手，我们要展示AI如何根据用户的需求推理和生成答案。然后，AI根据当前的用户请求进行推理，提供了具体的花卉建议并解释了为什么选择这些建议。</p><p>在这个过程中，AI需要理解客户的需求之后，按部就班的思考，然后给出最符合逻辑的回答。</p><h3 id="CoT的模板设计"><a href="#CoT的模板设计" class="headerlink" title="CoT的模板设计"></a>CoT的模板设计</h3><p>针对这个聊天机器人的需求，我设计了下面这样的思维链模板。</p><blockquote><p>作为一个为花店电商公司工作的AI助手，我的目标是帮助客户根据他们的喜好做出明智的决定。</p><p>我会按部就班的思考，先理解客户的需求，然后考虑各种鲜花的涵义，最后根据这个需求，给出我的推荐。</p><p>同时，我也会向客户解释我这样推荐的原因。</p><p><strong>示例 1：</strong></p><p>人类：我想找一种象征爱情的花。</p><p>AI：首先，我理解你正在寻找一种可以象征爱情的花。在许多文化中，红玫瑰被视为爱情的象征，这是因为它们的红色通常与热情和浓烈的感情联系在一起。因此，考虑到这一点，我会推荐红玫瑰。红玫瑰不仅能够象征爱情，同时也可以传达出强烈的感情，这是你在寻找的。</p><p><strong>示例 2：</strong></p><p>人类：我想要一些独特和奇特的花。</p><p>AI：从你的需求中，我理解你想要的是独一无二和引人注目的花朵。兰花是一种非常独特并且颜色鲜艳的花，它们在世界上的许多地方都被视为奢侈品和美的象征。因此，我建议你考虑兰花。选择兰花可以满足你对独特和奇特的要求，而且，兰花的美丽和它们所代表的力量和奢侈也可能会吸引你。</p></blockquote><p>AI的模板开始于对其角色的阐述，并给出了一些先前的对话示例（Few-Shot Learning）来帮助AI理解如何处理这种类型的请求。这些示例展示了AI如何根据思维链进行思考，给出深思熟虑之后的答案。</p><h3 id="程序的完整框架"><a href="#程序的完整框架" class="headerlink" title="程序的完整框架"></a>程序的完整框架</h3><p>程序的完整代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 设置环境变量和API密钥import osos.environ["OPENAI_API_KEY"] = '你的OpenAI API Key'# 创建聊天模型from langchain.chat_models import ChatOpenAIllm = ChatOpenAI(temperature=0)# 设定 AI 的角色和目标role_template = "你是一个为花店电商公司工作的AI助手, 你的目标是帮助客户根据他们的喜好做出明智的决定"# CoT 的关键部分，AI 解释推理过程，并加入一些先前的对话示例（Few-Shot Learning）cot_template = """作为一个为花店电商公司工作的AI助手，我的目标是帮助客户根据他们的喜好做出明智的决定。我会按部就班的思考，先理解客户的需求，然后考虑各种鲜花的涵义，最后根据这个需求，给出我的推荐。同时，我也会向客户解释我这样推荐的原因。示例 1:  人类：我想找一种象征爱情的花。  AI：首先，我理解你正在寻找一种可以象征爱情的花。在许多文化中，红玫瑰被视为爱情的象征，这是因为它们的红色通常与热情和浓烈的感情联系在一起。因此，考虑到这一点，我会推荐红玫瑰。红玫瑰不仅能够象征爱情，同时也可以传达出强烈的感情，这是你在寻找的。示例 2:  人类：我想要一些独特和奇特的花。  AI：从你的需求中，我理解你想要的是独一无二和引人注目的花朵。兰花是一种非常独特并且颜色鲜艳的花，它们在世界上的许多地方都被视为奢侈品和美的象征。因此，我建议你考虑兰花。选择兰花可以满足你对独特和奇特的要求，而且，兰花的美丽和它们所代表的力量和奢侈也可能会吸引你。"""from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplatesystem_prompt_role = SystemMessagePromptTemplate.from_template(role_template)system_prompt_cot = SystemMessagePromptTemplate.from_template(cot_template)# 用户的询问human_template = "&#123;human_input&#125;"human_prompt = HumanMessagePromptTemplate.from_template(human_template)# 将以上所有信息结合为一个聊天提示chat_prompt = ChatPromptTemplate.from_messages([system_prompt_role, system_prompt_cot, human_prompt])prompt = chat_prompt.format_prompt(human_input="我想为我的女朋友购买一些花。她喜欢粉色和紫色。你有什么建议吗?").to_messages()# 接收用户的询问，返回回答结果response = llm(prompt)print(response)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>程序中，首先设置环境变量OpenAI的API密钥，以便能够使用OpenAI的GPT-4模型。然后创建聊天模型：通过调用 ChatOpenAI 类，创建了一个聊天模型。设置 temperature&#x3D;0 可以让模型生成更确定性的回答，即输出更倾向于最可能的结果。</p><p>接着定义了AI的角色和目标，该AI为花店电商公司的助手，其目标是根据客户的喜好来提供购买建议。紧接着，定义 CoT 模板，其中包括了AI的角色和目标描述、思考链条以及遵循思考链条的一些示例，显示了AI如何理解问题，并给出建议。</p><p>之后，我使用了PromptTemplate的from_template方法，来生成相应的询问模板。其中包括用于指导模型的SystemMessagePromptTemplate和用于传递人类问题的HumanMessagePromptTemplate。</p><p>然后，我使用了ChatPromptTemplate.from_messages方法，整合上述定义的角色，CoT模板和用户询问，生成聊天提示。</p><p>最后，将生成的聊天提示输入模型中，获得模型的回答，并打印出来。</p><p>在Few-Shot CoT提示的指引之下，模型针对我们的问题，从问题中的具体需求出发，返回了不错的建议。</p><p><em><strong>现在，根据你的需求：你正在寻找你的女朋友喜欢的粉色和紫色的花。</strong></em></p><p><em><strong>首先，我从理解你的需求出发，只会推荐粉色或紫色，或者两者的组合的花。这些可能包括粉色的玫瑰，紫色的兰花，或者是粉色和紫色的花的混合花束。玫瑰是象征爱情和亲情的经典符号，而兰花象征着美丽和力量。这两种花都蕴含很棒的内涵。当然了，无论你选择哪种花卉，重要的是表达出你对她的爱和关心。记得附上一张温馨的贺卡，写下你的真挚祝福。</strong></em></p><h2 id="Tree-of-Thought"><a href="#Tree-of-Thought" class="headerlink" title="Tree of Thought"></a>Tree of Thought</h2><p>CoT这种思想，为大模型带来了更好的答案，然而，对于需要探索或预判战略的复杂任务来说，传统或简单的提示技巧是不够的。基于CoT的思想，Yao和Long等人几乎在同一时间在论文《 <a href="https://arxiv.org/pdf/2305.10601.pdf">思维之树：使用大型语言模型进行深思熟虑的问题解决</a>》和《 <a href="https://arxiv.org/pdf/2305.08291.pdf">大型语言模型指导的思维之树</a>》中，进一步提出了思维树（Tree of Thoughts，ToT）框架，该框架基于思维链提示进行了总结，引导语言模型探索把思维作为中间步骤来解决通用问题。</p><p>ToT是一种解决复杂问题的框架，它在需要多步骤推理的任务中，引导语言模型搜索一棵由连贯的语言序列（解决问题的中间步骤）组成的思维树，而不是简单地生成一个答案。ToT框架的核心思想是：让模型生成和评估其思维的能力，并将其与搜索算法（如广度优先搜索和深度优先搜索）结合起来，进行系统性地探索和验证。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/6eec83ffe1a5f37d245520535d65f8a0.png"></p><p>ToT 框架为每个任务定义具体的思维步骤和每个步骤的候选项数量。例如，要解决一个数学推理任务，先把它分解为3个思维步骤，并为每个步骤提出多个方案，并保留最优的5个候选方案。然后在多条思维路径中搜寻最优的解决方案。</p><p>这种方法的优势在于，模型可以通过观察和评估其自身的思维过程，更好地解决问题，而不仅仅是基于输入生成输出。这对于需要深度推理的复杂任务非常有用。此外，通过引入强化学习、集束搜索等技术，可以进一步提高搜索策略的性能，并让模型在解决新问题或面临未知情况时有更好的表现。</p><p>下面我们应用ToT的思想，给出一个鲜花运营方面的示例。</p><blockquote><p>假设一个顾客在鲜花网站上询问：“我想为我的妻子购买一束鲜花，但我不确定应该选择哪种鲜花。她喜欢淡雅的颜色和花香。”</p><p>AI（使用ToT框架）：</p><p><strong>思维步骤1</strong>：理解顾客的需求。</p><p>顾客想为妻子购买鲜花。</p><p>顾客的妻子喜欢淡雅的颜色和花香。</p><p><strong>思维步骤2</strong>：考虑可能的鲜花选择。</p><p>候选1：百合，因为它有淡雅的颜色和花香。</p><p>候选2：玫瑰，选择淡粉色或白色，它们通常有花香。</p><p>候选3：紫罗兰，它有淡雅的颜色和花香。</p><p>候选4：桔梗，它的颜色淡雅但不一定有花香。</p><p>候选5：康乃馨，选择淡色系列，它们有淡雅的花香。</p><p><strong>思维步骤3</strong>：根据顾客的需求筛选最佳选择。</p><p>百合和紫罗兰都符合顾客的需求，因为它们都有淡雅的颜色和花香。</p><p>淡粉色或白色的玫瑰也是一个不错的选择。</p><p>桔梗可能不是最佳选择，因为它可能没有花香。</p><p>康乃馨是一个可考虑的选择。</p><p><strong>思维步骤4</strong>：给出建议。</p><p>“考虑到您妻子喜欢淡雅的颜色和花香，我建议您可以选择百合或紫罗兰。淡粉色或白色的玫瑰也是一个很好的选择。希望这些建议能帮助您做出决策！”</p></blockquote><p>这个例子，可以作为FewShot示例之一，传递给模型，让他学着实现ToT。</p><p>通过在具体的步骤中产生多条思考路径，ToT 框架为解决复杂问题提供了一种新的方法，这种方法结合了语言模型的生成能力、搜索算法以及强化学习，以达到更好的效果。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>这节课我们介绍了Chain of Thought（CoT，即“思维链”）和Tree of Thoughts（ToT，即“思维树”）这两个非常有趣的概念，并探讨了如何利用它们引导大型语言模型进行更深入的推理。</p><ul><li><p>CoT的核心思想是通过生成一系列中间推理步骤来增强模型的推理能力。在Few-Shot CoT和Zero-Shot CoT两种应用方法中，前者通过提供链式思考示例传递给模型，后者则直接告诉模型进行要按部就班的推理。</p></li><li><p>ToT进一步扩展了CoT的思想，通过搜索由连贯的语言序列组成的思维树来解决复杂问题。我通过一个鲜花选择的实例，展示了如何在实际应用中使用ToT框架。</p><p>有朋友在GitHub上开了一个 <a href="https://github.com/kyegomez/tree-of-thoughts">Repo</a>，专门给大家介绍ToT的应用方法和实例，他们还给出了几个非常简单的通用ToT提示语，就像下面这样。</p></li></ul><blockquote><p>请你模拟三位出色、逻辑性强的专家合作回答一个问题。每个人都详细地解释他们的思考过程，考虑到其他人之前的解释，并公开承认错误。在每一步，只要可能，每位专家都会在其他人的思考基础上进行完善和建设，并承认他们的贡献。他们继续，直到对问题有一个明确的答案。为了清晰起见，您的整个回应应该是一个Markdown表格。</p><p>问题是…</p></blockquote><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d719e10a2b045f5a70993b6135ef503c.png" alt="图片"></p><p>如果你有兴趣，可以去这个Repo里面看一看。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>我们的CoT实战示例中使用的是Few-Shot CoT提示，请你把它换为Zero-Shot CoT，跑一下程序，看看结果。</li><li>请你设计一个你工作场景中的任务需求，然后用ToT让大语言模型帮你解决问题。</li></ol><p>期待在留言区看到你的分享，我们一起交流探讨，共创一个好的学习氛围。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>论文，自我一致性提升了语言模型中的思维链推理能力， <a href="https://arxiv.org/pdf/2205.11916.pdf">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>，Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., &amp; Zhou, D. (2023). Self-Consistency Improves Chain of Thought Reasoning in Language Models. Proceedings of the International Conference on Learning Representations (ICLR). arXiv preprint arXiv:2203.11171.</li><li>论文，大语言模型是零样本推理者， <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf">Large Language Models are Zero-Shot Reasoners</a>，Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., &amp; Iwasawa, Y. (2023). Large Language Models are Zero-Shot Reasoners. arXiv preprint arXiv:2205.11916v4.</li><li>论文，思维之树：使用大型语言模型进行深思熟虑的问题解决， <a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a>，Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., &amp; Narasimhan, K. (2023). Tree of Thoughts: Deliberate Problem Solving with Large Language Models. arXiv preprint arXiv:2305.10601.</li><li>论文，大型语言模型指导的思维之树， <a href="https://arxiv.org/abs/2305.08291">Large Language Model Guided Tree-of-Thought</a>，Long, J. (2023). Large Language Model Guided Tree-of-Thought. arXiv preprint arXiv:2305.08291.</li><li>GitHub链接， <a href="https://github.com/kyegomez/tree-of-thoughts">tree-of-thoughts</a>，把ToT算法导入你的大模型应用，目前3.3K颗星</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;05｜提示工程（下）：用思维链和思维树提升模型思考质量&quot;&gt;&lt;a href=&quot;#05｜提示工程（下）：用思维链和思维树提升模型思考质量&quot; class=&quot;headerlink&quot; title=&quot;05｜提示工程（下）：用思维链和思维树提升模型思考质量&quot;&gt;&lt;/a&gt;05｜提示</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>06｜调用模型：使用OpenAI API还是微调开源Llama2/ChatGLM？</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3239959551.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3239959551.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.432Z</updated>
    
    <content type="html"><![CDATA[<h1 id="06｜调用模型：使用OpenAI-API还是微调开源Llama2-ChatGLM？"><a href="#06｜调用模型：使用OpenAI-API还是微调开源Llama2-ChatGLM？" class="headerlink" title="06｜调用模型：使用OpenAI API还是微调开源Llama2&#x2F;ChatGLM？"></a>06｜调用模型：使用OpenAI API还是微调开源Llama2&#x2F;ChatGLM？</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>之前，我们花了两节课的内容讲透了提示工程的原理以及LangChain中的具体使用方式。今天，我们来着重讨论Model I&#x2F;O中的第二个子模块，LLM。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/cd7e1506af5b6a8e382c2c9eab4d7481.jpg"></p><p>让我们带着下面的问题来开始这一节课的学习。大语言模型，不止ChatGPT一种。调用OpenAI的API，当然方便且高效，不过，如果我就是想用其他的模型（比如说开源的Llama2或者ChatGLM），该怎么做？再进一步，如果我就是想在本机上从头训练出来一个新模型，然后在LangChain中使用自己的模型，又该怎么做？</p><p>关于大模型的微调（或称精调）、预训练、重新训练、乃至从头训练，这是一个相当大的话题，不仅仅需要足够的知识和经验，还需要大量的语料数据、GPU硬件和强大的工程能力。别说一节课了，我想两三个专栏也不一定能讲全讲透。不过，我可以提纲挈领地把大模型的训练流程和使用方法给你缕一缕。这样你就能体验到，在LangChain中使用自己微调的模型是完全没问题的。</p><h2 id="大语言模型发展史"><a href="#大语言模型发展史" class="headerlink" title="大语言模型发展史"></a>大语言模型发展史</h2><p>说到语言模型，我们不妨先从其发展史中去了解一些关键信息。</p><p>Google 2018 年的论文名篇Attention is all you need，提出了Transformer架构，也给这一次AI的腾飞点了火。Transformer是几乎所有预训练模型的核心底层架构。基于Transformer预训练所得的大规模语言模型也被叫做“基础模型”（Foundation Model 或Base Model）。</p><p>在这个过程中，模型学习了词汇、语法、句子结构以及上下文信息等丰富的语言知识。这种在大量数据上学到的知识，为后续的下游任务（如情感分析、文本分类、命名实体识别、问答系统等）提供了一个通用的、丰富的语言表示基础，为解决许多复杂的NLP问题提供了可能。</p><p>在预训练模型出现的早期，BERT毫无疑问是最具代表性的，也是影响力最大的模型。BERT通过同时学习文本的前向和后向上下文信息，实现对句子结构的深入理解。BERT之后，各种大型预训练模型如雨后春笋般地涌现，自然语言处理（NLP）领域进入了一个新时代。这些模型推动了NLP技术的快速发展，解决了许多以前难以应对的问题，比如翻译、文本总结、聊天对话等等，提供了强大的工具。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/7f1108deceaa4b5281ed431598f1b0a6.jpg"></p><p>当然，现今的预训练模型的趋势是参数越来越多，模型也越来越大，训练一次的费用可达几百万美元。这样大的开销和资源的耗费，只有世界顶级大厂才能够负担得起，普通的学术组织和高等院校很难在这个领域继续引领科技突破，这种现象开始被普通研究人员所诟病。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/95828d4e2234e7130bb2d500455092ef.jpg"></p><h2 id="预训练-微调的模式"><a href="#预训练-微调的模式" class="headerlink" title="预训练+微调的模式"></a>预训练+微调的模式</h2><p>不过，话虽如此，大型预训练模型的确是工程师的福音。因为，经过预训练的大模型中所习得的语义信息和所蕴含的语言知识，能够非常容易地向下游任务迁移。NLP应用人员可以对模型的头部或者部分参数根据自己的需要进行适应性的调整，这通常涉及在相对较小的有标注数据集上进行有监督学习，让模型适应特定任务的需求。</p><p>这就是对预训练模型的微调（Fine-tuning）。微调过程相比于从头训练一个模型要快得多，且需要的数据量也要少得多，这使得作为工程师的我们能够更高效地开发和部署各种NLP解决方案。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/75edd66d67ec8a20326b69514c9d9daf.jpg"></p><p>图中的“具体任务”，其实也可以更换为“具体领域”。那么总结来说：</p><ul><li><strong>预训练</strong>：在大规模无标注文本数据上进行模型的训练，目标是让模型学习自然语言的基础表达、上下文信息和语义知识，为后续任务提供一个通用的、丰富的语言表示基础。</li><li><strong>微调</strong>：在预训练模型的基础上，可以根据特定的下游任务对模型进行微调。现在你经常会听到各行各业的人说： _我们的优势就是领域知识嘛！我们比不过国内外大模型，我们可以拿开源模型做垂直领域嘛！做垂类模型！_—— 啥叫垂类？指的其实就是根据领域数据微调开源模型这件事儿。</li></ul><p>这种预训练+微调的大模型应用模式优势明显。首先，预训练模型能够将大量的通用语言知识迁移到各种下游任务上，作为应用人员，我们不需要自己寻找语料库，从头开始训练大模型，这减少了训练时间和数据需求；其次，微调过程可以快速地根据特定任务进行优化，简化了模型部署的难度；最后，预训练+微调的架构具有很强的可扩展性，可以方便地应用于各种自然语言处理任务，大大提高了NLP技术在实际应用中的可用性和普及程度，给我们带来了巨大的便利。</p><p>好，下面咱们开始一步步地使用开源模型。今天我要带你玩的模型主要是Meta（Facebook）推出的Llama2。当然你可以去Llama的官网下载模型，然后通过Llama官方 <a href="https://github.com/facebookresearch/llama">GitHub</a> 中提供的方法来调用它。但是，我还是会推荐你从HuggingFace下载并导入模型。因为啊，前天百川，昨天千问，今天流行Llama，明天不就流行别的了嘛。模型总在变，但是HuggingFace一直在那里，支持着各种开源模型。我们学东西，尽量选择学一次能够复用的知识。</p><h2 id="用-HuggingFace-跑开源模型"><a href="#用-HuggingFace-跑开源模型" class="headerlink" title="用 HuggingFace 跑开源模型"></a>用 HuggingFace 跑开源模型</h2><h3 id="注册并安装-HuggingFace"><a href="#注册并安装-HuggingFace" class="headerlink" title="注册并安装 HuggingFace"></a>注册并安装 HuggingFace</h3><p>第一步，还是要登录 <a href="https://huggingface.co/">HuggingFace</a> 网站，并拿到专属于你的Token。（如果你做了前面几节课的实战案例，那么你应该已经有这个API Token了）</p><p>第二步，用 <code>pip install transformers</code> 安装HuggingFace Library。详见 <a href="https://huggingface.co/docs/transformers/installation">这里</a>。</p><p>第三步，在命令行中运行 <code>huggingface-cli login</code>，设置你的API Token。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/5fa0c088652c8776f5ec50a059b1b1e6.png"></p><p>当然，也可以在程序中设置你的API Token，但是这不如在命令行中设置来得安全。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入HuggingFace API Tokenimport osos.environ['HUGGINGFACEHUB_API_TOKEN'] = '你的HuggingFace API Token'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="申请使用-Meta-的-Llama2-模型"><a href="#申请使用-Meta-的-Llama2-模型" class="headerlink" title="申请使用 Meta 的 Llama2 模型"></a>申请使用 Meta 的 Llama2 模型</h3><p>在HuggingFace的Model中，找到 <a href="https://huggingface.co/meta-llama/Llama-2-7b">meta-llama&#x2F;Llama-2-7b</a>。注意，各种各样版本的Llama2模型多如牛毛，我们这里用的是最小的7B版。此外，还有13b\70b\chat版以及各种各样的非Meta官方版。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/88a4b8d60cc93e77c3573663aa096217.png"></p><p>选择meta-llama&#x2F;Llama-2-7b这个模型后，你能够看到这个模型的基本信息。如果你是第一次用Llama，你需要申请Access，因为我已经申请过了，所以屏幕中间有句话：“You have been granted access to this model”。从申请到批准，大概是几分钟的事儿。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/46c59c59545720ccff6d7c560792d4ce.png"></p><h3 id="通过-HuggingFace-调用-Llama"><a href="#通过-HuggingFace-调用-Llama" class="headerlink" title="通过 HuggingFace 调用 Llama"></a>通过 HuggingFace 调用 Llama</h3><p>好，万事俱备，现在我们可以使用HuggingFace的Transformers库来调用Llama啦！</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入必要的库from transformers import AutoTokenizer, AutoModelForCausalLM# 加载预训练模型的分词器tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")# 加载预训练的模型# 使用 device_map 参数将模型自动加载到可用的硬件设备上，例如GPUmodel = AutoModelForCausalLM.from_pretrained(          "meta-llama/Llama-2-7b-chat-hf",          device_map = 'auto')# 定义一个提示，希望模型基于此提示生成故事prompt = "请给我讲个玫瑰的爱情故事?"# 使用分词器将提示转化为模型可以理解的格式，并将其移动到GPU上inputs = tokenizer(prompt, return_tensors="pt").to("cuda")# 使用模型生成文本，设置最大生成令牌数为2000outputs = model.generate(inputs["input_ids"], max_new_tokens=2000)# 将生成的令牌解码成文本，并跳过任何特殊的令牌，例如[CLS], [SEP]等response = tokenizer.decode(outputs[0], skip_special_tokens=True)# 打印生成的响应print(response)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段程序是一个很典型的HuggingFace的Transformers库的用例，该库提供了大量预训练的模型和相关的工具。</p><ul><li>导入AutoTokenizer：这是一个用于自动加载预训练模型的相关分词器的工具。分词器负责将文本转化为模型可以理解的数字格式。</li><li>导入AutoModelForCausalLM：这是用于加载因果语言模型（用于文本生成）的工具。</li><li>使用from_pretrained方法来加载预训练的分词器和模型。其中， <code>device_map = &#39;auto&#39;</code> 是为了自动地将模型加载到可用的设备上，例如GPU。</li><li>然后，给定一个提示（prompt）： <code>&quot;请给我讲个玫瑰的爱情故事?&quot;</code>，并使用分词器将该提示转换为模型可以接受的格式， <code>return_tensors=&quot;pt&quot;</code> 表示返回PyTorch张量。语句中的 <code>.to(&quot;cuda&quot;)</code> 是GPU设备格式转换，因为我在GPU上跑程序，不用这个的话会报错，如果你使用CPU，可以试一下删掉它。</li><li>最后使用模型的 <code>.generate()</code> 方法生成响应。 <code>max_new_tokens=2000</code> 限制生成的文本的长度。使用分词器的 <code>.decode() </code> 方法将输出的数字转化回文本，并且跳过任何特殊的标记。</li></ul><p>因为是在本地进行推理，耗时时间比较久。在我的机器上，大概需要30s～2min产生结果。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/933b7b11512bd06a977027cbbfd8d198.png"></p><p>这样的回答肯定不能直接用做商业文案，而且，我的意思是玫瑰花相关的故事，它明显把玫瑰理解成一个女孩的名字了。所以，开源模型，尤其是7B的小模型和Open AI的ChatGPT还是有一定差距的。</p><h2 id="LangChain-和-HuggingFace-的接口"><a href="#LangChain-和-HuggingFace-的接口" class="headerlink" title="LangChain 和 HuggingFace 的接口"></a>LangChain 和 HuggingFace 的接口</h2><p>讲了半天，LangChain未出场。下面让我们看一看，如何把HuggingFace里面的模型接入LangChain。</p><h3 id="通过-HuggingFace-Hub"><a href="#通过-HuggingFace-Hub" class="headerlink" title="通过 HuggingFace Hub"></a>通过 HuggingFace Hub</h3><p>第一种集成方式，是通过HuggingFace Hub。HuggingFace Hub 是一个开源模型中心化存储库，主要用于分享、协作和存储预训练模型、数据集以及相关组件。</p><p>我们给出一个HuggingFace Hub 和LangChain集成的代码示例。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入HuggingFace API Tokenimport osos.environ['HUGGINGFACEHUB_API_TOKEN'] = '你的HuggingFace API Token'# 导入必要的库from langchain import PromptTemplate, HuggingFaceHub, LLMChain# 初始化HF LLMllm = HuggingFaceHub(    repo_id="google/flan-t5-small",    #repo_id="meta-llama/Llama-2-7b-chat-hf",)# 创建简单的question-answering提示模板template = """Question: &#123;question&#125;              Answer: """# 创建Promptprompt = PromptTemplate(template=template, input_variables=["question"])# 调用LLM Chain --- 我们以后会详细讲LLM Chainllm_chain = LLMChain(    prompt=prompt,    llm=llm)# 准备问题question = "Rose is which type of flower?"# 调用模型并返回结果print(llm_chain.run(question))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看出，这个集成过程非常简单，只需要在HuggingFaceHub类的repo_id中指定模型名称，就可以直接下载并使用模型，模型会自动下载到HuggingFace的Cache目录，并不需要手工下载。</p><p>初始化LLM，创建提示模板，生成提示的过程，你已经很熟悉了。这段代码中有一个新内容是我通过llm_chain来调用了LLM。这段代码也不难理解，有关Chain的概念我们以后还会详述。</p><p>不过，我尝试使用meta-llama&#x2F;Llama-2-7b-chat-hf这个模型时，出现了错误，因此我只好用比较旧的模型做测试。我随便选择了google&#x2F;flan-t5-small，问了它一个很简单的问题，想看看它是否知道玫瑰是哪一种花。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/5bfc31eacb422fcd1d148bb1a2b3bf71.png"></p><p>模型告诉我，玫瑰是花。对，答案只有一个字，flower。这…不得不说，2023年之前的模型，和2023年之后的模型，水平没得比。以前的模型能说话就不错了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/yyc2177bc3c06f1d738f26985b9fbd3e.png"></p><h3 id="通过-HuggingFace-Pipeline"><a href="#通过-HuggingFace-Pipeline" class="headerlink" title="通过 HuggingFace Pipeline"></a>通过 HuggingFace Pipeline</h3><p>既然HuggingFace Hub还不能完成Llama-2的测试，让我们来尝试另外一种方法，HuggingFace Pipeline。HuggingFace 的 Pipeline 是一种高级工具，它简化了多种常见自然语言处理（NLP）任务的使用流程，使得用户不需要深入了解模型细节，也能够很容易地利用预训练模型来做任务。</p><p>让我来看看下面的示例：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 指定预训练模型的名称model = "meta-llama/Llama-2-7b-chat-hf"# 从预训练模型中加载词汇器from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained(model)# 创建一个文本生成的管道import transformersimport torchpipeline = transformers.pipeline(    "text-generation",    model=model,    torch_dtype=torch.float16,    device_map="auto",    max_length = 1000)# 创建HuggingFacePipeline实例from langchain import HuggingFacePipelinellm = HuggingFacePipeline(pipeline = pipeline,                          model_kwargs = &#123;'temperature':0&#125;)# 定义输入模板，该模板用于生成花束的描述template = """              为以下的花束生成一个详细且吸引人的描述：              花束的详细信息：              ```&#123;flower_details&#125;```           """# 使用模板创建提示from langchain import PromptTemplate,  LLMChainprompt = PromptTemplate(template=template,                     input_variables=["flower_details"])# 创建LLMChain实例from langchain import PromptTemplatellm_chain = LLMChain(prompt=prompt, llm=llm)# 需要生成描述的花束的详细信息flower_details = "12支红玫瑰，搭配白色满天星和绿叶，包装在浪漫的红色纸中。"# 打印生成的花束描述print(llm_chain.run(flower_details))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里简单介绍一下代码中使用到的transformers pipeline的配置参数。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/41yyb05408bd6a16e349f89279548f7e.jpg"></p><p>生成的结果之一如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/1bb303ec8bd8150d23bebc79035af13c.jpg"></p><p>此结果不敢恭维。但是，后续的测试告诉我，这很有可能是7B这个模型太小，尽管有形成中文的相应能力，但是能力不够强大，也就导致了这样的结果。</p><p>至此，通过HuggingFace接口调用各种开源模型的尝试成功结束。下面，我们进行最后一个测试，看看LangChain到底能否直接调用本地模型。</p><h2 id="用-LangChain-调用自定义语言模型"><a href="#用-LangChain-调用自定义语言模型" class="headerlink" title="用 LangChain 调用自定义语言模型"></a>用 LangChain 调用自定义语言模型</h2><p>最后，我们来尝试回答这节课开头提出的问题，假设你就是想训练属于自己的模型。而且出于商业秘密的原因，不想开源它，不想上传到HuggingFace，就是要在本机运行模型。此时应该如何利用LangChain的功能？</p><p>我们可以创建一个LLM的衍生类，自己定义模型。而LLM这个基类，则位于langchain.llms.base中，通过from langchain.llms.base import LLM语句导入。</p><p>这个自定义的LLM类只需要实现一个方法：</p><ul><li>_call方法：用于接收输入字符串并返回响应字符串。</li></ul><p>以及一个可选方法：</p><ul><li>_identifying_params方法：用于帮助打印此类的属性。</li></ul><p>下面，让我们先从HuggingFace的 <a href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main">这里</a>，下载一个llama-2-7b-chat.ggmlv3.q4_K_S.bin模型，并保存在本地。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/54c0ec3cbe3c3cyy6988de10f619b51c.png"></p><p>你可能会质疑我，不是说自己训练，自己微调，不再用HuggingFace了吗？</p><p>不好意思，容许我解释一下。自己训练一个能用的模型没那么容易。这个模型，它并不是原始的Llama模型，而是TheBloke这位老兄用他的手段为我们量化过的新模型，你也可以理解成，他已经为我们压缩或者说微调了Llama模型。</p><blockquote><p>量化是AI模型大小和性能优化的常用技术，它将模型的权重简化到较少的位数，以减少模型的大小和计算需求，让大模型甚至能够在CPU上面运行。当你看到模型的后缀有GGML或者GPTQ，就说明模型已经被量化过，其中GPTQ 是一种仅适用于 GPU 的特定格式。GGML 专为 CPU 和 Apple M 系列设计，但也可以加速 GPU 上的某些层。llama-cpp-python这个包就是为了实现GGML而制作的。</p></blockquote><p>所以，这里你就假设，咱们下载下来的llama-2-7b-chat.ggmlv3.q4_K_S.bin这个模型，就是你自己微调过的。将来你真的微调了Llama2、ChatGLM、百川或者千问的开源版，甚至是自己从头训练了一个mini-ChatGPT，你也可以保存为you_own_model.bin的格式，就按照下面的方式加载到LangChain之中。</p><p>然后，为了使用llama-2-7b-chat.ggmlv3.q4_K_S.bin这个模型，你需要安装 pip install llama-cpp-python 这个包。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入需要的库from llama_cpp import Llamafrom typing import Optional, List, Mapping, Anyfrom langchain.llms.base import LLM# 模型的名称和路径常量MODEL_NAME = 'llama-2-7b-chat.ggmlv3.q4_K_S.bin'MODEL_PATH = '/home/huangj/03_Llama/'# 自定义的LLM类，继承自基础LLM类class CustomLLM(LLM):    model_name = MODEL_NAME    # 该方法使用Llama库调用模型生成回复    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:        prompt_length = len(prompt) + 5        # 初始化Llama模型，指定模型路径和线程数        llm = Llama(model_path=MODEL_PATH+MODEL_NAME, n_threads=4)        # 使用Llama模型生成回复        response = llm(f"Q: &#123;prompt&#125; A: ", max_tokens=256)        # 从返回的回复中提取文本部分        output = response['choices'][0]['text'].replace('A: ', '').strip()        # 返回生成的回复，同时剔除了问题部分和额外字符        return output[prompt_length:]    # 返回模型的标识参数，这里只是返回模型的名称    @property    def _identifying_params(self) -> Mapping[str, Any]:        return &#123;"name_of_model": self.model_name&#125;    # 返回模型的类型，这里是"custom"    @property    def _llm_type(self) -> str:        return "custom"# 初始化自定义LLM类llm = CustomLLM()# 使用自定义LLM生成一个回复result = llm("昨天有一个客户抱怨他买了花给女朋友之后，两天花就枯了，你说作为客服我应该怎么解释？")# 打印生成的回复print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码中需要解释的内容不多，基本上就是CustomLLM类的构建和使用，类内部通过Llama类来实现大模型的推理功能，然后直接返回模型的回答。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/0275183b3863e602c59afb94707aca59.jpg"></p><p>似乎Llama经过量化之后，虽然仍读得懂中文，但是不会讲中文了。</p><p>翻译成中文，他的回答是这样的。</p><p><em>当客户抱怨他们为女朋友买的花在两天内就枯萎了，我会以礼貌和专业的方式这样解释：</em></p><p><em>“感谢您把这个问题告诉我们。对于给您带来的任何不便，我深感抱歉。有可能这些花没有被正确地存储或照料，这可能影响了它们的生命期。我们始终以提供高质量的产品为荣，但有时可能会出现意外的问题。请您知道，我们非常重视您的满意度并随时为您提供帮助。您希望我为您提供替换或退款吗？”</em></p><p>看上去，除了中文能力不大灵光之外，Llama2的英文表现真的非常完美，和GPT3.5差距不是很大，要知道：</p><ol><li>这可是开源模型，而且是允许商业的免费模型。</li><li>这是在本机 CPU 的环境下运行的，模型的推理速度还是可以接受的。</li><li>这仅仅是Llama的最小版本，也就是7B的量化版，就达到了这么好的效果。</li></ol><p>基于上述三点原因，我给Llama2打98.5分。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>今天的课程到此就结束了，相信你学到了很多新东西吧。的确，进入大模型开发这个领域，就好像打开了通往新世界的一扇门，有太多的新知识，等待着你去探索。</p><p>现在，你已经知道大模型训练涉及在大量数据上使用深度学习算法，通常需要大量计算资源和时间。训练后，模型可能不完全适合特定任务，因此需要微调，即在特定数据集上继续训练，以使模型更适应该任务。为了减小部署模型的大小和加快推理速度，模型还会经过量化，即将模型参数从高精度格式减少到较低精度。</p><p>如果你想继续深入学习大模型，那么有几个工具你不得不接着研究。</p><ul><li>PyTorch是一个流行的深度学习框架，常用于模型的训练和微调。</li><li>HuggingFace是一个开源社区，提供了大量预训练模型和微调工具，尤其是NLP任务。</li><li>LangChain则擅长于利用大语言模型的推理功能，开发新的工具或应用，完成特定的任务。</li></ul><p>这些工具和库在AI模型的全生命周期中起到关键作用，使研究者和开发者更容易开发和部署高效的AI系统。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li><p>现在请你再回答一下，什么时候应该使用OpenAI的API？什么时候应该使用开源模型？或者自己开发&#x2F;微调的模型？</p><p>提示：的确，文中没有给出这个问题的答案。因为这个问题并没有标准答案。</p></li><li><p>请你使用HuggingFace的Transformers库，下载新的模型进行推理，比较它们的性能。</p></li><li><p>请你在LangChain中，使用HuggingFaceHub和HuggingFace Pipeline这两种接口，调用当前最流行的大语言模型。</p><p>提示：HuggingFace Model 页面，有模型下载量的当月排序，当月下载最多的模型就是最流行的模型。</p></li></ol><p>期待在留言区看到你的分享，我们一起交流探讨，共创一个良好的学习氛围。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>Llama2，开源的可商用类ChatGPT模型， <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Facebook链接</a>、 <a href="https://github.com/facebookresearch/llama">GitHub链接</a></li><li>HuggingFace <a href="https://huggingface.co/docs/transformers/index">Transformer</a> 文档</li><li>PyTorch 官方 <a href="https://pytorch.org/tutorials/">教程</a>、 <a href="https://pytorch.org/docs/stable/index.html">文档</a></li><li><a href="https://github.com/PanQiWei/AutoGPTQ">AutoGPTQ</a> 基于GPTQ算法的大模型量化工具包</li><li><a href="https://github.com/ggerganov/llama.cpp">Llama CPP</a> 支持 <a href="https://github.com/ggerganov/ggml">GGML</a>，目标是在MacBook（或类似的非GPU的普通家用硬件环境）上使用4位整数量化运行Llama模型</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;06｜调用模型：使用OpenAI-API还是微调开源Llama2-ChatGLM？&quot;&gt;&lt;a href=&quot;#06｜调用模型：使用OpenAI-API还是微调开源Llama2-ChatGLM？&quot; class=&quot;headerlink&quot; title=&quot;06｜调用模型：使用</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>07｜输出解析：用OutputParser生成鲜花推荐列表</title>
    <link href="https://zhuansun.github.io/geekbang/posts/2050531098.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/2050531098.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.435Z</updated>
    
    <content type="html"><![CDATA[<h1 id="07｜输出解析：用OutputParser生成鲜花推荐列表"><a href="#07｜输出解析：用OutputParser生成鲜花推荐列表" class="headerlink" title="07｜输出解析：用OutputParser生成鲜花推荐列表"></a>07｜输出解析：用OutputParser生成鲜花推荐列表</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>首先请你回忆一下 <a href="https://time.geekbang.org/column/article/700699">第4课</a> 中我们学了什么: 为一些花和价格生成吸引人的描述，并将这些描述和原因存储到一个CSV文件中。为了实现这个目标，程序调用了OpenAI模型，并利用了结构化输出解析器，以及一些数据处理和存储的工具。</p><p>今天我要带着你深入研究一下LangChain中的输出解析器，并用一个新的解析器——Pydantic 解析器来重构第4课中的程序。这节课也是模型I&#x2F;O框架的最后一讲。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/6215fdd31373523a46bb02f86283522d.jpg"></p><p>下面先来看看LangChain中的输出解析器究竟是什么，有哪些种类。</p><h2 id="LangChain-中的输出解析器"><a href="#LangChain-中的输出解析器" class="headerlink" title="LangChain 中的输出解析器"></a>LangChain 中的输出解析器</h2><p>语言模型输出的是文本，这是给人类阅读的。但很多时候，你可能想要获得的是程序能够处理的结构化信息。这就是输出解析器发挥作用的地方。</p><p>输出解析器是 <strong>一种专用于处理和构建语言模型响应的类</strong>。一个基本的输出解析器类通常需要实现两个核心方法。</p><ul><li>get_format_instructions：这个方法需要返回一个字符串，用于指导如何格式化语言模型的输出，告诉它应该如何组织并构建它的回答。</li><li>parse：这个方法接收一个字符串（也就是语言模型的输出）并将其解析为特定的数据结构或格式。这一步通常用于确保模型的输出符合我们的预期，并且能够以我们需要的形式进行后续处理。</li></ul><p>还有一个可选的方法。</p><ul><li>parse_with_prompt：这个方法接收一个字符串（也就是语言模型的输出）和一个提示（用于生成这个输出的提示），并将其解析为特定的数据结构。这样，你可以根据原始提示来修正或重新解析模型的输出，确保输出的信息更加准确和贴合要求。</li></ul><p>下面是一个基于上述描述的简单伪代码示例：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">class OutputParser:    def __init__(self):        pass    def get_format_instructions(self):        # 返回一个字符串，指导如何格式化模型的输出        pass    def parse(self, model_output):        # 解析模型的输出，转换为某种数据结构或格式        pass    def parse_with_prompt(self, model_output, prompt):        # 基于原始提示解析模型的输出，转换为某种数据结构或格式        pass<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在LangChain中，通过实现get_format_instructions、parse 和 parse_with_prompt 这些方法，针对不同的使用场景和目标，设计了各种输出解析器。让我们来逐一认识一下。</p><ol><li>列表解析器（List Parser）：这个解析器用于处理模型生成的输出，当需要模型的输出是一个列表的时候使用。例如，如果你询问模型“列出所有鲜花的库存”，模型的回答应该是一个列表。</li><li>日期时间解析器（Datetime Parser）：这个解析器用于处理日期和时间相关的输出，确保模型的输出是正确的日期或时间格式。</li><li>枚举解析器（Enum Parser）：这个解析器用于处理预定义的一组值，当模型的输出应该是这组预定义值之一时使用。例如，如果你定义了一个问题的答案只能是“是”或“否”，那么枚举解析器可以确保模型的回答是这两个选项之一。</li><li>结构化输出解析器（Structured Output Parser）：这个解析器用于处理复杂的、结构化的输出。如果你的应用需要模型生成具有特定结构的复杂回答（例如一份报告、一篇文章等），那么可以使用结构化输出解析器来实现。</li><li>Pydantic（JSON）解析器：这个解析器用于处理模型的输出，当模型的输出应该是一个符合特定格式的JSON对象时使用。它使用Pydantic库，这是一个数据验证库，可以用于构建复杂的数据模型，并确保模型的输出符合预期的数据模型。</li><li>自动修复解析器（Auto-Fixing Parser）：这个解析器可以自动修复某些常见的模型输出错误。例如，如果模型的输出应该是一段文本，但是模型返回了一段包含语法或拼写错误的文本，自动修复解析器可以自动纠正这些错误。</li><li>重试解析器（RetryWithErrorOutputParser）：这个解析器用于在模型的初次输出不符合预期时，尝试修复或重新生成新的输出。例如，如果模型的输出应该是一个日期，但是模型返回了一个字符串，那么重试解析器可以重新提示模型生成正确的日期格式。</li></ol><p>上面的各种解析器中，前三种很容易理解，而结构化输出解析器你已经用过了。所以接下来我们重点讲一讲Pydantic（JSON）解析器、自动修复解析器和重试解析器。</p><h2 id="Pydantic（JSON）解析器实战"><a href="#Pydantic（JSON）解析器实战" class="headerlink" title="Pydantic（JSON）解析器实战"></a>Pydantic（JSON）解析器实战</h2><p>Pydantic (JSON) 解析器应该是最常用也是最重要的解析器，我带着你用它来重构鲜花文案生成程序。</p><blockquote><p>Pydantic 是一个 Python 数据验证和设置管理库，主要基于 Python 类型提示。尽管它不是专为 JSON 设计的，但由于 JSON 是现代 Web 应用和 API 交互中的常见数据格式，Pydantic 在处理和验证 JSON 数据时特别有用。</p></blockquote><h3 id="第一步：创建模型实例"><a href="#第一步：创建模型实例" class="headerlink" title="第一步：创建模型实例"></a>第一步：创建模型实例</h3><p>先通过环境变量设置OpenAI API密钥，然后使用LangChain库创建了一个OpenAI的模型实例。这里我们仍然选择了text-davinci-003作为大语言模型。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># ------Part 1# 设置OpenAI API密钥import osos.environ["OPENAI_API_KEY"] = '你的OpenAI API Key'# 创建模型实例from langchain import OpenAImodel = OpenAI(model_name='text-davinci-003')<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="第二步：定义输出数据的格式"><a href="#第二步：定义输出数据的格式" class="headerlink" title="第二步：定义输出数据的格式"></a>第二步：定义输出数据的格式</h3><p>先创建了一个空的DataFrame，用于存储从模型生成的描述。接下来，通过一个名为FlowerDescription的Pydantic BaseModel类，定义了期望的数据格式（也就是数据的结构）。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># ------Part 2# 创建一个空的DataFrame用于存储结果import pandas as pddf = pd.DataFrame(columns=["flower_type", "price", "description", "reason"])# 数据准备flowers = ["玫瑰", "百合", "康乃馨"]prices = ["50", "30", "20"]# 定义我们想要接收的数据格式from pydantic import BaseModel, Fieldclass FlowerDescription(BaseModel):    flower_type: str = Field(description="鲜花的种类")    price: int = Field(description="鲜花的价格")    description: str = Field(description="鲜花的描述文案")    reason: str = Field(description="为什么要这样写这个文案")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这里我们用到了负责数据格式验证的Pydantic库来创建带有类型注解的类FlowerDescription，它可以自动验证输入数据，确保输入数据符合你指定的类型和其他验证条件。</p><p>Pydantic有这样几个特点。</p><ol><li>数据验证：当你向Pydantic类赋值时，它会自动进行数据验证。例如，如果你创建了一个字段需要是整数，但试图向它赋予一个字符串，Pydantic会引发异常。</li><li>数据转换：Pydantic不仅进行数据验证，还可以进行数据转换。例如，如果你有一个需要整数的字段，但你提供了一个可以转换为整数的字符串，如 <code>&quot;42&quot;</code>，Pydantic会自动将这个字符串转换为整数42。</li><li>易于使用：创建一个Pydantic类就像定义一个普通的Python类一样简单。只需要使用Python的类型注解功能，即可在类定义中指定每个字段的类型。</li><li>JSON支持：Pydantic类可以很容易地从JSON数据创建，并可以将类的数据转换为JSON格式。</li></ol><p>下面，我们基于这个Pydantic数据格式类来创建LangChain的输出解析器。</p><h3 id="第三步：创建输出解析器"><a href="#第三步：创建输出解析器" class="headerlink" title="第三步：创建输出解析器"></a>第三步：创建输出解析器</h3><p>在这一步中，我们创建输出解析器并获取输出格式指示。先使用LangChain库中的PydanticOutputParser创建了输出解析器，该解析器将用于解析模型的输出，以确保其符合FlowerDescription的格式。然后，使用解析器的get_format_instructions方法获取了输出格式的指示。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># ------Part 3# 创建输出解析器from langchain.output_parsers import PydanticOutputParseroutput_parser = PydanticOutputParser(pydantic_object=FlowerDescription)# 获取输出格式指示format_instructions = output_parser.get_format_instructions()# 打印提示print("输出格式：",format_instructions)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>程序输出如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">输出格式： The output should be formatted as a JSON instance that conforms to the JSON schema below.As an example, for the schema &#123;"properties": &#123;"foo": &#123;"title": "Foo", "description": "a list of strings", "type": "array", "items": &#123;"type": "string"&#125;&#125;&#125;, "required": ["foo"]&#125;&#125;the object &#123;"foo": ["bar", "baz"]&#125; is a well-formatted instance of the schema. The object &#123;"properties": &#123;"foo": ["bar", "baz"]&#125;&#125; is not well-formatted.Here is the output schema:&#123;"properties": &#123;"flower_type": &#123;"title": "Flower Type", "description": "\u9c9c\u82b1\u7684\u79cd\u7c7b", "type": "string"&#125;, "price": &#123;"title": "Price", "description": "\u9c9c\u82b1\u7684\u4ef7\u683c", "type": "integer"&#125;, "description": &#123;"title": "Description", "description": "\u9c9c\u82b1\u7684\u63cf\u8ff0\u6587\u6848", "type": "string"&#125;, "reason": &#123;"title": "Reason", "description": "\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u5199\u8fd9\u4e2a\u6587\u6848", "type": "string"&#125;&#125;, "required": ["flower_type", "price", "description", "reason"]&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面这个输出，这部分是通过output_parser.get_format_instructions()方法生成的，这是Pydantic (JSON) 解析器的核心价值，值得你好好研究研究。同时它也算得上是一个很清晰的提示模板，能够为模型提供良好的指导，描述了模型输出应该符合的格式。（其中description中的中文被转成了UTF-8编码。）</p><p>它指示模型输出JSON Schema的形式，定义了一个有效的输出应该包含哪些字段，以及这些字段的数据类型。例如，它指定了 <code>&quot;flower_type&quot;</code> 字段应该是字符串类型， <code>&quot;price&quot;</code> 字段应该是整数类型。这个指示中还提供了一个例子，说明了什么是一个格式良好的输出。</p><p>下面，我们会把这个内容也传输到模型的提示中， <strong>让输入模型的提示和输出解析器的要求相互吻合，前后就呼应得上</strong>。</p><h3 id="第四步：创建提示模板"><a href="#第四步：创建提示模板" class="headerlink" title="第四步：创建提示模板"></a>第四步：创建提示模板</h3><p>我们定义了一个提示模板，该模板将用于为模型生成输入提示。模板中包含了你需要模型填充的变量（如价格和花的种类），以及之前获取的输出格式指示。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># ------Part 4# 创建提示模板from langchain import PromptTemplateprompt_template = """您是一位专业的鲜花店文案撰写员。对于售价为 &#123;price&#125; 元的 &#123;flower&#125; ，您能提供一个吸引人的简短中文描述吗？&#123;format_instructions&#125;"""# 根据模板创建提示，同时在提示中加入输出解析器的说明prompt = PromptTemplate.from_template(prompt_template,       partial_variables=&#123;"format_instructions": format_instructions&#125;)# 打印提示print("提示：", prompt)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">提示：input_variables=['flower', 'price']output_parser=Nonepartial_variables=&#123;'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema &#123;"properties": &#123;"foo": &#123;"title": "Foo", "description": "a list of strings", "type": "array", "items": &#123;"type": "string"&#125;&#125;&#125;,"required": ["foo"]&#125;&#125;\nthe object &#123;"foo": ["bar", "baz"]&#125; is a well-formatted instance of the schema.The object &#123;"properties": &#123;"foo": ["bar", "baz"]&#125;&#125; is not well-formatted.\n\nHere is the output schema:\n```\n&#123;"properties": &#123;"flower_type": &#123;"title": "Flower Type", "description": "\\u9c9c\\u82b1\\u7684\\u79cd\\u7c7b", "type": "string"&#125;,"price": &#123;"title": "Price", "description": "\\u9c9c\\u82b1\\u7684\\u4ef7\\u683c", "type": "integer"&#125;,"description": &#123;"title": "Description", "description": "\\u9c9c\\u82b1\\u7684\\u63cf\\u8ff0\\u6587\\u6848", "type": "string"&#125;,"reason": &#123;"title": "Reason", "description": "\\u4e3a\\u4ec0\\u4e48\\u8981\\u8fd9\\u6837\\u5199\\u8fd9\\u4e2a\\u6587\\u6848", "type": "string"&#125;&#125;,"required": ["flower_type", "price", "description", "reason"]&#125;\n```'&#125;template='您是一位专业的鲜花店文案撰写员。\n对于售价为 &#123;price&#125; 元的 &#123;flower&#125; ，您能提供一个吸引人的简短中文描述吗？\n&#123;format_instructions&#125;'template_format='f-string'validate_template=True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这就是包含了format_instructions信息的提示模板。</p><ol><li><code>input_variables=[&#39;flower&#39;, &#39;price&#39;]</code>：这是一个包含你想要在模板中使用的输入变量的列表。我们在模板中使用了 <code>&#39;flower&#39;</code> 和 <code>&#39;price&#39;</code> 两个变量，后面我们会用具体的值（如玫瑰、20元）来替换这两个变量。</li><li><code>output_parser=None</code>：这是你可以选择在模板中使用的一个输出解析器。在此例中，我们并没有选择在模板中使用输出解析器，而是在模型外部进行输出解析，所以这里是 <code>None</code>。</li><li><code>partial_variables</code>：包含了你想要在模板中使用，但在生成模板时无法立即提供的变量。在这里，我们通过 <code>&#39;format_instructions&#39;</code> 传入输出格式的详细说明。</li><li><code>template</code>：这是模板字符串本身。它包含了你想要模型生成的文本的结构。在此例中，模板字符串是你询问鲜花描述的问题，以及关于输出格式的说明。</li><li><code>template_format=&#39;f-string&#39;</code>：这是一个表示模板字符串格式的选项。此处是f-string格式。</li><li><code>validate_template=True</code>：表示是否在创建模板时检查模板的有效性。这里选择了在创建模板时进行检查，以确保模板是有效的。</li></ol><p>总的来说，这个提示模板是一个用于生成模型输入的工具。你可以在模板中定义需要的输入变量，以及模板字符串的格式和结构，然后使用这个模板来为每种鲜花生成一个描述。</p><p>后面，我们还要把实际的信息，循环传入提示模板，生成一个个的具体提示。下面让我们继续。</p><h3 id="第五步：生成提示，传入模型并解析输出"><a href="#第五步：生成提示，传入模型并解析输出" class="headerlink" title="第五步：生成提示，传入模型并解析输出"></a>第五步：生成提示，传入模型并解析输出</h3><p>这部分是程序的主体，我们循环来处理所有的花和它们的价格。对于每种花，都根据提示模板创建了输入，然后获取模型的输出。然后使用之前创建的解析器来解析这个输出，并将解析后的输出添加到DataFrame中。最后，你打印出了所有的结果，并且可以选择将其保存到CSV文件中。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># ------Part 5for flower, price in zip(flowers, prices):    # 根据提示准备模型的输入    input = prompt.format(flower=flower, price=price)    # 打印提示    print("提示：", input)    # 获取模型的输出    output = model(input)    # 解析模型的输出    parsed_output = output_parser.parse(output)    parsed_output_dict = parsed_output.dict()  # 将Pydantic格式转换为字典    # 将解析后的输出添加到DataFrame中    df.loc[len(df)] = parsed_output.dict()# 打印字典print("输出的数据：", df.to_dict(orient='records'))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这一步中，你使用你的模型和输入提示（由鲜花种类和价格组成）生成了一个具体鲜花的文案需求（同时带有格式描述），然后传递给大模型，也就是说，提示模板中的 flower 和 price，此时都被具体的花取代了，而且模板中的 {format_instructions}，也被替换成了 JSON Schema 中指明的格式信息。</p><p>具体来说，输出的一个提示是这样的：</p><blockquote><p><strong>提示</strong>： 您是一位专业的鲜花店文案撰写员。</p><p>对于售价为 20 元的 康乃馨 ，您能提供一个吸引人的简短中文描述吗？</p><p>The output should be formatted as a JSON instance that conforms to the JSON schema below.</p><p>As an example, for the schema {“properties”: {“foo”: {“title”: “Foo”, “description”: “a list of strings”, “type”: “array”, “items”: {“type”: “string”}}}, “required”: [“foo”]}}</p><p>the object {“foo”: [“bar”, “baz”]} is a well-formatted instance of the schema. The object {“properties”: {“foo”: [“bar”, “baz”]}} is not well-formatted.</p><p>Here is the output schema:</p><pre class="line-numbers language-none"><code class="language-none">&#123;&quot;properties&quot;: &#123;&quot;flower_type&quot;: &#123;&quot;title&quot;: &quot;Flower Type&quot;, &quot;description&quot;: &quot;\u9c9c\u82b1\u7684\u79cd\u7c7b&quot;, &quot;type&quot;: &quot;string&quot;&#125;, &quot;price&quot;: &#123;&quot;title&quot;: &quot;Price&quot;, &quot;description&quot;: &quot;\u9c9c\u82b1\u7684\u4ef7\u683c&quot;, &quot;type&quot;: &quot;integer&quot;&#125;, &quot;description&quot;: &#123;&quot;title&quot;: &quot;Description&quot;, &quot;description&quot;: &quot;\u9c9c\u82b1\u7684\u63cf\u8ff0\u6587\u6848&quot;, &quot;type&quot;: &quot;string&quot;&#125;, &quot;reason&quot;: &#123;&quot;title&quot;: &quot;Reason&quot;, &quot;description&quot;: &quot;\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u5199\u8fd9\u4e2a\u6587\u6848&quot;, &quot;type&quot;: &quot;string&quot;&#125;&#125;, &quot;required&quot;: [&quot;flower_type&quot;, &quot;price&quot;, &quot;description&quot;, &quot;reason&quot;]&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></blockquote><p>下面，程序解析模型的输出。在这一步中，你使用你之前定义的输出解析器（output_parser）将模型的输出解析成了一个FlowerDescription的实例。FlowerDescription是你之前定义的一个Pydantic类，它包含了鲜花的类型、价格、描述以及描述的理由。</p><p>然后，将解析后的输出添加到DataFrame中。在这一步中，你将解析后的输出（即FlowerDescription实例）转换为一个字典，并将这个字典添加到你的DataFrame中。这个DataFrame是你用来存储所有鲜花描述的。</p><p>模型的最后输出如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">输出的数据：[&#123;'flower_type': 'Rose', 'price': 50, 'description': '玫瑰是最浪漫的花，它具有柔和的粉红色，有着浓浓的爱意，价格实惠，50元就可以拥有一束玫瑰。', 'reason': '玫瑰代表着爱情，是最浪漫的礼物，以实惠的价格，可以让您尽情体验爱的浪漫。'&#125;,&#123;'flower_type': '百合', 'price': 30, 'description': '这支百合，柔美的花蕾，在你的手中摇曳，仿佛在与你深情的交谈', 'reason': '营造浪漫氛围'&#125;,&#123;'flower_type': 'Carnation', 'price': 20, 'description': '艳丽缤纷的康乃馨，带给你温馨、浪漫的气氛，是最佳的礼物选择！', 'reason': '康乃馨是一种颜色鲜艳、芬芳淡雅、具有浪漫寓意的鲜花，非常适合作为礼物，而且20元的价格比较实惠。'&#125;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>因此，Pydantic的优点就是容易解析，而解析之后的字典格式的列表在进行数据分析、处理和存储时非常方便。每个字典代表一条记录，它的键（ 即 <code>&quot;flower_type&quot;</code>、 <code>&quot;price&quot;</code>、 <code>&quot;description&quot;</code> 和 <code>&quot;reason&quot;</code>）是字段名称，对应的值是这个字段的内容。这样一来，每个字段都对应一列，每个字典就是一行，适合以DataFrame的形式来表示和处理。</p><h2 id="自动修复解析器（OutputFixingParser）实战"><a href="#自动修复解析器（OutputFixingParser）实战" class="headerlink" title="自动修复解析器（OutputFixingParser）实战"></a>自动修复解析器（OutputFixingParser）实战</h2><p>下面咱们来看看如何使用自动修复解析器。</p><p>首先，让我们来设计一个解析时出现的错误。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入所需要的库和模块from langchain.output_parsers import PydanticOutputParserfrom pydantic import BaseModel, Fieldfrom typing import List# 使用Pydantic创建一个数据格式，表示花class Flower(BaseModel):    name: str = Field(description="name of a flower")    colors: List[str] = Field(description="the colors of this flower")# 定义一个用于获取某种花的颜色列表的查询flower_query = "Generate the charaters for a random flower."# 定义一个格式不正确的输出misformatted = "&#123;'name': '康乃馨', 'colors': ['粉红色','白色','红色','紫色','黄色']&#125;"# 创建一个用于解析输出的Pydantic解析器，此处希望解析为Flower格式parser = PydanticOutputParser(pydantic_object=Flower)# 使用Pydantic解析器解析不正确的输出parser.parse(misformatted)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码如果运行，会出现错误。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">langchain.schema.output_parser.OutputParserException: Failed to parse Flower from completion &#123;'name': '康乃馨', 'colors': ['粉红色','白色']&#125;. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个错误消息来自Python的内建JSON解析器发现我们输入的JSON格式不正确。程序尝试用PydanticOutputParser来解析JSON字符串时，Python期望属性名称被双引号包围，但在给定的JSON字符串中是单引号。</p><p>当这个错误被触发后，程序进一步引发了一个自定义异常：OutputParserException，它提供了更多关于错误的上下文。这个自定义异常的消息表示在尝试解析flower对象时遇到了问题。</p><p>刚才说了，问题在于misformatted字符串的内容：</p><p><code>&quot;&#123;&#39;name&#39;: &#39;康乃馨&#39;, &#39;colors&#39;: [&#39;粉红色&#39;,&#39;白色&#39;,&#39;红色&#39;,&#39;紫色&#39;,&#39;黄色&#39;]&#125;&quot;</code></p><p>应该改为：</p><p><code>&#39;&#123;&quot;name&quot;: &quot;康乃馨&quot;, &quot;colors&quot;: [&quot;粉红色&quot;,&quot;白色&quot;,&quot;红色&quot;,&quot;紫色&quot;,&quot;黄色&quot;]&#125;&#39;</code></p><p>这样，你的JSON字符串就会使用正确的双引号格式，应该可以被正确地解析。</p><p>不过，这里我并不想这样解决问题，而是尝试使用OutputFixingParser来帮助咱们自动解决类似的格式错误。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 从langchain库导入所需的模块from langchain.chat_models import ChatOpenAIfrom langchain.output_parsers import OutputFixingParser# 设置OpenAI API密钥import osos.environ["OPENAI_API_KEY"] = '你的OpenAI API Key'# 使用OutputFixingParser创建一个新的解析器，该解析器能够纠正格式不正确的输出new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())# 使用新的解析器解析不正确的输出result = new_parser.parse(misformatted) # 错误被自动修正print(result) # 打印解析后的输出结果<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>用上面的新的new_parser来代替Parser进行解析，你会发现，JSON格式的错误问题被解决了，程序不再出错。</p><p>输出如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">name='Rose' colors=['red', 'pink', 'white']<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这里的秘密在于，在OutputFixingParser内部，调用了原有的PydanticOutputParser，如果成功，就返回；如果失败，它会将格式错误的输出以及格式化的指令传递给大模型，并要求LLM进行相关的修复。</p><p>神奇吧，大模型不仅给我们提供知识，还随时帮助分析并解决程序出错的信息。</p><h2 id="重试解析器（RetryWithErrorOutputParser）实战"><a href="#重试解析器（RetryWithErrorOutputParser）实战" class="headerlink" title="重试解析器（RetryWithErrorOutputParser）实战"></a>重试解析器（RetryWithErrorOutputParser）实战</h2><p>OutputFixingParser不错，但它只能做简单的格式修复。如果出错的不只是格式，比如，输出根本不完整，有缺失内容，那么仅仅根据输出和格式本身，是无法修复它的。</p><p>此时，通过实现输出解析器中parse_with_prompt方法，LangChain提供的重试解析器可以帮助我们利用大模型的推理能力根据原始提示找回相关信息。</p><p>我们通过分析一个重试解析器的用例来理解上面的这段话。</p><p>首先还是设计一个解析过程中的错误。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 定义一个模板字符串，这个模板将用于生成提问template = """Based on the user question, provide an Action and Action Input for what step should be taken.&#123;format_instructions&#125;Question: &#123;query&#125;Response:"""# 定义一个Pydantic数据格式，它描述了一个"行动"类及其属性from pydantic import BaseModel, Fieldclass Action(BaseModel):    action: str = Field(description="action to take")    action_input: str = Field(description="input to the action")# 使用Pydantic格式Action来初始化一个输出解析器from langchain.output_parsers import PydanticOutputParserparser = PydanticOutputParser(pydantic_object=Action)# 定义一个提示模板，它将用于向模型提问from langchain.prompts import PromptTemplateprompt = PromptTemplate(    template="Answer the user query.\n&#123;format_instructions&#125;\n&#123;query&#125;\n",    input_variables=["query"],    partial_variables=&#123;"format_instructions": parser.get_format_instructions()&#125;,)prompt_value = prompt.format_prompt(query="What are the colors of Orchid?")# 定义一个错误格式的字符串bad_response = '&#123;"action": "search"&#125;'parser.parse(bad_response) # 如果直接解析，它会引发一个错误<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>由于bad_response只提供了action字段，而没有提供action_input字段，这与Action数据格式的预期不符，所以解析会失败。</p><p>我们首先尝试用OutputFixingParser来解决这个错误。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.output_parsers import OutputFixingParserfrom langchain.chat_models import ChatOpenAIfix_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())parse_result = fix_parser.parse(bad_response)print('OutputFixingParser的parse结果:',parse_result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>OutputFixingParser的parse结果： <code>action=&#39;search&#39; action_input=&#39;query&#39;</code></p><p>我们来看看这个尝试解决了什么问题，没解决什么问题。</p><p><strong>解决的问题有：</strong></p><ul><li>不完整的数据：原始的bad_response只提供了action字段而没有action_input字段。OutputFixingParser已经填补了这个缺失，为action_input字段提供了值 <code>&#39;query&#39;</code>。</li></ul><p><strong>没解决的问题有：</strong></p><ul><li>具体性：尽管OutputFixingParser为action_input字段提供了默认值 <code>&#39;query&#39;</code>，但这并不具有描述性。真正的查询是 “Orchid（兰花）的颜色是什么？”。所以，这个修复只是提供了一个通用的值，并没有真正地回答用户的问题。</li><li>可能的误导： <code>&#39;query&#39;</code> 可能被误解为一个指示，要求进一步查询某些内容，而不是作为实际的查询输入。</li></ul><p>当然，还有更鲁棒的选择，我们最后尝试一下RetryWithErrorOutputParser这个解析器。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 初始化RetryWithErrorOutputParser，它会尝试再次提问来得到一个正确的输出from langchain.output_parsers import RetryWithErrorOutputParserfrom langchain.llms import OpenAIretry_parser = RetryWithErrorOutputParser.from_llm(    parser=parser, llm=OpenAI(temperature=0))parse_result = retry_parser.parse_with_prompt(bad_response, prompt_value)print('RetryWithErrorOutputParser的parse结果:',parse_result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个解析器没有让我们失望，成功地还原了格式，甚至也根据传入的原始提示，还原了action_input字段的内容。</p><p>RetryWithErrorOutputParser的parse结果： <code>action=&#39;search&#39; action_input=&#39;colors of Orchid&#39;</code></p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>结构化解析器和Pydantic解析器都旨在从大型语言模型中获取格式化的输出。结构化解析器更适合简单的文本响应，而Pydantic解析器则提供了对复杂数据结构和类型的支持。选择哪种解析器取决于应用的具体需求和输出的复杂性。</p><p>自动修复解析器主要适用于纠正小的格式错误，它更加“被动”，仅在原始输出出现问题时进行修复。重试解析器则可以处理更复杂的问题，包括格式错误和内容缺失。它通过重新与模型交互，使得输出更加完整和符合预期。</p><p>在选择哪种解析器时，需要考虑具体的应用场景。如果仅面临格式问题，自动修复解析器可能足够；但如果输出的完整性和准确性至关重要，那么重试解析器可能是更好的选择。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>到目前为止，我们已经使用了哪些LangChain输出解析器？请你说一说它们的用法和异同。同时也请你尝试使用其他类型的输出解析器，并把代码与大家分享。</li><li>为什么大模型能够返回JSON格式的数据，输出解析器用了什么魔法让大模型做到了这一点？</li><li>自动修复解析器的“修复”功能具体来说是怎样实现的？请做debug，研究一下LangChain在调用大模型之前如何设计“提示”。</li><li>重试解析器的原理是什么？它主要实现了解析器类的哪个可选方法？</li></ol><p>题目较多，可以选择性思考，期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>工具： <a href="https://docs.pydantic.dev/latest/">Pydantic</a> 是一个Python库，用于数据验证，可以确保数据符合特定的格式</li><li>文档：LangChain中的各种 <a href="https://python.langchain.com/docs/modules/model_io/output_parsers/">Output Parsers</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;07｜输出解析：用OutputParser生成鲜花推荐列表&quot;&gt;&lt;a href=&quot;#07｜输出解析：用OutputParser生成鲜花推荐列表&quot; class=&quot;headerlink&quot; title=&quot;07｜输出解析：用OutputParser生成鲜花推荐列表&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>08｜链（上）：写一篇完美鲜花推文？用SequencialChain链接不同的组件</title>
    <link href="https://zhuansun.github.io/geekbang/posts/2782525328.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/2782525328.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.438Z</updated>
    
    <content type="html"><![CDATA[<h1 id="08｜链（上）：写一篇完美鲜花推文？用SequencialChain链接不同的组件"><a href="#08｜链（上）：写一篇完美鲜花推文？用SequencialChain链接不同的组件" class="headerlink" title="08｜链（上）：写一篇完美鲜花推文？用SequencialChain链接不同的组件"></a>08｜链（上）：写一篇完美鲜花推文？用SequencialChain链接不同的组件</h1><p>你好，我是黄佳。欢迎来到LangChain实战课！</p><p>到这节课，我们已经学到了不少LangChain的应用，也体会到了LangChain功能的强大。但也许你心里开始出现了一个疑问：LangChain，其中的 <strong>Chain</strong> 肯定是关键组件，为什么我们还没有讲到呢？</p><p>这的确是个好问题。对于简单的应用程序来说，直接调用LLM就已经足够了。因此，在前几节课的示例中，我们主要通过LangChain中提供的提示模板、模型接口以及输出解析器就实现了想要的功能。</p><h2 id="什么是-Chain"><a href="#什么是-Chain" class="headerlink" title="什么是 Chain"></a>什么是 Chain</h2><p>但是，如果你想开发更复杂的应用程序，那么就需要通过 “Chain” 来链接LangChain的各个组件和功能——模型之间彼此链接，或模型与其他组件链接。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e26993dd3957bfd2947424abb9de7cde.png"></p><p>这种将多个组件相互链接，组合成一个链的想法简单但很强大。它简化了复杂应用程序的实现，并使之更加模块化，能够创建出单一的、连贯的应用程序，从而使调试、维护和改进应用程序变得容易。</p><p><strong>说到链的实现和使用，也简单。</strong></p><ul><li>首先LangChain通过设计好的接口，实现一个具体的链的功能。例如，LLM链（LLMChain）能够接受用户输入，使用 PromptTemplate 对其进行格式化，然后将格式化的响应传递给 LLM。这就相当于把整个Model I&#x2F;O的流程封装到链里面。</li><li>实现了链的具体功能之后，我们可以通过将多个链组合在一起，或者将链与其他组件组合来构建更复杂的链。</li></ul><p>所以你看，链在内部把一系列的功能进行封装，而链的外部则又可以组合串联。 <strong>链其实可以被视为LangChain中的一种基本功能单元。</strong></p><p>LangChain中提供了很多种类型的预置链，目的是使各种各样的任务实现起来更加方便、规范。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/8b580b2b8e0fc8515d271165a46101c3.jpg"></p><p>我们先使用一下最基础也是最常见的LLMChain。</p><h2 id="LLMChain：最简单的链"><a href="#LLMChain：最简单的链" class="headerlink" title="LLMChain：最简单的链"></a>LLMChain：最简单的链</h2><p>LLMChain围绕着语言模型推理功能又添加了一些功能，整合了PromptTemplate、语言模型（LLM或聊天模型）和 Output Parser，相当于把Model I&#x2F;O放在一个链中整体操作。它使用提示模板格式化输入，将格式化的字符串传递给 LLM，并返回 LLM 输出。</p><p>举例来说，如果我想让大模型告诉我某种花的花语，如果不使用链，代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">#----第一步 创建提示# 导入LangChain中的提示模板from langchain import PromptTemplate# 原始字符串模板template = "&#123;flower&#125;的花语是?"# 创建LangChain模板prompt_temp = PromptTemplate.from_template(template)# 根据模板创建提示prompt = prompt_temp.format(flower='玫瑰')# 打印提示的内容print(prompt)#----第二步 创建并调用模型# 导入LangChain中的OpenAI模型接口from langchain import OpenAI# 创建模型实例model = OpenAI(temperature=0)# 传入提示，调用模型，返回结果result = model(prompt)print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">玫瑰的花语是?爱情、浪漫、美丽、永恒、誓言、坚贞不渝。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>此时Model I&#x2F;O的实现分为两个部分，提示模板的构建和模型的调用独立处理。</p><p>如果使用链，代码结构则显得更简洁。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入所需的库from langchain import PromptTemplate, OpenAI, LLMChain# 原始字符串模板template = "&#123;flower&#125;的花语是?"# 创建模型实例llm = OpenAI(temperature=0)# 创建LLMChainllm_chain = LLMChain(    llm=llm,    prompt=PromptTemplate.from_template(template))# 调用LLMChain，返回结果result = llm_chain("玫瑰")print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'flower': '玫瑰', 'text': '\n\n爱情、浪漫、美丽、永恒、誓言、坚贞不渝。'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在这里，我们就把提示模板的构建和模型的调用封装在一起了。</p><h2 id="链的调用方式"><a href="#链的调用方式" class="headerlink" title="链的调用方式"></a>链的调用方式</h2><p>链有很多种调用方式。</p><h3 id="直接调用"><a href="#直接调用" class="headerlink" title="直接调用"></a>直接调用</h3><p>刚才我们是直接调用的链对象。当我们像函数一样调用一个对象时，它实际上会调用该对象内部实现的__call__方法。</p><p>如果你的提示模板中包含多个变量，在调用链的时候，可以使用字典一次性输入它们。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">prompt = PromptTemplate(    input_variables=["flower", "season"],    template="&#123;flower&#125;在&#123;season&#125;的花语是?",)llm_chain = LLMChain(llm=llm, prompt=prompt)print(llm_chain(&#123;    'flower': "玫瑰",    'season': "夏季" &#125;))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'flower': '玫瑰', 'season': '夏季', 'text': '\n\n玫瑰在夏季的花语是爱的誓言，热情，美丽，坚定的爱情。'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="通过run方法"><a href="#通过run方法" class="headerlink" title="通过run方法"></a>通过run方法</h3><p>通过run方法，也等价于直接调用_call_函数。</p><p>语句：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">llm_chain("玫瑰")<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>等价于：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">llm_chain.run("玫瑰")<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="通过predict方法"><a href="#通过predict方法" class="headerlink" title="通过predict方法"></a>通过predict方法</h3><p>predict方法类似于run，只是输入键被指定为关键字参数而不是 Python 字典。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">result = llm_chain.predict(flower="玫瑰")print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="通过apply方法"><a href="#通过apply方法" class="headerlink" title="通过apply方法"></a>通过apply方法</h3><p>apply方法允许我们针对输入列表运行链，一次处理多个输入。</p><p>示例如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># apply允许您针对输入列表运行链input_list = [    &#123;"flower": "玫瑰",'season': "夏季"&#125;,    &#123;"flower": "百合",'season': "春季"&#125;,    &#123;"flower": "郁金香",'season': "秋季"&#125;]result = llm_chain.apply(input_list)print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">'''[&#123;'text': '\n\n玫瑰在夏季的花语是“恋爱”、“热情”和“浪漫”。'&#125;,&#123;'text': '\n\n百合在春季的花语是“爱情”和“友谊”。'&#125;, &#123;'text': '\n\n郁金香在秋季的花语表达的是“热情”、“思念”、“爱恋”、“回忆”和“持久的爱”。'&#125;]'''<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="通过generate方法"><a href="#通过generate方法" class="headerlink" title="通过generate方法"></a>通过generate方法</h3><p>generate方法类似于apply，只不过它返回一个LLMResult对象，而不是字符串。LLMResult通常包含模型生成文本过程中的一些相关信息，例如令牌数量、模型名称等。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">result = llm_chain.generate(input_list)print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">generations=[[Generation(text='\n\n玫瑰在夏季的花语是“热情”、“爱情”和“幸福”。',generation_info=&#123;'finish_reason': 'stop', 'logprobs': None&#125;)],[Generation(text='\n\n春季的花语是爱情、幸福、美满、坚贞不渝。',generation_info=&#123;'finish_reason': 'stop', 'logprobs': None&#125;)],[Generation(text='\n\n秋季的花语是“思念”。银色的百合象征着“真爱”，而淡紫色的郁金香则象征着“思念”，因为它们在秋天里绽放的时候，犹如在思念着夏天的温暖。',generation_info=&#123;'finish_reason': 'stop', 'logprobs': None&#125;)]]llm_output=&#123;'token_usage': &#123;'completion_tokens': 243, 'total_tokens': 301, 'prompt_tokens': 58&#125;, 'model_name': 'text-davinci-003'&#125;run=[RunInfo(run_id=UUID('13058cca-881d-4b76-b0cf-0f9c831af6c4')),RunInfo(run_id=UUID('7f38e33e-bab5-4d03-b77c-f50cd195affb')),RunInfo(run_id=UUID('7a1e45fd-77ee-4133-aab0-431147186db8'))]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Sequential-Chain：顺序链"><a href="#Sequential-Chain：顺序链" class="headerlink" title="Sequential Chain：顺序链"></a>Sequential Chain：顺序链</h2><p>好，到这里，你已经掌握了最基本的LLMChain的用法。下面，我要带着你用Sequential Chain 把几个LLMChain串起来，形成一个顺序链。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/48f3f524ecf2d2yyeb11fd54yyf99f36.png"></p><p>这个示例中，我们的目标是这样的：</p><ul><li>第一步，我们假设大模型是一个植物学家，让他给出某种特定鲜花的知识和介绍。</li><li>第二步，我们假设大模型是一个鲜花评论者，让他参考上面植物学家的文字输出，对鲜花进行评论。</li><li>第三步，我们假设大模型是易速鲜花的社交媒体运营经理，让他参考上面植物学家和鲜花评论者的文字输出，来写一篇鲜花运营文案。</li></ul><p>下面我们就来一步步地实现这个示例。</p><p>首先，导入所有需要的库。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 设置OpenAI API密钥import osos.environ["OPENAI_API_KEY"] = '你的OpenAI API Key'from langchain.llms import OpenAIfrom langchain.chains import LLMChainfrom langchain.prompts import PromptTemplatefrom langchain.chains import SequentialChain<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后，添加第一个LLMChain，生成鲜花的知识性说明。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 这是第一个LLMChain，用于生成鲜花的介绍，输入为花的名称和种类llm = OpenAI(temperature=.7)template = """你是一个植物学家。给定花的名称和类型，你需要为这种花写一个200字左右的介绍。花名: &#123;name&#125;颜色: &#123;color&#125;植物学家: 这是关于上述花的介绍:"""prompt_template = PromptTemplate(input_variables=["name", "color"], template=template)introduction_chain = LLMChain(llm=llm, prompt=prompt_template, output_key="introduction")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接着，添加第二个LLMChain，根据鲜花的知识性说明生成评论。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 这是第二个LLMChain，用于根据鲜花的介绍写出鲜花的评论llm = OpenAI(temperature=.7)template = """你是一位鲜花评论家。给定一种花的介绍，你需要为这种花写一篇200字左右的评论。鲜花介绍:&#123;introduction&#125;花评人对上述花的评论:"""prompt_template = PromptTemplate(input_variables=["introduction"], template=template)review_chain = LLMChain(llm=llm, prompt=prompt_template, output_key="review")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接着，添加第三个LLMChain，根据鲜花的介绍和评论写出一篇自媒体的文案。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 这是第三个LLMChain，用于根据鲜花的介绍和评论写出一篇自媒体的文案template = """你是一家花店的社交媒体经理。给定一种花的介绍和评论，你需要为这种花写一篇社交媒体的帖子，300字左右。鲜花介绍:&#123;introduction&#125;花评人对上述花的评论:&#123;review&#125;社交媒体帖子:"""prompt_template = PromptTemplate(input_variables=["introduction", "review"], template=template)social_post_chain = LLMChain(llm=llm, prompt=prompt_template, output_key="social_post_text")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后，添加SequentialChain，把前面三个链串起来。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 这是总的链，我们按顺序运行这三个链overall_chain = SequentialChain(    chains=[introduction_chain, review_chain, social_post_chain],    input_variables=["name", "color"],    output_variables=["introduction","review","social_post_text"],    verbose=True)# 运行链，并打印结果result = overall_chain(&#123;"name":"玫瑰", "color": "黑色"&#125;)print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最终的输出如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Entering new  chain...> Finished chain.&#123;'name': '玫瑰', 'color': '黑色','introduction': '\n\n黑色玫瑰，这是一种对传统玫瑰花的独特颠覆，它的出现挑战了我们对玫瑰颜色的固有认知。它的花瓣如煤炭般黑亮，反射出独特的微光，而花蕊则是金黄色的，宛如夜空中的一颗星，强烈的颜色对比营造出一种前所未有的视觉效果。在植物学中，黑色玫瑰的出现无疑提供了一种新的研究方向，对于我们理解花朵色彩形成的机制有着重要的科学价值。','review': '\n\n黑色玫瑰，这不仅仅是一种花朵，更是一种完全颠覆传统的艺术表现形式。黑色的花瓣仿佛在诉说一种不可言喻的悲伤与神秘，而黄色的蕊瓣犹如漆黑夜空中的一抹亮色，给人带来无尽的想象。它将悲伤与欢乐，神秘与明亮完美地结合在一起，这是一种全新的视觉享受，也是一种对生活理解的深度表达。','social_post_text': '\n欢迎来到我们的自媒体平台，今天，我们要向您展示的是我们的全新产品——黑色玫瑰。这不仅仅是一种花，这是一种对传统观念的挑战，一种视觉艺术的革新，更是一种生活态度的象征。这种别样的玫瑰花，其黑色花瓣宛如漆黑夜空中闪烁的繁星，富有神秘的深度感，给人一种前所未有的视觉冲击力。这种黑色，它不是冷酷、不是绝望，而是充满着独特的魅力和力量。而位于黑色花瓣之中的金黄色花蕊，则犹如星星中的灵魂，默默闪烁，给人带来无尽的遐想，充满活力与生机。黑色玫瑰的存在，不仅挑战了我们对于玫瑰传统颜色的认知，它更是一种生动的生命象征，象征着那些坚韧、独特、勇敢面对生活的人们。黑色的花瓣中透露出一种坚韧的力量，而金黄的花蕊则是生活中的希望，二者的结合恰好象征了生活中的喜怒哀乐，体现了人生的百态。'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>至此，我们就通过两个LLM链和一个顺序链，生成了一篇完美的文案。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>LangChain为我们提供了好用的“链”，帮助我们把多个组件像链条一样连接起来。这个“链条”其实就是一系列组件的调用顺序，这个顺序里还可以包括其他的“链条”。</p><p>我们可以使用多种方法调用链，也可以根据开发时的需求选择各种不同的链。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/5fe2366c3e8294a61cb44d33b9d79638.png"></p><p>除去最常见的LLMChain和SequenceChain之外，LangChain中还自带大量其他类型的链，封装了各种各样的功能。你可以看一看这些链的实现细节，并尝试着使用它们。</p><p>下一节课，我们会继续介绍另外一种好用的链，RouterChain。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>在 <a href="https://time.geekbang.org/column/article/699451">第3课</a> 中，我们曾经用提示模板生成过一段鲜花的描述，代码如下：</li></ol><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">for flower, price in zip(flowers, prices):    # 根据提示准备模型的输入    input = prompt.format(flower_name=flower, price=price)    # 获取模型的输出    output = model(input)    # 解析模型的输出    parsed_output = output_parser.parse(output)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>请你使用LLMChain重构提示的format和获取模型输出部分，完成相同的功能。</p><p>提示：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">    llm_chain = LLMChain(        llm=model,        prompt=prompt)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol><li>上一道题目中，我要求你把提示的format和获取模型输出部分整合到LLMChain中，其实你还可以更进一步，把output_parser也整合到LLMChain中，让程序结构进一步简化，请你尝试一下。</li></ol><p>提示：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">    llm_chain = LLMChain(        llm=model,        prompt=prompt,        output_parser=output_parser)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol><li>选择一个LangChain中的链（我们没用到的类型），尝试使用它解决一个问题，并分享你的用例和代码。</li></ol><p>题目较多，可以选择性思考，期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>GitHub上各种各样的 <a href="https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/chains">链</a></li><li>代码， <a href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/llm.py">LLMChain</a> 的实现细节</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;08｜链（上）：写一篇完美鲜花推文？用SequencialChain链接不同的组件&quot;&gt;&lt;a href=&quot;#08｜链（上）：写一篇完美鲜花推文？用SequencialChain链接不同的组件&quot; class=&quot;headerlink&quot; title=&quot;08｜链（上）：写一</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>09｜链（下）：想学“育花”还是“插花”？用RouterChain确定客户意图</title>
    <link href="https://zhuansun.github.io/geekbang/posts/4058055232.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/4058055232.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.441Z</updated>
    
    <content type="html"><![CDATA[<h1 id="09｜链（下）：想学“育花”还是“插花”？用RouterChain确定客户意图"><a href="#09｜链（下）：想学“育花”还是“插花”？用RouterChain确定客户意图" class="headerlink" title="09｜链（下）：想学“育花”还是“插花”？用RouterChain确定客户意图"></a>09｜链（下）：想学“育花”还是“插花”？用RouterChain确定客户意图</h1><p>你好，我是黄佳。欢迎来到LangChain实战课！</p><p>上一节课中，我带着你学习了Chain的基本概念，还使用了LLMChain和SequencialChain，这一节课，我们再来看看其他类型的一些Chain的用法。</p><h2 id="任务设定"><a href="#任务设定" class="headerlink" title="任务设定"></a>任务设定</h2><p>首先，还是先看一下今天要完成一个什么样的任务。</p><p>这里假设咱们的鲜花运营智能客服ChatBot通常会接到两大类问题。</p><ol><li><strong>鲜花养护</strong>（保持花的健康、如何浇水、施肥等）</li><li><strong>鲜花装饰</strong>（如何搭配花、如何装饰场地等）</li></ol><p>你的需求是， <strong>如果接到的是第一类问题，你要给ChatBot A指示；如果接到第二类的问题，你要给ChatBot B指示</strong>。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d8491e696c03f49a331c94e31d20e559.jpg"></p><p>我们可以根据这两个场景来构建两个不同的目标链。遇到不同类型的问题，LangChain会通过RouterChain来自动引导大语言模型选择不同的模板。</p><p>当然我们的运营过程会遇到更多种类的问题，你只需要通过同样的方法扩充逻辑即可。</p><h2 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h2><p>RouterChain，也叫路由链，能动态选择用于给定输入的下一个链。我们会根据用户的问题内容，首先使用路由器链确定问题更适合哪个处理模板，然后将问题发送到该处理模板进行回答。如果问题不适合任何已定义的处理模板，它会被发送到默认链。</p><p>在这里，我们会用LLMRouterChain和MultiPromptChain（也是一种路由链）组合实现路由功能，该MultiPromptChain会调用LLMRouterChain选择与给定问题最相关的提示，然后使用该提示回答问题。</p><p><strong>具体步骤如下：</strong></p><ol><li>构建处理模板：为鲜花护理和鲜花装饰分别定义两个字符串模板。</li><li>提示信息：使用一个列表来组织和存储这两个处理模板的关键信息，如模板的键、描述和实际内容。</li><li>初始化语言模型：导入并实例化语言模型。</li><li>构建目标链：根据提示信息中的每个模板构建了对应的LLMChain，并存储在一个字典中。</li><li>构建LLM路由链：这是决策的核心部分。首先，它根据提示信息构建了一个路由模板，然后使用这个模板创建了一个LLMRouterChain。</li><li>构建默认链：如果输入不适合任何已定义的处理模板，这个默认链会被触发。</li><li>构建多提示链：使用MultiPromptChain将LLM路由链、目标链和默认链组合在一起，形成一个完整的决策系统。</li></ol><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>下面，就是用路由链自动选择处理模板的具体代码实现。</p><h3 id="构建提示信息的模板"><a href="#构建提示信息的模板" class="headerlink" title="构建提示信息的模板"></a>构建提示信息的模板</h3><p>首先，我们针对两种场景，构建两个提示信息的模板。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 构建两个场景的模板flower_care_template = """你是一个经验丰富的园丁，擅长解答关于养花育花的问题。                        下面是需要你来回答的问题:                        &#123;input&#125;"""flower_deco_template = """你是一位网红插花大师，擅长解答关于鲜花装饰的问题。                        下面是需要你来回答的问题:                        &#123;input&#125;"""# 构建提示信息prompt_infos = [    &#123;        "key": "flower_care",        "description": "适合回答关于鲜花护理的问题",        "template": flower_care_template,    &#125;,    &#123;        "key": "flower_decoration",        "description": "适合回答关于鲜花装饰的问题",        "template": flower_deco_template,    &#125;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="初始化语言模型"><a href="#初始化语言模型" class="headerlink" title="初始化语言模型"></a>初始化语言模型</h3><p>接下来，我们初始化语言模型。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 初始化语言模型from langchain.llms import OpenAIimport osos.environ["OPENAI_API_KEY"] = '你的OpenAI Key'llm = OpenAI()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="构建目标链"><a href="#构建目标链" class="headerlink" title="构建目标链"></a>构建目标链</h3><p>下面，我们循环prompt_infos这个列表，构建出两个目标链，分别负责处理不同的问题。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 构建目标链from langchain.chains.llm import LLMChainfrom langchain.prompts import PromptTemplatechain_map = &#123;&#125;for info in prompt_infos:    prompt = PromptTemplate(template=info['template'],                            input_variables=["input"])    print("目标提示:\n",prompt)    chain = LLMChain(llm=llm, prompt=prompt,verbose=True)    chain_map[info["key"]] = chain<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里，目标链提示是这样的：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">目标提示:input_variables=['input']output_parser=None partial_variables=&#123;&#125;template='你是一个经验丰富的园丁，擅长解答关于养花育花的问题。\n                        下面是需要你来回答的问题:\n&#123;input&#125;' template_format='f-string'validate_template=True目标提示:input_variables=['input']output_parser=None partial_variables=&#123;&#125;template='你是一位网红插花大师，擅长解答关于鲜花装饰的问题。\n                        下面是需要你来回答的问题:\n&#123;input&#125;' template_format='f-string'validate_template=True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于每个场景，我们创建一个 LLMChain（语言模型链）。每个链会根据其场景模板生成对应的提示，然后将这个提示送入语言模型获取答案。</p><h3 id="构建路由链"><a href="#构建路由链" class="headerlink" title="构建路由链"></a>构建路由链</h3><p>下面，我们构建路由链，负责查看用户输入的问题，确定问题的类型。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 构建路由链from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParserfrom langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE as RounterTemplatedestinations = [f"&#123;p['key']&#125;: &#123;p['description']&#125;" for p in prompt_infos]router_template = RounterTemplate.format(destinations="\n".join(destinations))print("路由模板:\n",router_template)router_prompt = PromptTemplate(    template=router_template,    input_variables=["input"],    output_parser=RouterOutputParser(),)print("路由提示:\n",router_prompt)router_chain = LLMRouterChain.from_llm(llm,                                       router_prompt,                                       verbose=True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-none"><code class="language-none">路由模板: Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.&lt;&lt; FORMATTING &gt;&gt;Return a markdown code snippet with a JSON object formatted to look like:&#96;&#96;&#96;json&#123;&#123;    &quot;destination&quot;: string \ name of the prompt to use or &quot;DEFAULT&quot;    &quot;next_inputs&quot;: string \ a potentially modified version of the original input&#125;&#125;&#96;&#96;&#96;REMEMBER: &quot;destination&quot; MUST be one of the candidate prompt names specified below OR it can be &quot;DEFAULT&quot; if the input is not well suited for any of the candidate prompts.REMEMBER: &quot;next_inputs&quot; can just be the original input if you don&#39;t think any modifications are needed.&lt;&lt; CANDIDATE PROMPTS &gt;&gt;flower_care: 适合回答关于鲜花护理的问题flower_decoration: 适合回答关于鲜花装饰的问题&lt;&lt; INPUT &gt;&gt;&#123;input&#125;&lt;&lt; OUTPUT &gt;&gt;路由提示:input_variables&#x3D;[&#39;input&#39;] output_parser&#x3D;RouterOutputParser(default_destination&#x3D;&#39;DEFAULT&#39;, next_inputs_type&#x3D;&lt;class &#39;str&#39;&gt;, next_inputs_inner_key&#x3D;&#39;input&#39;)partial_variables&#x3D;&#123;&#125;template&#x3D;&#39;Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n\n&lt;&lt; FORMATTING &gt;&gt;\nReturn a markdown code snippet with a JSON object formatted to look like:\n&#96;&#96;&#96;json\n&#123;&#123;\n &quot;destination&quot;: string \\ name of the prompt to use or &quot;DEFAULT&quot;\n    &quot;next_inputs&quot;: string \\ a potentially modified version of the original input\n&#125;&#125;\n&#96;&#96;&#96;\n\nREMEMBER: &quot;destination&quot; MUST be one of the candidate prompt names specified below OR it can be &quot;DEFAULT&quot; if the input is not well suited for any of the candidate prompts.\nREMEMBER: &quot;next_inputs&quot; can just be the original input if you don\&#39;t think any modifications are needed.\n\n&lt;&lt; CANDIDATE PROMPTS &gt;&gt;\nflower_care: 适合回答关于鲜花护理的问题\nflower_decoration: 适合回答关于鲜花装饰的问题\n\n&lt;&lt; INPUT &gt;&gt;\n&#123;input&#125;\n\n&lt;&lt; OUTPUT &gt;&gt;\n&#39;template_format&#x3D;&#39;f-string&#39;validate_template&#x3D;True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里我说一下路由器链是如何构造提示信息，来引导大模型查看用户输入的问题并确定问题的类型的。</p><p>先看路由模板部分，这段模板字符串是一个指导性的说明，目的是引导语言模型正确处理用户的输入，并将其定向到适当的模型提示。</p><p><strong>1. 路由模板的解释</strong></p><p>路由模板是路由功能得以实现的核心。我们来详细分解一下这个模板的每个部分。</p><p><strong>引言</strong></p><blockquote><p>Given a raw text input to a language model select the model prompt best suited for the input.</p></blockquote><p>这是一个简单的引导语句，告诉模型你将给它一个输入，它需要根据这个输入选择最适合的模型提示。</p><blockquote><p>You will be given the names of the available prompts and a description of what the prompt is best suited for.</p></blockquote><p>这里进一步提醒模型，它将获得各种模型提示的名称和描述。</p><blockquote><p>You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.</p></blockquote><p>这是一个可选的步骤，告诉模型它可以更改原始输入以获得更好的响应。</p><p><strong>格式说明(&lt;&lt; FORMATTING &gt;&gt;)</strong></p><p>指导模型如何格式化其输出，使其以特定的方式返回结果。</p><blockquote><p>Return a markdown code snippet with a JSON object formatted to look like:</p></blockquote><p>表示模型的输出应该是一个 Markdown 代码片段，其中包含一个特定格式的 JSON 对象。</p><p>下面的代码块显示了期望的 JSON 结构，其中 destination 是模型选择的提示名称（或“DEFAULT”），而 next_inputs 是可能被修订的原始输入。</p><p><strong>额外的说明和要求</strong></p><blockquote><p>REMEMBER: “destination” MUST be one of the candidate prompt names specified below OR it can be “DEFAULT”…</p></blockquote><p>这是一个重要的指导，提醒模型 “destination” 字段的值必须是下面列出的提示之一或是 “DEFAULT”。</p><blockquote><p>REMEMBER: “next_inputs” can just be the original input if you don’t think any modifications are needed.</p></blockquote><p>这再次强调，除非模型认为有必要，否则原始输入不需要修改。</p><p><strong>候选提示(&lt;&lt; CANDIDATE PROMPTS &gt;&gt;)</strong></p><p>列出了两个示例模型提示及其描述：</p><ul><li>“flower_care: 适合回答关于鲜花护理的问题”，适合处理与花卉护理相关的问题。</li><li>“flower_decoration: 适合回答关于鲜花装饰的问题”，适合处理与花卉装饰相关的问题。</li></ul><p><strong>输入&#x2F;输出部分</strong></p><blockquote><p>&lt;&lt; INPUT &gt;&gt;\n{input}\n\n&lt;&lt; OUTPUT &gt;&gt;\n：</p></blockquote><p>这部分为模型提供了一个格式化的框架，其中它将接收一个名为 {input} 的输入，并在此后的部分输出结果。</p><p>总的来说，这个模板的目的是让模型知道如何处理用户的输入，并根据提供的提示列表选择一个最佳的模型提示来回应。</p><p><strong>2. 路由提示的解释</strong></p><p>路由提示 (router_prompt）则根据路由模板，生成了具体传递给LLM的路由提示信息。</p><ul><li>其中input_variables 指定模板接收的输入变量名，这里只有 <code>&quot;input&quot;</code>。</li><li>output_parser 是一个用于解析模型输出的对象，它有一个默认的目的地和一个指向下一输入的键。</li><li>template 是实际的路由模板，用于给模型提供指示。这就是刚才详细解释的模板内容。</li><li>template_format 指定模板的格式，这里是 <code>&quot;f-string&quot;</code>。</li><li>validate_template 是一个布尔值，如果为 True，则会在使用模板前验证其有效性。</li></ul><p>简而言之，这个构造允许你将用户的原始输入送入路由器，然后路由器会决定将该输入发送到哪个具体的模型提示，或者是否需要对输入进行修订以获得最佳的响应。</p><h3 id="构建默认链"><a href="#构建默认链" class="headerlink" title="构建默认链"></a>构建默认链</h3><p>除了处理目标链和路由链之外，我们还需要准备一个默认链。如果路由链没有找到适合的链，那么，就以默认链进行处理。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 构建默认链from langchain.chains import ConversationChaindefault_chain = ConversationChain(llm=llm,                                  output_key="text",                                  verbose=True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="构建多提示链"><a href="#构建多提示链" class="headerlink" title="构建多提示链"></a>构建多提示链</h3><p>最后，我们使用MultiPromptChain类把前几个链整合在一起，实现路由功能。这个MultiPromptChain类是一个多路选择链，它使用一个LLM路由器链在多个提示之间进行选择。</p><p><strong>MultiPromptChain中有三个关键元素。</strong></p><ul><li>router_chain（类型RouterChain）：这是用于决定目标链和其输入的链。当给定某个输入时，这个router_chain决定哪一个destination_chain应该被选中，以及传给它的具体输入是什么。</li><li>destination_chains（类型Mapping[str, LLMChain]）：这是一个映射，将名称映射到可以将输入路由到的候选链。例如，你可能有多种处理文本输入的方法（或“链”），每种方法针对特定类型的问题。destination_chains可以是这样一个字典： <code>&#123;&#39;weather&#39;: weather_chain, &#39;news&#39;: news_chain&#125;</code>。在这里，weather_chain可能专门处理与天气相关的问题，而news_chain处理与新闻相关的问题。</li><li>default_chain（类型LLMChain）：当 router_chain 无法将输入映射到destination_chains中的任何一个链时，LLMChain 将使用此默认链。这是一个备选方案，确保即使路由器不能决定正确的链，也总有一个链可以处理输入。</li></ul><p><strong>它的工作流程如下：</strong></p><ol><li>输入首先传递给router_chain。</li><li>router_chain根据某些标准或逻辑决定应该使用哪一个destination_chain。</li><li>输入随后被路由到选定的destination_chain，该链进行处理并返回结果。</li><li>如果router_chain不能决定正确的destination_chain，则输入会被传递给default_chain。</li></ol><p>这样，MultiPromptChain就为我们提供了一个在多个处理链之间动态路由输入的机制，以得到最相关或最优的输出。</p><p>实现代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 构建多提示链from langchain.chains.router import MultiPromptChainchain = MultiPromptChain(    router_chain=router_chain,    destination_chains=chain_map,    default_chain=default_chain,    verbose=True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="运行路由链"><a href="#运行路由链" class="headerlink" title="运行路由链"></a>运行路由链</h2><p>好了，至此我们的链路已经准备好了。现在开始提出各种问题，测试一下我们的链。</p><p><strong>测试A：</strong></p><p>print(chain.run(“如何为玫瑰浇水？”))</p><p>输出：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/89d0bfac97b259b93240a10cf777d9a2.png"></p><p><strong>测试B：</strong></p><p>print(chain.run(“如何为婚礼场地装饰花朵？”))</p><p>输出：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/4f848ca6592476358a25bf91996aa0ed.png"></p><p><strong>测试C：</strong></p><p>print(chain.run(“如何考入哈佛大学？”))</p><p>输出：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/acd4a69df2cef81b1f7bcf33f9b4bb12.png"></p><p>这三个测试，分别被路由到了三个不同的目标链，其中两个是我们预设的“专家类型”目标链，而第三个问题：如何考入哈佛大学？被模型一眼看穿，并不属于任何鲜花运营业务场景，路由链把它抛入了一个 “default chain” —— ConversationChain 去解决。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>在这个示例中，我们看到了LLMRouterChain以及MultiPromptChain。其中，LLMRouterChain继承自RouterChain；而MultiPromptChain则继承自MultiRouteChain。</p><p>整体上，我们通过MultiPromptChain把其他链组织起来，完成了路由功能。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">chain = MultiPromptChain(    router_chain=router_chain,    destination_chains=chain_map,    default_chain=default_chain,    verbose=True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在LangChain的 chains -&gt; router -&gt; base.py 文件中，可以看到RouterChain和MultiRouteChain的代码实现。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>通过verbose&#x3D;True这个选项的设定，在输出时显示了链的开始和结束日志，从而得到其相互调用流程。请你尝试把该选项设置为False，看一看输出结果有何不同。</li><li>在这个例子中，我们使用了ConversationChain作为default_chain，这个Chain是LLMChain的子类，你能否把这个Chain替换为LLMChain？</li></ol><p>期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>代码，RouterChain和MultiRouteChain的 <a href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/router/base.py">实现细节</a></li><li>代码，MultiPromptChain的 <a href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/router/multi_prompt.py">实现细节</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;09｜链（下）：想学“育花”还是“插花”？用RouterChain确定客户意图&quot;&gt;&lt;a href=&quot;#09｜链（下）：想学“育花”还是“插花”？用RouterChain确定客户意图&quot; class=&quot;headerlink&quot; title=&quot;09｜链（下）：想学“育花”</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>10｜记忆：通过Memory记住客户上次买花时的对话细节</title>
    <link href="https://zhuansun.github.io/geekbang/posts/2669479270.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/2669479270.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.445Z</updated>
    
    <content type="html"><![CDATA[<h1 id="10｜记忆：通过Memory记住客户上次买花时的对话细节"><a href="#10｜记忆：通过Memory记住客户上次买花时的对话细节" class="headerlink" title="10｜记忆：通过Memory记住客户上次买花时的对话细节"></a>10｜记忆：通过Memory记住客户上次买花时的对话细节</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>在默认情况下，无论是LLM还是代理都是无状态的，每次模型的调用都是独立于其他交互的。也就是说，我们每次通过API开始和大语言模型展开一次新的对话，它都不知道你其实昨天或者前天曾经和它聊过天了。</p><p>你肯定会说，不可能啊，每次和ChatGPT聊天的时候，ChatGPT明明白白地记得我之前交待过的事情。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c9907bc695521228cdfb5d3f75c13897.png"></p><p>的确如此，ChatGPT之所以能够记得你之前说过的话，正是因为它使用了 <strong>记忆（Memory）机制</strong>，记录了之前的对话上下文，并且把这个上下文作为提示的一部分，在最新的调用中传递给了模型。在聊天机器人的构建中，记忆机制非常重要。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e26993dd3957bfd2947424abb9de7cde.png"></p><h2 id="使用ConversationChain"><a href="#使用ConversationChain" class="headerlink" title="使用ConversationChain"></a>使用ConversationChain</h2><p>不过，在开始介绍LangChain中记忆机制的具体实现之前，先重新看一下我们上一节课曾经见过的ConversationChain。</p><p>这个Chain最主要的特点是，它提供了包含AI 前缀和人类前缀的对话摘要格式，这个对话格式和记忆机制结合得非常紧密。</p><p>让我们看一个简单的示例，并打印出ConversationChain中的内置提示模板，你就会明白这个对话格式的意义了。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain import OpenAIfrom langchain.chains import ConversationChain# 初始化大语言模型llm = OpenAI(    temperature=0.5,    model_name="text-davinci-003")# 初始化对话链conv_chain = ConversationChain(llm=llm)# 打印对话的模板print(conv_chain.prompt.template)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.Current conversation:&#123;history&#125;Human: &#123;input&#125;AI:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里的提示为人类（我们）和人工智能（ text-davinci-003 ）之间的对话设置了一个基本对话框架：这是 <strong>人类和</strong> <strong>AI</strong> <strong>之间的友好对话。AI</strong> <strong>非常健谈并从其上下文中提供了大量的具体细节。</strong> (The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. )</p><p>同时，这个提示试图通过说明以下内容来减少幻觉，也就是尽量减少模型编造的信息：</p><p><strong>“如果</strong> <strong>AI</strong> <strong>不知道问题的答案，它就会如实说它不知道。”</strong>（If the AI does not know the answer to a question, it truthfully says it does not know.）</p><p>之后，我们看到两个参数 {history} 和 {input}。</p><ul><li><strong>{history}</strong> 是存储会话记忆的地方，也就是人类和人工智能之间对话历史的信息。</li><li><strong>{input}</strong> 是新输入的地方，你可以把它看成是和ChatGPT对话时，文本框中的输入。</li></ul><p>这两个参数会通过提示模板传递给 LLM，我们希望返回的输出只是对话的延续。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c11b24c318dbd762f13781e3e40f9b7c.png"></p><p><strong>那么当有了</strong> <strong>{history}</strong> <strong>参数，以及</strong> <strong>Human</strong> <strong>和</strong> <strong>AI</strong> <strong>这两个前缀，我们就能够把历史对话信息存储在提示模板中，并作为新的提示内容在新一轮的对话过程中传递给模型。—— 这就是记忆机制的原理</strong>。</p><p>下面就让我们来在ConversationChain中加入记忆功能。</p><h2 id="使用ConversationBufferMemory"><a href="#使用ConversationBufferMemory" class="headerlink" title="使用ConversationBufferMemory"></a>使用ConversationBufferMemory</h2><p>在LangChain中，通过ConversationBufferMemory（ <strong>缓冲记忆</strong>）可以实现最简单的记忆机制。</p><p>下面，我们就在对话链中引入ConversationBufferMemory。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain import OpenAIfrom langchain.chains import ConversationChainfrom langchain.chains.conversation.memory import ConversationBufferMemory# 初始化大语言模型llm = OpenAI(    temperature=0.5,    model_name="text-davinci-003")# 初始化对话链conversation = ConversationChain(    llm=llm,    memory=ConversationBufferMemory())# 第一天的对话# 回合1conversation("我姐姐明天要过生日，我需要一束生日花束。")print("第一次对话后的记忆:", conversation.memory.buffer)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">第一次对话后的记忆:Human: 我姐姐明天要过生日，我需要一束生日花束。AI:  哦，你姐姐明天要过生日，那太棒了！我可以帮你推荐一些生日花束，你想要什么样的？我知道有很多种，比如玫瑰、康乃馨、郁金香等等。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>在下一轮对话中，这些记忆会作为一部分传入提示。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 回合2conversation("她喜欢粉色玫瑰，颜色是粉色的。")print("第二次对话后的记忆:", conversation.memory.buffer)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">第二次对话后的记忆:Human: 我姐姐明天要过生日，我需要一束生日花束。AI:  哦，你姐姐明天要过生日，那太棒了！我可以帮你推荐一些生日花束，你想要什么样的？我知道有很多种，比如玫瑰、康乃馨、郁金香等等。Human: 她喜欢粉色玫瑰，颜色是粉色的。AI:  好的，那我可以推荐一束粉色玫瑰的生日花束给你。你想要多少朵？我可以帮你定制一束，比如说十朵、二十朵或者更多？<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面，我们继续对话，同时打印出此时提示模板的信息。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 回合3 （第二天的对话）conversation("我又来了，还记得我昨天为什么要来买花吗？")print("/n第三次对话后时提示:/n",conversation.prompt.template)print("/n第三次对话后的记忆:/n", conversation.memory.buffer)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>模型输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">Human: 我姐姐明天要过生日，我需要一束生日花束。AI:  哦，你姐姐明天要过生日，那太棒了！我可以帮你推荐一些生日花束，你想要什么样的？我知道有很多种，比如玫瑰、康乃馨、郁金香等等。Human: 她喜欢粉色玫瑰，颜色是粉色的。AI:  好的，那我可以推荐一束粉色玫瑰的生日花束给你，你想要多少朵？Human: 我又来了，还记得我昨天为什么要来买花吗？AI:  是的，我记得你昨天来买花是因为你姐姐明天要过生日，你想要买一束粉色玫瑰的生日花束给她。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>实际上，这些聊天历史信息，都被传入了ConversationChain的提示模板中的 {history} 参数，构建出了包含聊天记录的新的提示输入。</p><p>有了记忆机制，LLM能够了解之前的对话内容，这样简单直接地存储所有内容为LLM提供了最大量的信息，但是新输入中也包含了更多的Token（所有的聊天历史记录），这意味着响应时间变慢和更高的成本。而且，当达到LLM的令牌数（上下文窗口）限制时，太长的对话无法被记住（对于text-davinci-003和gpt-3.5-turbo，每次的最大输入限制是4096个Token）。</p><p>下面我们来看看针对Token太多、聊天历史记录过长的一些解决方案。</p><h2 id="使用ConversationBufferWindowMemory"><a href="#使用ConversationBufferWindowMemory" class="headerlink" title="使用ConversationBufferWindowMemory"></a>使用ConversationBufferWindowMemory</h2><p>说到记忆，我们人类的大脑也不是无穷无尽的。所以说，有的时候事情太多，我们只能把有些遥远的记忆抹掉。毕竟，最新的经历最鲜活，也最重要。</p><p>ConversationBufferWindowMemory 是 <strong>缓冲窗口记忆</strong>，它的思路就是只保存最新最近的几次人类和AI的互动。因此，它在之前的“缓冲记忆”基础上增加了一个窗口值 k。这意味着我们只保留一定数量的过去互动，然后“忘记”之前的互动。</p><p>下面看一下示例。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain import OpenAIfrom langchain.chains import ConversationChainfrom langchain.chains.conversation.memory import ConversationBufferWindowMemory# 创建大语言模型实例llm = OpenAI(    temperature=0.5,    model_name="text-davinci-003")# 初始化对话链conversation = ConversationChain(    llm=llm,    memory=ConversationBufferWindowMemory(k=1))# 第一天的对话# 回合1result = conversation("我姐姐明天要过生日，我需要一束生日花束。")print(result)# 回合2result = conversation("她喜欢粉色玫瑰，颜色是粉色的。")# print("\n第二次对话后的记忆:\n", conversation.memory.buffer)print(result)# 第二天的对话# 回合3result = conversation("我又来了，还记得我昨天为什么要来买花吗？")print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一回合的输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'input': '我姐姐明天要过生日，我需要一束生日花束。','history': '', 'response': ' 哦，你姐姐明天要过生日！那太棒了！你想要一束什么样的花束呢？有很多种类可以选择，比如玫瑰花束、康乃馨花束、郁金香花束等等，你有什么喜欢的吗？'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>第二回合的输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'input': '她喜欢粉色玫瑰，颜色是粉色的。','history': 'Human: 我姐姐明天要过生日，我需要一束生日花束。\nAI:  哦，你姐姐明天要过生日！那太棒了！你想要一束什么样的花束呢？有很多种类可以选择，比如玫瑰花束、康乃馨花束、郁金香花束等等，你有什么喜欢的吗？','response': ' 好的，那粉色玫瑰花束怎么样？我可以帮你找到一束非常漂亮的粉色玫瑰花束，你觉得怎么样？'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>第三回合的输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'input': '我又来了，还记得我昨天为什么要来买花吗？','history': 'Human: 她喜欢粉色玫瑰，颜色是粉色的。\nAI:  好的，那粉色玫瑰花束怎么样？我可以帮你找到一束非常漂亮的粉色玫瑰花束，你觉得怎么样？','response': '  当然记得，你昨天来买花是为了给你喜欢的人送一束粉色玫瑰花束，表达你对TA的爱意。'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>在给定的例子中，设置 k&#x3D;1，这意味着窗口只会记住与AI之间的最新的互动，即只保留上一次的人类回应和AI的回应。</p><p>在第三个回合，当我们询问“还记得我昨天为什么要来买花吗？”，由于我们只保留了最近的互动（k&#x3D;1），模型已经忘记了正确的答案。所以，虽然它说记得，但只能模糊地说出“喜欢的人”，而没有说关键字“姐姐”。不过，如果（我是说如果哈）在第二个回合，模型能回答“我可以帮你 <strong>为你姐姐</strong> 找到…”，那么，尽管我们没有第一回合的历史记录，但凭着上一个回合的信息，模型还是有可能推断出昨天来的人买花的真实意图。</p><p>尽管这种方法不适合记住遥远的互动，但它非常擅长限制使用的Token数量。如果只需要记住最近的互动，缓冲窗口记忆是一个很好的选择。但是，如果需要混合远期和近期的互动信息，则还有其他选择。</p><h2 id="使用ConversationSummaryMemory"><a href="#使用ConversationSummaryMemory" class="headerlink" title="使用ConversationSummaryMemory"></a>使用ConversationSummaryMemory</h2><p>上面说了，如果模型在第二轮回答的时候，能够说出“我可以帮你为你姐姐找到…”，那么在第三轮回答时，即使窗口大小 k&#x3D;1，还是能够回答出正确答案。</p><p>这是为什么？</p><p>因为模型 <strong>在回答新问题的时候，对之前的问题进行了总结性的重述</strong>。</p><p>ConversationSummaryMemory（ <strong>对话总结记忆</strong>）的思路就是将对话历史进行汇总，然后再传递给 {history} 参数。这种方法旨在通过对之前的对话进行汇总来避免过度使用 Token。</p><p>ConversationSummaryMemory有这么几个核心特点。</p><ol><li>汇总对话：此方法不是保存整个对话历史，而是每次新的互动发生时对其进行汇总，然后将其添加到之前所有互动的“运行汇总”中。</li><li>使用LLM进行汇总：该汇总功能由另一个LLM驱动，这意味着对话的汇总实际上是由AI自己进行的。</li><li>适合长对话：对于长对话，此方法的优势尤为明显。虽然最初使用的 Token 数量较多，但随着对话的进展，汇总方法的增长速度会减慢。与此同时，常规的缓冲内存模型会继续线性增长。</li></ol><p>下面，我们来看看使用ConversationSummaryMemory的代码示例。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.chains.conversation.memory import ConversationSummaryMemory# 初始化对话链conversation = ConversationChain(    llm=llm,    memory=ConversationSummaryMemory(llm=llm))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一回合的输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'input': '我姐姐明天要过生日，我需要一束生日花束。','history': '','response': ' 我明白，你需要一束生日花束。我可以为你提供一些建议吗？我可以推荐一些花束给你，比如玫瑰，康乃馨，百合，仙客来，郁金香，满天星等等。挑选一束最适合你姐姐的生日花束吧！'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>第二回合的输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'input': '她喜欢粉色玫瑰，颜色是粉色的。','history': "\nThe human asked what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human then asked the AI for advice on what type of flower bouquet to get for their sister's birthday, to which the AI provided a variety of suggestions.",'response': ' 为了为你的姐姐的生日准备一束花，我建议你搭配粉色玫瑰和白色康乃馨。你可以在玫瑰花束中添加一些紫色的满天星，或者添加一些绿叶以增加颜色对比。这将是一束可爱的花束，让你姐姐的生日更加特别。'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>第三回合的输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'input': '我又来了，还记得我昨天为什么要来买花吗？','history': "\n\nThe human asked what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human then asked the AI for advice on what type of flower bouquet to get for their sister's birthday, to which the AI suggested pink roses and white carnations with the addition of purple aster flowers and green leaves for contrast. This would make a lovely bouquet to make the sister's birthday extra special.",'response': ' 确实，我记得你昨天想买一束花给你的姐姐作为生日礼物。我建议你买粉红色的玫瑰花和白色的康乃馨花，再加上紫色的雏菊花和绿叶，这样可以让你的姐姐的生日更加特别。'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>看得出来，这里的 <code>&#39;history&#39;</code>，不再是之前人类和AI对话的简单复制粘贴，而是经过了总结和整理之后的一个综述信息。</p><p>这里，我们 <strong>不仅仅利用了LLM来回答每轮问题，还利用LLM来对之前的对话进行总结性的陈述，以节约Token数量</strong>。这里，帮我们总结对话的LLM，和用来回答问题的LLM，可以是同一个大模型，也可以是不同的大模型。</p><p>ConversationSummaryMemory的优点是对于长对话，可以减少使用的 Token 数量，因此可以记录更多轮的对话信息，使用起来也直观易懂。不过，它的缺点是，对于较短的对话，可能会导致更高的 Token 使用。另外，对话历史的记忆完全依赖于中间汇总LLM的能力，还需要为汇总LLM使用 Token，这增加了成本，且并不限制对话长度。</p><p>通过对话历史的汇总来优化和管理 Token 的使用，ConversationSummaryMemory 为那些预期会有多轮的、长时间对话的场景提供了一种很好的方法。然而，这种方法仍然受到 Token 数量的限制。在一段时间后，我们仍然会超过大模型的上下文窗口限制。</p><p>而且，总结的过程中并没有区分近期的对话和长期的对话（通常情况下近期的对话更重要），所以我们还要继续寻找新的记忆管理方法。</p><h2 id="使用ConversationSummaryBufferMemory"><a href="#使用ConversationSummaryBufferMemory" class="headerlink" title="使用ConversationSummaryBufferMemory"></a>使用ConversationSummaryBufferMemory</h2><p>我要为你介绍的最后一种记忆机制是ConversationSummaryBufferMemory，即 <strong>对话总结缓冲记忆</strong>，它是一种 <strong>混合记忆</strong> 模型，结合了上述各种记忆机制，包括ConversationSummaryMemory 和 ConversationBufferWindowMemory的特点。这种模型旨在在对话中总结早期的互动，同时尽量保留最近互动中的原始内容。</p><p>它是通过max_token_limit这个参数做到这一点的。当最新的对话文字长度在300字之内的时候，LangChain会记忆原始对话内容；当对话文字超出了这个参数的长度，那么模型就会把所有超过预设长度的内容进行总结，以节省Token数量。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.chains.conversation.memory import ConversationSummaryBufferMemory# 初始化对话链conversation = ConversationChain(    llm=llm,    memory=ConversationSummaryBufferMemory(        llm=llm,        max_token_limit=300))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一回合的输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'input': '我姐姐明天要过生日，我需要一束生日花束。','history': '','response': ' 哇，你姐姐要过生日啊！那太棒了！我建议你去买一束色彩鲜艳的花束，因为这样可以代表你给她的祝福和祝愿。你可以去你家附近的花店，或者也可以从网上订购，你可以看看有没有特别的花束，比如彩色玫瑰或者百合花，它们会更有特色。'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>第二回合的输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'input': '她喜欢粉色玫瑰，颜色是粉色的。','history': 'Human: 我姐姐明天要过生日，我需要一束生日花束。\nAI:  哇，你姐姐要过生日啊！那太棒了！我建议你去买一束色彩鲜艳的花束，因为这样可以代表你给她的祝福和祝愿。你可以去你家附近的花店，或者也可以从网上订购，你可以看看有没有特别的花束，比如彩色玫瑰或者百合花，它们会更有特色。','response': ' 好的，那粉色玫瑰就是一个很好的选择！你可以买一束粉色玫瑰花束，这样你姐姐会很开心的！你可以在花店里找到粉色玫瑰，也可以从网上订购，你可以根据你的预算，选择合适的数量。另外，你可以考虑添加一些装饰，比如细绳、彩带或者小礼品'&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>第三回合的输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&#123;'input': '我又来了，还记得我昨天为什么要来买花吗？','history': "System: \nThe human asked the AI for advice on buying a bouquet for their sister's birthday. The AI suggested buying a vibrant bouquet as a representation of their wishes and blessings, and recommended looking for special bouquets like colorful roses or lilies for something more unique.\nHuman: 她喜欢粉色玫瑰，颜色是粉色的。\nAI:  好的，那粉色玫瑰就是一个很好的选择！你可以买一束粉色玫瑰花束，这样你姐姐会很开心的！你可以在花店里找到粉色玫瑰，也可以从网上订购，你可以根据你的预算，选择合适的数量。另外，你可以考虑添加一些装饰，比如细绳、彩带或者小礼品",'response': ' 是的，我记得你昨天来买花是为了给你姐姐的生日。你想买一束粉色玫瑰花束来表达你的祝福和祝愿，你可以在花店里找到粉色玫瑰，也可以从网上订购，你可以根据你的预算，选择合适的数量。另外，你可以考虑添加一些装饰，比如细绳、彩带或者小礼品&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>不难看出，在第二回合，记忆机制完整地记录了第一回合的对话，但是在第三回合，它察觉出前两轮的对话已经超出了300个字节，就把早期的对话加以总结，以节省Token资源。</p><p>ConversationSummaryBufferMemory的优势是通过总结可以回忆起较早的互动，而且有缓冲区确保我们不会错过最近的互动信息。当然，对于较短的对话，ConversationSummaryBufferMemory也会增加Token数量。</p><p>总体来说，ConversationSummaryBufferMemory为我们提供了大量的灵活性。它是我们迄今为止的唯一记忆类型，可以回忆起较早的互动并完整地存储最近的互动。在节省Token数量方面，ConversationSummaryBufferMemory与其他方法相比，也具有竞争力。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>好的，今天我给你介绍了一种对话链和四种类型的对话记忆机制，那么我们可以通过一个表格对这四种类型的记忆做一个整体比较。</p><p>四种记忆机制的比较如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/a06b5db35405b74yy317de917eacbdc0.jpg"></p><p>网上还有人总结了一个示意图，体现出了当对话轮次逐渐增加时，各种记忆机制对Token的消耗数量。意图向我们表达的是：有些记忆机制，比如说ConversationSummaryBufferMemory和ConversationSummaryMemory，在对话轮次较少的时候可能会浪费一些Token，但是多轮对话过后，Token的节省就逐渐体现出来了。</p><p>当然ConversationBufferWindowMemory对于Token的节省最为直接，但是它会完全遗忘掉K轮之前的对话内容，因此对于某些场景也不是最佳选择。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c56yyd7eb61637687de448512yy426ea.png"></p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>在你的客服聊天机器人设计中，你会首先告知客户：“亲，我的记忆能力有限，只能记住和你的最近10次对话哦。如果我忘了之前的对话，请你体谅我。” 当有了这样的预设，你会为你的ChatBot选择那种记忆机制？</li><li>尝试改变示例程序ConversationBufferWindowMemory中的k值，并增加对话轮次，看看记忆效果。</li><li>尝试改变示例程序ConversationSummaryBufferMemory中的max_token_limit值，看看记忆效果。</li></ol><p>期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>代码，ConversationBufferMemory的 <a href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/memory/buffer.py">实现细节</a></li><li>代码，ConversationSummaryMemory的 <a href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/memory/summary.py">实现细节</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;10｜记忆：通过Memory记住客户上次买花时的对话细节&quot;&gt;&lt;a href=&quot;#10｜记忆：通过Memory记住客户上次买花时的对话细节&quot; class=&quot;headerlink&quot; title=&quot;10｜记忆：通过Memory记住客户上次买花时的对话细节&quot;&gt;&lt;/a&gt;10</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>11｜代理（上）：ReAct框架，推理与行动的协同</title>
    <link href="https://zhuansun.github.io/geekbang/posts/879533637.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/879533637.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.447Z</updated>
    
    <content type="html"><![CDATA[<h1 id="11｜代理（上）：ReAct框架，推理与行动的协同"><a href="#11｜代理（上）：ReAct框架，推理与行动的协同" class="headerlink" title="11｜代理（上）：ReAct框架，推理与行动的协同"></a>11｜代理（上）：ReAct框架，推理与行动的协同</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>在之前介绍的思维链（CoT）中，我向你展示了 LLMs 执行推理轨迹的能力。在给出答案之前，大模型通过中间推理步骤（尤其是与少样本提示相结合）能够实现复杂的推理，获得更好的结果，以完成更具挑战的任务。</p><p>然而，仅仅应用思维链推理并不能解决大模型的固有问题： <strong>无法主动更新自己的知识，导致出现事实幻觉</strong>。也就是说，因为缺乏和外部世界的接触，大模型只拥有训练时见过的知识，以及提示信息中作为上下文提供的附加知识。如果你问的问题超出它的知识范围，要么大模型向你坦白：“我的训练时间截至XXXX年XX月XX日”，要么它就会开始一本正经地胡说。</p><p>下面这张图就属于第二种情况，我制作的一个Prompt骗过了大模型，它会误以为我引述的很多虚构的东西是事实，而且它还会顺着这个思路继续胡编乱造。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/50050ee434877dc4617a7cfe49386a45.png"></p><p>这个问题如何解决呢？</p><p>也不难。你可以让大模型先在本地知识库中进行搜索，检查一下提示中的信息的真实性，如果真实，再进行输出；如果不真实，则进行修正。如果本地知识库找不到相应的信息，可以调用工具进行外部搜索，来检查提示信息的真实性。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/7032d003ac36e858cbb53f90bb4f3a1b.jpg"></p><p>上面所说的无论本地知识库还是搜索引擎，都不是封装在大模型内部的知识，我们把它们称为“外部工具”。</p><h2 id="代理的作用"><a href="#代理的作用" class="headerlink" title="代理的作用"></a>代理的作用</h2><p>每当你遇到这种需要模型做自主判断、自行调用工具、自行决定下一步行动的时候，Agent（也就是代理）就出场了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e26993dd3957bfd2947424abb9de7cde.png"></p><p>代理就像一个多功能的接口，它能够接触并使用一套工具。根据用户的输入，代理会决定调用哪些工具。它不仅可以同时使用多种工具，而且可以将一个工具的输出数据作为另一个工具的输入数据。</p><p>在LangChain中使用代理，我们只需要理解下面三个元素。</p><ul><li><strong>大模型</strong>：提供逻辑的引擎，负责生成预测和处理输入。</li><li>与之交互的 <strong>外部工具</strong>：可能包括数据清洗工具、搜索引擎、应用程序等。</li><li>控制交互的 <strong>代理</strong>：调用适当的外部工具，并管理整个交互过程的流程。</li></ul><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/9a9550e7df156d15975dc027b3201d31.png"></p><p>上面的思路看似简单，其实很值得我们仔细琢磨。</p><p>这个过程有很多地方需要大模型自主判断下一步行为（也就是操作）要做什么，如果不加引导，那大模型本身是不具备这个能力的。比如下面这一系列的操作：</p><ul><li>什么时候开始在本地知识库中搜索（这个比较简单，毕竟是第一个步骤，可以预设）？</li><li>怎么确定本地知识库的检索已经完成，可以开始下一步？</li><li>调用哪一种外部搜索工具（比如Google引擎）？</li><li>如何确定外部搜索工具返回了想要找的内容？</li><li>如何确定信息真实性的检索已经全部完成，可以开始下一步？</li></ul><p>那么，LangChain中的代理是怎样自主计划、自行判断，并执行行动的呢？</p><h2 id="ReAct框架"><a href="#ReAct框架" class="headerlink" title="ReAct框架"></a>ReAct框架</h2><p>这里我要请你思考一下：如果你接到一个新任务，你将如何做出决策并完成下一步的行动？</p><p>比如说，你在运营花店的过程中，经常会经历天气变化而导致的鲜花售价变化，那么，每天早上你会如何为你的鲜花定价？</p><p>也许你会告诉我，我会去Google上面查一查今天的鲜花成本价啊（ <strong>行动</strong>），也就是我预计的进货的价格，然后我会根据这个价格的高低（ <strong>观察</strong>），来确定我要加价多少（ <strong>思考</strong>），最后计算出一个售价（ <strong>行动</strong>）！</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/58bdbe17948a0ed2d52ceb3557194a12.png"></p><p>你看，在这个简单的例子中，你有观察、有思考，然后才会具体行动。这里的观察和思考，我们统称为推理（Reasoning）过程，推理指导着你的行动（Acting）。</p><p>我们今天要讲的 <strong>ReAct 框架的灵感正是来自“行动”和“推理”之间的协同作用，这种协同作用使得咱们人类能够学习新任务并做出决策或推理</strong>。这个框架，也是大模型能够作为“智能代理”，自主、连续、交错地生成推理轨迹和任务特定操作的理论基础。</p><p>先和你说明一点，此 ReAct 并非指代流行的前端开发框架React，它在这里专指如何指导大语言模型推理和行动的一种思维框架。这个思维框架是Shunyu Yao等人在ICLR 2023会议论文《 <a href="https://arxiv.org/pdf/2210.03629.pdf">ReAct: Synergizing Reasoning and Acting in Language Models</a>》（ReAct：在语言模型中协同推理和行动）中提出的。</p><p>这篇文章的一个关键启发在于： <strong>大语言模型可以通过生成推理痕迹和任务特定行动来实现更大的协同作用</strong>。</p><p>具体来说，就是引导模型生成一个任务解决轨迹：观察环境-进行思考-采取行动，也就是观察-思考-行动。那么，再进一步进行简化，就变成了推理-行动，也就是Reasoning-Acting框架。</p><p>其中，Reasoning包括了对当前环境和状态的观察，并生成推理轨迹。这使模型能够诱导、跟踪和更新操作计划，甚至处理异常情况。Acting在于指导大模型采取下一步的行动，比如与外部源（如知识库或环境）进行交互并且收集信息，或者给出最终答案。</p><p>ReAct的每一个推理过程都会被详细记录在案，这也改善大模型解决问题时的可解释性和可信度，而且这个框架在各种语言和决策任务中都得到了很好的效果。</p><p>下面让我们用一个具体的示例来说明这一点。比如我给出大模型这样一个任务：在一个虚拟环境中找到一个胡椒瓶并将其放在一个抽屉里。</p><p>在这个任务中，没有推理能力的模型不能够在房间的各个角落中进行寻找，或者在找到胡椒瓶之后不能够判断下一步的行动，因而无法完成任务。如果使用ReAct，这一系列子目标将被具体地捕获在每一个思考过程中。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/638e1b0098211b1b622283e0f7100596.png"></p><p>现在，让我们回到开始的时候我们所面临的问题。仅仅使用思维链（CoT）提示，LLMs能够执行推理轨迹，以完成算术和常识推理等问题，但这样的模型因为缺乏和外部世界的接触或无法更新自己的知识，会导致幻觉的出现。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/1189768e0ae5b6199fd6db301d2401c8.png"></p><p>而将 ReAct框架和思维链（CoT）结合使用，则能够让大模型在推理过程同时使用内部知识和获取到的外部信息，从而给出更可靠和实际的回应，也提高了 LLMs 的可解释性和可信度。</p><p>LangChain正是通过Agent类，将ReAct框架进行了完美封装和实现，这一下子就赋予了大模型极大的自主性（Autonomy）， <strong>你的大模型现在从一个仅仅可以通过自己内部知识进行对话聊天的</strong> <strong>Bot</strong> <strong>，</strong> <strong>飞升为了一个有手有脚能使用工具的智能代理</strong>。</p><p>ReAct框架会提示 LLMs 为任务生成推理轨迹和操作，这使得代理能系统地执行动态推理来创建、维护和调整操作计划，同时还支持与外部环境（例如Google搜索、Wikipedia）的交互，以将额外信息合并到推理中。</p><h2 id="通过代理实现ReAct框架"><a href="#通过代理实现ReAct框架" class="headerlink" title="通过代理实现ReAct框架"></a>通过代理实现ReAct框架</h2><p>下面，就让我们用LangChain中最为常用的 <strong>ZERO_SHOT_REACT_DESCRIPTION</strong> ——这种常用代理类型，来剖析一下LLM是如何在ReAct框架的指导之下进行推理的。</p><p>此处，我们要给代理一个任务，这个任务是找到玫瑰的当前市场价格，然后计算出加价15%后的新价格。</p><p>在开始之前，有一个准备工作，就是你需要在 <a href="https://serpapi.com/">serpapi.com</a> 注册一个账号，并且拿到你的 SERPAPI_API_KEY，这个就是我们要为大模型提供的 Google 搜索工具。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/1841f5d709cd27f1000ee9a5b593325b.png"></p><p>先安装SerpAPI的包。</p><pre class="line-numbers language-none"><code class="language-none">pip install google-search-results<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>设置好OpenAI和SerpAPI的API密钥。</p><pre class="line-numbers language-none"><code class="language-none"># 设置OpenAI和SERPAPI的API密钥import osos.environ[&quot;OPENAI_API_KEY&quot;] &#x3D; &#39;Your OpenAI API Key&#39;os.environ[&quot;SERPAPI_API_KEY&quot;] &#x3D; &#39;Your SerpAPI API Key&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>再导入所需的库。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.agents import load_toolsfrom langchain.agents import initialize_agentfrom langchain.agents import AgentTypefrom langchain.llms import OpenAI<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>然后加载将用于控制代理的语言模型。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">llm = OpenAI(temperature=0)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>接下来，加载一些要使用的工具，包括serpapi（这是调用Google搜索引擎的工具）以及llm-math（这是通过LLM进行数学计算的工具）。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">tools = load_tools(["serpapi", "llm-math"], llm=llm)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>最后，让我们使用工具、语言模型和代理类型来初始化代理。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>好了，现在我们让代理来回答我刚才提出的问题了！目前市场上玫瑰花的平均价格是多少？如果我在此基础上加价15%卖出，应该如何定价？</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">agent.run("目前市场上玫瑰花的平均价格是多少？如果我在此基础上加价15%卖出，应该如何定价？")<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>大模型成功遵循了ReAct框架，它输出的思考与行动轨迹如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">> Entering new  chain... I need to find the current market price of roses and then calculate the new price with a 15% markup.Action: SearchAction Input: "Average price of roses"Observation: According to the study, the average price for a dozen roses in the United States is $80.16. The Empire State hovers closer to that number than its neighbors, with a bouquet setting back your average New Yorker $78.33.Thought: I need to calculate the new price with a 15% markup.Action: CalculatorAction Input: 80.16 * 1.15Observation: Answer: 92.18399999999998Thought: I now know the final answer.Final Answer: The new price with a 15% markup would be $92.18.> Finished chain.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c99893b6d8311d9ac95aeb8d818e1914.png"></p><p>可以看到，ZERO_SHOT_REACT_DESCRIPTION类型的智能代理在LangChain中，自动形成了一个完善的思考与行动链条，而且给出了正确的答案。</p><p>你可以对照下面这个表格，再巩固一下这个链条中的每一个环节。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/56fbe79e086052895f301383c27f4a0c.jpg"></p><p>这个思维链条中，智能代理有思考、有观察、有行动，成功通过搜索和计算两个操作，完成了任务。在下一讲中，我们将继续深入剖析LangChain中的不同类型的代理，并利用它完成更为复杂的任务。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>这节课我们介绍了什么是LangChain中的代理，更重要的是，我们介绍了代理自主行动的驱动力—— ReAct框架。</p><p>通过ReAct框架，大模型将被引导生成一个任务解决轨迹，即观察环境-进行思考-采取行动。观察和思考阶段被统称为推理（Reasoning），而实施下一步行动的阶段被称为行动（Acting）。在每一步推理过程中，都会详细记录下来，这也改善了大模型解决问题时的可解释性和可信度。</p><ul><li>在推理阶段，模型对当前环境和状态进行观察，并生成推理轨迹，从而使模型能够诱导、跟踪和更新操作计划，甚至处理异常情况。</li><li>在行动阶段，模型会采取下一步的行动，如与外部源（如知识库或环境）进行交互并收集信息，或给出最终答案。</li></ul><p>ReAct框架的这些优点，使得它在未来的发展中具有巨大的潜力。随着技术的进步，我们可以期待ReAct框架将能够处理更多、更复杂的任务。特别是随着具身智能的发展，ReAct框架将能够使智能代理在虚拟或实际环境中进行更复杂的交互。例如，智能代理可能会在虚拟环境中进行导航，或者在实际环境中操作物理对象。这将大大扩展AI的应用范围，使得它们能够更好地服务于我们的生活和工作。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>在ReAct框架中，推理和行动各自代表什么？其相互之间的关系如何？</li><li>为什么说ReAct框架能改善大模型解决问题时的可解释性和可信度？</li><li>你能否说一说LangChain中的代理和链的核心差异？</li></ol><p>期待在留言区看到你的思考，如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面延伸阅读的两篇论文。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>论文， <a href="https://arxiv.org/abs/2210.03629">ReAct：在语言模型中协同推理和行动</a>，Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp; Cao, Y. (2023). ReAct: Synergizing Reasoning and Acting in Language Models. arXiv preprint arXiv:2210.03629</li><li>论文， <a href="https://arxiv.org/abs/2303.09014">ART：大型语言模型的自动多步推理和工具使用</a>， Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., &amp; Ribeiro, M. T. (2023). ART: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014.</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;11｜代理（上）：ReAct框架，推理与行动的协同&quot;&gt;&lt;a href=&quot;#11｜代理（上）：ReAct框架，推理与行动的协同&quot; class=&quot;headerlink&quot; title=&quot;11｜代理（上）：ReAct框架，推理与行动的协同&quot;&gt;&lt;/a&gt;11｜代理（上）：Re</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>12｜代理（中）：AgentExecutor究竟是怎样驱动模型和工具完成任务的？</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1848002715.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1848002715.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.450Z</updated>
    
    <content type="html"><![CDATA[<h1 id="12｜代理（中）：AgentExecutor究竟是怎样驱动模型和工具完成任务的？"><a href="#12｜代理（中）：AgentExecutor究竟是怎样驱动模型和工具完成任务的？" class="headerlink" title="12｜代理（中）：AgentExecutor究竟是怎样驱动模型和工具完成任务的？"></a>12｜代理（中）：AgentExecutor究竟是怎样驱动模型和工具完成任务的？</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>上节课中，你了解了ReAct框架的原理，最后我给你留了一道思考题，让你说一说LangChain中的“代理”和“链”的差异究竟是什么。</p><p>我的答案是： <strong>在链中，一系列操作被硬编码（在代码中）。在代理中，语言模型被用作推理引擎来确定要采取哪些操作以及按什么顺序执行这些操作。</strong></p><p>下面这个图，就展现出了Agent接到任务之后，自动进行推理，然后自主调用工具完成任务的过程。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/aeb7497d833b0b3188fbc7152282b0e3.jpg"></p><p>那么，你看LangChain，乃至整个大模型应用开发的核心理念就呼之欲出了。这个核心理念就是 <strong>操作的序列并非硬编码在代码中，而是使用语言模型（如GPT-3或GPT-4）来选择执行的操作序列</strong>。</p><p>这里，我又一次重复了上一段话，显得有点啰嗦，但是这个思路真的是太重要了，它也凸显了LLM作为AI自主决定程序逻辑这个编程新范式的价值，我希望你仔细认真地去理解。</p><h2 id="Agent-的关键组件"><a href="#Agent-的关键组件" class="headerlink" title="Agent 的关键组件"></a>Agent 的关键组件</h2><p>在LangChain的代理中，有这样几个关键组件。</p><ol><li><p><strong>代理</strong>（Agent）：这个类决定下一步执行什么操作。它由一个语言模型和一个提示（prompt）驱动。提示可能包含代理的性格（也就是给它分配角色，让它以特定方式进行响应）、任务的背景（用于给它提供更多任务类型的上下文）以及用于激发更好推理能力的提示策略（例如ReAct）。LangChain中包含很多种不同类型的代理。</p></li><li><p><strong>工具</strong>（Tools）：工具是代理调用的函数。这里有两个重要的考虑因素：一是让代理能访问到正确的工具，二是以最有帮助的方式描述这些工具。如果你没有给代理提供正确的工具，它将无法完成任务。如果你没有正确地描述工具，代理将不知道如何使用它们。LangChain提供了一系列的工具，同时你也可以定义自己的工具。</p></li><li><p><strong>工具包</strong>（Toolkits）：工具包是一组用于完成特定目标的彼此相关的工具，每个工具包中包含多个工具。比如LangChain的Office365工具包中就包含连接Outlook、读取邮件列表、发送邮件等一系列工具。当然LangChain中还有很多其他工具包供你使用。</p></li><li><p><strong>代理执行器</strong>（AgentExecutor）：代理执行器是代理的运行环境，它调用代理并执行代理选择的操作。执行器也负责处理多种复杂情况，包括处理代理选择了不存在的工具的情况、处理工具出错的情况、处理代理产生的无法解析成工具调用的输出的情况，以及在代理决策和工具调用进行观察和日志记录。</p></li></ol><p>总的来说，代理就是一种用语言模型做出决策、调用工具来执行具体操作的系统。通过设定代理的性格、背景以及工具的描述，你可以定制代理的行为，使其能够根据输入的文本做出理解和推理，从而实现自动化的任务处理。而代理执行器（AgentExecutor）就是上述机制得以实现的引擎。</p><p>在这一讲中，我们将深入LangChain源代码的内部，揭示代理是如何通过代理执行器来自动决策的。</p><h2 id="深挖-AgentExcutor-的运行机制"><a href="#深挖-AgentExcutor-的运行机制" class="headerlink" title="深挖 AgentExcutor 的运行机制"></a>深挖 AgentExcutor 的运行机制</h2><p>让我们先来回顾一下上一讲中的关键代码。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">llm = OpenAI(temperature=0) # 大语言模型tools = load_tools(["serpapi", "llm-math"], llm=llm) # 工具-搜索和数学运算agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) # 代理agent.run("目前市场上玫瑰花的平均价格是多少？如果我在此基础上加价15%卖出，应该如何定价？") # 运行代理<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>在这段代码中，模型、工具、代理的初始化，以及代理运行的过程都极为简洁。但是，LangChain的内部封装的逻辑究竟是怎样的？我希望带着你至少弄清楚两个问题。</p><ol><li>代理每次给大模型的具体提示是什么样子的？能够让模型给出下一步的行动指南，这个提示的秘密何在？</li><li>代理执行器是如何按照ReAct框架来调用模型，接收模型的输出，并根据这个输出来调用工具，然后又根据工具的返回结果生成新的提示的。</li></ol><p>运行代码后我们得到下面的日志。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/106f1c0f2f34b77473d2b18616a30a73.jpg"></p><p>要回答上面的两个问题，仅仅观察LangChain输出的Log是不够的。我们需要深入到LangChain的程序内部，深挖一下AgentExcutor的运行机制。</p><h3 id="开始-Debug"><a href="#开始-Debug" class="headerlink" title="开始 Debug"></a>开始 Debug</h3><p>现在，请你用你的代码编辑工具（比如VS Code）在agent.run这个语句设置一个断点，用 “Step Into” 功能深入几层LangChain内部代码，直到我们进入了 <strong>agent.py文件的AgentExecutor类的内部方法 _take_next_step</strong>。</p><p>这个 _take_next_step 方法掌控着下一步要调用什么的计划，你可以看到self.agent.plan方法被调用，这是计划开始之处。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/99869f62yy0c82a35797d0fc6712736d.jpg"></p><p>注意：如果使用VS Code，要把launch.json的justMycode项设置为false才可以debug LangChain包中的代码。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d5accaa1f9a751e7678e7634f2a56942.jpg"></p><h3 id="第一轮思考：模型决定搜索"><a href="#第一轮思考：模型决定搜索" class="headerlink" title="第一轮思考：模型决定搜索"></a>第一轮思考：模型决定搜索</h3><p>在AgentExecutor 的_take_next_step 方法的驱动下，我们进一步Debug， <strong>深入self.agent.plan方法</strong>，来到了整个行为链条的第一步—— <strong>Plan</strong>，这个Plan的具体细节是由Agent类的Plan方法来完成的，你可以看到，输入的问题将会被传递给llm_chain，然后接收llm_chain调用大模型的返回结果。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/1d60291f18dc4087b7e166ac5d69d849.jpg"></p><p>再往前进一步，我们就要开始调用大模型了，那么，LangChain到底传递给了大模型什么具体的提示信息，让大模型能够主动进行工具的选择呢？秘密在 <strong>LLMChain类的generate方法</strong> 中，我们可以看到提示的具体内容。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/2e441106b2e8b04eb1806b4f0f46b251.jpg"></p><p>在Debug过程中，你可以观察prompt，也就是提示的具体内容，这里我把这个提示Copy出来，你可以看一下。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">0: StringPromptValue(text='Answer the following questions as best you can. You have access to the following tools:\n\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\nCalculator: Useful for when you need to answer questions about math.\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Search, Calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: 目前市场上玫瑰花的平均价格是多少？如果我在此基础上加价15%卖出，应该如何定价？\nThought:<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>我来给你详细拆解一下这个prompt。注意，下面的解释文字不是原始提示，而是我添加的说明。</p><blockquote><p>0: StringPromptValue(text&#x3D;’Answer the following questions as best you can. You have access to the following tools:\n\n</p></blockquote><p>这句提示是让模型尽量回答问题，并告诉模型拥有哪些工具。</p><blockquote><p>Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\n</p></blockquote><p>这是向模型介绍第一个工具：搜索。</p><blockquote><p>Calculator: Useful for when you need to answer questions about math.\n\n</p></blockquote><p>这是向模型介绍第二个工具：计算器。</p><blockquote><p>Use the following format:\n\n （指导模型使用下面的格式）</p><p>Question: the input question you must answer\n （问题）</p><p>Thought: you should always think about what to do\n （思考）</p><p>Action: the action to take, should be one of [Search, Calculator]\n （行动）</p><p>Action Input: the input to the action\n （行动的输入）</p><p>Observation: the result of the action\n… （观察：行动的返回结果）</p><p>(this Thought&#x2F;Action&#x2F;Action Input&#x2F;Observation can repeat N times)\n （上面这个过程可以重复多次）</p><p>Thought: I now know the final answer\n （思考：现在我知道最终答案了）</p><p>Final Answer: the final answer to the original input question\n\n （最终答案）</p></blockquote><p>上面，就是给模型的思考框架。具体解释可以看一下括号中的文字</p><blockquote><p>Begin!\n\n</p></blockquote><p>现在开始！</p><blockquote><p>Question: 目前市场上玫瑰花的平均价格是多少？如果我在此基础上加价15%卖出，应该如何定价？\nThought:’)</p></blockquote><p>具体问题，也就是具体任务。</p><p>上面我一句句拆解的这个提示词，就是Agent之所以能够趋动大模型，进行 <strong>思考-行动-观察行动结果-再思考-再行动-再观察</strong> 这个循环的核心秘密。有了这样的提示词，模型就会不停地思考、行动，直到模型判断出问题已经解决，给出最终答案，跳出循环。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e1d0551d616f798740bfcbc3da983b07.jpg"><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/5e9a4139170699bce6c2500d51332090.jpg"></p><p>那么，调用大模型之后，模型具体返回了什么结果呢？</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ddd55cb0bf32117a88e9f829d4906092.jpg"></p><p>在Debug过程中，我们发现调用模型之后的outputs中包含下面的内容。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">0: LLMResult(generations=[[Generation(text=' I need to find the current market price of roses and then calculate the new price with a 15% markup.\nAction: Search\nAction Input: "Average price of roses"', generation_info=&#123;'finish_reason': 'stop', 'logprobs': None&#125;)]],llm_output=&#123;'token_usage': &#123;'completion_tokens': 36, 'total_tokens': 294, 'prompt_tokens': 258&#125;, 'model_name': 'text-davinci-003'&#125;, run=None)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>把上面的内容拆解如下：</p><blockquote><p>‘text’: ’ I need to find the current market price of roses and then calculate the new price with a 15% markup.\n （Text：问题文本）</p><p>Action: Search\n （行动：搜索）</p><p>Action Input: “Average price of roses”’ （行动的输入：搜索玫瑰平均价格）</p></blockquote><p>看来，模型知道面对这个问题， <strong>它自己根据现有知识解决不了，下一步行动是需要选择工具箱中的搜索工具</strong>。而此时，命令行中也输出了模型的第一步计划——调用搜索工具。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/406abee8ce87448bf62110574cb84680.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/35510020fd06f7ec0e7572977ab14a8c.jpg"></p><p>现在模型知道了要调用什么工具，第一轮的Plan部分就结束了。下面，我们就来到了AgentExecutor 的_take_next_step 的 <strong>工具调用部分。</strong></p><p>在这里，因为模型返回了Action为Search，OutputParse解析了这个结果之后，LangChain很清楚地知道，Search工具会被调用。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/2f9ef860f7f5b68d1c880c4d088b8c47.jpg"></p><p>工具调用完成之后，我们就拥有了一个对当前工具调用的 <strong>Observation</strong>，也就是当前工具调用的结果。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/40c6796eca3448f16e3d57987f9b4413.jpg"></p><p>下一步，我们要再次调用大模型，形成新的 <strong>Thought</strong>，看看任务是否已经完成了，或者仍需要再次调用工具（新的工具或者再次调用同一工具）。</p><h3 id="第二轮思考：模型决定计算"><a href="#第二轮思考：模型决定计算" class="headerlink" title="第二轮思考：模型决定计算"></a>第二轮思考：模型决定计算</h3><p>因为任务尚未完成，第二轮思考开始，程序重新进入了Plan环节。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/14422cdffed757470b2a60d0d0f325c6.jpg"></p><p>此时，LangChain的LLM Chain根据目前的input，也就是历史对话记录生成了新的提示信息。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d0d604056dc85b2b570651d084099801.jpg"></p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">0: StringPromptValue(text='Answer the following questions as best you can. You have access to the following tools:\n\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\nCalculator: Useful for when you need to answer questions about math.\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Search, Calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: 目前市场上玫瑰花的平均价格是多少？如果我在此基础上加价15%卖出，应该如何定价？\nThought: I need to find the current market price of roses and then calculate the new price with a 15% markup.\nAction: Search\nAction Input: "Average price of roses"\nObservation: The average price for a dozen roses in the U.S. is $80.16. The state where a dozen roses cost the most is Hawaii at $108.33. That\'s 35% more expensive than the national average. A dozen roses are most affordable in Pennsylvania, costing $66.15 on average.\nThought:<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>我们再来拆解一下这个prompt。</p><blockquote><p>0: StringPromptValue(text&#x3D;’Answer the following questions as best you can. You have access to the following tools:\n\n</p></blockquote><p>这句提示是让模型尽量回答问题，并告诉模型拥有哪些工具。</p><blockquote><p>Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\n</p></blockquote><p>这是向模型介绍第一个工具：搜索。</p><blockquote><p>Calculator: Useful for when you need to answer questions about math.\n\n</p></blockquote><p>这是向模型介绍第二个工具：计算器。</p><blockquote><p>Use the following format:\n\n （指导模型使用下面的格式）</p><p>Question: the input question you must answer\n （问题）</p><p>Thought: you should always think about what to do\n （思考）</p><p>Action: the action to take, should be one of [Search, Calculator]\n （行动）</p><p>Action Input: the input to the action\n （行动的输入）</p><p>Observation: the result of the action\n… （观察：行动的返回结果）</p><p>(this Thought&#x2F;Action&#x2F;Action Input&#x2F;Observation can repeat N times)\n （上面这个过程可以重复多次）</p><p>Thought: I now know the final answer\n （思考：现在我知道最终答案了）</p><p>Final Answer: the final answer to the original input question\n\n （最终答案）</p></blockquote><p>上面是一段比较细节的解释说明，看一下括号中的文字。</p><blockquote><p>Begin!\n\n</p></blockquote><p>现在开始！</p><blockquote><p>Question: 目前市场上玫瑰花的平均价格是多少？如果我在此基础上加价15%卖出，应该如何定价？\n</p></blockquote><p>具体问题，也就是具体任务。</p><p>这句之前的提示，与我们在第一轮思考时看到的完全相同。</p><blockquote><p>Thought: I need to find the current market price of roses and then calculate the new price with a 15% markup.\n （思考：我需要找到玫瑰花的价格，并加入15%的加价）</p><p>Action: Search\nAction （行动：搜索）</p><p>Input: “Average price of roses”\n （行动的输入：玫瑰花的平均价格）</p><p>Observation: The average price for a dozen roses in the U.S. is $80.16. The state where a dozen roses cost the most is Hawaii at $108.33. That’s 35% more expensive than the national average. A dozen roses are most affordable in Pennsylvania, costing $66.15 on average.\n （观察：这里时搜索工具返回的玫瑰花价格信息）</p></blockquote><blockquote><p>Thought:’</p></blockquote><p>思考：后面是大模型应该进一步推理的内容。</p><p>大模型根据上面这个提示，返回了下面的output信息。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/a05de0f3b03182a7fc6428f307401c9d.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/cd2a2730736af44a3842b80f50yyfdce.jpg"></p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">AgentAction(tool='Calculator', tool_input='80.16 * 1.15', log=' I need to calculate the new price with a 15% markup.\nAction: Calculator\nAction Input: 80.16 * 1.15')<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个输出显示，模型告诉自己，“我需要计算新的Price，在搜索结果的基础上加价15%”，并确定Action为计算器，输入计算器工具的指令为80.16*1.15。 <strong>这是一个非常有逻辑性的思考。</strong></p><p>经过解析之后的Thought在命令行中的输出如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ee390d3baa268d97cc9c4b4c8cbc0076.jpg"></p><p>有了上面的Thought做指引，AgentExecutor调用了第二个工具：LLMMath。现在开始计算。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/71a3861377d741bce1379c2a9cd9eee1.jpg"></p><p>因为这个数学工具也是调用LLM，我们可以看一下内部的提示，看看这个工具是怎样指导LLM做数学计算的。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c9bc03e84ddf6966a819c4a221becee4.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f8a4e1ff0744829f4ce3157da35b8ef3.jpg"></p><p>这个提示，我把它拷贝出来，也拆解一下。</p><blockquote><p>0: StringPromptValue(text&#x3D;’Translate a math problem into a expression that can be executed using Python’s numexpr library. Use the output of running this code to answer the question.\n\n</p></blockquote><p>指定模型用Python的数学库来编程解决数学问题，而不是自己计算。这就规避了大模型数学推理能力弱的局限。</p><blockquote><p>Question: ${Question with math problem.}\n （问题）</p><p>text\n${single line mathematical expression that solves the problem} n```\n （问题的数学描述）</p><p>…numexpr.evaluate(text)…\n``` （通过Python库运行问题的数学描述）</p><p>output\n${Output of running the code}\n```\n （输出的Python代码运行结果）</p><p>Answer: ${Answer}\n\n （问题的答案）</p><p>Begin.\n\n （开始）</p></blockquote><p>从这里开始是两个数学式的解题示例。</p><blockquote><p>Question: What is 37593 * 67?\n</p><p>```text\n37593 * 67\n```</p><p>\n…numexpr.evaluate(“37593 * 67”)…\n</p><p>```output\n2518731\n```\n</p><p>Answer: 2518731\n\n</p><p>Question: 37593^(1&#x2F;5)\n</p><p>```text\n37593**(1&#x2F;5)\n```\n…</p><p>numexpr.evaluate(“37593**(1&#x2F;5)”)…\n</p><p>```output\n8.222831614237718\n```\n</p><p>Answer: 8.222831614237718\n\n</p></blockquote><p>两个数学式的解题示例结束。</p><blockquote><p>Question: 80.16 * 1.15\n’)</p></blockquote><p>这里是玫瑰花问题的具体描述。</p><p>下面，就是模型返回结果。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f43a49f3c6592a94be0d6ffa0bfa99c4.jpg"></p><p>在LLMChain内部，根据Python代码进行了计算，因此final_ansewer也已经算好了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/b2bd0d29d9368164eyy9yyf038b974d8.jpg"></p><p>至此，LangChain的BaseTool返回的Observation如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/86841a3b2d90ecbd686c54fc25ddf703.jpg"></p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">observation'Answer: 92.18399999999998'<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/823eyy2ed415d7ab87f1e411d9172525.jpg"></p><p>命令行中也输出了当前数学工具调用后的Observation结果：92.18。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/1d07de9cb80bf06e64d6c4fe49391b7f.jpg"></p><h3 id="第三轮思考：模型完成任务"><a href="#第三轮思考：模型完成任务" class="headerlink" title="第三轮思考：模型完成任务"></a>第三轮思考：模型完成任务</h3><p>第三轮思考开始。此时，Executor的Plan应该进一步把当前的新结果传递给大模型，不出所料的话，大模型应该有足够的智慧判断出任务此时已经成功地完成了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ea36a2be7da26f303a966218d530ac65.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/92c28162c1bf5c2c525f6204658f6438.jpg"></p><p>下面是目前最新的 prompt。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">0: StringPromptValue(text='Answer the following questions as best you can. You have access to the following tools:\n\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\nCalculator: Useful for when you need to answer questions about math.\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Search, Calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: 目前市场上玫瑰花的平均价格是多少？如果我在此基础上加价15%卖出，应该如何定价？\nThought: I need to find the current market price of roses and then calculate the new price with a 15% markup.\nAction: Search\nAction Input: "Average price of roses"\nObservation: The average price for a dozen roses in the U.S. is $80.16. The state where a dozen roses cost the most is Hawaii at $108.33. That\'s 35% more expensive than the national average. A dozen roses are most affordable in Pennsylvania, costing $66.15 on average.\nThought: I need to calculate the new price with a 15% markup.\nAction: Calculator\nAction Input: 80.16 * 1.15\nObservation: Answer: 92.18399999999998\nThought:')<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>我们再来拆解一下这个最终的prompt。</p><blockquote><p>0: StringPromptValue(text&#x3D;’Answer the following questions as best you can. You have access to the following tools:\n\n</p></blockquote><p>这句提示是让模型尽量回答问题，并告诉模型拥有哪些工具。</p><blockquote><p>Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\n</p></blockquote><p>这是向模型介绍第一个工具：搜索。</p><blockquote><p>Calculator: Useful for when you need to answer questions about math.\n\n</p></blockquote><p>这是向模型介绍第二个工具：计算器。</p><blockquote><p>Use the following format:\n\n （指导模型使用下面的格式）</p><p>Question: the input question you must answer\n （问题）</p><p>Thought: you should always think about what to do\n （思考）</p><p>Action: the action to take, should be one of [Search, Calculator]\n （行动）</p><p>Action Input: the input to the action\n （行动的输入）</p><p>Observation: the result of the action\n… （观察：行动的返回结果）</p><p>(this Thought&#x2F;Action&#x2F;Action Input&#x2F;Observation can repeat N times)\n （上面这个过程可以重复多次）</p><p>Thought: I now know the final answer\n （思考：现在我知道最终答案了）</p><p>Final Answer: the final answer to the original input question\n\n （最终答案）</p></blockquote><p>仍然是比较细节的说明，看括号文字。</p><blockquote><p>Begin!\n\n</p></blockquote><p>现在开始！</p><blockquote><p>Question: 目前市场上玫瑰花的平均价格是多少？如果我在此基础上加价15%卖出，应该如何定价？\n</p></blockquote><p>具体问题，也就是具体任务。</p><blockquote><p>Thought: I need to find the current market price of roses and then calculate the new price with a 15% markup.\n （思考：我需要找到玫瑰花的价格，并加入15%的加价）</p><p>Action: Search\nAction （行动：搜索）</p><p>Input: “Average price of roses”\n （行动的输入：玫瑰花的平均价格）</p><p>Observation: The average price for a dozen roses in the U.S. is $80.16. The state where a dozen roses cost the most is Hawaii at $108.33. That’s 35% more expensive than the national average. A dozen roses are most affordable in Pennsylvania, costing $66.15 on average.\n （观察：这里时搜索工具返回的玫瑰花价格信息）</p></blockquote><p>这句之前的提示，与我们在第二轮思考时看到的完全相同。</p><blockquote><p>Thought: I need to calculate the new price with a 15% markup.\n （思考：我需要计算玫瑰花15%的加价）</p><p>Action: Calculator\n （行动：计算器工具）</p><p>Action Input: 80.16 * 1.15\n （行动输入：一个数学式）</p><p>Observation: Answer: 92.18399999999998\n （观察：计算得到的答案）</p><p>Thought:’ （思考）</p></blockquote><p>可见，每一轮的提示都跟随着模型的思维链条，逐步递进，逐步完善。环环相扣，最终结果也就呼之欲出了。</p><p>继续Debug，发现模型在这一轮思考之后的输出中终于包含了 “ **I now know the final answer.**”，这说明模型意识到任务已经成功地完成了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/7d94a6yy78f9858ff53db67011e9a615.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/a0f4febfb2ea1978ff8fafyyb52c6920.jpg"></p><p>此时，AgentExcutor的plan方法返回一个 <strong>AgentFinish</strong> <strong>实例</strong>，这表示代理经过对输出的检查，其内部逻辑判断出任务已经完成，思考和行动的循环要结束了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/2234d186828cb5c7d27ca89e3d356aac.jpg"></p><p>至此，整个链条完成，AgentExecutor 的任务结束。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/45e60142ebfa7da7a93c73edd54a18af.jpg"></p><p>在命令行中，模型输出 <code>Thought: I now know the final answer. </code>（我已经知道最终的答案）。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/3bc53515c70d4f2d03b7a82599cc1612.jpg"></p><p>最终答案：玫瑰的平均价格是 80.16 美元，加价15%后，是 92.18 美元。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>这一课中，我们深入到AgentExecutor的代码内部，深挖其运行机制，了解了AgentExecutor是如何通过计划和工具调用，一步一步完成Thought、Action和Observation的。</p><p>如果我们审视一下AgentExecutor 的代码实现，会发现AgentExecutor这个类是作为链（Chain）而存在，同时也为代理执行各种工具，完成任务。它会接收代理的计划，并执行代理思考链路中每一步的行动。</p><p>AgentExecutor中最重要的方法是步骤处理方法，_take_next_step方法。它用于在思考-行动-观察的循环中采取单步行动。先调用代理的计划，查找代理选择的工具，然后使用选定的工具执行该计划（此时把输入传给工具），从而获得观察结果，然后继续思考，直到输出是 AgentFinish 类型，循环才会结束。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li><p>请你在 <a href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/agents/agent.py">agent.py</a> 文件中找到AgentExecutor类。</p></li><li><p>请你在AgentExecutor类中找到_take_next_step方法，对应本课的内容，分析AgentExecutor类是怎样实现Plan和工具调用的。</p></li></ol><p>期待在留言区看到你的分享，如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h1 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h1><ol><li>代码，AgentExecutor类的 <a href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/agents/agent.py">实现细节</a></li><li>代码，LLMChain类的 <a href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/llm.py">实现细节</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;12｜代理（中）：AgentExecutor究竟是怎样驱动模型和工具完成任务的？&quot;&gt;&lt;a href=&quot;#12｜代理（中）：AgentExecutor究竟是怎样驱动模型和工具完成任务的？&quot; class=&quot;headerlink&quot; title=&quot;12｜代理（中）：Age</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>13｜代理（下）：结构化工具对话、Self-Ask with Search以及Plan and execute代理</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1457749217.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1457749217.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.453Z</updated>
    
    <content type="html"><![CDATA[<h1 id="13｜代理（下）：结构化工具对话、Self-Ask-with-Search以及Plan-and-execute代理"><a href="#13｜代理（下）：结构化工具对话、Self-Ask-with-Search以及Plan-and-execute代理" class="headerlink" title="13｜代理（下）：结构化工具对话、Self-Ask with Search以及Plan and execute代理"></a>13｜代理（下）：结构化工具对话、Self-Ask with Search以及Plan and execute代理</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>在上一讲中，我们深入LangChain程序内部机制，探索了AgentExecutor究竟是如何思考（Thought）、执行（Execute&#x2F;Act）和观察（Observe）的，这些步骤之间的紧密联系就是代理在推理（Reasoning）和工具调用过程中的“生死因果”。</p><p>现在我们趁热打铁，再学习几种更为复杂的代理：Structured Tool Chat（结构化工具对话）代理、Self-Ask with Search（自主询问搜索）代理、Plan and execute（计划与执行） 代理。</p><h2 id="什么是结构化工具"><a href="#什么是结构化工具" class="headerlink" title="什么是结构化工具"></a>什么是结构化工具</h2><p>LangChain的第一个版本是在 2022 年 11 月推出的，当时的设计是基于 ReAct 论文构建的，主要围绕着代理和工具的使用，并将二者集成到提示模板的框架中。</p><p>早期的工具使用十分简单，AgentExecutor引导模型经过推理调用工具时，仅仅能够生成两部分内容：一是工具的名称，二是输入工具的内容。而且，在每一轮中，代理只被允许使用一个工具，并且输入内容只能是一个简单的字符串。这种简化的设计方式是为了让模型的任务变得更简单，因为进行复杂的操作可能会使得执行过程变得不太稳定。</p><p>不过，随着语言模型的发展，尤其是出现了如 gpt-3.5-turbo 和 GPT-4 这样的模型，推理能力逐渐增强，也为代理提供了更高的稳定性和可行性。这就使得 LangChain 开始考虑放宽工具使用的限制。</p><p>2023年初，LangChain 引入了“多操作”代理框架，允许代理计划执行多个操作。在此基础上，LangChain 推出了结构化工具对话代理，允许更复杂、多方面的交互。通过指定AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION 这个代理类型，代理能够调用包含一系列复杂工具的“ <strong>结构化工具箱</strong>”，组合调用其中的多个工具，完成批次相关的任务集合。</p><p>举例来说，结构化工具的示例包括：</p><ol><li>文件管理工具集：支持所有文件系统操作，如写入、搜索、移动、复制、列目录和查找。</li><li>Web 浏览器工具集：官方的 PlayWright 浏览器工具包，允许代理访问网站、点击、提交表单和查询数据。</li></ol><p>下面，我们就以 PlayWright 工具包为例，来实现一个结构化工具对话代理。</p><p>先来看一看什么是 PlayWright 工具包。</p><h2 id="什么是-Playwright"><a href="#什么是-Playwright" class="headerlink" title="什么是 Playwright"></a>什么是 Playwright</h2><p>Playwright是一个开源的自动化框架，它可以让你模拟真实用户操作网页，帮助开发者和测试者自动化网页交互和测试。用简单的话说，它就像一个“机器人”，可以按照你给的指令去浏览网页、点击按钮、填写表单、读取页面内容等等，就像一个真实的用户在使用浏览器一样。</p><p>Playwright支持多种浏览器，比如Chrome、Firefox、Safari等，这意味着你可以用它来测试你的网站或测试应用在不同的浏览器上的表现是否一致。</p><p>下面我们先用 <code>pip install playwright</code> 安装Playwright工具。</p><p>不过，如果只用pip安装Playwright工具安装包，就使用它，还不行，会得到下面的信息。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/5cb10de270599b427a4efa9655ceb1c7.jpg"></p><p>因此我们还需要通过 <code>playwright install</code> 命令来安装三种常用的浏览器工具。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/335f98d28232d1a7160f1d48f334d56d.jpg"></p><p>现在，一切就绪，我们可以通过Playwright浏览器工具来访问一个测试网页。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from playwright.sync_api import sync_playwrightdef run():    # 使用Playwright上下文管理器    with sync_playwright() as p:        # 使用Chromium，但你也可以选择firefox或webkit        browser = p.chromium.launch()        # 创建一个新的页面        page = browser.new_page()        # 导航到指定的URL        page.goto('https://langchain.com/')        # 获取并打印页面标题        title = page.title()        print(f"Page title is: &#123;title&#125;")        # 关闭浏览器        browser.close()if __name__ == "__main__":    run()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个简单的Playwright脚本，它打开了一个新的浏览器实例。过程是：导航到指定的URL；获取页面标题并打印页面的标题；最后关闭浏览器。</p><p>输出如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">Page title is: LangChain<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个脚本展示了Playwright的工作方式，一切都是在命令行里面直接完成。它不需要我们真的去打开Chome网页，然后手工去点击菜单栏、拉动进度条等。</p><p>下面这个表，我列出了使用命令行进行自动化网页测试的优势。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/0a5909f879b043b5f17d7c8ea5a88a20.jpg"></p><p>现在你了解了Playwright这个工具包的基本思路，下面我们就开始使用它来作为工具集，来实现结构化工具对话代理。</p><h2 id="使用结构化工具对话代理"><a href="#使用结构化工具对话代理" class="headerlink" title="使用结构化工具对话代理"></a>使用结构化工具对话代理</h2><p>在这里，我们要使用的Agent类型是STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION。要使用的工具则是PlayWrightBrowserToolkit，这是LangChain中基于PlayWrightBrowser包封装的工具箱，它继承自 BaseToolkit类。</p><p>PlayWrightBrowserToolkit 为 PlayWright 浏览器提供了一系列交互的工具，可以在同步或异步模式下操作。</p><p>其中具体的工具就包括：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ce51ayya392733c6b55ec3568caaac46.jpg"></p><p>下面，我们就来看看结构化工具对话代理是怎样通过组合调用PlayWrightBrowserToolkit中的各种工具，自动完成我们交给它的任务。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.agents.agent_toolkits import PlayWrightBrowserToolkitfrom langchain.tools.playwright.utils import create_async_playwright_browserasync_browser = create_async_playwright_browser()toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)tools = toolkit.get_tools()print(tools)from langchain.agents import initialize_agent, AgentTypefrom langchain.chat_models import ChatAnthropic, ChatOpenAI# LLM不稳定，对于这个任务，可能要多跑几次才能得到正确结果llm = ChatOpenAI(temperature=0.5)agent_chain = initialize_agent(    tools,    llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,)async def main():    response = await agent_chain.arun("What are the headers on python.langchain.com?")    print(response)import asyncioloop = asyncio.get_event_loop()loop.run_until_complete(main())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这个示例中，我们询问大模型，网页python.langchain.com中有哪些标题目录？</p><p>很明显，大模型不可能包含这个网页的内部信息，因为ChatGPT完成训练的那一年（2021年9月），LangChain还不存在。因此，大模型不可避免地需要通过PlayWrightBrowser工具来解决问题。</p><h3 id="第一轮思考"><a href="#第一轮思考" class="headerlink" title="第一轮思考"></a>第一轮思考</h3><p>代理进入AgentExecutor Chain之后的第一轮思考如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/6a5718eef084ac988a23e5488e967302.jpg"></p><p>这里，我对上述思考做一个具体说明。</p><blockquote><p>I can use the “navigate_browser” tool to visit the website and then use the “get_elements” tool to retrieve the headers. Let me do that.</p></blockquote><p>这是第一轮思考，大模型知道自己没有相关信息，决定使用PlayWrightBrowserToolkit工具箱中的 navigate_browser 工具。</p><blockquote><p>Action:```{“action”: “navigate_browser”, “action_input”: {“url”: “ <a href="https://python.langchain.com/">https://python.langchain.com</a>”}}```</p></blockquote><p>行动：通过Playwright浏览器访问这个网站。</p><blockquote><p>Observation: Navigating to <a href="https://python.langchain.com/">https://python.langchain.com</a> returned status code 200</p></blockquote><p>观察：成功得到浏览器访问的返回结果。</p><p>在第一轮思考过程中，模型决定使用PlayWrightBrowserToolkit中的navigate_browser工具。</p><h3 id="第二轮思考"><a href="#第二轮思考" class="headerlink" title="第二轮思考"></a>第二轮思考</h3><p>下面是大模型的第二轮思考。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/663de1fda23de782af9233328ca5c2e5.jpg"></p><p>还是对上述思考做一个具体说明。</p><blockquote><p>Thought:Now that I have successfully navigated to the website, I can use the “get_elements” tool to retrieve the headers. I will specify the CSS selector for the headers and retrieve their text.</p></blockquote><p>第二轮思考：模型决定使用PlayWrightBrowserToolkit工具箱中的另一个工具 get_elements，并且指定CSS selector只拿标题的文字。</p><blockquote><p>Action: ```{“action”: “get_elements”, “action_input”: {“selector”: “h1, h2, h3, h4, h5, h6”, “attributes”: [“innerText”]}}```</p></blockquote><p>行动：用Playwright的 get_elements 工具去拿网页中各级标题的文字。</p><blockquote><p>Observation: [{“innerText”: “Introduction”}, {“innerText”: “Get started​”}, {“innerText”: “Modules​”}, {“innerText”: “Model I&#x2F;O​”}, {“innerText”: “Data connection​”}, {“innerText”: “Chains​”}, {“innerText”: “Agents​”}, {“innerText”: “Memory​”}, {“innerText”: “Callbacks​”}, {“innerText”: “Examples, ecosystem, and resources​”}, {“innerText”: “Use cases​”}, {“innerText”: “Guides​”}, {“innerText”: “Ecosystem​”}, {“innerText”: “Additional resources​”}, {“innerText”: “Support”}, {“innerText”: “API reference​”}]</p></blockquote><p>观察：成功地拿到了标题文本。</p><p>在第二轮思考过程中，模型决定使用PlayWrightBrowserToolkit中的get_elements工具。</p><h3 id="第三轮思考"><a href="#第三轮思考" class="headerlink" title="第三轮思考"></a>第三轮思考</h3><p>下面是大模型的第三轮思考。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/01e427d582973da438c67940f132166d.jpg"></p><p>对上述思考做一个具体说明。</p><blockquote><p>Thought:The headers on python.langchain.com are:</p><ol><li><p>Introduction</p><p>… …</p></li><li><p>API reference</p></li></ol></blockquote><p>第三轮思考：模型已经找到了网页中的所有标题。</p><blockquote><p>Action:</p><pre class="line-numbers language-none"><code class="language-none">&#123;  &quot;action&quot;: &quot;Final Answer&quot;,  &quot;action_input&quot;: &quot;The headers on python.langchain.com are: 1. Introduction 2. Get started 3. Modules 4. Model I&#x2F;O 5. Data connection 6. Chains 7. Agents 8. Memory 9. Callbacks 10. Examples, ecosystem, and resources 11. Use cases 12. Guides 13. Ecosystem 14. Additional resources 15. Support 16. API reference&quot;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></blockquote><p>行动：给出最终答案。</p><p>AgentExecutor Chain结束之后，成功输出python.langchain.com这个页面中各级标题的具体内容。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/961a5c0cc2b9c19d7147b2120608a663.jpg"></p><p>在这个过程中，结构化工具代理组合调用了Playwright工具包中的两种不同工具，自主完成了任务。</p><h2 id="使用-Self-Ask-with-Search-代理"><a href="#使用-Self-Ask-with-Search-代理" class="headerlink" title="使用 Self-Ask with Search 代理"></a>使用 Self-Ask with Search 代理</h2><p>讲完了Structured Tool Chat代理，我们再来看看Self-Ask with Search代理。</p><p>Self-Ask with Search 也是LangChain中的一个有用的代理类型（SELF_ASK_WITH_SEARCH）。它利用一种叫做 “Follow-up Question（追问）”加“Intermediate Answer（中间答案）”的技巧，来辅助大模型寻找事实性问题的过渡性答案，从而引出最终答案。</p><p>这是什么意思？让我通过示例来给你演示一下，你就明白了。在这个示例中，我们使用SerpAPIWrapper作为工具，用OpenAI作为语言模型，创建Self-Ask with Search代理。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain import OpenAI, SerpAPIWrapperfrom langchain.agents import initialize_agent, Toolfrom langchain.agents import AgentTypellm = OpenAI(temperature=0)search = SerpAPIWrapper()tools = [    Tool(        name="Intermediate Answer",        func=search.run,        description="useful for when you need to ask with search",    )]self_ask_with_search = initialize_agent(    tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)self_ask_with_search.run(    "使用玫瑰作为国花的国家的首都是哪里?")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>该代理对于这个问题的输出如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/dd6dcfa6c90384abc80640fe5ea1850d.jpg"></p><p>其实，细心的你可能会发现，“ **使用玫瑰作为国花的国家的首都是哪里?**”这个问题不是一个简单的问题，它其实是一个多跳问题—— <strong>在问题和最终答案之间，存在中间过程</strong>。</p><p>多跳问题（Multi-hop question）是指为了得到最终答案，需要进行多步推理或多次查询。这种问题不能直接通过单一的查询或信息源得到答案，而是需要跨越多个信息点，或者从多个数据来源进行组合和整合。</p><p>也就是说，问题的答案依赖于另一个子问题的答案，这个子问题的答案可能又依赖于另一个问题的答案。这就像是一连串的问题跳跃，对于人类来说，解答这类问题可能需要从不同的信息源中寻找一系列中间答案，然后结合这些中间答案得出最终结论。</p><p>“使用玫瑰作为国花的国家的首都是哪里？”这个问题并不直接询问哪个国家使用玫瑰作为国花，也不是直接询问英国的首都是什么。而是先要推知使用玫瑰作为国花的国家（英国）之后，进一步询问这个国家的首都。这就需要多跳查询。</p><p>为什么 Self-Ask with Search 代理适合解决多跳问题呢？有下面几个原因。</p><ol><li><strong>工具集合</strong>：代理包含解决问题所必须的搜索工具，可以用来查询和验证多个信息点。这里我们在程序中为代理武装了SerpAPIWrapper工具。</li><li><strong>逐步逼近</strong>：代理可以根据第一个问题的答案，提出进一步的问题，直到得到最终答案。这种逐步逼近的方式可以确保答案的准确性。</li><li><strong>自我提问与搜索</strong>：代理可以自己提问并搜索答案。例如，首先确定哪个国家使用玫瑰作为国花，然后确定该国家的首都是什么。</li><li><strong>决策链</strong>：代理通过一个决策链来执行任务，使其可以跟踪和处理复杂的多跳问题，这对于解决需要多步推理的问题尤为重要。</li></ol><p>在上面的例子中，通过大模型的两次follow-up追问，搜索工具给出了两个中间答案，最后给出了问题的最终答案——伦敦。</p><h2 id="使用-Plan-and-execute-代理"><a href="#使用-Plan-and-execute-代理" class="headerlink" title="使用 Plan and execute 代理"></a>使用 Plan and execute 代理</h2><p>在这节课的最后，我再给你介绍一种比较新的代理类型：Plan and execute 代理。</p><p>计划和执行代理通过首先计划要做什么，然后执行子任务来实现目标。这个想法是受到 <a href="https://arxiv.org/abs/2305.04091">Plan-and-Solve</a> 论文的启发。论文中提出了计划与解决（Plan-and-Solve）提示。它由两部分组成：首先，制定一个计划，并将整个任务划分为更小的子任务；然后按照该计划执行子任务。</p><p>这种代理的独特之处在于，它的计划和执行不再是由同一个代理所完成，而是：</p><ul><li>计划由一个大语言模型代理（负责推理）完成。</li><li>执行由另一个大语言模型代理（负责调用工具）完成。</li></ul><p>因为这个代理比较新，它隶属于LangChain的实验包langchain_experimental，所以你需要先安装langchain_experimental这个包。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">pip install -U langchain langchain_experimental<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>下面我们来使用一下这个代理。在这里，我们创建了Plan and execute代理，这个代理和之前看到的代理不同，它有一个Planner，有一个Executor，它们可以是不同的模型。</p><p>当然，在这个示例中，我们都使用了ChatOpenAI模型。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.chat_models import ChatOpenAIfrom langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_plannerfrom langchain.llms import OpenAIfrom langchain import SerpAPIWrapperfrom langchain.agents.tools import Toolfrom langchain import LLMMathChainsearch = SerpAPIWrapper()llm = OpenAI(temperature=0)llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)tools = [    Tool(        name = "Search",        func=search.run,        description="useful for when you need to answer questions about current events"    ),    Tool(        name="Calculator",        func=llm_math_chain.run,        description="useful for when you need to answer questions about math"    ),]model = ChatOpenAI(temperature=0)planner = load_chat_planner(model)executor = load_agent_executor(model, tools, verbose=True)agent = PlanAndExecute(planner=planner, executor=executor, verbose=True)agent.run("在纽约，100美元能买几束玫瑰?")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/fd28e5f19a6a8b8ef9c4d68b3e5c0d38.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/8ea16266717acf88a2fedb72283744b2.jpg"></p><p>在上面输出中，PlanAndExecute 链的调用流程以及代理的思考过程，我就留给你来做分析了，相信你可以把握住Plan and execute代理解决问题的基本脉络。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>这节课是Agent的最后一课，也是LangChain所有基础知识的最后一课。我给你总结了两张的表。</p><p>第一个表，是LangChain中常见的代理类型和它们的介绍。在这些代理中，有很多我们已经一起使用过了，有些则需要你自己去阅读相关文档，自己去探索它的使用方法。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ee248367eef96616690831498519eeae.jpg"></p><p>第二个表，是我对LangChain各个组件的一个简明总结。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e26993dd3957bfd2947424abb9de7cde.png"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/577333985abb70b890d94bf99fe58ed9.jpg"></p><p>上面这个图片，相信此时你已经不再陌生了，也掌握了它们的精髓所在。</p><p>最后还有一个问题值得讲一讲，就是图中的 Indexes，到底是什么，其实这个Indexes是LangChang早期版本的一个组件，现在已经被整合到Retrieval（数据检索）这个单元中了。而Retrieval（包括Indexes），讲的其实就是如何把离散的文档及其他信息做嵌入，存储到向量数据库中，然后再提取的过程。这个过程我们在 <a href="https://time.geekbang.org/column/article/699451">第3课</a> 已经讲过，在后面的课程中还会再深入介绍。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e3yyf61d8ccc0b2ba47a76dfc1fdf190.jpg"></p><p>此外，在LangChain文档中，新的6大组件中其实还有一个模块——Callbacks，目前我们尚未涉及，在后续的课程中也会介绍。</p><p>好了，LangChain的基础知识就讲到这里，从下节课起，我们将整合以前学过的各个组件的内容，为你讲解更多偏重具体应用的内容。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>在结构化工具对话代理的示例中，请你打印出PlayWrightBrowserToolkit中的所有具体工具名称的列表。</li></ol><p>提示：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">tools = toolkit.get_tools()print(tools)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol><li>在Plan and execute代理的示例中，请你分析PlanAndExecute、AgentExecutor和LLMMathChain链的调用流程以及代理的思考过程。</li></ol><p>期待在留言区看到你的分享，如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>代码 Github <a href="https://github.com/microsoft/playwright">Playwright</a> 工具包</li><li>论文 <a href="https://arxiv.org/abs/2305.04091">“计划与解决”提示：通过大型语言模型改进Zero-Shot链式思考推理</a> Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., &amp; Lim, E.-P. (2023). Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. arXiv preprint arXiv:2305.04091.</li></ol><h2 id="放假通知"><a href="#放假通知" class="headerlink" title="放假通知"></a>放假通知</h2><p>相信细心的同学已经发现了，我们这个专栏的更新节奏还是很快的，前面的内容基本接近工作日日更。从内容的重要程度来说，基础篇其实相当重要，值此中秋&amp;国庆双节长假来临之际，希望大家能好好休息，也能空出一段时间好好复习前面所学，所以我们的专栏计划停更一周，10月9日恢复正常更新，也期待你能把前面的思考题都做一做，我会在留言区等你的分享，与你交流探讨。最后祝大家小长假愉快，中秋阖家团圆！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;13｜代理（下）：结构化工具对话、Self-Ask-with-Search以及Plan-and-execute代理&quot;&gt;&lt;a href=&quot;#13｜代理（下）：结构化工具对话、Self-Ask-with-Search以及Plan-and-execute代理&quot; clas</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>14｜工具和工具箱：LangChain中的Tool和Toolkits一览</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3752167682.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3752167682.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.457Z</updated>
    
    <content type="html"><![CDATA[<h1 id="14｜工具和工具箱：LangChain中的Tool和Toolkits一览"><a href="#14｜工具和工具箱：LangChain中的Tool和Toolkits一览" class="headerlink" title="14｜工具和工具箱：LangChain中的Tool和Toolkits一览"></a>14｜工具和工具箱：LangChain中的Tool和Toolkits一览</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>这节课我们来一起看一看LangChain中各种强大的工具（Tool），以及如何使用它们。</p><p>在之前的几节课中，我们深入讲解了LangChain中的代理。未来的AI Agent，应该就是以LLM为核心控制器的代理系统。而 <strong>工具，则是代理身上延展出的三头六臂，是代理的武器，代理通过工具来与世界进行交互，控制并改造世界</strong>。</p><h2 id="工具是代理的武器"><a href="#工具是代理的武器" class="headerlink" title="工具是代理的武器"></a>工具是代理的武器</h2><p>LangChain之所以强大，第一是大模型的推理能力强大，第二则是工具的执行能力强大！孙猴子法力再强，没有金箍棒，也降伏不了妖怪。大模型再能思考，没有工具也不行。</p><p>工具是代理可以用来与世界交互的功能。这些工具可以是通用实用程序（例如搜索），也可以是其他链，甚至其他的代理。</p><p>那么到底什么是工具？在LangChain中，工具是如何发挥作用的？</p><p>LangChain通过提供一个统一的框架来集成功能的具体实现。在这个框架中，每个功能都被封装成一个工具。每个工具都有自己的输入和输出，以及处理这些输入和生成输出的方法。</p><p>当代理接收到一个任务时，它会根据任务的类型和需求，通过大模型的推理，来选择合适的工具处理这个任务。这个选择过程可以基于各种策略，例如基于工具的性能，或者基于工具处理特定类型任务的能力。</p><p>一旦选择了合适的工具，LangChain就会将任务的输入传递给这个工具，然后工具会处理这些输入并生成输出。这个输出又经过大模型的推理，可以被用作其他工具的输入，或者作为最终结果，被返回给用户。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ebcyyaccd79133c03f417c45c225d1b6.png"></p><p>通过这种方式，LangChain 大大延展了大模型的功能。大模型的推理，加上工具的调用，都集成在一个系统中，而这个系统可以处理多种类型的任务。这提高了系统的灵活性和可扩展性，也大大简化了开发者的工作。</p><h2 id="如何加载工具"><a href="#如何加载工具" class="headerlink" title="如何加载工具"></a>如何加载工具</h2><p>在程序中，可以使用以下代码片段加载工具。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.agents import load_toolstool_names = [...]tools = load_tools(tool_names)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>某些工具（例如链、代理）可能需要 LLM 来初始化它们。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.agents import load_toolstool_names = [...]llm = ...tools = load_tools(tool_names, llm=llm)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="LangChain-支持的工具一览"><a href="#LangChain-支持的工具一览" class="headerlink" title="LangChain 支持的工具一览"></a>LangChain 支持的工具一览</h2><p>下面，我给你列出目前LangChain中所支持的工具。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e2f8a0318b4f1da7f0e756e87761d95b.jpg"></p><p>当然这个列表随着时间的推移会越来越长，也就意味着LangChain的功能会越来越强大。</p><h2 id="使用-arXiv-工具开发科研助理"><a href="#使用-arXiv-工具开发科研助理" class="headerlink" title="使用 arXiv 工具开发科研助理"></a>使用 arXiv 工具开发科研助理</h2><p>其中有一些工具，比如SerpAPI，你已经用过了，这里我们再来用一下arXiv工具。arXiv本身就是一个论文研究的利器，里面的论文数量比AI顶会还早、还多、还全。那么把它以工具的形式集成到LangChain中，能让你在研究学术最新进展时如虎添翼。</p><blockquote><p>arXiv是一个提供免费访问的预印本库，供研究者在正式出版前上传和分享其研究工作。它成立于1991年，最初是作为物理学预印本数据库开始的，但后来扩展到了数学、计算机科学、生物学、经济学等多个领域。</p><p>预印本是研究者完成的、但尚未经过同行评议或正式出版的论文。Arxiv允许研究者上传这些预印本，使其他研究者可以在正式出版之前查看、评论和使用这些工作。这样，研究的发现可以更快地传播和分享，促进学术交流。</p></blockquote><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 设置OpenAI API的密钥import osos.environ["OPENAI_API_KEY"] = 'Your Key'# 导入库from langchain.chat_models import ChatOpenAIfrom langchain.agents import load_tools, initialize_agent, AgentType# 初始化模型和工具llm = ChatOpenAI(temperature=0.0)tools = load_tools(    ["arxiv"],)# 初始化链agent_chain = initialize_agent(    tools,    llm,    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,)# 运行链agent_chain.run("介绍一下2005.14165这篇论文的创新点?")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>首先，我们还是来研究一下ZERO_SHOT_REACT_DESCRIPTION这个Agent是怎么通过提示来引导模型调用工具的。</p><blockquote><p>“prompts”: [</p><p>    “Answer the following questions as best you can. You have access to the following tools:\n\n</p></blockquote><p>首先告诉模型，要尽力回答问题，但是可以访问下面的工具。</p><blockquote><p><strong>arxiv:</strong> A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n\n</p></blockquote><p>arxiv工具：一个围绕Arxiv.org的封装工具。当你需要回答关于物理学、数学、计算机科学、定量生物学、定量金融、统计学、电气工程和经济学的问题时，来自arxiv.org上的科学文章非常有用。同时还告诉模型：输入这个工具的内容应该是搜索查询。</p><blockquote><p>Use the following format:\n\n</p></blockquote><p>指导模型输出下面的内容。</p><blockquote><p>Question: the input question you must answer\n （问题：需要回答的问题）</p><p>Thought: you should always think about what to do\n （思考：应该总是思考下一步做什么）</p><p>Action: the action to take, should be one of [arxiv]\n （行动：从具体工具列表中选择行动——这里只有arxiv一个工具）</p><p>Action Input: the input to the action\n （行动的输入：输入工具的内容）</p><p>Observation: the result of the action\n… （观察：工具返回的结果）</p><p>(this Thought&#x2F;Action&#x2F;Action Input&#x2F;Observation can repeat N times)\n （上面 Thought&#x2F;Action&#x2F;Action Input&#x2F;Observation 的过程将重复N次）</p><p>Thought: I now know the final answer\n （现在我知道最终答案了）</p><p>Final Answer: the final answer to the original input question\n\n （原始问题的最终答案）</p></blockquote><blockquote><p>**Begin!**\n\n</p></blockquote><p>现在开始！</p><blockquote><p><strong>Question</strong>: ‘Chain-of-Thought Prompting Elicits Reasoning in Large Language Models’这篇论文的创新点\n</p></blockquote><p>真正的问题在此。</p><blockquote><p>Thought:”</p></blockquote><p>开始思考吧！</p><p>然后，我们来看看Chain的运行过程。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/6e1195d608d47fbe5b67131c1fe32357.jpg"></p><p>其中，代理的思考过程中的第一个返回结果如下：</p><blockquote><p>“text”: “ I need to read the paper to understand the innovation\n （思考：我需要阅读文章才能理解创新点）</p><p>Action: arxiv\n （行动：arxiv工具）</p><p>Action Input: ‘Chain-of-Thought Prompting Elicits Reasoning in Large Language Models’”, （行动的输入：论文的标题）</p></blockquote><p>因为在之前的提示中，LangChain告诉大模型，对于Arxiv工具的输入总是以搜索的形式出现，因此尽管我指明了论文的ID，Arxiv还是根据这篇论文的关键词搜索到了3篇相关论文的信息。</p><p>模型对这些信息进行了总结，认为信息已经完善，并给出了最终答案。</p><blockquote><p>Thought: I now know the final answer</p></blockquote><p>想法：我现在知道了最终答案。</p><blockquote><p>Final Answer: The innovation of the paper ‘Chain-of-Thought Prompting Elicits Reasoning in Large Language Models’ is the introduction of a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting, which significantly improves the ability of large language models to perform complex reasoning.”</p></blockquote><p>最终答案：这篇名为《链式思考提示促使大型语言模型进行推理》的论文的创新之处在于，引入了一种简单的方法，即链式思考提示，在提示中提供了一些链式思考的示例，这大大提高了大型语言模型执行复杂推理的能力。</p><h2 id="LangChain-中的工具箱一览"><a href="#LangChain-中的工具箱一览" class="headerlink" title="LangChain 中的工具箱一览"></a>LangChain 中的工具箱一览</h2><p>下面，我给你列出了目前LangChain中所支持的工具箱。每个工具箱中都有一系列工具。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c87be0638409b278c2657a66f45aa927.jpg"></p><h2 id="使用-Gmail-工具箱开发个人助理"><a href="#使用-Gmail-工具箱开发个人助理" class="headerlink" title="使用 Gmail 工具箱开发个人助理"></a>使用 Gmail 工具箱开发个人助理</h2><p>刚才，你使用了arXiv工具帮助你做了一些科研工作。你当然还希望你的AI Agent能够成为你的全能自动助理，你开发出的智能应用应该能帮你检查邮件、写草稿，甚至发邮件、写文档，对吧？</p><p>上面这一切的一切，LangChain当然能够安排上！</p><ul><li>通过Gmail工具箱，你可以通过LangChain应用检查邮件、删除垃圾邮件，甚至让它帮你撰写邮件草稿。</li><li>通过Office365工具箱，你可以让LangChain应用帮你读写文档、总结文档，甚至做PPT。</li><li>通过GitHub工具箱，你可以指示LangChain应用来检查最新的代码，Commit Changes、Merge Branches，甚至尝试让大模型自动回答 Issues 中的问题——反正大模型解决代码问题的能力本来就更强。</li></ul><p>这些都不再是梦想。</p><p>下面咱们从一个最简单的应用开始。</p><p><strong>目标：我要让AI应用来访问我的Gmail邮件，让他每天早晨检查一次我的邮箱，看看“易速鲜花”的客服有没有给我发信息。</strong>（因为我可能正在焦急地等待他们的退款😁）</p><p>现在开始。</p><h3 id="第一步：在-Google-Cloud-中设置你的应用程序接口"><a href="#第一步：在-Google-Cloud-中设置你的应用程序接口" class="headerlink" title="第一步：在 Google Cloud 中设置你的应用程序接口"></a>第一步：在 Google Cloud 中设置你的应用程序接口</h3><p>这个步骤你要跟着Gmail API的官方配置 <a href="https://developers.google.com/gmail/api/quickstart/python?hl=zh-cn#authorize_credentials_for_a_desktop_application">链接</a> 完成，这个和LangChain无关。蛮复杂的，你需要有点耐心。跟着流程一步步配置就好了。</p><p>下面是我在这个设置过程中截取的一部分图片，只是供你参考。详细配置你要follow Google的官方说明。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/8a3c72f48c231bd2d886b4d99e9f3321.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/3822d1effb90c855c133acdecea2eaab.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/96a788e8a1f7d4f32e3d23eb94cce8f3.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/0f746cfa48ba60c0fe98e657cb3yyb29.jpg"></p><p>下面这个OAuth同意屏幕里面的配置非常重要，你的智能代理能做什么，不能做什么，就看你怎么给权限了！</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/195ec3590bb075ecff42911f13d2f22f.jpg"></p><p>所有设置都完成之后，在OAuth客户段已创建这个页面，你拥有了开发密钥。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f6829a70c320161a1002ee3380c5b1b0.jpg"></p><h3 id="第二步：根据密钥生成开发-Token"><a href="#第二步：根据密钥生成开发-Token" class="headerlink" title="第二步：根据密钥生成开发 Token"></a>第二步：根据密钥生成开发 Token</h3><p>在这一步之前，你可能需要安装一些相关的包。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">pip install --upgrade google-api-python-clientpip install --upgrade google-auth-oauthlibpip install --upgrade google-auth-httplib2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>然后，把密钥下载下来，保存为credentials.json。</p><p>运行下面的代码，生成token.json。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from __future__ import print_functionimport os.pathfrom google.auth.transport.requests import Requestfrom google.oauth2.credentials import Credentialsfrom google_auth_oauthlib.flow import InstalledAppFlowfrom googleapiclient.discovery import buildfrom googleapiclient.errors import HttpError# If modifying these scopes, delete the file token.json.SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']def main():    """Shows basic usage of the Gmail API.    Lists the user's Gmail labels.    """    creds = None    # The file token.json stores the user's access and refresh tokens, and is    # created automatically when the authorization flow completes for the first    # time.    if os.path.exists('token.json'):        creds = Credentials.from_authorized_user_file('token.json', SCOPES)    # If there are no (valid) credentials available, let the user log in.    if not creds or not creds.valid:        if creds and creds.expired and creds.refresh_token:            creds.refresh(Request())        else:            flow = InstalledAppFlow.from_client_secrets_file(                'credentials.json', SCOPES)            creds = flow.run_local_server(port=8088)        # Save the credentials for the next run        with open('token.json', 'w') as token:            token.write(creds.to_json())    try:        # Call the Gmail API        service = build('gmail', 'v1', credentials=creds)        results = service.users().labels().list(userId='me').execute()        labels = results.get('labels', [])        if not labels:            print('No labels found.')            return        print('Labels:')        for label in labels:            print(label['name'])    except HttpError as error:        # TODO(developer) - Handle errors from gmail API.        print(f'An error occurred: &#123;error&#125;')if __name__ == '__main__':    main()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是Google API网站提供的标准示例代码，里面给了读取权限（gmail.readonly）的Token，如果你要编写邮件，甚至发送邮件，需要根据需求来调整权限。更多细节可以参阅Google API的 <a href="https://cloud.google.com/compute/docs/apis?hl=zh-cn">文档</a>。</p><p>这个程序会生成一个token.json文件，是有相关权限的开发令牌。这个文件在LangChain应用中需要和密钥一起使用。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/541c541b377063b49d74ddc53f41d578.jpg"></p><p>把密钥和Token文件都放在程序的同一个目录中，你就可以开始开发应用程序了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f23144b35b44fef8d900d0d50c9da6b4.jpg"></p><h3 id="第三步：用-LangChain-框架开发-Gmail-App"><a href="#第三步：用-LangChain-框架开发-Gmail-App" class="headerlink" title="第三步：用 LangChain 框架开发 Gmail App"></a>第三步：用 LangChain 框架开发 Gmail App</h3><p>这段代码的核心目的是连接到Gmail API，查询用户的邮件，并通过LangChain的Agent框架智能化地调用API（用语言而不是具体API），与邮件进行互动。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 设置OpenAI API的密钥import osos.environ["OPENAI_API_KEY"] = 'Your Key'# 导入与Gmail交互所需的工具包from langchain.agents.agent_toolkits import GmailToolkit# 初始化Gmail工具包toolkit = GmailToolkit()# 从gmail工具中导入一些有用的功能from langchain.tools.gmail.utils import build_resource_service, get_gmail_credentials# 获取Gmail API的凭证，并指定相关的权限范围credentials = get_gmail_credentials(    token_file="token.json",  # Token文件路径    scopes=["https://mail.google.com/"],  # 具有完全的邮件访问权限    client_secrets_file="credentials.json",  # 客户端的秘密文件路径)# 使用凭证构建API资源服务api_resource = build_resource_service(credentials=credentials)toolkit = GmailToolkit(api_resource=api_resource)# 获取工具tools = toolkit.get_tools()print(tools)# 导入与聊天模型相关的包from langchain.chat_models import ChatOpenAIfrom langchain.agents import initialize_agent, AgentType# 初始化聊天模型llm = ChatOpenAI(temperature=0, model='gpt-4')# 通过指定的工具和聊天模型初始化agentagent = initialize_agent(    tools=toolkit.get_tools(),    llm=llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,)# 使用agent运行一些查询或指令result = agent.run(    "今天易速鲜花客服给我发邮件了么？最新的邮件是谁发给我的？")# 打印结果print(result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码的核心部分主要是连接到Gmail API，获取用户的邮件数据，并通过特定的 Agent 查询这些数据。</p><p>你的请求是查询今天是否收到了来自“易速鲜花客服”的邮件，以及最新邮件的发送者是谁。 <strong>这个请求是模糊的，是自然语言格式，具体调用什么API，由Agent、Tool也就是Gmail API它俩商量着来。</strong> 这与我们之前所进行的清晰的、具体API调用式的应用开发迥然不同。</p><p>第一次运行程序，会进行一些确认，并让我Login我的Gmail。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/0e2a7df295caa50512552e05ea3def37.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/3208ff117674ebf3f08eac6118393e51.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/0cc81560c4bc412104b5144a474c5530.jpg"></p><p>之后，我就得到了智能助手的回答！</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/455f8cb0138cd3860869e5eee74f8ecf.jpg"></p><p>她说： <strong>主人，看起来你没有收到“易速鲜花”的邮件耶，还需要我帮你做些什么吗？</strong> 真的很贴心，这样的话，我每天早晨就不需要自己去检查邮件啦！</p><p>后来，我又问她，那么谁给我发来了新邮件呢？</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c95a8e75cdc78a7da4960c8f2yyf8be4.jpg"></p><p>她告诉我说，Medium - Programing 给我发了一篇 VS code 的 10 个 tips 的文章，还有Kubernetes的点子啥的。</p><p>嗯，这是我订阅的内容。下一步，我还可以让她针对这些内容给我总结总结！这也是她的强项！</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>学到现在，你应该对LangChain 的核心价值有了更深的感悟吧。它的价值，在于它将模型运行和交互的复杂性进行了封装和抽象化，为开发者提供了一个更简单、更直观的接口来利用大模型。</p><ul><li><p><strong>集成多模型和多策略：</strong> LangChain 提供了一种方法，使得多个模型或策略能够在一个统一的框架下工作。例如，arXiv 是一个单独的工具，它负责处理特定的任务。这种工具可以与其他工具（例如用于处理自然语言查询或者数据库查询的工具）一起作为一个集成的系统存在。这样，你可以轻松地创建一个系统，该系统可以处理多种类型的输入并执行多种任务，而不必为每个任务单独写代码。</p></li><li><p><strong>更易于交互和维护：</strong> 通过 LangChain，你可以更方便地管理和维护你的工具和模型。LangChain 提供的工具和代理（Agent）抽象使得开发者可以将关注点从底层实现细节转向实现应用的高层逻辑。而且，LangChain封装了像模型的加载、输入输出的处理、工具的调度等底层任务，使得开发者能够更专注于如何组合这些工具以解决实际问题。</p></li><li><p><strong>适应性：</strong> LangChain 提供的架构允许你轻松地添加新的工具或模型，或者替换现有的工具或模型。这种灵活性使得你的系统可以很容易地适应新的需求或改变。</p></li><li><p><strong>可解释性：</strong> LangChain 还提供了对模型决策的可解释性。在你的示例中，LangChain 提供的对话历史和工具选择的记录可以帮助理解系统做出某些决策的原因。</p></li></ul><p>总的来说，尽管直接调用模型可能对于单一任务或简单应用来说足够了，但是当你需要处理更复杂的场景，例如需要协调多个模型或工具，或者需要处理多种类型的输入时，使用像 LangChain 这样的框架可以大大简化你的工作。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li><p>上面Gmail的示例中我只是展示了邮件读取功能，你能否让你的AI助理帮你写邮件的草稿甚至发送邮件？</p></li><li><p>你可否尝试使用GitHub工具开发一些App来自动完成一部分GitHub任务，比如查看Issues、Merge Branches之类的事儿。</p></li></ol><p>提示：参考此 <a href="https://docs.github.com/en/apps/creating-github-apps/registering-a-github-app/registering-a-github-app">链接</a> 创建 GitHub App，以及LangChain的 <a href="https://python.langchain.com/docs/integrations/toolkits/github">参考文档</a>。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/1bc0dcd6e05133f934ed926cdcc9eb2e.jpg"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e037cf6460826e189811ea2af4bb96ea.jpg"></p><p>期待在留言区看到你的分享，如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>文档：LangChain中集成的所有 <a href="https://python.langchain.com/docs/integrations/tools/">工具</a></li><li>文档：LangChain中集成的所有 <a href="https://python.langchain.com/docs/integrations/toolkits/">工具箱</a></li><li>文档：Google Cloud <a href="https://cloud.google.com/compute/docs/apis?hl=zh-cn">API</a></li><li>文档：Github REST <a href="https://support.github.com/features/rest-api">API</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;14｜工具和工具箱：LangChain中的Tool和Toolkits一览&quot;&gt;&lt;a href=&quot;#14｜工具和工具箱：LangChain中的Tool和Toolkits一览&quot; class=&quot;headerlink&quot; title=&quot;14｜工具和工具箱：LangChain中</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>15｜检索增强生成：通过RAG助力鲜花运营</title>
    <link href="https://zhuansun.github.io/geekbang/posts/948258930.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/948258930.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.460Z</updated>
    
    <content type="html"><![CDATA[<h1 id="15｜检索增强生成：通过RAG助力鲜花运营"><a href="#15｜检索增强生成：通过RAG助力鲜花运营" class="headerlink" title="15｜检索增强生成：通过RAG助力鲜花运营"></a>15｜检索增强生成：通过RAG助力鲜花运营</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>在 <a href="https://time.geekbang.org/column/article/699436">第2课</a> 中，我曾经带着你完成了一个基于本地文档的问答系统。用当下时髦的话说，你实现了一个RAG 应用。</p><p>什么是RAG？其全称为Retrieval-Augmented Generation，即检索增强生成，它结合了检索和生成的能力，为文本序列生成任务引入外部知识。RAG将传统的语言生成模型与大规模的外部知识库相结合，使模型在生成响应或文本时可以动态地从这些知识库中检索相关信息。这种结合方法旨在增强模型的生成能力，使其能够产生更为丰富、准确和有根据的内容，特别是在需要具体细节或外部事实支持的场合。</p><p>RAG 的工作原理可以概括为几个步骤。</p><ol><li><strong>检索</strong> <strong>：</strong> 对于给定的输入（问题），模型首先使用检索系统从大型文档集合中查找相关的文档或段落。这个检索系统通常基于密集向量搜索，例如ChromaDB、Faiss这样的向量数据库。</li><li><strong>上下文编码</strong> <strong>：</strong> 找到相关的文档或段落后，模型将它们与原始输入（问题）一起编码。</li><li><strong>生成</strong> <strong>：</strong> 使用编码的上下文信息，模型生成输出（答案）。这通常当然是通过大模型完成的。</li></ol><p>RAG 的一个关键特点是，它不仅仅依赖于训练数据中的信息，还可以从大型外部知识库中检索信息。这使得RAG模型特别适合处理在训练数据中未出现的问题。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f326343298bc0bc540978604203a3e0d.jpg"></p><p>RAG类的任务，目前企业实际应用场景中的需求量相当大，也是LangChain所关注的一个重点内容。在这节课中，我会对LangChain中所有与之相关的工具进行一个梳理，便于你把握LangChain在这个领域中都能够做到些什么。</p><h2 id="文档加载"><a href="#文档加载" class="headerlink" title="文档加载"></a>文档加载</h2><p>RAG的第一步是文档加载。LangChain 提供了多种类型的文档加载器，以加载各种类型的文档（HTML、PDF、代码），并与该领域的其他主要提供商如 Airbyte 和 Unstructured.IO 进行了集成。</p><p>下面给出常用的文档加载器列表。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/2af251fa78768b54a7d6a4a96423a867.jpg"></p><h2 id="文本转换"><a href="#文本转换" class="headerlink" title="文本转换"></a>文本转换</h2><p>加载文档后，下一个步骤是对文本进行转换，而最常见的文本转换就是把长文档分割成更小的块（或者是片，或者是节点），以适合模型的上下文窗口。LangChain 有许多内置的文档转换器，可以轻松地拆分、组合、过滤和以其他方式操作文档。</p><h3 id="文本分割器"><a href="#文本分割器" class="headerlink" title="文本分割器"></a>文本分割器</h3><p>把长文本分割成块听起来很简单，其实也存在一些细节。文本分割的质量会影响检索的结果质量。理想情况下，我们希望将语义相关的文本片段保留在一起。</p><p>LangChain中，文本分割器的工作原理如下：</p><ol><li>将文本分成小的、具有语义意义的块（通常是句子）。</li><li>开始将这些小块组合成一个更大的块，直到达到一定的大小。</li><li>一旦达到该大小，一个块就形成了，可以开始创建新文本块。这个新文本块和刚刚生成的块要有一些重叠，以保持块之间的上下文。</li></ol><p>因此，LangChain提供的各种文本拆分器可以帮助你从下面几个角度设定你的分割策略和参数：</p><ol><li>文本如何分割</li><li>块的大小</li><li>块之间重叠文本的长度</li></ol><p>这些文本分割器的说明和示例如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/517c22ba8c7d78a755d5b29ec16d3e83.jpg"></p><p>你可能会关心，文本分割在实践，有哪些具体的考量因素，我总结了下面几点。</p><p><strong>首先，就是LLM 的具体限制。</strong> GPT-3.5-turbo支持的上下文窗口为4096个令牌，这意味着输入令牌和生成的输出令牌的总和不能超过4096，否则会出错。为了保证不超过这个限制，我们可以预留约2000个令牌作为输入提示，留下约2000个令牌作为返回的消息。这样，如果你提取出了五个相关信息块，那么每个片的大小不应超过400个令牌。</p><p><strong>此外，文本分割策略的选择和任务类型相关。</strong></p><ul><li>需要细致查看文本的任务，最好使用较小的分块。例如，拼写检查、语法检查和文本分析可能需要识别文本中的单个单词或字符。垃圾邮件识别、查找剽窃和情感分析类任务，以及搜索引擎优化、主题建模中常用的关键字提取任务也属于这类细致任务。</li><li>需要全面了解文本的任务，则使用较大的分块。例如，机器翻译、文本摘要和问答任务需要理解文本的整体含义。而自然语言推理、问答和机器翻译需要识别文本中不同部分之间的关系。还有创意写作，都属于这种粗放型的任务。</li></ul><p><strong>最后，你也要考虑所分割的文本的性质。</strong> 例如，如果文本结构很强，如代码或HTML，你可能想使用较大的块，如果文本结构较弱，如小说或新闻文章，你可能想使用较小的块。</p><p>你可以反复试验不同大小的块和块与块之间重叠窗口的大小，找到最适合你特定问题的解决方案。</p><h3 id="其他形式的文本转换"><a href="#其他形式的文本转换" class="headerlink" title="其他形式的文本转换"></a>其他形式的文本转换</h3><p>除拆分文本之外，LangChain中还集成了各种工具对文档执行的其他类型的转换。下面让我们对其进行逐点分析。</p><ol><li>过滤冗余的文档：使用 EmbeddingsRedundantFilter 工具可以识别相似的文档并过滤掉冗余信息。这意味着如果你有多份高度相似或几乎相同的文档，这个功能可以帮助识别并删除这些多余的副本，从而节省存储空间并提高检索效率。</li><li>翻译文档：通过与工具 doctran 进行集成，可以将文档从一种语言翻译成另一种语言。</li><li>提取元数据：通过与工具 doctran 进行集成，可以从文档内容中提取关键信息（如日期、作者、关键字等），并将其存储为元数据。元数据是描述文档属性或内容的数据，这有助于更有效地管理、分类和检索文档。</li><li>转换对话格式：通过与工具 doctran 进行集成，可以将对话式的文档内容转化为问答（Q&#x2F;A）格式，从而更容易地提取和查询特定的信息或回答。这在处理如访谈、对话或其他交互式内容时非常有用。</li></ol><p>所以说，文档转换不仅限于简单的文本拆分，还可以包含附加的操作，这些操作的目的都是更好地准备和优化文档，以供后续生成更好的索引和检索功能。</p><h2 id="文本嵌入"><a href="#文本嵌入" class="headerlink" title="文本嵌入"></a>文本嵌入</h2><p>文本块形成之后，我们就通过LLM来做嵌入（Embeddings），将文本转换为数值表示，使得计算机可以更容易地处理和比较文本。OpenAI、Cohere、Hugging Face 中都有能做文本嵌入的模型。</p><p>Embeddings 会创建一段文本的向量表示，让我们可以在向量空间中思考文本，并执行语义搜索之类的操作，在向量空间中查找最相似的文本片段。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/b54fc88694120820cd1afea29946d9ba.png"></p><p>LangChain中的Embeddings 类是设计用于与文本嵌入模型交互的类。这个类为所有这些提供者提供标准接口。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 初始化Embedding类from langchain.embeddings import OpenAIEmbeddingsembeddings_model = OpenAIEmbeddings()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>它提供两种方法：</p><ol><li>第一种是 embed_documents 方法，为文档创建嵌入。这个方法接收多个文本作为输入，意味着你可以一次性将多个文档转换为它们的向量表示。</li><li>第二种是 embed_query 方法，为查询创建嵌入。这个方法只接收一个文本作为输入，通常是用户的搜索查询。</li></ol><p><strong>为</strong> <strong>什么需要两种方法？</strong> 虽然看起来这两种方法都是为了文本嵌入，但是LangChain将它们分开了。原因是一些嵌入提供者对于文档和查询使用的是不同的嵌入方法。文档是要被搜索的内容，而查询是实际的搜索请求。这两者可能因为其性质和目的，而需要不同的处理或优化。</p><p>embed_documents 方法的示例代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">embeddings = embeddings_model.embed_documents(    [        "您好，有什么需要帮忙的吗？",        "哦，你好！昨天我订的花几天送达",        "请您提供一些订单号？",        "12345678",    ])len(embeddings), len(embeddings[0])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">(4, 1536)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>embed_documents 方法的示例代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">embedded_query = embeddings_model.embed_query("刚才对话中的订单号是多少?")embedded_query[:3]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">[-0.0029746221837547455, -0.007710168602107487, 0.00923260021751183]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="存储嵌入"><a href="#存储嵌入" class="headerlink" title="存储嵌入"></a>存储嵌入</h2><p>计算嵌入可能是一个时间消耗大的过程。为了加速这一过程，我们可以将计算出的嵌入存储或临时缓存，这样在下次需要它们时，就可以直接读取，无需重新计算。</p><h3 id="缓存存储"><a href="#缓存存储" class="headerlink" title="缓存存储"></a>缓存存储</h3><p>CacheBackedEmbeddings是一个支持缓存的嵌入式包装器，它可以将嵌入缓存在键值存储中。具体操作是：对文本进行哈希处理，并将此哈希值用作缓存的键。</p><p>要初始化一个CacheBackedEmbeddings，主要的方式是使用from_bytes_store。其需要以下参数：</p><ul><li>underlying_embedder：实际计算嵌入的嵌入器。</li><li>document_embedding_cache：用于存储文档嵌入的缓存。</li><li>namespace（可选）：用于文档缓存的命名空间，避免与其他缓存发生冲突。</li></ul><p><strong>不同的缓存策略如下：</strong></p><ol><li>InMemoryStore：在内存中缓存嵌入。主要用于单元测试或原型设计。如果需要长期存储嵌入，请勿使用此缓存。</li><li>LocalFileStore：在本地文件系统中存储嵌入。适用于那些不想依赖外部数据库或存储解决方案的情况。</li><li>RedisStore：在Redis数据库中缓存嵌入。当需要一个高速且可扩展的缓存解决方案时，这是一个很好的选择。</li></ol><p>在内存中缓存嵌入的示例代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入内存存储库，该库允许我们在RAM中临时存储数据from langchain.storage import InMemoryStore# 创建一个InMemoryStore的实例store = InMemoryStore()# 导入与嵌入相关的库。OpenAIEmbeddings是用于生成嵌入的工具，而CacheBackedEmbeddings允许我们缓存这些嵌入from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings# 创建一个OpenAIEmbeddings的实例，这将用于实际计算文档的嵌入underlying_embeddings = OpenAIEmbeddings()# 创建一个CacheBackedEmbeddings的实例。# 这将为underlying_embeddings提供缓存功能，嵌入会被存储在上面创建的InMemoryStore中。# 我们还为缓存指定了一个命名空间，以确保不同的嵌入模型之间不会出现冲突。embedder = CacheBackedEmbeddings.from_bytes_store(    underlying_embeddings,  # 实际生成嵌入的工具    store,  # 嵌入的缓存位置    namespace=underlying_embeddings.model  # 嵌入缓存的命名空间)# 使用embedder为两段文本生成嵌入。# 结果，即嵌入向量，将被存储在上面定义的内存存储中。embeddings = embedder.embed_documents(["你好", "智能鲜花客服"])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>解释下这段代码。首先我们在内存中设置了一个存储空间，然后初始化了一个嵌入工具，该工具将实际生成嵌入。之后，这个嵌入工具被包装在一个缓存工具中，用于为两段文本生成嵌入。</p><p>至于其他两种缓存器，嵌入的使用方式也不复杂，你可以参考LangChain文档自行学习。</p><h3 id="向量数据库（向量存储）"><a href="#向量数据库（向量存储）" class="headerlink" title="向量数据库（向量存储）"></a>向量数据库（向量存储）</h3><p>更常见的存储向量的方式是通过向量数据库（Vector Store）来保存它们。LangChain支持非常多种向量数据库，其中有很多是开源的，也有很多是商用的。比如Elasticsearch、Faiss、Chroma和Qdrant等等。</p><p>因为选择实在是太多了，我也给你列出来了一个表。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/2eb52480f790fd3281ae905ee1c58077.jpg"></p><p>那么问题来了，面对这么多种类的向量数据库，应该如何选择呢？</p><p>这就涉及到许多技术和业务层面的考量，你应该 <strong>根据具体需求进行选型</strong>。</p><ol><li>数据规模和速度需求：考虑你的数据量大小以及查询速度的要求。一些向量数据库在处理大规模数据时更加出色，而另一些在低延迟查询中表现更好。</li><li>持久性和可靠性：根据你的应用场景，确定你是否需要数据的高可用性、备份和故障转移功能。</li><li>易用性和社区支持：考虑向量数据库的学习曲线、文档的完整性以及社区的活跃度。</li><li>成本：考虑总体拥有成本，包括许可、硬件、运营和维护成本。</li><li>特性：考虑你是否需要特定的功能，例如多模态搜索等。</li><li>安全性：确保向量数据库符合你的安全和合规要求。</li></ol><p>在进行向量数据库的评测时，进行 <strong>性能基准测试</strong> 是了解向量数据库实际表现的关键。这可以帮助你评估查询速度、写入速度、并发性能等。</p><p>没有“最好”的向量数据库，只有“最适合”的向量数据库。在你的需求上做些研究和测试，确保你选择的向量数据库满足你的业务和技术要求就好。</p><h2 id="数据检索"><a href="#数据检索" class="headerlink" title="数据检索"></a>数据检索</h2><p>在LangChain中，Retriever，也就是检索器，是数据检索模块的核心入口，它通过非结构化查询返回相关的文档。</p><h3 id="向量存储检索器"><a href="#向量存储检索器" class="headerlink" title="向量存储检索器"></a>向量存储检索器</h3><p>向量存储检索器是最常见的，它主要支持向量检索。当然LangChain也有支持其他类型存储格式的检索器。</p><p>下面实现一个端到端的数据检索功能，我们通过VectorstoreIndexCreator来创建索引，并在索引的query方法中，通过vectorstore类的as_retriever方法，把向量数据库（Vector Store）直接作为检索器，来完成检索任务。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 设置OpenAI的API密钥import osos.environ["OPENAI_API_KEY"] = 'Your OpenAI Key'# 导入文档加载器模块，并使用TextLoader来加载文本文件from langchain.document_loaders import TextLoaderloader = TextLoader('LangChainSamples/OneFlower/易速鲜花花语大全.txt', encoding='utf8')# 使用VectorstoreIndexCreator来从加载器创建索引from langchain.indexes import VectorstoreIndexCreatorindex = VectorstoreIndexCreator().from_loaders([loader])# 定义查询字符串, 使用创建的索引执行查询query = "玫瑰花的花语是什么？"result = index.query(query)print(result) # 打印查询结果<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">玫瑰花的花语是爱情、热情、美丽。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>你可能会觉得，这个数据检索过程太简单了。这就要归功于LangChain的强大封装能力。如果我们审视一下位于vectorstore.py中的VectorstoreIndexCreator类的代码，你就会发现，它其中封装了vectorstore、embedding以及text_splitter，甚至document loader（如果你使用from_documents方法的话）。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">class VectorstoreIndexCreator(BaseModel):    """Logic for creating indexes."""    vectorstore_cls: Type[VectorStore] = Chroma    embedding: Embeddings = Field(default_factory=OpenAIEmbeddings)    text_splitter: TextSplitter = Field(default_factory=_get_default_text_splitter)    vectorstore_kwargs: dict = Field(default_factory=dict)    class Config:        """Configuration for this pydantic object."""        extra = Extra.forbid        arbitrary_types_allowed = True    def from_loaders(self, loaders: List[BaseLoader]) -> VectorStoreIndexWrapper:        """Create a vectorstore index from loaders."""        docs = []        for loader in loaders:            docs.extend(loader.load())        return self.from_documents(docs)    def from_documents(self, documents: List[Document]) -> VectorStoreIndexWrapper:        """Create a vectorstore index from documents."""        sub_docs = self.text_splitter.split_documents(documents)        vectorstore = self.vectorstore_cls.from_documents(            sub_docs, self.embedding, **self.vectorstore_kwargs        )        return VectorStoreIndexWrapper(vectorstore=vectorstore)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因此，上面的检索功能就相当于我们第2课中讲过的一系列工具的整合。而我们也可以用下面的代码，来显式地指定索引创建器的vectorstore、embedding以及text_splitter，并把它们替换成你所需要的工具，比如另外一种向量数据库或者别的Embedding模型。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.text_splitter import CharacterTextSplittertext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)from langchain.vectorstores import Chromafrom langchain.embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()index_creator = VectorstoreIndexCreator(    vectorstore_cls=Chroma,    embedding=OpenAIEmbeddings(),    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>那么，下一个问题是 index.query(query)，又是如何完成具体的检索及文本生成任务的呢？我们此处既没有看到大模型，又没有看到LangChain的文档检索工具（比如我们在第2课中见过的QARetrival链）。</p><p>秘密仍然存在于源码中，在VectorStoreIndexWrapper类的query方法中，可以看到，在调用方法的同时，RetrievalQA链被启动，以完成检索功能。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">class VectorStoreIndexWrapper(BaseModel):    """Wrapper around a vectorstore for easy access."""    vectorstore: VectorStore    class Config:        """Configuration for this pydantic object."""        extra = Extra.forbid        arbitrary_types_allowed = True    def query(        self,        question: str,        llm: Optional[BaseLanguageModel] = None,        retriever_kwargs: Optional[Dict[str, Any]] = None,        **kwargs: Any    ) -> str:        """Query the vectorstore."""        llm = llm or OpenAI(temperature=0)        retriever_kwargs = retriever_kwargs or &#123;&#125;        chain = RetrievalQA.from_chain_type(            llm, retriever=self.vectorstore.as_retriever(**retriever_kwargs), **kwargs        )        return chain.run(question)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面我们用到的向量存储检索器，是向量存储类的轻量级包装器，使其符合检索器接口。它使用向量存储中的搜索方法（例如相似性搜索和 MMR）来查询向量存储中的文本。</p><h3 id="各种类型的检索器"><a href="#各种类型的检索器" class="headerlink" title="各种类型的检索器"></a>各种类型的检索器</h3><p>除向量存储检索器之外，LangChain中还提供很多种其他的检索工具。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f87c2d22bb1e71419ee129c9871724a8.jpg"></p><p>这些检索工具，各有其功能特点，你可以查找它们的文档说明，并尝试使用。</p><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>在本节课的最后，我们来看看LangChain中的索引（Index）。简单的说，索引是一种高效地管理和定位文档信息的方法，确保每个文档具有唯一标识并便于检索。</p><p>尽管在 <a href="https://time.geekbang.org/column/article/699436">第2课</a> 的示例中，我们并没有显式的使用到索引就完成了一个RAG任务，但在复杂的信息检索任务中，有效地管理和索引文档是关键的一步。LangChain 提供的索引 API 为开发者带来了一个高效且直观的解决方案。具体来说，它的优势包括：</p><ul><li>避免重复内容：确保你的向量存储中不会有冗余数据。</li><li>只更新更改的内容：能检测哪些内容已更新，避免不必要的重写。</li><li>省时省钱：不对未更改的内容重新计算嵌入，从而减少了计算资源的消耗。</li><li>优化搜索结果：减少重复和不相关的数据，从而提高搜索的准确性。</li></ul><p>LangChain 利用了记录管理器（RecordManager）来跟踪哪些文档已经被写入向量存储。</p><p>在进行索引时，API 会对每个文档进行哈希处理，确保每个文档都有一个唯一的标识。这个哈希值不仅仅基于文档的内容，还考虑了文档的元数据。</p><p>一旦哈希完成，以下信息会被保存在记录管理器中：</p><ul><li>文档哈希：基于文档内容和元数据计算出的唯一标识。</li><li>写入时间：记录文档何时被添加到向量存储中。</li><li>源 ID：这是一个元数据字段，表示文档的原始来源。</li></ul><p>这种方法确保了即使文档经历了多次转换或处理，也能够精确地跟踪它的状态和来源，确保文档数据被正确管理和索引。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>这节课的内容非常多，而且我给出了很多表格供你查询之用，信息量很大。同时，你可以复习 <a href="https://time.geekbang.org/column/article/699436">第2课</a> 的内容，我希望你对RAG的流程有个更深的理解。</p><p>通过检索增强生成来存储和搜索非结构化数据的最常见方法是，给这些非结构化的数据做嵌入并存储生成的嵌入向量，然后在查询时给要查询的文本也做嵌入，并检索与嵌入查询“最相似”的嵌入向量。向量数据库则负责存储嵌入数据，并为你执行向量的搜索。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/39ab4b67b2689e6daf9a83bc5895b684.jpg"></p><p>你看，RAG实际上是为非结构化数据创建了一个“地图”。当用户有查询请求时，该查询同样被嵌入，然后你的应用程序会在这个“地图”中寻找与之最匹配的位置，从而快速准确地检索信息。</p><p>在我们的鲜花运营场景中，RAG当然可以在很多方面发挥巨大的作用。你的鲜花有各种各样的品种、颜色和花语，这些数据往往是自然的、松散的，也就是非结构化的。使用RAG，你可以通过嵌入向量，把库存的鲜花与相关的非结构化信息（如花语、颜色、产地等）关联起来。当客户或者员工想要查询某种鲜花的信息时，系统可以快速地提供准确的答案。</p><p>此外，RAG还可以应用于订单管理。每个订单，无论是客户的姓名、地址、购买的鲜花种类，还是订单状态，都可以被视为非结构化数据。通过RAG，我们可以轻松地嵌入并检索这些订单，为客户提供实时的订单更新、跟踪和查询服务。</p><p>当然，对于订单这样的信息，更常见的情况仍是把它们组织成结构化的数据，存储在数据库中（至少也是CSV或者Excel表中），以便高效、精准地查询。那么，LLM能否帮助我们查询数据库表中的条目呢？在下一课中，我将为你揭晓答案。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>请你尝试使用一种文本分割器来给你的文档分块。</li><li>请你尝试使用一种新的向量数据库来存储你的文本嵌入。</li><li>请你尝试使用一种新的检索器来提取信息。</li></ol><p>期待在留言区看到你的实践成果，如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>Github： <a href="https://github.com/psychic-api/doctran/tree/main">doctran</a>，辅助LangChain进行文本转换</li><li>文档：LangChain中 <a href="https://python.langchain.com/docs/modules/data_connection/indexing">Indexing</a> 的说明</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;15｜检索增强生成：通过RAG助力鲜花运营&quot;&gt;&lt;a href=&quot;#15｜检索增强生成：通过RAG助力鲜花运营&quot; class=&quot;headerlink&quot; title=&quot;15｜检索增强生成：通过RAG助力鲜花运营&quot;&gt;&lt;/a&gt;15｜检索增强生成：通过RAG助力鲜花运营&lt;/</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>17｜回调函数：在AI应用中引入异步通信机制</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1156965186.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1156965186.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.465Z</updated>
    
    <content type="html"><![CDATA[<h1 id="17｜回调函数：在AI应用中引入异步通信机制"><a href="#17｜回调函数：在AI应用中引入异步通信机制" class="headerlink" title="17｜回调函数：在AI应用中引入异步通信机制"></a>17｜回调函数：在AI应用中引入异步通信机制</h1><p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>这节课我们一起来学习一下LangChain中的回调函数。</p><h2 id="回调函数和异步编程"><a href="#回调函数和异步编程" class="headerlink" title="回调函数和异步编程"></a>回调函数和异步编程</h2><p>回调函数，你可能并不陌生。它是函数A作为参数传给另一个函数B，然后在函数B内部执行函数A。当函数B完成某些操作后，会调用（即“回调”）函数A。这种编程模式常见于处理异步操作，如事件监听、定时任务或网络请求。</p><blockquote><p>在编程中，异步通常是指代码不必等待某个操作完成（如I&#x2F;O操作、网络请求、数据库查询等）就可以继续执行的能力。异步机制的实现涉及事件循环、任务队列和其他复杂的底层机制。这与同步编程形成对比，在同步编程中，操作必须按照它们出现的顺序完成。</p></blockquote><p>下面是回调函数的一个简单示例。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">def compute(x, y, callback):    result = x + y    callback(result)def print_result(value):    print(f"The result is: &#123;value&#125;")def square_result(value):    print(f"The squared result is: &#123;value**2&#125;")# 使用print_result作为回调compute(3, 4, print_result)  # 输出: The result is: 7# 使用square_result作为回调compute(3, 4, square_result)  # 输出: The squared result is: 49<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>不过，上面这个程序中并没有体现出异步操作。虽然回调函数这种编程模式常见于处理异步操作，但回调函数本身并不代表异步。回调只是一种编程模式，允许你在某个操作完成时（无论是否异步）执行某些代码。</p><p>而下面的例子，就是在异步操作时使用回调函数的示例。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">import asyncioasync def compute(x, y, callback):    print("Starting compute...")    await asyncio.sleep(0.5)  # 模拟异步操作    result = x + y    # callback(result)    print("Finished compute...")def print_result(value):    print(f"The result is: &#123;value&#125;")async def another_task():    print("Starting another task...")    await asyncio.sleep(1)    print("Finished another task...")async def main():    print("Main starts...")    task1 = asyncio.create_task(compute(3, 4, print_result))    task2 = asyncio.create_task(another_task())    await task1    await task2    print("Main ends...")asyncio.run(main())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个示例中，当我们调用 asyncio.create_task(compute(3, 4, print_result))，compute函数开始执行。当它遇到 await asyncio.sleep(2) 时，它会暂停，并将控制权交还给事件循环。这时，事件循环可以选择开始执行another_task，这是另一个异步任务。这样，你可以清晰地看到，尽管compute函数还没有完成，another_task函数也得以开始执行并完成。这就是异步编程，允许你同时执行多个操作，而不需要等待一个完成后再开始另一个。</p><h2 id="LangChain-中的-Callback-处理器"><a href="#LangChain-中的-Callback-处理器" class="headerlink" title="LangChain 中的 Callback 处理器"></a>LangChain 中的 Callback 处理器</h2><p>LangChain 的 Callback 机制允许你在应用程序的不同阶段进行自定义操作，如日志记录、监控和数据流处理，这个机制通过 CallbackHandler（回调处理器）来实现。</p><p>回调处理器是LangChain中实现 CallbackHandler 接口的对象，为每类可监控的事件提供一个方法。当该事件被触发时，CallbackManager 会在这些处理器上调用适当的方法。</p><p>BaseCallbackHandler是最基本的回调处理器，你可以继承它来创建自己的回调处理器。它包含了多种方法，如on_llm_start&#x2F;on_chat（当 LLM 开始运行时调用）和on_llm_error（当 LLM 出现错误时调用）等。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f393b0aa5b0b4fa795c27b5e04cae491.jpg"></p><p>LangChain 也提供了一些内置的处理器，例如 StdOutCallbackHandler，它会将所有事件记录到标准输出。还有FileCallbackHandler，会将所有的日志记录到一个指定的文件中。</p><h2 id="在组件中使用回调处理器"><a href="#在组件中使用回调处理器" class="headerlink" title="在组件中使用回调处理器"></a>在组件中使用回调处理器</h2><p>在 LangChain 的各个组件，如 Chains、Models、Tools、Agents 等，都提供了两种类型的回调设置方法：构造函数回调和请求回调。你可以在初始化 LangChain 时将回调处理器传入，或者在单独的请求中使用回调。例如，当你想要在整个链的所有请求中进行日志记录时，可以在初始化时传入处理器；而当你只想在某个特定请求中使用回调时，可以在请求时传入。</p><p>这两者的区别，我给你整理了一下。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/a593e19a4c3693365756a5c34a96355e.jpg"></p><p>下面这段示例代码，使用 LangChain 执行了一个简单的任务，结合使用 LangChain 的回调机制与 loguru 日志库，将相关事件同时输出到标准输出和 <code>&quot;output.log&quot;</code> 文件中。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from loguru import loggerfrom langchain.callbacks import FileCallbackHandlerfrom langchain.chains import LLMChainfrom langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatelogfile = "output.log"logger.add(logfile, colorize=True, enqueue=True)handler = FileCallbackHandler(logfile)llm = OpenAI()prompt = PromptTemplate.from_template("1 + &#123;number&#125; = ")# this chain will both print to stdout (because verbose=True) and write to 'output.log'# if verbose=False, the FileCallbackHandler will still write to 'output.log'chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler], verbose=True)answer = chain.run(number=2)logger.info(answer)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中，初始化LLMChain时指定的 verbose 参数，就等同于将一个输出到控制台的回调处理器添加到你的对象中。这个在你调试程序时非常有用，因为它会将所有事件的信息输出到控制台。</p><p>简而言之，LangChain 通过回调系统提供了一种灵活的方式，来监控和操作应用程序的不同阶段。</p><h2 id="自定义回调函数"><a href="#自定义回调函数" class="headerlink" title="自定义回调函数"></a>自定义回调函数</h2><p>我们也可以通过BaseCallbackHandler和AsyncCallbackHandler来自定义回调函数。下面是一个示例。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">import asynciofrom typing import Any, Dict, Listfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import LLMResult, HumanMessagefrom langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler# 创建同步回调处理器class MyFlowerShopSyncHandler(BaseCallbackHandler):    def on_llm_new_token(self, token: str, **kwargs) -> None:        print(f"获取花卉数据: token: &#123;token&#125;")# 创建异步回调处理器class MyFlowerShopAsyncHandler(AsyncCallbackHandler):    async def on_llm_start(        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any    ) -> None:        print("正在获取花卉数据...")        await asyncio.sleep(0.5)  # 模拟异步操作        print("花卉数据获取完毕。提供建议...")    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:        print("整理花卉建议...")        await asyncio.sleep(0.5)  # 模拟异步操作        print("祝你今天愉快！")# 主要的异步函数async def main():    flower_shop_chat = ChatOpenAI(        max_tokens=100,        streaming=True,        callbacks=[MyFlowerShopSyncHandler(), MyFlowerShopAsyncHandler()],    )    # 异步生成聊天回复    await flower_shop_chat.agenerate([[HumanMessage(content="哪种花卉最适合生日？只简单说3种，不超过50字")]])# 运行主异步函数asyncio.run(main())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这个鲜花店客服的程序中，当客户问及关于鲜花的建议时，我们使用了一个同步和一个异步回调。</p><p>MyFlowerShopSyncHandler 是一个同步回调，每当新的Token生成时，它就简单地打印出正在获取的鲜花数据。</p><p>而 MyFlowerShopAsyncHandler 则是异步的，当客服开始提供鲜花建议时，它会模拟数据的异步获取。在建议完成后，它还会模拟一个结束的操作，如向客户发出感谢。</p><p>这种结合了同步和异步操作的方法，使得程序能够更有效率地处理客户请求，同时提供实时反馈。</p><p><strong>这里的异步体现在这样几个方面。</strong></p><ol><li><p>模拟延时操作：在MyFlowerShopAsyncHandler中，我们使用了await asyncio.sleep(0.5)来模拟其他请求异步获取花卉信息的过程。当执行到这个await语句时，当前的on_llm_start函数会“暂停”，释放控制权回到事件循环。这意味着，在这个sleep期间，其他异步任务（如其他客户的请求）可以被处理。</p></li><li><p>回调机制：当ChatOpenAI在处理每个新Token时，它会调用on_llm_new_token方法。因为这是一个同步回调，所以它会立即输出。但是，开始和结束的异步回调on_llm_start和on_llm_end在开始和结束时都有一个小的延时操作，这是通过await asyncio.sleep(0.5)模拟的。</p></li><li><p>事件循环：Python的syncio库提供了一个事件循环，允许多个异步任务并发运行。在我们的例子中，虽然看起来所有的操作都是按顺序发生的，但由于我们使用了异步操作和回调，如果有其他并发任务，它们可以在await暂停期间运行。</p></li></ol><p>为了更清晰地展示异步的优势，通常我们会在程序中同时运行多个异步任务，并观察它们如何“并发”执行。但在这个简单的例子中，我们主要是通过模拟延时来展示异步操作的基本机制。</p><p>因此说，回调函数为异步操作提供了一个机制，使你可以定义“当操作完成时要做什么”，而异步机制的真正实现涉及更深层次的底层工作，如事件循环和任务调度。</p><h2 id="用-get-openai-callback-构造令牌计数器"><a href="#用-get-openai-callback-构造令牌计数器" class="headerlink" title="用 get_openai_callback 构造令牌计数器"></a>用 get_openai_callback 构造令牌计数器</h2><p>下面，我带着你使用LangChain中的回调函数来构造一个令牌计数器。这个计数功能对于监控大模型的会话消耗以及成本控制十分重要。</p><p>在构造令牌计数器之前，我们来回忆一下 <a href="https://time.geekbang.org/column/article/704183">第10课</a> 中的记忆机制。我们用下面的代码生成了ConversationBufferMemory。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain import OpenAIfrom langchain.chains import ConversationChainfrom langchain.chains.conversation.memory import ConversationBufferMemory# 初始化大语言模型llm = OpenAI(    temperature=0.5,    model_name="text-davinci-003")# 初始化对话链conversation = ConversationChain(    llm=llm,    memory=ConversationBufferMemory())# 第一天的对话# 回合1conversation("我姐姐明天要过生日，我需要一束生日花束。")print("第一次对话后的记忆:", conversation.memory.buffer)# 回合2conversation("她喜欢粉色玫瑰，颜色是粉色的。")print("第二次对话后的记忆:", conversation.memory.buffer)# 回合3 （第二天的对话）conversation("我又来了，还记得我昨天为什么要来买花吗？")print("/n第三次对话后时提示:/n",conversation.prompt.template)print("/n第三次对话后的记忆:/n", conversation.memory.buffer)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>同时，我们也给出了各种记忆机制对Token的消耗数量的估算示意图。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/b605f14e7c9151c5172fff5860285e52.png"></p><p>不过，这张图毕竟是估算，要真正地衡量出每种记忆机制到底耗费了多少个Token，那就需要回调函数上场了。</p><p>下面，我们通过回调函数机制，重构这段程序。为了做到这一点，我们首先需要确保在与大语言模型进行交互时，使用了get_openai_callback上下文管理器。</p><blockquote><p>在Python中，一个上下文管理器通常用于管理资源，如文件或网络连接，这些资源在使用前需要设置，在使用后需要清理。上下文管理器经常与with语句一起使用，以确保资源正确地设置和清理。</p><p>get_openai_callback被设计用来监控与OpenAI交互的Token数量。当你进入该上下文时，它会通过监听器跟踪Token的使用。当你退出上下文时，它会清理监听器并提供一个Token的总数。通过这种方式，它充当了一个回调机制，允许你在特定事件发生时执行特定的操作或收集特定的信息。</p></blockquote><p>具体代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain import OpenAIfrom langchain.chains import ConversationChainfrom langchain.chains.conversation.memory import ConversationBufferMemoryfrom langchain.callbacks import get_openai_callback# 初始化大语言模型llm = OpenAI(temperature=0.5, model_name="text-davinci-003")# 初始化对话链conversation = ConversationChain(    llm=llm,    memory=ConversationBufferMemory())# 使用context manager进行token countingwith get_openai_callback() as cb:    # 第一天的对话    # 回合1    conversation("我姐姐明天要过生日，我需要一束生日花束。")    print("第一次对话后的记忆:", conversation.memory.buffer)    # 回合2    conversation("她喜欢粉色玫瑰，颜色是粉色的。")    print("第二次对话后的记忆:", conversation.memory.buffer)    # 回合3 （第二天的对话）    conversation("我又来了，还记得我昨天为什么要来买花吗？")    print("/n第三次对话后时提示:/n",conversation.prompt.template)    print("/n第三次对话后的记忆:/n", conversation.memory.buffer)# 输出使用的tokensprint("\n总计使用的tokens:", cb.total_tokens)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里，我使用了get_openai_callback上下文管理器来监控与ConversationChain的交互。这允许我们计算在这些交互中使用的总Tokens数。</p><p>输出：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">总计使用的tokens: 966<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>下面，我再添加了一个additional_interactions异步函数，用于演示如何在多个并发交互中计算Tokens。</p><blockquote><p>当我们讨论异步交互时，指的是我们可以启动多个任务，它们可以并发（而不是并行）地运行，并且不会阻塞主线程。在Python中，这是通过asyncio库实现的，它使用事件循环来管理并发的异步任务。</p></blockquote><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">import asyncio# 进行更多的异步交互和token计数async def additional_interactions():    with get_openai_callback() as cb:        await asyncio.gather(            *[llm.agenerate(["我姐姐喜欢什么颜色的花？"]) for _ in range(3)]        )    print("\n另外的交互中使用的tokens:", cb.total_tokens)# 运行异步函数asyncio.run(additional_interactions())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>简单解释一下。</p><ol><li><code>async def</code>：这表示additional_interactions是一个异步函数。它可以使用await关键字在其中挂起执行，允许其他异步任务继续。</li><li><code>await asyncio.gather(...)</code>：这是asyncio库提供的一个非常有用的方法，用于并发地运行多个异步任务。它会等待所有任务完成，然后继续执行。</li><li><code>*[llm.agenerate([&quot;我姐姐喜欢什么颜色的花？&quot;]) for _ in range(3)]</code>：这实际上是一个Python列表解析，它生成了3个 llm.agenerate(…)的异步调用。asyncio.gather将并发地运行这3个调用。</li></ol><p>由于这3个llm.agenerate调用是并发的，所以它们不会按顺序执行，而是几乎同时启动，并在各自完成时返回。这意味着，即使其中一个调用由于某种原因需要更长时间，其他调用也不会被阻塞，它们会继续并完成。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>回调函数是计算机科学中一个重要和广泛应用的概念，它允许我们在特定的时间或条件下执行特定的代码。</p><p>回调函数在开发过程中有很多应用场景。</p><ol><li>异步编程：在JavaScript中，回调函数常常用于异步编程。例如，当你发送一个AJAX请求到服务器时，你可以提供一个回调函数，这个函数将在服务器的响应到达时被调用。</li><li>事件处理：在许多编程语言和框架中，回调函数被用作事件处理器。例如，你可能会写一个回调函数来处理用户的点击事件，当用户点击某个按钮时，这个函数就会被调用。</li><li>定时器：你可以使用回调函数来创建定时器。例如，你可以使用JavaScript的setTimeout或setInterval函数，并提供一个回调函数，这个函数会在指定的时间过后被调用。</li></ol><p>在 LangChain 中，回调机制同样为用户提供了灵活性和自定义能力，以便更好地控制和响应事件。CallbackHandler允许开发者在链的特定阶段或条件下注入自定义的行为，例如异步编程中的响应处理、事件驱动编程中的事件处理等。这为 LangChain 提供了灵活性和扩展性，使其能够适应各种应用场景。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li><p>我通过get_openai_callback重构了ConversationBufferMemory的程序，你能否把这个令牌计数器实现到其他记忆机制中？</p></li><li><p>在LangChain开发过程中，可以在构造函数中引入回调机制，我给出了一个示例，你能否尝试在请求过程（run&#x2F;apply方法）中引入回调机制？</p></li></ol><p>提示：请求回调常用在流式传输的实现中。在传统的传输中，我们必须等待这个函数生成所有数据后才能开始处理。在流式传输中，我们可以在数据被生成时立即开始处理。如果你想将单个请求的输出流式传输到一个WebSocket，你可以将一个Callback处理器传递给 call() 方法。</p><p>期待在留言区看到你的分享，如果觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>GitHub 代码： <a href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/callbacks/base.py">CallbackHandler</a> 中的可监控事件和方法</li><li>文档：LangChain中的 <a href="https://python.langchain.com/docs/modules/callbacks/">回调</a> 机制</li><li>文档：什么是 <a href="https://www.zhihu.com/question/19801131">回调函数</a>（知乎）</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;17｜回调函数：在AI应用中引入异步通信机制&quot;&gt;&lt;a href=&quot;#17｜回调函数：在AI应用中引入异步通信机制&quot; class=&quot;headerlink&quot; title=&quot;17｜回调函数：在AI应用中引入异步通信机制&quot;&gt;&lt;/a&gt;17｜回调函数：在AI应用中引入异步通信</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>16｜连接数据库：通过链和代理查询鲜花信息</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3353709308.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3353709308.html</id>
    <published>2023-10-20T09:48:40.000Z</published>
    <updated>2023-12-06T03:49:11.462Z</updated>
    
    <content type="html"><![CDATA[<h1 id="16｜连接数据库：通过链和代理查询鲜花信息"><a href="#16｜连接数据库：通过链和代理查询鲜花信息" class="headerlink" title="16｜连接数据库：通过链和代理查询鲜花信息"></a>16｜连接数据库：通过链和代理查询鲜花信息</h1><p>你好，我是黄佳，欢迎来到 LangChain 实战课！</p><p>一直以来，在计算机编程和数据库管理领域，所有的操作都需要通过严格、专业且结构化的语法来完成。这就是结构化查询语言（SQL）。当你想从一个数据库中提取信息或进行某种操作时，你需要使用这种特定的语言明确地告诉计算机你的要求。这不仅需要我们深入了解正在使用的技术，还需要对所操作的数据有充分的了解。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/829798a55a330518c7c95dcac948890d.png"></p><p>你需要拥有一个程序员基本的技能和知识才能有效地与计算机交互。不过，随着人工智能的兴起和大语言模型的发展，情况开始发生变化。</p><p>现在，我们正进入一个 <strong>全新的编程范式</strong>，其中机器学习和自然语言处理技术使得与计算机的交互变得更加自然。这意味着，我们可以用更加接近我们日常话语的自然语言来与计算机交流。例如，不用复杂的SQL语句查询数据库，我们可以简单地问：“请告诉我去年的销售额是多少？” 计算机能够理解这个问题，并给出相应的答案。</p><p>这种转变不仅使得非技术人员更容易与计算机交互，还为开发者提供了更大的便利性。简而言之，我们从“ <strong>告诉计算机每一步怎么做</strong>”，转变为“ <strong>告诉计算机我们想要什么</strong>”，整个过程变得更加人性化和高效。</p><h2 id="新的数据库查询范式"><a href="#新的数据库查询范式" class="headerlink" title="新的数据库查询范式"></a>新的数据库查询范式</h2><p>下面这个图，非常清晰地解释了这个以LLM为驱动引擎，从自然语言的（模糊）询问，到自然语言的查询结果输出的流程。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/60ffbcbe5a891ae74a12d3d5d24cf426.jpg"></p><p>这种范式结合了自然语言处理和传统数据库查询的功能，为用户提供了一个更为直观和高效的交互方式。下面我来解释下这个过程。</p><ol><li>提出问题：用户用自然语言提出一个问题，例如“去年的总销售额是多少？”。</li><li>LLM理解并转译：LLM首先会解析这个问题，理解其背后的意图和所需的信息。接着，模型会根据解析的内容，生成相应的SQL查询语句，例如 “SELECT SUM(sales) FROM sales_data WHERE year &#x3D; ‘last_year’;”。</li><li>执行SQL查询：生成的SQL查询语句会被发送到相应的数据库进行执行。数据库处理这个查询，并返回所需的数据结果。</li><li>LLM接收并解释结果：当数据库返回查询结果后，LLM会接收到这些数据。然后，LLM会开始解析这些数据，并将其转化为更容易被人类理解的答案格式。</li><li>提供答案：最后，LLM将结果转化为自然语言答案，并返回给用户。例如“去年的总销售额为1,000,000元”。</li></ol><p>你看，用户不需要知道数据库的结构，也不需要具备编写SQL的技能。他们只需要用自然语言提问，然后就可以得到他们所需的答案。这大大简化了与数据库的交互过程，并为各种应用场景提供了巨大的潜力。</p><h2 id="实战案例背景信息"><a href="#实战案例背景信息" class="headerlink" title="实战案例背景信息"></a>实战案例背景信息</h2><p>下面我们将通过LangChain实现这个新的数据库应用开发范式。</p><p>在这个实战案例中，我们的所有业务数据都存储在数据库中，而目标则是通过自然语言来为销售的每一种鲜花数据创建各种查询。这样，无论是员工还是顾客，当他们想了解某种鲜花的价格时，都可以快速地生成适当的查询语句。</p><p><strong>这就大大简化了查询过程和难度。</strong></p><p>首先，这个应用可以被简单地用作一个查询工具，允许员工在存货或销售系统中快速查找价格。员工不再需要记住复杂的查询语句或进行手动搜索，只需选择鲜花种类，告诉系统他所想要的东西，系统就会为他们生成正确的查询。</p><p>其次，这个模板也可以被整合到一个聊天机器人或客服机器人中。顾客可以直接向机器人询问：“红玫瑰的价格是多少？” 机器人会根据输入内容来调用LangChain和LLM，生成适当的查询，然后返回确切的价格给顾客。这样，不仅提高了服务效率，还增强了用户体验。</p><p>了解完项目的背景信息，下面我们就开始行动吧！</p><h2 id="创建数据库表"><a href="#创建数据库表" class="headerlink" title="创建数据库表"></a>创建数据库表</h2><p>首先，让我们创建一系列的数据库表，存储易速鲜花的业务数据。</p><p>这里，我们使用SQLite作为我们的示例数据库。它提供了轻量级的磁盘文件数据库，并不需要单独的服务器进程或系统，应用程序可以直接与数据库文件交互。同时，它也不需要配置、安装或管理，非常适合桌面应用、嵌入式应用或初创企业的简单需求。</p><p>SQLite支持ACID（原子性、一致性、隔离性、持久性），这意味着你的数据库操作即使在系统崩溃或电源失败的情况下也是安全的。虽然SQLite被认为是轻量级的，但它支持大多数SQL的标准特性，包括事务、触发器和视图。</p><p>因此，它也特别适用于那些不需要大型数据库系统带来的全部功能，但仍然需要数据持久性的应用程序，如移动应用或小型Web应用。当然，也非常适合我们做Demo。</p><p>sqlite3库，则是Python内置的轻量级SQLite数据库。通过sqlite3库，Python为开发者提供了一个简单、直接的方式来创建、查询和管理SQLite数据库。当你安装Python时，sqlite3模块已经包含在内，无需再进行额外的安装。</p><p>基于这个sqlite3库，创建业务数据的代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入sqlite3库import sqlite3# 连接到数据库conn = sqlite3.connect('FlowerShop.db')cursor = conn.cursor()# 执行SQL命令来创建Flowers表cursor.execute('''        CREATE TABLE Flowers (            ID INTEGER PRIMARY KEY,            Name TEXT NOT NULL,            Type TEXT NOT NULL,            Source TEXT NOT NULL,            PurchasePrice REAL,            SalePrice REAL,            StockQuantity INTEGER,            SoldQuantity INTEGER,            ExpiryDate DATE,            Description TEXT,            EntryDate DATE DEFAULT CURRENT_DATE        );    ''')# 插入5种鲜花的数据flowers = [    ('Rose', 'Flower', 'France', 1.2, 2.5, 100, 10, '2023-12-31', 'A beautiful red rose'),    ('Tulip', 'Flower', 'Netherlands', 0.8, 2.0, 150, 25, '2023-12-31', 'A colorful tulip'),    ('Lily', 'Flower', 'China', 1.5, 3.0, 80, 5, '2023-12-31', 'An elegant white lily'),    ('Daisy', 'Flower', 'USA', 0.7, 1.8, 120, 15, '2023-12-31', 'A cheerful daisy flower'),    ('Orchid', 'Flower', 'Brazil', 2.0, 4.0, 50, 2, '2023-12-31', 'A delicate purple orchid')]for flower in flowers:    cursor.execute('''        INSERT INTO Flowers (Name, Type, Source, PurchasePrice, SalePrice, StockQuantity, SoldQuantity, ExpiryDate, Description)        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);    ''', flower)# 提交更改conn.commit()# 关闭数据库连接conn.close()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>首先，我们连接到FlowerShop.db数据库。然后，我们创建一个名为Flowers的新表，此表将存储与每种鲜花相关的各种数据。</p><p>该表有以下字段：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d569yy552347e51ba9514a183yyef731.jpg"></p><p>接着，我们创建了一个名为flowers的列表，其中包含5种鲜花的所有相关数据。使用for循环，我们遍历flowers列表，并将每种鲜花的数据插入到Flowers表中。然后提交这些更改，把它们保存到数据库中。最后，我们关闭与数据库的连接。</p><h2 id="用-Chain-查询数据库"><a href="#用-Chain-查询数据库" class="headerlink" title="用 Chain 查询数据库"></a>用 Chain 查询数据库</h2><p>因为LangChain的数据库查询功能较新，目前还处于实验阶段，因此，需要先安装langchain-experimental包，这个包含有实验性的LangChain新功能。</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">pip install langchain-experimental<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>下面，我们就开始通过SQLDatabaseChain来查询数据库。代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain"># 导入langchain的实用工具和相关的模块from langchain.utilities import SQLDatabasefrom langchain.llms import OpenAIfrom langchain_experimental.sql import SQLDatabaseChain# 连接到FlowerShop数据库（之前我们使用的是Chinook.db）db = SQLDatabase.from_uri("sqlite:///FlowerShop.db")# 创建OpenAI的低级语言模型（LLM）实例，这里我们设置温度为0，意味着模型输出会更加确定性llm = OpenAI(temperature=0, verbose=True)# 创建SQL数据库链实例，它允许我们使用LLM来查询SQL数据库db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)# 运行与鲜花运营相关的问题response = db_chain.run("有多少种不同的鲜花？")print(response)response = db_chain.run("哪种鲜花的存货数量最少？")print(response)response = db_chain.run("平均销售价格是多少？")print(response)response = db_chain.run("从法国进口的鲜花有多少种？")print(response)response = db_chain.run("哪种鲜花的销售量最高？")print(response)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里，我们导入必要的LangChain模块，然后连接到FlowerShop数据库，初始化OpenAI的LLM实例。之后用SQLDatabaseChain来创建一个从LLM到数据库的链接。</p><p>最后，用db_chain.run()方法来查询多个与鲜花运营相关的问题，Chain的内部会把这些自然语言转换为SQL语句，并查询数据库表，得到查询结果之后，又通过LLM把这个结果转换成自然语言。</p><p>因此，Chain的输出结果是我们可以理解的，也是可以直接传递给Chatbot的人话。</p><p>输出如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/04b4de7c9fd93a98cc58e6de2f8aaaaa.jpg"></p><p>SQLDatabaseChain调用大语言模型，完美地完成了从自然语言（输入）到自然语言（输出）的新型SQL查询。</p><h2 id="用-Agent-查询数据库"><a href="#用-Agent-查询数据库" class="headerlink" title="用 Agent 查询数据库"></a>用 Agent 查询数据库</h2><p>除了通过Chain完成数据库查询之外，LangChain 还可以通过SQL Agent来完成查询任务。相比SQLDatabaseChain，使用 SQL 代理有一些优点。</p><ul><li>它可以根据数据库的架构以及数据库的内容回答问题（例如它会检索特定表的描述）。</li><li>它具有纠错能力，当执行生成的查询遇到错误时，它能够捕获该错误，然后正确地重新生成并执行新的查询。</li></ul><p>LangChain使用create_sql_agent函数来初始化代理，通过这个函数创建的SQL代理包含SQLDatabaseToolkit，这个工具箱中包含以下工具：</p><ul><li>创建并执行查询</li><li>检查查询语法</li><li>检索数据表的描述</li></ul><p>在这些工具的辅助之下，代理可以趋动LLM完成SQL查询任务。代码如下：</p><pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">from langchain.utilities import SQLDatabasefrom langchain.llms import OpenAIfrom langchain.agents import create_sql_agentfrom langchain.agents.agent_toolkits import SQLDatabaseToolkitfrom langchain.agents.agent_types import AgentType# 连接到FlowerShop数据库db = SQLDatabase.from_uri("sqlite:///FlowerShop.db")llm = OpenAI(temperature=0, verbose=True)# 创建SQL Agentagent_executor = create_sql_agent(    llm=llm,    toolkit=SQLDatabaseToolkit(db=db, llm=llm),    verbose=True,    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,)# 使用Agent执行SQL查询questions = [    "哪种鲜花的存货数量最少？",    "平均销售价格是多少？",]for question in questions:    response = agent_executor.run(question)    print(response)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>问题1的输出如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/97bd63f7bf63eb90a33eb88829a3d118.jpg"></p><p>问题2的输出如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/749ddf06803b961a16856494c33a163b.jpg"></p><p>可以看到，和Chain直接生成SQL语句不同，代理会使用 ReAct 风格的提示。首先，它思考之后，将先确定第一个action是使用工具 sql_db_list_tables，然后观察该工具所返回的表格，思考后再确定下一个 action是sql_db_schema，也就是创建SQL语句，逐层前进，直到得到答案。</p><h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>今天的内容很容易理解，又很开脑洞。我最想强调的，仍然是从“告诉计算机要做什么”的编程范式向“告诉计算机我们想要什么”的范式的转变。</p><p>这种转变具有深远的意义。</p><ol><li>更大的可达性：不再需要深入的技术知识或特定的编程背景。这意味着非技术人员，比如业务分析师、项目经理甚至是终端用户，都可以直接与数据交互。</li><li>高效率与生产力：传统的编程方法需要大量的时间和努力，尤其是在复杂的数据操作中。自然语言处理和理解能够显著减少这种负担，使得复杂的数据操作变得更加直观。</li><li>错误的减少：许多编程错误源于对特定语法或结构的误解，通过使用自然语言，这些源于误解的错误将大大减少。</li><li>人与机器的紧密结合：在这种新范式下，机器更像是人类的合作伙伴，而不仅仅是一个工具。它们可以理解我们的需求，并为我们提供解决方案，而无需我们明确指导每一步。</li></ol><p>但这种转变也带来了挑战。</p><ol><li>模糊性的问题：自然语言本身是模糊的，机器必须能够准确地解释这种模糊性，并在必要时寻求澄清。</li><li>对现有系统的依赖：虽然自然语言查询看起来很有吸引力，但许多现有系统可能不支持或不兼容这种新范式。</li><li>过度依赖：如果过于依赖机器为我们做决策，那么我们可能会失去对数据的深入了解和对结果的质疑。</li></ol><p>我们正处于一个技术变革的时刻，自然语言与编程之间的界限正在消失。对于那些愿意接受和采纳这种新范式的人来说，未来充满了无限的可能性。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><ol><li>LangChain中用Chain和Agent来查询数据库，这两种方式有什么异同？</li><li>你能否深入上面这两种方法的代码，看一看它们的底层实现。尤其是要看LangChain是如何做提示工程，指导模型生成 SQL 代码的。</li></ol><p>期待在留言区看到你的分享，如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h2><ol><li>文档：LangChain中关于数据库接口的 <a href="https://python.langchain.com/docs/modules/chains/popular/sqlite">官方文档</a></li><li>工具：上面的文档中提到了 <a href="https://docs.smith.langchain.com/">LangSmith</a> 工具，用于调试、测试和评估LangChain开发的LLM应用程序</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;16｜连接数据库：通过链和代理查询鲜花信息&quot;&gt;&lt;a href=&quot;#16｜连接数据库：通过链和代理查询鲜花信息&quot; class=&quot;headerlink&quot; title=&quot;16｜连接数据库：通过链和代理查询鲜花信息&quot;&gt;&lt;/a&gt;16｜连接数据库：通过链和代理查询鲜花信息&lt;/</summary>
      
    
    
    
    <category term="LangChain实战课" scheme="https://zhuansun.github.io/geekbang/categories/LangChain%E5%AE%9E%E6%88%98%E8%AF%BE/"/>
    
    
  </entry>
  
</feed>
