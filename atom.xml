<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>geekbang</title>
  
  
  <link href="https://zhuansun.github.io/geekbang/atom.xml" rel="self"/>
  
  <link href="https://zhuansun.github.io/geekbang/"/>
  <updated>2023-12-15T14:43:23.840Z</updated>
  <id>https://zhuansun.github.io/geekbang/</id>
  
  <author>
    <name>码农张三</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>01 | 架构到底是指什么？</title>
    <link href="https://zhuansun.github.io/geekbang/posts/2639868199.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/2639868199.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.840Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-架构到底是指什么？"><a href="#01-架构到底是指什么？" class="headerlink" title="01 | 架构到底是指什么？"></a>01 | 架构到底是指什么？</h1><p>你好，我是华仔。</p><p>2018年，我发布了《从0开始学架构》这门课程，分享了我之前在电信业务和移动互联网业务方面的经验和感悟。</p><p>后来，我转岗去了蚂蚁国际，从事更加复杂的支付业务。为什么说支付业务更加复杂？因为它涉及的关联方多、业务流程长、业务模型复杂，对安全、高可用、高性能等都有更高的要求。我有幸参与了一个海外钱包从0到1的建设过程，积累了不少实战经验，于是对复杂业务的架构设计有了新的理解。</p><p>2020年，因为身体等原因，我离开了蚂蚁国际，之后就一边休养身体，一边系统地总结梳理过去的经验，先后创作了《大厂晋升指南》和《架构实战营》。在打磨课程和与各位同学交流的过程中，我对于架构学习的难点和应用的痛点，又有了新的心得体会。</p><p>因此，我决定更新这门课程的部分内容，把这些新的收获也全部分享出来，希望能帮助你与时俱进地提升架构水平。</p><h2 id="架构到底是指什么"><a href="#架构到底是指什么" class="headerlink" title="架构到底是指什么"></a>架构到底是指什么</h2><p>对于技术人员来说，“架构”是一个再常见不过的词了。我们会对新员工培训整个系统的架构，参加架构设计评审，学习业界开源系统（例如MySQL和Hadoop）的架构，研究大公司的架构实现（例如微信架构和淘宝架构）……</p><p>虽然“架构”这个词很常见，但如果深究一下，“架构”到底是指什么，大部分人就搞不清楚了。例如以下这些问题，你能够准确地回答吗？</p><ol><li>微信有架构，微信的登录系统也有架构，微信的支付系统也有架构，当我们谈微信架构时，到底是在谈什么架构？</li><li>Linux有架构，MySQL有架构，JVM也有架构，使用Java开发、MySQL存储、跑在Linux上的业务系统也有架构，应该关注哪个架构呢？</li><li>架构和框架是什么关系？有什么区别？</li></ol><p>身为架构师，如果你连架构的定义都搞不清楚，那么无论是自己设计架构、给别人讲解架构，还是学习别人的架构，都会暴露问题，要么无从下手，要么张冠李戴。这无疑会成为你面试、晋升和带领团队工作时的绊脚石。</p><p>比如有些同学明明在系统架构上做了不少有价值的工作，但是在给晋升面试的评委讲解的时候，只会说“我们是微服务架构”，然后就不知道讲什么了。结果得到的评价大打折扣，晋升失败，非常可惜。</p><p>要想准确地理解架构的定义，关键就在于把三组容易混淆的概念梳理清楚：</p><ol><li>系统与子系统</li><li>模块与组件</li><li>框架与架构</li></ol><h2 id="系统与子系统"><a href="#系统与子系统" class="headerlink" title="系统与子系统"></a>系统与子系统</h2><p>我们先来看维基百科定义的“系统”：</p><blockquote><p>系统泛指由一群有关联的个体组成，根据某种规则运作，能完成个别元件不能单独完成的工作的群体。它的意思是“总体”“整体”或“联盟”。</p></blockquote><p>我来提炼一下里面的关键内容。</p><ol><li><strong>关联</strong>：系统是由一群有关联的个体组成的，没有关联的个体堆在一起不能成为一个系统。例如，把一个发动机和一台PC放在一起不能称之为一个系统，把发动机、底盘、轮胎、车架组合起来才能成为一台汽车。</li><li><strong>规则</strong>：系统内的个体需要按照指定的规则运作，而不是单个个体各自为政。规则规定了系统内个体分工和协作的方式。例如，汽车发动机负责产生动力，然后通过变速器和传动轴，将动力输出到车轮上，从而驱动汽车前进。</li><li><strong>能力</strong>：系统能力与个体能力有本质的差别，系统能力不是个体能力之和，而是产生了新的能力。例如，汽车能够载重前进，而发动机、变速器、传动轴、车轮本身都不具备这样的能力。</li></ol><p>我们再来看子系统的定义：</p><blockquote><p>子系统也是由一群有关联的个体所组成的系统，多半会是更大系统中的一部分。</p></blockquote><p>其实，子系统的定义和系统定义是一样的，只是观察的角度有差异，一个系统可能是另外一个更大系统的子系统。</p><p>按照这个定义，系统和子系统比较容易理解，我们以微信为例来做一个分析：</p><ol><li>微信本身是一个系统，包含聊天、登录、支付、朋友圈等子系统。</li><li>朋友圈这个系统又包括动态、评论、点赞等子系统。</li><li>评论这个系统可能又包括防刷子系统、审核子系统、发布子系统、存储子系统。</li><li>评论审核子系统不再包含业务意义上的子系统，而是包括各个模块或者组件，这些模块或者组件本身也是另外一个维度上的系统。例如，MySQL、Redis等是存储系统，但不是业务子系统。</li></ol><p>现在，我们可以回答第一个问题了。一个系统的架构，只包括 <strong>顶层</strong> 这一个层级的架构，而不包括下属子系统层级的架构。所以微信架构，就是指微信系统这个层级的架构。当然，微信的子系统，比如支付系统，也有它自己的架构，同样只包括 <strong>顶层</strong>。</p><h2 id="模块与组件"><a href="#模块与组件" class="headerlink" title="模块与组件"></a>模块与组件</h2><p>模块和组件两个概念在实际工作中很容易混淆，我们经常能够听到类似这样的说法：</p><ul><li>MySQL模块主要负责存储数据，而Elasticsearch模块主要负责数据搜索。</li><li>我们有安全加密组件、有审核组件。</li><li>App的下载模块使用了第三方的组件。</li></ul><p>造成这种现象的主要原因是，模块与组件的定义并不好理解，也不能很好地进行区分。我们来看看这两者在维基百科上的定义：</p><blockquote><p>软件模块（Module）是一套一致而互相有紧密关连的软件组织。它分别包含了程序和数据结构两部分。现代软件开发往往利用模块作为合成的单位。模块的接口表达了由该模块提供的功能和调用它时所需的元素。模块是可能分开被编写的单位。这使它们可再用和允许人员同时协作、编写及研究不同的模块。</p><p>软件组件定义为自包含的、可编程的、可重用的、与语言无关的软件单元，软件组件可以很容易被用于组装应用程序中。</p></blockquote><p>可能你看完这两个定义后一头雾水，还是不知道这两者有什么区别。造成这种现象的根本原因是， <strong>模块和组件都是系统的组成部分，只是从不同的角度拆分系统而已</strong>。</p><p>从业务逻辑的角度来拆分系统后，得到的单元就是“模块”；从物理部署的角度来拆分系统后，得到的单元就是“组件”。划分模块的主要目的是职责分离；划分组件的主要目的是单元复用。</p><p>其实，“组件”的英文Component也可翻译成中文的“零件”一词。“零件”更容易理解一些，它是一个物理的概念，并且具备“独立且可替换”的特点。</p><p>我以一个最简单的网站系统来为例。假设我们要做一个学生信息管理系统，这个系统从逻辑的角度来拆分，可以分为“登录注册模块”“个人信息模块”和“个人成绩模块”；从物理的角度来拆分，可以拆分为Nginx、Web服务器和MySQL。</p><p>现在，我们可以回答第二个问题了。如果你是业务系统的架构师，首先需要思考怎么从业务逻辑的角度把系统拆分成一个个模块 <strong>角色</strong>，其次需要思考怎么从物理部署的角度把系统拆分成组件 <strong>角色，</strong> 例如选择MySQL作为存储系统。但是对于MySQL内部的体系架构（Parser、Optimizer、Caches&amp;Buffers和Storage Engines等），你其实是可以不用关注的，也不需要在你的业务系统架构中展现这些内容。</p><h2 id="框架与架构"><a href="#框架与架构" class="headerlink" title="框架与架构"></a>框架与架构</h2><p>框架是和架构比较相似的概念，且两者有较强的关联关系，所以在实际工作中，这两个概念有时我们容易分不清楚。参考维基百科上框架与架构的定义，我来解释两者的区别。</p><blockquote><p>软件框架（Software framework）通常指的是为了实现某个业界标准或完成特定基本任务的软件组件规范，也指为了实现某个软件组件规范时，提供规范所要求之基础功能的软件产品。</p></blockquote><p>我来提炼一下其中关键部分：</p><ol><li>框架是组件规范：例如，MVC就是一种最常见的开发规范，类似的还有MVP、MVVM、J2EE等框架。</li><li>框架提供基础功能的产品：例如，Spring MVC是MVC的开发框架，除了满足MVC的规范，Spring提供了很多基础功能来帮助我们实现功能，包括注解（@Controller等）、Spring Security、Spring JPA等很多基础功能。</li></ol><blockquote><p>软件架构指软件系统的“基础结构”，创造这些基础结构的准则，以及对这些结构的描述。</p></blockquote><p>单纯从定义的角度来看，框架和架构的区别还是比较明显的： <strong>框架关注的是“规范”，架构关注的是“结构”</strong>。</p><p>框架的英文是Framework，架构的英文是Architecture，Spring MVC的英文文档标题就是“Web MVC framework”。</p><p>虽然如此，在实际工作中我们却经常碰到一些似是而非的说法，例如：</p><ul><li><p>我们的系统是MVC架构。</p></li><li><p>我们需要将Android App重构为MVP架构。</p></li><li><p>我们的系统基于SSH框架开发。</p></li><li><p>我们是SSH的架构。</p></li><li><p>XX系统是基于Spring MVC框架开发，标准的MVC架构。</p><p>……</p></li></ul><p>究竟什么说法是对的，什么说法是错的呢？</p><p>其实这些说法都是对的。造成这种现象的根本原因隐藏于架构的定义中，关键就是“基础结构”这个概念，并没有明确说是从什么角度来分解的。采用不同的角度或者维度，可以将系统划分为不同的结构，其实我在“模块与组件”中的“学生管理系统”示例已经包含了这点。</p><p>从业务逻辑的角度分解，“学生管理系统”的架构是：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/af3f5d6afe79d8c37b727606f749a1a8.jpg" alt="图片"></p><p>从物理部署的角度分解，“学生管理系统”的架构是：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/28ca0b7912ea0dda4a9fd4ceec75bf69.jpg" alt="图片"></p><p>从开发规范的角度分解，“学生管理系统”可以采用标准的MVC框架来开发，因此架构又变成了MVC架构：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/3e5f788e9dceb7f2cd9eb79d0d92fd1d.jpg" alt="图片"></p><p>这些“架构”，都是“学生管理系统”正确的架构，只是从不同的角度来分解而已，这也是IBM的RUP将软件架构视图分为著名的“ <strong>4+1视图</strong>”的原因。</p><p>现在，我们可以回答第三个问题了。框架是一整套开发规范，架构是某一套开发规范下的具体落地方案，包括各个模块之间的 <strong>组合关系</strong> 以及它们协同起来完成功能的 <strong>运作规则</strong>。</p><h2 id="重新定义架构：4R架构"><a href="#重新定义架构：4R架构" class="headerlink" title="重新定义架构：4R架构"></a>重新定义架构：4R架构</h2><p>参考维基百科的定义，再结合我自己的一些理解和思考，我将软件架构重新定义为： <strong>软件架构指软件系统的顶层（Rank）结构，它定义了系统由哪些角色（Role）组成，角色之间的关系（Relation）和运作规则（Rule）。</strong></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/670a502889683719f63846762a710ec1.jpg" alt="图片"></p><p>因为这个定义中的4个关键词，都可以用R开头的英文单词来表示，分别是Rank、Role、Relation和Rule，所以我把定义简称为“4R架构定义”，每个R的详细解释如下。</p><p>第一个R，Rank。它是指软件架构是分层的，对应“系统”和“子系统”的分层关系。通常情况下，我们只需要关注某一层的架构，最多展示相邻两层的架构，而不需要把每一层的架构全部糅杂在一起。无论是架构设计还是画架构图，都应该采取“自顶向下，逐步细化”的方式。以微信为例，Rank的含义如下所示：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/452ce48209b1e9ea77484e68dbb8f0b1.jpg" alt="图片"></p><p>注：L0\L1\L2指层级，一个L0往下可以分解多个L1，一个L1可以往下分解多个L2，以此类推，一般建议不超过5层（L0~L4）。</p><p>第二个R，Role。它是指软件系统包含哪些角色，每个角色都会负责系统的一部分功能。架构设计最重要的工作之一就是将系统拆分为多个角色。最常见的微服务拆分其实就是将整体复杂的 <strong>业务系统</strong> 按照业务领域的方式，拆分为多个微服务，每个微服务就是系统的一个角色。</p><p>第三个R，Relation。它是指软件系统的角色之间的关系，对应到架构图中其实就是连接线，角色之间的关系不能乱连，任何关系最后都需要代码来实现，包括连接方式（HTTP、TCP、UDP和串口等）、数据协议（JSON、XML和二进制等）以及具体的接口等。</p><p>第四个R，Rule。它是指软件系统角色之间如何协作来完成系统功能。我们在前面解读什么是“系统”的时候提到过：系统能力不是个体能力之和，而是产生了新的能力。那么这个新能力具体如何完成的呢？具体哪些角色参与了这个新能力呢？这就是Rule所要表达的内容。在架构设计的时候，核心的业务场景都需要设计Rule。</p><p>在实际工作中，为了方便理解，Rank、Role和Relation是通过系统架构图来展示的，而Rule是通过系统序列图（System Sequence Diagram）来展示的。</p><p>我们以一个简化的支付系统为例，支付系统架构图如下所示：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/952cdceaa1bd5ed9f5fb039733dabafc.jpg" alt="图片"></p><p>“扫码支付”这个核心场景的系统序列图如下所示：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/0e7a35a01b62e5590566c09eff6b19ea.jpg" alt="图片"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你梳理了与架构有关的几个容易混淆的概念，包括系统与子系统、模块与组件、框架与架构，并且提炼出了4R架构定义，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧。你原来理解的架构是如何定义的？对比我今天讲的架构定义，你觉得差异在哪里？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%"  frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/6458" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;01-架构到底是指什么？&quot;&gt;&lt;a href=&quot;#01-架构到底是指什么？&quot; class=&quot;headerlink&quot; title=&quot;01 | 架构到底是指什么？&quot;&gt;&lt;/a&gt;01 | 架构到底是指什么？&lt;/h1&gt;&lt;p&gt;你好，我是华仔。&lt;/p&gt;
&lt;p&gt;2018年，我发布了</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>02 | 架构设计的历史背景</title>
    <link href="https://zhuansun.github.io/geekbang/posts/2835423006.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/2835423006.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.843Z</updated>
    
    <content type="html"><![CDATA[<h1 id="02-架构设计的历史背景"><a href="#02-架构设计的历史背景" class="headerlink" title="02 | 架构设计的历史背景"></a>02 | 架构设计的历史背景</h1><p>理解了架构的有关概念和定义之后，今天，我会给你讲讲架构设计的历史背景。我认为，如果想要深入理解一个事物的本质，最好的方式就是去追寻这个事物出现的历史背景和推动因素。我们先来简单梳理一下软件开发进化的历史，探索一下软件架构出现的历史背景。</p><h2 id="机器语言（1940年之前）"><a href="#机器语言（1940年之前）" class="headerlink" title="机器语言（1940年之前）"></a>机器语言（1940年之前）</h2><p>最早的软件开发使用的是“ <strong>机器语言</strong>”，直接使用二进制码0和1来表示机器可以识别的指令和数据。例如，在8086机器上完成“s&#x3D;768+12288-1280”的数学运算，机器码如下：</p><pre class="line-numbers language-none"><code class="language-none">101100000000000000000011000001010000000000110000001011010000000000000101<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>不用多说，不管是当时的程序员，还是现在的程序员，第一眼看到这样一串东西时，肯定是一头雾水，因为这实在是太难看懂了，这还只是一行运算，如果要输出一个“hello world”，面对几十上百行这样的0&#x2F;1串，眼睛都要花了！</p><p>看都没法看，更何况去写这样的程序，如果不小心哪个地方敲错了，将1敲成了0，例如：</p><pre class="line-numbers language-none"><code class="language-none">101100000000000000000011000001010000000000110000001011000000000000000101<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>如果要找出这个程序中的错误，程序员的心里阴影面积有多大？</p><p>归纳一下，机器语言的主要问题是三难： <strong>太难写、太难读、太难改</strong>！</p><h2 id="汇编语言（20世纪40年代）"><a href="#汇编语言（20世纪40年代）" class="headerlink" title="汇编语言（20世纪40年代）"></a>汇编语言（20世纪40年代）</h2><p>为了解决机器语言编写、阅读、修改复杂的问题， <strong>汇编语言</strong> 应运而生。汇编语言又叫“ <strong>符号语言</strong>”，用助记符代替机器指令的操作码，用地址符号（Symbol）或标号（Label）代替指令或操作数的地址。</p><p>例如，为了完成“将寄存器BX的内容送到AX中”的简单操作，汇编语言和机器语言分别如下。</p><pre class="line-numbers language-none"><code class="language-none">机器语言：1000100111011000汇编语言：mov ax,bx<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>相比机器语言来说，汇编语言就清晰得多了。mov是操作，ax和bx是寄存器代号，mov ax,bx语句基本上就是“将寄存器BX的内容送到AX”的简化版的翻译，即使不懂汇编，单纯看到这样一串语言，至少也能明白大概意思。</p><p>汇编语言虽然解决了机器语言读写复杂的问题，但本质上还是 <strong>面向机器</strong> 的，因为写汇编语言需要我们精确了解计算机底层的知识。例如，CPU指令、寄存器、段地址等底层的细节。这对于程序员来说同样很复杂，因为程序员需要将现实世界中的问题和需求按照机器的逻辑进行翻译。例如，对于程序员来说，在现实世界中面对的问题是4 + 6 &#x3D; ？。而要用汇编语言实现一个简单的加法运算，代码如下：</p><pre class="line-numbers language-none"><code class="language-none">.section .data  a: .int 10  b: .int 20  format: .asciz &quot;%d\n&quot;.section .text.global _start_start:  movl a, %edx  addl b, %edx  pushl %edx  pushl $format  call printf  movl $0, (%esp)  call exit<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这还只是实现一个简单的加法运算所需要的汇编程序，可以想象一下，实现一个四则运算的程序会更加复杂，更不用说用汇编写一个操作系统了！</p><p>除了编写本身复杂，还有另外一个复杂的地方在于：不同CPU的汇编指令和结构是不同的。例如，Intel的CPU和Motorola的CPU指令不同，同样一个程序，为Intel的CPU写一次，还要为Motorola的CPU再写一次，而且指令完全不同。</p><h2 id="高级语言（20世纪50年代）"><a href="#高级语言（20世纪50年代）" class="headerlink" title="高级语言（20世纪50年代）"></a>高级语言（20世纪50年代）</h2><p>为了解决汇编语言的问题，计算机前辈们从20世纪50年代开始又设计了多个 <strong>高级语言</strong>，最初的高级语言有下面几个，并且这些语言至今还在特定的领域继续使用。</p><ul><li><p>Fortran：1955年，名称取自”FORmula TRANslator”，即公式翻译器，由约翰·巴科斯（John Backus）等人发明。</p></li><li><p>LISP：1958年，名称取自”LISt Processor”，即枚举处理器，由约翰·麦卡锡（John McCarthy）等人发明。</p></li><li><p>Cobol：1959年，名称取自”Common Business Oriented Language”，即通用商业导向语言，由葛丽丝·霍普（Grace Hopper）发明。</p></li></ul><p>为什么称这些语言为“高级语言”呢？原因在于这些语言让程序员不需要关注机器底层的低级结构和逻辑，而只要关注具体的问题和业务即可。</p><p>还是以4 + 6&#x3D;？这个加法为例，如果用LISP语言实现，只需要简单一行代码即可：</p><pre class="line-numbers language-none"><code class="language-none">(+ 4 6)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>除此以外，通过编译程序的处理，高级语言可以被编译为适合不同CPU指令的机器语言。程序员只要写一次程序，就可以在多个不同的机器上编译运行，无须根据不同的机器指令重写整个程序。</p><h2 id="第一次软件危机与结构化程序设计（20世纪60年代-20世纪70年代）"><a href="#第一次软件危机与结构化程序设计（20世纪60年代-20世纪70年代）" class="headerlink" title="第一次软件危机与结构化程序设计（20世纪60年代~20世纪70年代）"></a>第一次软件危机与结构化程序设计（20世纪60年代~20世纪70年代）</h2><p>高级语言的出现，解放了程序员，但好景不长，随着软件的规模和复杂度的大大增加，20世纪60年代中期开始爆发了第一次软件危机，典型表现有软件质量低下、项目无法如期完成、项目严重超支等，因为软件而导致的重大事故时有发生。例如，1963年美国（ <a href="http://en.wikipedia.org/wiki/Mariner_1">http://en.wikipedia.org/wiki/Mariner_1</a>）的水手一号火箭发射失败事故，就是因为一行FORTRAN代码错误导致的。</p><p>软件危机最典型的例子莫过于IBM的System&#x2F;360的操作系统开发。佛瑞德·布鲁克斯（Frederick P. Brooks, Jr.）作为项目主管，率领2000多个程序员夜以继日地工作，共计花费了5000人一年的工作量，写出将近100万行的源码，总共投入5亿美元，是美国的“曼哈顿”原子弹计划投入的1&#x2F;4。尽管投入如此巨大，但项目进度却一再延迟，软件质量也得不到保障。布鲁克斯后来基于这个项目经验而总结的《人月神话》一书，成了畅销的软件工程书籍。</p><p>为了解决问题，在1968、1969年连续召开两次著名的NATO会议，会议正式创造了“软件危机”一词，并提出了针对性的解决方法“软件工程”。虽然“软件工程”提出之后也曾被视为软件领域的银弹，但后来事实证明，软件工程同样无法根除软件危机，只能在一定程度上缓解软件危机。</p><p>差不多同一时间，“结构化程序设计”作为另外一种解决软件危机的方案被提了出来。艾兹赫尔·戴克斯特拉（Edsger Dijkstra）于1968年发表了著名的《GOTO有害论》论文，引起了长达数年的论战，并由此产生了 <strong>结构化程序设计方法</strong>。同时，第一个结构化的程序语言Pascal也在此时诞生，并迅速流行起来。</p><p>结构化程序设计的主要特点是抛弃goto语句，采取“自顶向下、逐步细化、模块化”的指导思想。结构化程序设计本质上还是一种面向过程的设计思想，但通过“自顶向下、逐步细化、模块化”的方法，将软件的复杂度控制在一定范围内，从而从整体上降低了软件开发的复杂度。结构化程序方法成为了20世纪70年代软件开发的潮流。</p><h2 id="第二次软件危机与面向对象（20世纪80年代）"><a href="#第二次软件危机与面向对象（20世纪80年代）" class="headerlink" title="第二次软件危机与面向对象（20世纪80年代）"></a>第二次软件危机与面向对象（20世纪80年代）</h2><p>结构化编程的风靡在一定程度上缓解了软件危机，然而随着硬件的快速发展，业务需求越来越复杂，以及编程应用领域越来越广泛，第二次软件危机很快就到来了。</p><p>第二次软件危机的根本原因还是在于软件生产力远远跟不上硬件和业务的发展。第一次软件危机的根源在于软件的“逻辑”变得非常复杂，而第二次软件危机主要体现在软件的“扩展”变得非常复杂。结构化程序设计虽然能够解决（也许用“缓解”更合适）软件逻辑的复杂性，但是对于业务变化带来的软件扩展却无能为力，软件领域迫切希望找到新的银弹来解决软件危机，在这种背景下， <strong>面向对象的思想</strong> 开始流行起来。</p><p>面向对象的思想并不是在第二次软件危机后才出现的，早在1967年的Simula语言中就开始提出来了，但第二次软件危机促进了面向对象的发展。 <strong>面向对象真正开始流行是在20世纪80年代，主要得益于C++的功劳，后来的Java、C#把面向对象推向了新的高峰。到现在为止，面向对象已经成为了主流的开发思想。</strong></p><p>虽然面向对象开始也被当作解决软件危机的银弹，但事实证明，和软件工程一样，面向对象也不是银弹，而只是一种新的软件方法而已。</p><h2 id="软件架构的历史背景"><a href="#软件架构的历史背景" class="headerlink" title="软件架构的历史背景"></a>软件架构的历史背景</h2><p>虽然早在20世纪60年代，戴克斯特拉这位上古大神就已经涉及软件架构这个概念了，但软件架构真正流行却是从20世纪90年代开始的，由于在Rational和Microsoft内部的相关活动，软件架构的概念开始越来越流行了。</p><p>与之前的各种新方法或者新理念不同的是，“软件架构”出现的背景并不是整个行业都面临类似相同的问题，“软件架构”也不是为了解决新的软件危机而产生的，这是怎么回事呢？</p><p>卡内基·梅隆大学的玛丽·肖（Mary Shaw）和戴维·加兰（David Garlan）对软件架构做了很多研究，他们在1994年的一篇文章《软件架构介绍》（An Introduction to Software Architecture）中写到：</p><blockquote><p>“When systems are constructed from many components, the organization of the overall system-the software architecture-presents a new set of design problems.”</p></blockquote><p>简单翻译一下：随着软件系统规模的增加，计算相关的算法和数据结构不再构成主要的设计问题；当系统由许多部分组成时，整个系统的组织，也就是所说的“软件架构”，导致了一系列新的设计问题。</p><p>这段话很好地解释了“软件架构”为何先在Rational或者Microsoft这样的大公司开始逐步流行起来。因为只有大公司开发的软件系统才具备较大规模，而只有规模较大的软件系统才会面临软件架构相关的问题，例如：</p><ul><li><p>系统规模庞大，内部耦合严重，开发效率低；</p></li><li><p>系统耦合严重，牵一发动全身，后续修改和扩展困难；</p></li><li><p>系统逻辑复杂，容易出问题，出问题后很难排查和修复。</p></li></ul><p>软件架构的出现有其历史必然性。20世纪60年代第一次软件危机引出了“结构化编程”，创造了“模块”概念；20世纪80年代第二次软件危机引出了“面向对象编程”，创造了“对象”概念；到了20世纪90年代“软件架构”开始流行，创造了“组件”概念。我们可以看到，“模块”“对象”“组件”本质上都是对达到一定规模的软件进行拆分，差别只是在于随着软件的复杂度不断增加，拆分的粒度越来越粗，拆分的层次越来越高。</p><p>《人月神话》中提到的IBM 360大型系统，开发时间是1964年，那个时候结构化编程都还没有提出来，更不用说软件架构了。如果IBM 360系统放在20世纪90年代开发，不管是质量还是效率、成本，都会比1964年开始做要好得多，当然，这样的话我们可能就看不到《人月神话》了。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你回顾了软件开发进化的历史，以及软件架构出现的历史背景，从历史发展的角度，希望对你深入了解架构设计的本质有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧。为何结构化编程、面向对象编程、软件工程、架构设计最后都没有成为软件领域的银弹？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%"  frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/6463" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;02-架构设计的历史背景&quot;&gt;&lt;a href=&quot;#02-架构设计的历史背景&quot; class=&quot;headerlink&quot; title=&quot;02 | 架构设计的历史背景&quot;&gt;&lt;/a&gt;02 | 架构设计的历史背景&lt;/h1&gt;&lt;p&gt;理解了架构的有关概念和定义之后，今天，我会给你讲讲架</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>03 | 架构设计的目的</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3795714184.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3795714184.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.846Z</updated>
    
    <content type="html"><![CDATA[<h1 id="03-架构设计的目的"><a href="#03-架构设计的目的" class="headerlink" title="03 | 架构设计的目的"></a>03 | 架构设计的目的</h1><p>周二，我们聊了架构出现的历史背景和推动因素。以史为鉴，对我们了解架构设计的目的很有帮助。谈到架构设计，相信每个技术人员都是耳熟能详，但如果深入探讨一下，“为何要做架构设计？”或者“架构设计目的是什么？”类似的问题，大部分人可能从来没有思考过，或者即使有思考，也没有太明确可信的答案。</p><h2 id="架构设计的误区"><a href="#架构设计的误区" class="headerlink" title="架构设计的误区"></a>架构设计的误区</h2><p>关于架构设计的目的，常见的误区有：</p><ul><li>因为架构很重要，所以要做架构设计</li></ul><p>这是一句正确的废话，架构是很重要，但架构为何重要呢？</p><p>例如：不做架构设计系统就跑不起来么？</p><p>其实不然，很多朋友尤其是经历了创业公司的朋友可能会发现，公司的初始产品可能没有架构设计，大伙撸起袖子简单讨论一下就开始编码了，根本没有正规的架构设计过程，而且也许产品开发速度还更快，上线后运行也还不错。</p><p>例如：做了架构设计就能提升开发效率么？</p><p>也不尽然，实际上有时候最简单的设计开发效率反而是最高的，架构设计毕竟需要投入时间和人力，这部分投入如果用来尽早编码，项目也许会更快。</p><p>例如：设计良好的架构能促进业务发展么？</p><p>好像有一定的道理，例如设计高性能的架构能够让用户体验更好，但反过来想，我们照抄微信的架构，业务就能达到微信的量级么？肯定不可能，不要说达到微信的量级，达到微信的1&#x2F;10做梦都要笑醒了。</p><ul><li>不是每个系统都要做架构设计吗</li></ul><p>这其实是知其然不知其所以然，系统确实要做架构设计，但还是不知道为何要做架构设计，反正大家都要做架构设计，所以做架构设计肯定没错。</p><p>这样的架构师或者设计师很容易走入生搬硬套业界其他公司已有架构的歧路，美其名曰“参考”“微改进”。一旦强行引入其他公司架构后，很可能会发现架构水土不服，或者运行起来很别扭等各种情况，最后往往不得不削足适履，或者不断重构，甚至无奈推倒重来。</p><ul><li>公司流程要求系统开发过程中必须有架构设计</li></ul><p>与此答案类似还有因为“架构师总要做点事情”，所以要做架构设计，其实都是舍本逐末。因为流程有规定，所以要做架构设计；因为架构师要做事，所以要做架构设计，这都是很表面地看问题，并没有真正理解为何要做架构设计，而且很多需求并不一定要进行架构设计。如果认为架构师一定要找点事做，流程一定要进行架构设计，就会出现事实上不需要架构设计但形式上却继续去做架构设计，不但浪费时间和人力，还会拖慢整体的开发进度。</p><ul><li>为了高性能、高可用、可扩展，所以要做架构设计</li></ul><p>能够给出这个答案，说明已经有了一定的架构经历或者基础，毕竟确实很多架构设计都是冲着高性能、高可用……等“高XX”的目标去的。</p><p>但往往持有这类观点的架构师和设计师会给项目带来巨大的灾难，这绝不是危言耸听，而是很多实际发生的事情，为什么会这样呢？因为这类架构师或者设计师不管三七二十一，不管什么系统，也不管什么业务，上来就要求“高性能、高可用、高扩展”，结果就会出现架构设计复杂无比，项目落地遥遥无期，团队天天吵翻天……等各种让人抓狂的现象，费尽九牛二虎之力将系统整上线，却发现运行不够稳定，经常出问题，出了问题很难解决，加个功能要改1个月……等各种继续让人抓狂的事件。</p><h2 id="架构设计的真正目的"><a href="#架构设计的真正目的" class="headerlink" title="架构设计的真正目的"></a>架构设计的真正目的</h2><p>那架构设计的真正目的究竟是什么？</p><p>从周二与你分享的架构设计的历史背景，可以看到，整个软件技术发展的历史，其实就是一部与“复杂度”斗争的历史，架构的出现也不例外。简而言之，架构也是为了应对软件系统复杂度而提出的一个解决方案，通过回顾架构产生的历史背景和原因，我们可以基本推导出答案： <strong>架构设计的主要目的是为了解决软件系统复杂度带来的问题</strong>。</p><p>这个结论虽然很简洁，但却是架构设计过程中需要时刻铭记在心的一条准则，为什么这样说呢？</p><p>首先，遵循这条准则能够让“新手”架构师 <strong>心中有数，而不是一头雾水</strong>。</p><p>新手架构师开始做架构设计的时候，心情都很激动，希望大显身手，甚至恨不得一出手就设计出世界上最牛的XX架构，从此走上人生巅峰，但真的面对具体的需求时，往往都会陷入一头雾水的状态：</p><p>“这么多需求，从哪里开始下手进行架构设计呢？”。</p><p>“架构设计要考虑高性能、高可用、高扩展……这么多高XX，全部设计完成估计要1个月，但老大只给了1周时间”。</p><p>“业界A公司的架构是X，B公司的方案是Y，两个差别比较大，该参考哪一个呢？”。</p><p>以上类似问题，如果明确了“架构设计是为了解决软件复杂度”原则后，就很好回答。</p><ul><li>“这么多需求，从哪里开始下手进行架构设计呢？”</li></ul><blockquote><p>通过熟悉和理解需求，识别系统复杂性所在的地方，然后针对这些复杂点进行架构设计。</p></blockquote><ul><li>“架构设计要考虑高性能、高可用、高扩展……这么多高XX，全部设计完成估计要1个月，但老大只给了1周时间”</li></ul><blockquote><p>架构设计并不是要面面俱到，不需要每个架构都具备高性能、高可用、高扩展等特点，而是要识别出复杂点然后有针对性地解决问题。</p></blockquote><ul><li>“业界A公司的架构是X，B公司的方案是Y，两个差别比较大，该参考哪一个呢？”</li></ul><blockquote><p>理解每个架构方案背后所需要解决的复杂点，然后才能对比自己的业务复杂点，参考复杂点相似的方案。</p></blockquote><p>其次，遵循这条准则能够让“老鸟”架构师 <strong>有的放矢，而不是贪大求全</strong>。</p><p>技术人员往往都希望自己能够做出最牛的东西，架构师也不例外，尤其是一些“老鸟”架构师，为了证明自己的技术牛，可能会陷入贪大求全的焦油坑而无法自拔。例如：</p><p>“我们的系统一定要做到每秒TPS 10万”。</p><p>“淘宝的架构是这么做的，我们也要这么做”。</p><p>“Docker现在很流行，我们的架构应该将Docker应用进来”。</p><p>以上这些想法，如果拿“架构设计是为了解决软件复杂度”这个原则来衡量，就很容易判断。</p><ul><li>“我们的系统一定要做到每秒TPS 10万”</li></ul><blockquote><p>如果系统的复杂度不是在性能这部分，TPS做到10万并没有什么用。</p></blockquote><ul><li>“淘宝的架构是这么做的，我们也要这么做”</li></ul><blockquote><p>淘宝的架构是为了解决淘宝业务的复杂度而设计的，淘宝的业务复杂度并不就是我们的业务复杂度，绝大多数业务的用户量都不可能有淘宝那么大。</p></blockquote><ul><li>“Docker现在很流行，我们的架构应该将Docker应用进来”</li></ul><blockquote><p>Docker不是万能的，只是为了解决资源重用和动态分配而设计的，如果我们的系统复杂度根本不是在这方面，引入Docker没有什么意义。</p></blockquote><h2 id="简单的复杂度分析案例"><a href="#简单的复杂度分析案例" class="headerlink" title="简单的复杂度分析案例"></a>简单的复杂度分析案例</h2><p>我来分析一个简单的案例，一起来看看如何将“架构设计的真正目的是为了解决软件系统复杂度带来的问题”这个指导思想应用到实践中。</p><p>假设我们需要设计一个大学的学生管理系统，其基本功能包括登录、注册、成绩管理、课程管理等。当我们对这样一个系统进行架构设计的时候，首先应识别其复杂度到底体现在哪里。</p><p>性能：一个学校的学生大约1 ~ 2万人，学生管理系统的访问频率并不高，平均每天单个学生的访问次数平均不到1次，因此性能这部分并不复杂，存储用MySQL完全能够胜任，缓存都可以不用，Web服务器用Nginx绰绰有余。</p><p>可扩展性：学生管理系统的功能比较稳定，可扩展的空间并不大，因此可扩展性也不复杂。</p><p>高可用：学生管理系统即使宕机2小时，对学生管理工作影响并不大，因此可以不做负载均衡，更不用考虑异地多活这类复杂的方案了。但是，如果学生的数据全部丢失，修复是非常麻烦的，只能靠人工逐条修复，这个很难接受，因此需要考虑存储高可靠，这里就有点复杂了。我们需要考虑多种异常情况：机器故障、机房故障，针对机器故障，我们需要设计MySQL同机房主备方案；针对机房故障，我们需要设计MySQL跨机房同步方案。</p><p>安全性：学生管理系统存储的信息有一定的隐私性，例如学生的家庭情况，但并不是和金融相关的，也不包含强隐私（例如玉照、情感）的信息，因此安全性方面只要做3个事情就基本满足要求了：Nginx提供ACL控制、用户账号密码管理、数据库访问权限控制。</p><p>成本：由于系统很简单，基本上几台服务器就能够搞定，对于一所大学来说完全不是问题，可以无需太多关注。</p><p>还有其他方面，如果有兴趣，你可以自行尝试去分析。通过我上面的分析，可以看到这个方案的主要复杂性体现在存储可靠性上，需要保证异常的时候，不要丢失所有数据即可（丢失几个或者几十个学生的信息问题不大），对应的架构如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/970f83d548b6b4a5c7903b3fc1f3b8d4.jpg"></p><p>学生管理系统虽然简单，但麻雀虽小五脏俱全，基本上能涵盖软件系统复杂度分析的各个方面，而且绝大部分技术人员都曾经自己设计或者接触过类似的系统，如果将这个案例和自己的经验对比，相信会有更多的收获。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你分析了架构设计的误区，结合周二讲的架构设计的历史背景，给出架构设计的主要目的是为了解决软件系统复杂度带来的问题，并分析了一个简单复杂度的案例，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧。请按照“架构设计的主要目的是为了解决软件复杂度带来的问题”这个指导思想来分析一下你目前的业务系统架构，看看是否和你当时分析的结果一样？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><p>最后给你推荐一个课程，极客时间新上线了《Java核心技术36讲》，由Oracle首席工程师杨晓峰老师给你精讲大厂Java面试题，帮你构建Java知识体系，你可以点击下方图片进入课程。</p><hr><iframe width="100%"  frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/6472" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;03-架构设计的目的&quot;&gt;&lt;a href=&quot;#03-架构设计的目的&quot; class=&quot;headerlink&quot; title=&quot;03 | 架构设计的目的&quot;&gt;&lt;/a&gt;03 | 架构设计的目的&lt;/h1&gt;&lt;p&gt;周二，我们聊了架构出现的历史背景和推动因素。以史为鉴，对我们了解架构</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>04 | 复杂度来源：高性能</title>
    <link href="https://zhuansun.github.io/geekbang/posts/55827629.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/55827629.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.850Z</updated>
    
    <content type="html"><![CDATA[<h1 id="04-复杂度来源：高性能"><a href="#04-复杂度来源：高性能" class="headerlink" title="04 | 复杂度来源：高性能"></a>04 | 复杂度来源：高性能</h1><p>周四，我为你讲了架构设计的主要目的是为了解决软件系统复杂度带来的问题。那么从今天开始，我将为你深入分析复杂度的6个来源，先来聊聊复杂度的来源之一高性能。</p><p>对性能孜孜不倦的追求是整个人类技术不断发展的根本驱动力。例如计算机，从电子管计算机到晶体管计算机再到集成电路计算机，运算性能从每秒几次提升到每秒几亿次。但伴随性能越来越高，相应的方法和系统复杂度也是越来越高。现代的计算机CPU集成了几亿颗晶体管，逻辑复杂度和制造复杂度相比最初的晶体管计算机，根本不可同日而语。</p><p>软件系统也存在同样的现象。最近几十年软件系统性能飞速发展，从最初的计算机只能进行简单的科学计算，到现在Google能够支撑每秒几万次的搜索。与此同时，软件系统规模也从单台计算机扩展到上万台计算机；从最初的单用户单工的字符界面Dos操作系统，到现在的多用户多工的Windows 10图形操作系统。</p><p>当然，技术发展带来了性能上的提升，不一定带来复杂度的提升。例如，硬件存储从纸带→磁带→磁盘→SSD，并没有显著带来系统复杂度的增加。因为新技术会逐步淘汰旧技术，这种情况下我们直接用新技术即可，不用担心系统复杂度会随之提升。只有那些并不是用来取代旧技术，而是开辟了一个全新领域的技术，才会给软件系统带来复杂度，因为软件系统在设计的时候就需要在这些技术之间进行判断选择或者组合。就像汽车的发明无法取代火车，飞机的出现也并不能完全取代火车，所以我们在出行的时候，需要考虑选择汽车、火车还是飞机，这个选择的过程就比较复杂了，要考虑价格、时间、速度、舒适度等各种因素。</p><p>软件系统中高性能带来的复杂度主要体现在两方面，一方面是 <strong>单台计算机内部为了高性能带来的复杂度</strong>；另一方面是 <strong>多台计算机集群为了高性能带来的复杂度</strong>。</p><h2 id="单机复杂度"><a href="#单机复杂度" class="headerlink" title="单机复杂度"></a>单机复杂度</h2><p>计算机内部复杂度最关键的地方就是操作系统。计算机性能的发展本质上是由硬件发展驱动的，尤其是CPU的性能发展。著名的“摩尔定律”表明了CPU的处理能力每隔18个月就翻一番；而将硬件性能充分发挥出来的关键就是操作系统，所以操作系统本身其实也是跟随硬件的发展而发展的，操作系统是软件系统的运行环境，操作系统的复杂度直接决定了软件系统的复杂度。</p><p>操作系统和性能最相关的就是 <strong>进程</strong> 和 <strong>线程</strong>。最早的计算机其实是没有操作系统的，只有输入、计算和输出功能，用户输入一个指令，计算机完成操作，大部分时候计算机都在等待用户输入指令，这样的处理性能很显然是很低效的，因为人的输入速度是远远比不上计算机的运算速度的。</p><p>为了解决手工操作带来的低效，批处理操作系统应运而生。批处理简单来说就是先把要执行的指令预先写下来（写到纸带、磁带、磁盘等），形成一个指令清单，这个指令清单就是我们常说的“任务”，然后将任务交给计算机去执行，批处理操作系统负责读取“任务”中的指令清单并进行处理，计算机执行的过程中无须等待人工手工操作，这样性能就有了很大的提升。</p><p>批处理程序大大提升了处理性能，但有一个很明显的缺点：计算机一次只能执行一个任务，如果某个任务需要从I&#x2F;O设备（例如磁带）读取大量的数据，在I&#x2F;O操作的过程中，CPU其实是空闲的，而这个空闲时间本来是可以进行其他计算的。</p><p>为了进一步提升性能，人们发明了“进程”，用进程来对应一个任务，每个任务都有自己独立的内存空间，进程间互不相关，由操作系统来进行调度。此时的CPU还没有多核和多线程的概念，为了达到多进程并行运行的目的，采取了分时的方式，即把CPU的时间分成很多片段，每个片段只能执行某个进程中的指令。虽然从操作系统和CPU的角度来说还是串行处理的，但是由于CPU的处理速度很快，从用户的角度来看，感觉是多进程在并行处理。</p><p>多进程虽然要求每个任务都有独立的内存空间，进程间互不相关，但从用户的角度来看，两个任务之间能够在运行过程中就进行通信，会让任务设计变得更加灵活高效。否则如果两个任务运行过程中不能通信，只能是A任务将结果写到存储，B任务再从存储读取进行处理，不仅效率低，而且任务设计更加复杂。为了解决这个问题，进程间通信的各种方式被设计出来了，包括管道、消息队列、信号量、共享存储等。</p><p>多进程让多任务能够并行处理任务，但本身还有缺点，单个进程内部只能串行处理，而实际上很多进程内部的子任务并不要求是严格按照时间顺序来执行的，也需要并行处理。例如，一个餐馆管理进程，排位、点菜、买单、服务员调度等子任务必须能够并行处理，否则就会出现某个客人买单时间比较长（比如说信用卡刷不出来），其他客人都不能点菜的情况。为了解决这个问题，人们又发明了线程，线程是进程内部的子任务，但这些子任务都共享同一份进程数据。为了保证数据的正确性，又发明了互斥锁机制。有了多线程后，操作系统调度的最小单位就变成了线程，而进程变成了操作系统分配资源的最小单位。</p><p>多进程多线程虽然让多任务并行处理的性能大大提升，但本质上还是分时系统，并不能做到时间上真正的并行。解决这个问题的方式显而易见，就是让多个CPU能够同时执行计算任务，从而实现真正意义上的多任务并行。目前这样的解决方案有3种：SMP（Symmetric Multi-Processor，对称多处理器结构）、NUMA（Non-Uniform Memory Access，非一致存储访问结构）、MPP（Massive Parallel Processing，海量并行处理结构）。其中SMP是我们最常见的，目前流行的多核处理器就是SMP方案。</p><p>操作系统发展到现在，如果我们要完成一个高性能的软件系统，需要考虑如多进程、多线程、进程间通信、多线程并发等技术点，而且这些技术 <strong>并不是最新的就是最好的，也不是非此即彼的选择</strong>。在做架构设计的时候，需要花费很大的精力来结合业务进行分析、判断、选择、组合，这个过程同样很复杂。举一个最简单的例子：Nginx可以用多进程也可以用多线程，JBoss采用的是多线程；Redis采用的是单进程，Memcache采用的是多线程，这些系统都实现了高性能，但内部实现差异却很大。</p><h2 id="集群的复杂度"><a href="#集群的复杂度" class="headerlink" title="集群的复杂度"></a>集群的复杂度</h2><p>虽然计算机硬件的性能快速发展，但和业务的发展速度相比，还是小巫见大巫了，尤其是进入互联网时代后，业务的发展速度远远超过了硬件的发展速度。例如：</p><ul><li>2016年“双11”支付宝每秒峰值达12万笔支付。</li><li>2017年春节微信红包收发红包每秒达到76万个。</li></ul><p>要支持支付和红包这种复杂的业务，单机的性能无论如何是无法支撑的，必须采用机器集群的方式来达到高性能。例如，支付宝和微信这种规模的业务系统，后台系统的机器数量都是万台级别的。</p><p>通过大量机器来提升性能，并不仅仅是增加机器这么简单，让多台机器配合起来达到高性能的目的，是一个复杂的任务，我针对常见的几种方式简单分析一下。</p><p>1.任务分配</p><p>任务分配的意思是指每台机器都可以处理完整的业务任务，不同的任务分配到不同的机器上执行。</p><p>我从最简单的一台服务器变两台服务器开始，来讲任务分配带来的复杂性，整体架构示意图如下。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/8ef42bd2536b3f1860f4a879223c2dc0.jpg"></p><p>从图中可以看到，1台服务器演变为2台服务器后，架构上明显要复杂多了，主要体现在：</p><ul><li><p>需要增加一个任务分配器，这个分配器可能是硬件网络设备（例如，F5、交换机等），可能是软件网络设备（例如，LVS），也可能是负载均衡软件（例如，Nginx、HAProxy），还可能是自己开发的系统。选择合适的任务分配器也是一件复杂的事情，需要综合考虑性能、成本、可维护性、可用性等各方面的因素。</p></li><li><p>任务分配器和真正的业务服务器之间有连接和交互（即图中任务分配器到业务服务器的连接线），需要选择合适的连接方式，并且对连接进行管理。例如，连接建立、连接检测、连接中断后如何处理等。</p></li><li><p>任务分配器需要增加分配算法。例如，是采用轮询算法，还是按权重分配，又或者按照负载进行分配。如果按照服务器的负载进行分配，则业务服务器还要能够上报自己的状态给任务分配器。</p></li></ul><p>这一大段描述，即使你可能还看不懂，但也应该感受到其中的复杂度了，更何况还要真正去实践和实现。</p><p>上面这个架构只是最简单地增加1台业务机器，我们假设单台业务服务器每秒能够处理5000次业务请求，那么这个架构理论上能够支撑10000次请求，实际上的性能一般按照8折计算，大约是8000次左右。</p><p>如果我们的性能要求继续提高，假设要求每秒提升到10万次，上面这个架构会出现什么问题呢？是不是将业务服务器增加到25台就可以了呢？显然不是，因为随着性能的增加，任务分配器本身又会成为性能瓶颈，当业务请求达到每秒10万次的时候，单台任务分配器也不够用了，任务分配器本身也需要扩展为多台机器，这时的架构又会演变成这个样子。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ac0e9979025df3dd7b8f6588860a9203.jpg"></p><p>这个架构比2台业务服务器的架构要复杂，主要体现在：</p><ul><li><p>任务分配器从1台变成了多台（对应图中的任务分配器1到任务分配器M），这个变化带来的复杂度就是需要将不同的用户分配到不同的任务分配器上（即图中的虚线“用户分配”部分），常见的方法包括DNS轮询、智能DNS、CDN（Content Delivery Network，内容分发网络）、GSLB设备（Global Server Load Balance，全局负载均衡）等。</p></li><li><p>任务分配器和业务服务器的连接从简单的“1对多”（1台任务分配器连接多台业务服务器）变成了“多对多”（多台任务分配器连接多台业务服务器）的网状结构。</p></li><li><p>机器数量从3台扩展到30台（一般任务分配器数量比业务服务器要少，这里我们假设业务服务器为25台，任务分配器为5台），状态管理、故障处理复杂度也大大增加。</p></li></ul><p>上面这两个例子都是以业务处理为例，实际上“任务”涵盖的范围很广， <strong>可以指完整的业务处理，也可以单指某个具体的任务</strong>。例如，“存储”“运算”“缓存”等都可以作为一项任务，因此存储系统、运算系统、缓存系统都可以按照任务分配的方式来搭建架构。此外，“任务分配器”也并不一定只能是物理上存在的机器或者一个独立运行的程序，也可以是嵌入在其他程序中的算法，例如Memcache的集群架构。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d2c94ac2aedbd4ayy5852d2be77b4081.jpg"></p><p>2.任务分解</p><p>通过任务分配的方式，我们能够突破单台机器处理性能的瓶颈，通过增加更多的机器来满足业务的性能需求，但如果业务本身也越来越复杂，单纯只通过任务分配的方式来扩展性能，收益会越来越低。例如，业务简单的时候1台机器扩展到10台机器，性能能够提升8倍（需要扣除机器群带来的部分性能损耗，因此无法达到理论上的10倍那么高），但如果业务越来越复杂，1台机器扩展到10台，性能可能只能提升5倍。造成这种现象的主要原因是业务越来越复杂，单台机器处理的性能会越来越低。为了能够继续提升性能，我们需要采取第二种方式： <strong>任务分解</strong>。</p><p>继续以上面“任务分配”中的架构为例，“业务服务器”如果越来越复杂，我们可以将其拆分为更多的组成部分，我以微信的后台架构为例。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/727f995c45cyy1652e135175c0f6b411.jpg"></p><p>通过上面的架构示意图可以看出，微信后台架构从逻辑上将各个子业务进行了拆分，包括：接入、注册登录、消息、LBS、摇一摇、漂流瓶、其他业务（聊天、视频、朋友圈等）。</p><p>通过这种任务分解的方式，能够把原来大一统但复杂的业务系统，拆分成小而简单但需要多个系统配合的业务系统。从业务的角度来看，任务分解既不会减少功能，也不会减少代码量（事实上代码量可能还会增加，因为从代码内部调用改为通过服务器之间的接口调用），那为何通过任务分解就能够提升性能呢？</p><p>主要有几方面的因素：</p><ul><li><strong>简单的系统更加容易做到高性能</strong></li></ul><p>系统的功能越简单，影响性能的点就越少，就更加容易进行有针对性的优化。而系统很复杂的情况下，首先是比较难以找到关键性能点，因为需要考虑和验证的点太多；其次是即使花费很大力气找到了，修改起来也不容易，因为可能将A关键性能点提升了，但却无意中将B点的性能降低了，整个系统的性能不但没有提升，还有可能会下降。</p><ul><li><strong>可以针对单个任务进行扩展</strong></li></ul><p>当各个逻辑任务分解到独立的子系统后，整个系统的性能瓶颈更加容易发现，而且发现后只需要针对有瓶颈的子系统进行性能优化或者提升，不需要改动整个系统，风险会小很多。以微信的后台架构为例，如果用户数增长太快，注册登录子系统性能出现瓶颈的时候，只需要优化登录注册子系统的性能（可以是代码优化，也可以简单粗暴地加机器），消息逻辑、LBS逻辑等其他子系统完全不需要改动。</p><p>既然将一个大一统的系统分解为多个子系统能够提升性能，那是不是划分得越细越好呢？例如，上面的微信后台目前是7个逻辑子系统，如果我们把这7个逻辑子系统再细分，划分为100个逻辑子系统，性能是不是会更高呢？</p><p>其实不然，这样做性能不仅不会提升，反而还会下降，最主要的原因是如果系统拆分得太细，为了完成某个业务，系统间的调用次数会呈指数级别上升，而系统间的调用通道目前都是通过网络传输的方式，性能远比系统内的函数调用要低得多。我以一个简单的图示来说明。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e7f71f230bb525e48ee3d62fa938cef7.jpg"></p><p>从图中可以看到，当系统拆分2个子系统的时候，用户访问需要1次系统间的请求和1次响应；当系统拆分为4个子系统的时候，系统间的请求次数从1次增长到3次；假如继续拆分下去为100个子系统，为了完成某次用户访问，系统间的请求次数变成了99次。</p><p>为了描述简单，我抽象出来一个最简单的模型：假设这些系统采用IP网络连接，理想情况下一次请求和响应在网络上耗费为1ms，业务处理本身耗时为50ms。我们也假设系统拆分对单个业务请求性能没有影响，那么系统拆分为2个子系统的时候，处理一次用户访问耗时为51ms；而系统拆分为100个子系统的时候，处理一次用户访问耗时竟然达到了149ms。</p><p>虽然系统拆分可能在某种程度上能提升业务处理性能，但提升性能也是有限的，不可能系统不拆分的时候业务处理耗时为50ms，系统拆分后业务处理耗时只要1ms，因为最终决定业务处理性能的还是业务逻辑本身，业务逻辑本身没有发生大的变化下，理论上的性能是有一个上限的，系统拆分能够让性能逼近这个极限，但无法突破这个极限。因此，任务分解带来的性能收益是有一个度的，并不是任务分解越细越好，而对于架构设计来说，如何把握这个粒度就非常关键了。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我给你讲了软件系统中高性能带来的复杂度主要体现的两方面，一是单台计算机内部为了高性能带来的复杂度；二是多台计算机集群为了高性能带来的复杂度，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧。你所在的业务体系中，高性能的系统采用的是哪种方式？目前是否有改进和提升的空间？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%"  frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/6605" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;04-复杂度来源：高性能&quot;&gt;&lt;a href=&quot;#04-复杂度来源：高性能&quot; class=&quot;headerlink&quot; title=&quot;04 | 复杂度来源：高性能&quot;&gt;&lt;/a&gt;04 | 复杂度来源：高性能&lt;/h1&gt;&lt;p&gt;周四，我为你讲了架构设计的主要目的是为了解决软件系统</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>06 | 复杂度来源：可扩展性</title>
    <link href="https://zhuansun.github.io/geekbang/posts/2461862411.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/2461862411.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.856Z</updated>
    
    <content type="html"><![CDATA[<h1 id="06-复杂度来源：可扩展性"><a href="#06-复杂度来源：可扩展性" class="headerlink" title="06 | 复杂度来源：可扩展性"></a>06 | 复杂度来源：可扩展性</h1><p>你好，我是华仔。复杂度来源前面已经讲了高性能和高可用，今天我们来聊聊可扩展性。</p><p>可扩展性是指，系统为了应对将来需求变化而提供的一种扩展能力，当有新的需求出现时，系统不需要或者仅需要少量修改就可以支持，无须整个系统重构或者重建。</p><p>由于软件系统固有的多变性，新的需求总会不断提出来，因此可扩展性显得尤其重要。在软件开发领域，面向对象思想的提出，就是为了解决可扩展性带来的问题；后来的设计模式，更是将可扩展性做到了极致。得益于设计模式的巨大影响力，几乎所有的技术人员对于可扩展性都特别重视。</p><p>设计具备良好可扩展性的系统，有两个基本条件：</p><ol><li><strong>正确预测变化</strong></li><li><strong>完美应对变化</strong></li></ol><p>但要达成这两个条件，本身也是一件复杂的事情，我来具体分析一下。</p><h2 id="预测变化"><a href="#预测变化" class="headerlink" title="预测变化"></a>预测变化</h2><p>软件系统与硬件或者建筑相比，有一个很大的差异：软件系统在发布后，还可以不断地修改和演进。</p><p>这就意味着 <strong>不断有新的需求需要实现</strong>。</p><p>如果新需求能够少改代码甚至不改代码就可以实现，那当然是皆大欢喜的，否则来一个需求就要求系统大改一次，成本会非常高，程序员心里也不爽（改来改去），产品经理也不爽（做得那么慢），老板也不爽（那么多人就只能干这么点事）。</p><p>因此作为架构师，我们总是试图去预测所有的变化，然后设计完美的方案来应对。当下一次需求真正来临时，架构师可以自豪地说：“这个我当时已经预测到了，架构已经完美地支持，只需要一两天工作量就可以了！”</p><p>然而理想是美好的，现实却是复杂的。有一句谚语：“唯一不变的是变化。”如果按照这个标准去衡量，架构师每个设计方案都要考虑可扩展性，例如：</p><ul><li>架构师准备设计一个简单的后台管理系统，当架构师考虑用MySQL存储数据时，是否要考虑后续需要用Oracle来存储？</li><li>当架构师设计用HTTP做接口协议时，是否要考虑要不要支持ProtocolBuffer？</li><li>甚至更离谱一点，架构师是否要考虑VR技术对架构的影响从而提前做好可扩展性？</li></ul><p>如果每个点都考虑可扩展性，架构师会不堪重负，架构设计也会异常庞大且最终无法落地。但架构师也不能完全不做预测，否则可能系统刚上线，马上来新的需求就需要重构，这同样意味着前期很多投入的工作量也白费了。</p><p>同时，“预测”这个词，本身就暗示了不可能每次预测都是准确的。如果预测的事情出错，我们期望中的需求迟迟不来，甚至被明确否定，那么基于预测做的架构设计就没什么作用，投入的工作量也就白费了。</p><p>综合分析，预测变化的复杂性在于：</p><ol><li>不能每个设计点都考虑可扩展性。</li><li>不能完全不考虑可扩展性。</li><li>所有的预测都存在出错的可能性。</li></ol><p>对于架构师来说，如何把握预测的程度和提升预测结果的准确性，是一件很复杂的事情，而且没有通用的标准可以简单套上去，更多是靠自己的经验、直觉。所以架构设计评审的时候，经常会出现两个设计师对某个判断争得面红耳赤的情况，原因就在于没有明确标准，不同的人理解和判断有偏差，而最终又只能选择其中一个判断。</p><h3 id="2年法则"><a href="#2年法则" class="headerlink" title="2年法则"></a>2年法则</h3><p>那么我们设计架构的时候要怎么办呢？根据以往的职业经历和思考，我提炼出一个“2年法则”供你参考： <strong>只预测2年内的可能变化，不要试图预测5年甚至10年后的变化。</strong></p><p>当然，你可能会有疑问：为什么一定是2年呢？有的行业变化快，有的行业变化慢，不应该是按照行业特点来选择具体的预测周期吗？</p><p>理论上来说确实如此，但实际操作的时候你会发现，如果你要给出一个让大家都信服的行业预测周期，其实是很难的。</p><p>我之所以说要预测2年，是因为变化快的行业，你能够预测2年已经足够了；而变化慢的行业，本身就变化慢，预测本身的意义不大，预测5年和预测2年的结果是差不多的。所以“2年法则”在大部分场景下都是适用的。</p><h2 id="应对变化"><a href="#应对变化" class="headerlink" title="应对变化"></a>应对变化</h2><p>假设架构师经验非常丰富，目光非常敏锐，看问题非常准，所有的变化都能准确预测，是否意味着可扩展性就很容易实现了呢？也没那么理想！因为预测变化是一回事，采取什么方案来应对变化，又是另外一个复杂的事情。即使预测很准确，如果方案不合适，则系统扩展一样很麻烦。</p><h3 id="方案一：提炼出“变化层”和“稳定层”"><a href="#方案一：提炼出“变化层”和“稳定层”" class="headerlink" title="方案一：提炼出“变化层”和“稳定层”"></a>方案一：提炼出“变化层”和“稳定层”</h3><p>第一种应对变化的常见方案是： <strong>将不变的部分封装在一个独立的“稳定层”，将“变化”封装在一个“变化层”</strong>（也叫“适配层”）。这种方案的核心思想是通过变化层来 <strong>隔离变化</strong>。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/9117222928cc441774df9be05dd815b1.jpg" alt="图片"></p><p>无论是变化层依赖稳定层，还是稳定层依赖变化层都是可以的，需要根据具体业务情况来设计。</p><p>如果系统需要支持XML、JSON、ProtocolBuffer三种接入方式，那么最终的架构就是“形式1”架构；如果系统需要支持MySQL、Oracle、DB2数据库存储，那么最终的架构就变成了“形式2”的架构了。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c80058572221851716f25f1db7dcf186.jpg" alt="图片"></p><p>无论采取哪种形式，通过剥离变化层和稳定层的方式应对变化，都会带来两个主要的复杂性相关的问题。</p><ol><li>变化层和稳定层如何拆分？</li></ol><p>对于哪些属于变化层，哪些属于稳定层，很多时候并不是像前面的示例（不同接口协议或者不同数据库）那样明确，不同的人有不同的理解，导致架构设计评审的时候可能吵翻天。</p><ol><li>变化层和稳定层之间的接口如何设计？</li></ol><p>对于稳定层来说，接口肯定是越稳定越好；但对于变化层来说，在有差异的多个实现方式中找出共同点，并且还要保证当加入新的功能时，原有的接口不需要太大修改，这是一件很复杂的事情，所以接口设计同样至关重要。</p><p>例如，MySQL的REPLACE INTO和Oracle的MERGE INTO语法和功能有一些差异，那么存储层如何向稳定层提供数据访问接口呢？是采取MySQL的方式，还是采取Oracle的方式，还是自适应判断？如果再考虑DB2的情况呢？</p><p>看到这里，相信你已经能够大致体会到接口设计的复杂性了。</p><h3 id="方案二：提炼出“抽象层”和“实现层”"><a href="#方案二：提炼出“抽象层”和“实现层”" class="headerlink" title="方案二：提炼出“抽象层”和“实现层”"></a>方案二：提炼出“抽象层”和“实现层”</h3><p>第二种常见的应对变化的方案是： <strong>提炼出一个“抽象层”和一个“实现层”</strong>。如果说方案一的核心思想是通过变化层来隔离变化，那么方案二的核心思想就是通过实现层来 <strong>封装变化</strong>。</p><p>因为抽象层的接口是稳定的不变的，我们可以基于抽象层的接口来实现统一的处理规则，而实现层可以根据具体业务需求定制开发不同的实现细节，所以当加入新的功能时，只要遵循处理规则然后修改实现层，增加新的实现细节就可以了，无须修改抽象层。</p><p>方案二典型的实践就是设计模式和规则引擎。考虑到绝大部分技术人员对设计模式都非常熟悉，我以设计模式为例来说明这种方案的复杂性。</p><p>下面是设计模式的“装饰者”模式的类关系图。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/933b2b11afa24b8ac6524e0a3dae9551.jpg" alt="图片"></p><p>图中的Component和Decorator就是抽象出来的规则，这个规则包括几部分：</p><ol><li>Component和Decorator类。</li><li>Decorator类继承Component类。</li><li>Decorator类聚合了Component类。</li></ol><p>这个规则一旦抽象出来后就固定了，不能轻易修改。例如，把规则3去掉，就无法实现装饰者模式的目的了。</p><p>装饰者模式相比传统的继承来实现功能，确实灵活很多。例如，《设计模式》中装饰者模式的样例“TextView”类的实现，用了装饰者之后，能够灵活地给TextView增加额外更多功能，包括可以增加边框、滚动条和背景图片等。这些功能上的组合不影响规则，只需要按照规则实现即可。</p><p>但装饰者模式相对普通的类实现模式，明显要复杂多了。本来一个函数或者一个类就能搞定的事情，现在要拆分成多个类，而且多个类之间必须按照装饰者模式来设计和调用。</p><p>规则引擎和设计模式类似，都是通过灵活的设计来达到可扩展的目的，但“灵活的设计”本身就是一件复杂的事情，不说别的，光是把23种设计模式全部理解和备注，都是一件很困难的事情。</p><h3 id="1写2抄3重构原则"><a href="#1写2抄3重构原则" class="headerlink" title="1写2抄3重构原则"></a>1写2抄3重构原则</h3><p>那么，我们在实际工作中具体如何来应对变化呢？Martin Fowler在他的经典书籍《重构》中给出一个“Rule of three”的原则，原文是“Three Strikes And You Refactor”，中文一般翻译为“事不过三，三则重构”。</p><ul><li>而我将其翻译为“1写2抄3重构”，也就是说你不要一开始就考虑复杂的可扩展性应对方法，而是等到第三次遇到类似的实现的时候再来重构，重构的时候采取隔离或者封装的方案。</li></ul><p>举个最简单的例子，假设你们的创新业务要对接第三方钱包，按照这个原则，就可以这样做：</p><ul><li><strong>1写</strong>：最开始你们选择了微信钱包对接，此时不需要考虑太多可扩展性，直接快速对照微信支付的API对接即可，因为业务是否能做起来还不确定。</li><li><strong>2抄</strong>：后来你们发现业务发展不错，决定要接入支付宝，此时还是可以不考虑可扩展，直接把原来微信支付接入的代码拷贝过来，然后对照支付宝的API，快速修改上线。</li><li><strong>3重构</strong>：因为业务发展不错，为了方便更多用户，你们决定接入银联云闪付，此时就需要考虑重构，参考设计模式的模板方法和策略模式将支付对接的功能进行封装。</li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我从预测变化和应对变化这两个设计可扩展性系统的条件，以及它们实现起来本身的复杂性，为你讲了复杂度来源之一的可扩展性，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧。你在具体代码中使用过哪些可扩展的技术？最终的效果如何？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%"  frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/6899" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;06-复杂度来源：可扩展性&quot;&gt;&lt;a href=&quot;#06-复杂度来源：可扩展性&quot; class=&quot;headerlink&quot; title=&quot;06 | 复杂度来源：可扩展性&quot;&gt;&lt;/a&gt;06 | 复杂度来源：可扩展性&lt;/h1&gt;&lt;p&gt;你好，我是华仔。复杂度来源前面已经讲了高性能</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>07 | 复杂度来源：低成本、安全、规模</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1217260707.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1217260707.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.859Z</updated>
    
    <content type="html"><![CDATA[<h1 id="07-复杂度来源：低成本、安全、规模"><a href="#07-复杂度来源：低成本、安全、规模" class="headerlink" title="07 | 复杂度来源：低成本、安全、规模"></a>07 | 复杂度来源：低成本、安全、规模</h1><p>关于复杂度来源，前面的专栏已经讲了高性能、高可用和可扩展性，今天我来聊聊复杂度另外三个来源低成本、安全和规模。</p><h2 id="低成本"><a href="#低成本" class="headerlink" title="低成本"></a>低成本</h2><p>当我们的架构方案只涉及几台或者十几台服务器时，一般情况下成本并不是我们重点关注的目标，但如果架构方案涉及几百上千甚至上万台服务器，成本就会变成一个非常重要的架构设计考虑点。例如，A方案需要10000台机器，B方案只需要8000台机器，单从比例来看，也就节省了20%的成本，但从数量来看，B方案能节省2000台机器，1台机器成本预算每年大约2万元，这样一年下来就能节省4000万元，4000万元成本不是小数目，给100人的团队发奖金每人可以发40万元了，这可是算得上天价奖金了。通过一个架构方案的设计，就能轻松节约几千万元，不但展现了技术的强大力量，也带来了可观的收益，对于技术人员来说，最有满足感的事情莫过于如此了。</p><p>当我们设计“高性能”“高可用”的架构时，通用的手段都是增加更多服务器来满足“高性能”和“高可用”的要求；而低成本正好与此相反，我们需要减少服务器的数量才能达成低成本的目标。因此，低成本本质上是与高性能和高可用冲突的，所以低成本很多时候不会是架构设计的首要目标，而是架构设计的附加约束。也就是说，我们首先设定一个成本目标，当我们根据高性能、高可用的要求设计出方案时，评估一下方案是否能满足成本目标，如果不行，就需要重新设计架构；如果无论如何都无法设计出满足成本要求的方案，那就只能找老板调整成本目标了。</p><p>低成本给架构设计带来的主要复杂度体现在， <strong>往往只有“创新”才能达到低成本目标</strong>。这里的“创新”既包括开创一个全新的技术领域（这个要求对绝大部分公司太高），也包括引入新技术，如果没有找到能够解决自己问题的新技术，那么就真的需要自己创造新技术了。</p><p>类似的新技术例子很多，我来举几个。</p><ul><li><p>NoSQL（Memcache、Redis等）的出现是为了解决关系型数据库无法应对高并发访问带来的访问压力。</p></li><li><p>全文搜索引擎（Sphinx、Elasticsearch、Solr）的出现是为了解决关系型数据库like搜索的低效的问题。</p></li><li><p>Hadoop的出现是为了解决传统文件系统无法应对海量数据存储和计算的问题。</p></li></ul><p>我再来举几个业界类似的例子。</p><ul><li><p>Facebook为了解决PHP的低效问题，刚开始的解决方案是HipHop PHP，可以将PHP语言翻译为C++语言执行，后来改为HHVM，将PHP翻译为字节码然后由虚拟机执行，和Java的JVM类似。</p></li><li><p>新浪微博将传统的Redis&#x2F;MC + MySQL方式，扩展为Redis&#x2F;MC + SSD Cache + MySQL方式，SSD Cache作为L2缓存使用，既解决了MC&#x2F;Redis成本过高，容量小的问题，也解决了穿透DB带来的数据库访问压力（来源： <a href="http://www.infoq.com/cn/articles/weibo-platform-archieture">http://www.infoq.com/cn/articles/weibo-platform-archieture</a> ）。</p></li><li><p>Linkedin为了处理每天5千亿的事件，开发了高效的Kafka消息系统。</p></li><li><p>其他类似将Ruby on Rails改为Java、Lua + redis改为Go语言实现的例子还有很多。</p></li></ul><p>无论是引入新技术，还是自己创造新技术，都是一件复杂的事情。引入新技术的主要复杂度在于需要去熟悉新技术，并且将新技术与已有技术结合起来；创造新技术的主要复杂度在于需要自己去创造全新的理念和技术，并且新技术跟旧技术相比，需要有质的飞跃。</p><p>相比来说，创造新技术复杂度更高，因此一般中小公司基本都是靠引入新技术来达到低成本的目标；而大公司更有可能自己去创造新的技术来达到低成本的目标，因为大公司才有足够的资源、技术和时间去创造新技术。</p><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><p>安全本身是一个庞大而又复杂的技术领域，并且一旦出问题，对业务和企业形象影响非常大。例如：</p><ul><li><p>2016年雅虎爆出史上最大规模信息泄露事件，逾5亿用户资料在2014年被窃取。</p></li><li><p>2016年10月美国遭史上最大规模DDoS攻击，东海岸网站集体瘫痪。</p></li><li><p>2013年10月，为全国4500多家酒店提供网络服务的浙江慧达驿站网络有限公司，因安全漏洞问题，致2千万条入住酒店的客户信息泄露，由此导致很多敲诈、家庭破裂的后续事件。</p></li></ul><p>正因为经常能够看到或者听到各类安全事件，所以大部分技术人员和架构师，对安全这部分会多一些了解和考虑。</p><p>从技术的角度来讲，安全可以分为两类：一类是功能上的安全，一类是架构上的安全。</p><p>1.功能安全</p><p>例如，常见的XSS攻击、CSRF攻击、SQL注入、Windows漏洞、密码破解等，本质上是因为系统实现有漏洞，黑客有了可乘之机。黑客会利用各种漏洞潜入系统，这种行为就像小偷一样，黑客和小偷的手法都是利用系统或家中不完善的地方潜入，并进行破坏或者盗取。因此形象地说， <strong>功能安全其实就是“防小偷”</strong>。</p><p>从实现的角度来看，功能安全更多地是和具体的编码相关，与架构关系不大。现在很多开发框架都内嵌了常见的安全功能，能够大大减少安全相关功能的重复开发，但框架只能预防常见的安全漏洞和风险（常见的XSS攻击、CSRF攻击、SQL注入等），无法预知新的安全问题，而且框架本身很多时候也存在漏洞（例如，流行的Apache Struts2就多次爆出了调用远程代码执行的高危漏洞，给整个互联网都造成了一定的恐慌）。所以功能安全是一个逐步完善的过程，而且往往都是在问题出现后才能有针对性的提出解决方案，我们永远无法预测系统下一个漏洞在哪里，也不敢说自己的系统肯定没有任何问题。换句话讲，功能安全其实也是一个“攻”与“防”的矛盾，只能在这种攻防大战中逐步完善，不可能在系统架构设计的时候一劳永逸地解决。</p><p>2.架构安全</p><p>如果说功能安全是“防小偷”，那么 <strong>架构安全就是“防强盗”</strong>。强盗会直接用大锤将门砸开，或者用炸药将围墙炸倒；小偷是偷东西，而强盗很多时候就是故意搞破坏，对系统的影响也大得多。因此架构设计时需要特别关注架构安全，尤其是互联网时代，理论上来说系统部署在互联网上时，全球任何地方都可以发起攻击。</p><p>传统的架构安全主要依靠防火墙，防火墙最基本的功能就是隔离网络，通过将网络划分成不同的区域，制定出不同区域之间的 <strong>访问控制策略</strong> 来控制不同信任程度区域间传送的数据流。例如，下图是一个典型的银行系统的安全架构。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/28e72e72d8691f1c869ea0db283e156b.png"></p><p>从图中你可以看到，整个系统根据不同的分区部署了多个防火墙来保证系统的安全。</p><p>防火墙的功能虽然强大，但性能一般，所以在传统的银行和企业应用领域应用较多。但在互联网领域，防火墙的应用场景并不多。因为互联网的业务具有海量用户访问和高并发的特点，防火墙的性能不足以支撑；尤其是互联网领域的DDoS攻击，轻则几GB，重则几十GB。2016年知名安全研究人员布莱恩·克莱布斯（Brian Krebs）的安全博客网站遭遇DDoS攻击，攻击带宽达665Gbps，是目前在网络犯罪领域已知的最大的拒绝服务攻击。这种规模的攻击，如果用防火墙来防，则需要部署大量的防火墙，成本会很高。例如，中高端一些的防火墙价格10万元，每秒能抗住大约25GB流量，那么应对这种攻击就需要将近30台防火墙，成本将近300万元，这还不包括维护成本，而这些防火墙设备在没有发生攻击的时候又没有什么作用。也就是说，如果花费几百万元来买这么一套设备，有可能几年都发挥不了任何作用。</p><p>就算是公司对钱不在乎，一般也不会堆防火墙来防DDoS攻击，因为DDoS攻击最大的影响是大量消耗机房的出口总带宽。不管防火墙处理能力有多强，当出口带宽被耗尽时，整个业务在用户看来就是不可用的，因为用户的正常请求已经无法到达系统了。防火墙能够保证内部系统不受冲击，但用户也是进不来的。对于用户来说，业务都已经受到影响了，至于是因为用户自己进不去，还是因为系统出故障，用户其实根本不会关心。</p><p>基于上述原因，互联网系统的架构安全目前并没有太好的设计手段来实现，更多地是依靠运营商或者云服务商强大的带宽和流量清洗的能力，较少自己来设计和实现。</p><h2 id="规模"><a href="#规模" class="headerlink" title="规模"></a>规模</h2><p>很多企业级的系统，既没有高性能要求，也没有双中心高可用要求，也不需要什么扩展性，但往往我们一说到这样的系统，很多人都会脱口而出：这个系统好复杂！为什么这样说呢？关键就在于这样的系统往往功能特别多，逻辑分支特别多。特别是有的系统，发展时间比较长，不断地往上面叠加功能，后来的人由于不熟悉整个发展历史，可能连很多功能的应用场景都不清楚，或者细节根本无法掌握，面对的就是一个黑盒系统，看不懂、改不动、不敢改、修不了，复杂度自然就感觉很高了。</p><p><strong>规模带来复杂度的主要原因就是“量变引起质变”</strong>，当数量超过一定的阈值后，复杂度会发生质的变化。常见的规模带来的复杂度有：</p><p>1.功能越来越多，导致系统复杂度指数级上升</p><p>例如，某个系统开始只有3大功能，后来不断增加到8大功能，虽然还是同一个系统，但复杂度已经相差很大了，具体相差多大呢？</p><p>我以一个简单的抽象模型来计算一下，假设系统间的功能都是两两相关的，系统的复杂度&#x3D;功能数量+功能之间的连接数量，通过计算我们可以看出：</p><ul><li><p>3个功能的系统复杂度&#x3D; 3 + 3 &#x3D; 6</p></li><li><p>8个功能的系统复杂度&#x3D; 8 + 28 &#x3D; 36</p></li></ul><p>可以看出，具备8个功能的系统的复杂度不是比具备3个功能的系统的复杂度多5，而是多了30，基本是指数级增长的，主要原因在于随着系统功能数量增多，功能之间的连接呈指数级增长。下图形象地展示了功能数量的增多带来了复杂度。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/00328479c77f39c22637a3a53b535629.png"></p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/3fcdf2386bc9158899bfc6f3625df81c.png"></p><p>通过肉眼就可以很直观地看出，具备8个功能的系统复杂度要高得多。</p><p>2.数据越来越多，系统复杂度发生质变</p><p>与功能类似，系统数据越来越多时，也会由量变带来质变，最近几年火热的“大数据”就是在这种背景下诞生的。大数据单独成为了一个热门的技术领域，主要原因就是数据太多以后，传统的数据收集、加工、存储、分析的手段和工具已经无法适应，必须应用新的技术才能解决。目前的大数据理论基础是Google发表的三篇大数据相关论文，其中Google File System是大数据文件存储的技术理论，Google Bigtable是列式数据存储的技术理论，Google MapReduce是大数据运算的技术理论，这三篇技术论文各自开创了一个新的技术领域。</p><p>即使我们的数据没有达到大数据规模，数据的增长也可能给系统带来复杂性。最典型的例子莫过于使用关系数据库存储数据，我以MySQL为例，MySQL单表的数据因不同的业务和应用场景会有不同的最优值，但不管怎样都肯定是有一定的限度的，一般推荐在5000万行左右。如果因为业务的发展，单表数据达到了10亿行，就会产生很多问题，例如：</p><ul><li><p>添加索引会很慢，可能需要几个小时，这几个小时内数据库表是无法插入数据的，相当于业务停机了。</p></li><li><p>修改表结构和添加索引存在类似的问题，耗时可能会很长。</p></li><li><p>即使有索引，索引的性能也可能会很低，因为数据量太大。</p></li><li><p>数据库备份耗时很长。</p></li><li><p>……</p></li></ul><p>因此，当MySQL单表数据量太大时，我们必须考虑将单表拆分为多表，这个拆分过程也会引入更多复杂性，例如：</p><ul><li>拆表的规则是什么？</li></ul><p>以用户表为例：是按照用户id拆分表，还是按照用户注册时间拆表？</p><ul><li>拆完表后查询如何处理？</li></ul><p>以用户表为例：假设按照用户id拆表，当业务需要查询学历为“本科”以上的用户时，要去很多表查询才能得到最终结果，怎么保证性能？</p><p>还有很多类似的问题这里不一一展开，后面的专栏还会讨论。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你分析了低成本给架构设计带来的主要复杂度体现在引入新技术或创造新技术，讨论了从功能安全和架构安全引入的复杂度，以及规模带来复杂度的主要原因是“量变引起质变”，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧。学习了6大复杂度来源后，结合你所在的业务，分析一下主要的复杂度是这其中的哪些部分？是否还有其他复杂度原因？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%"  frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/6990" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;07-复杂度来源：低成本、安全、规模&quot;&gt;&lt;a href=&quot;#07-复杂度来源：低成本、安全、规模&quot; class=&quot;headerlink&quot; title=&quot;07 | 复杂度来源：低成本、安全、规模&quot;&gt;&lt;/a&gt;07 | 复杂度来源：低成本、安全、规模&lt;/h1&gt;&lt;p&gt;关于</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>10 | 架构设计流程：识别复杂度</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1718282363.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1718282363.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.868Z</updated>
    
    <content type="html"><![CDATA[<h1 id="10-架构设计流程：识别复杂度"><a href="#10-架构设计流程：识别复杂度" class="headerlink" title="10 | 架构设计流程：识别复杂度"></a>10 | 架构设计流程：识别复杂度</h1><p>从今天开始，我将分4期，结合复杂度来源和架构设计原则，通过一个模拟的设计场景“前浪微博”，和你一起看看在实践中究竟如何进行架构设计。今天先来看架构设计流程第1步：识别复杂度。</p><h2 id="架构设计第1步：识别复杂度"><a href="#架构设计第1步：识别复杂度" class="headerlink" title="架构设计第1步：识别复杂度"></a>架构设计第1步：识别复杂度</h2><p>我在前面讲过，架构设计的本质目的是为了解决软件系统的复杂性，所以在我们设计架构时，首先就要分析系统的复杂性。只有正确分析出了系统的复杂性，后续的架构设计方案才不会偏离方向；否则，如果对系统的复杂性判断错误，即使后续的架构设计方案再完美再先进，都是南辕北辙，做的越好，错的越多、越离谱。</p><p>例如，如果一个系统的复杂度本来是业务逻辑太复杂，功能耦合严重，架构师却设计了一个TPS达到50000&#x2F;秒的高性能架构，即使这个架构最终的性能再优秀也没有任何意义，因为架构没有解决正确的复杂性问题。</p><p>架构的复杂度主要来源于“高性能”“高可用”“可扩展”等几个方面，但架构师在具体判断复杂性的时候，不能生搬硬套，认为任何时候架构都必须同时满足这三方面的要求。实际上大部分场景下，复杂度只是其中的某一个，少数情况下包含其中两个，如果真的出现同时需要解决三个或者三个以上的复杂度，要么说明这个系统之前设计的有问题，要么可能就是架构师的判断出现了失误，即使真的认为要同时满足这三方面的要求，也必须要进行优先级排序。</p><p>例如，专栏前面提到过的“亿级用户平台”失败的案例，设计对标腾讯的QQ，按照腾讯QQ的用户量级和功能复杂度进行设计，高性能、高可用、可扩展、安全等技术一应俱全，一开始就设计出了40多个子系统，然后投入大量人力开发了将近1年时间才跌跌撞撞地正式上线。上线后发现之前的过度设计完全是多此一举，而且带来很多问题：</p><ul><li><p>系统复杂无比，运维效率低下，每次业务版本升级都需要十几个子系统同步升级，操作步骤复杂，容易出错，出错后回滚还可能带来二次问题。</p></li><li><p>每次版本开发和升级都需要十几个子系统配合，开发效率低下。</p></li><li><p>子系统数量太多，关系复杂，小问题不断，而且出问题后定位困难。</p></li><li><p>开始设计的号称TPS 50000&#x2F;秒的系统，实际TPS连500都不到。</p></li></ul><p>由于业务没有发展，最初的设计人员陆续离开，后来接手的团队，无奈又花了2年时间将系统重构，合并很多子系统，将原来40多个子系统合并成不到20个子系统，整个系统才逐步稳定下来。</p><p>如果运气真的不好，接手了一个每个复杂度都存在问题的系统，那应该怎么办呢？答案是一个个来解决问题，不要幻想一次架构重构解决所有问题。例如这个“亿级用户平台”的案例，后来接手的团队其实面临几个主要的问题：系统稳定性不高，经常出各种莫名的小问题；系统子系统数量太多，系统关系复杂，开发效率低；不支持异地多活，机房级别的故障会导致业务整体不可用。如果同时要解决这些问题，就可能会面临这些困境：</p><ul><li><p>要做的事情太多，反而感觉无从下手。</p></li><li><p>设计方案本身太复杂，落地时间遥遥无期。</p></li><li><p>同一个方案要解决不同的复杂性，有的设计点是互相矛盾的。例如，要提升系统可用性，就需要将数据及时存储到硬盘上，而硬盘刷盘反过来又会影响系统性能。</p></li></ul><p>因此，正确的做法是 <strong>将主要的复杂度问题列出来，然后根据业务、技术、团队等综合情况进行排序，优先解决当前面临的最主要的复杂度问题</strong>。“亿级用户平台”这个案例，团队就优先选择将子系统的数量降下来，后来发现子系统数量降下来后，不但开发效率提升了，原来经常发生的小问题也基本消失了，于是团队再在这个基础上做了异地多活方案，也取得了非常好的效果。</p><p>对于按照复杂度优先级解决的方式，存在一个普遍的担忧：如果按照优先级来解决复杂度，可能会出现解决了优先级排在前面的复杂度后，解决后续复杂度的方案需要将已经落地的方案推倒重来。这个担忧理论上是可能的，但现实中几乎是不可能出现的，原因在于软件系统的可塑性和易变性。对于同一个复杂度问题，软件系统的方案可以有多个，总是可以挑出综合来看性价比最高的方案。</p><p>即使架构师决定要推倒重来，这个新的方案也必须能够同时解决已经被解决的复杂度问题，一般来说能够达到这种理想状态的方案基本都是依靠新技术的引入。例如，Hadoop能够将高可用、高性能、大容量三个大数据处理的复杂度问题同时解决。</p><p>识别复杂度对架构师来说是一项挑战，因为原始的需求中并没有哪个地方会明确地说明复杂度在哪里，需要架构师在理解需求的基础上进行分析。有经验的架构师可能一看需求就知道复杂度大概在哪里；如果经验不足，那只能采取“排查法”，从不同的角度逐一进行分析。</p><h2 id="识别复杂度实战"><a href="#识别复杂度实战" class="headerlink" title="识别复杂度实战"></a>识别复杂度实战</h2><p>我们假想一个创业公司，名称叫作“前浪微博”。前浪微博的业务发展很快，系统也越来越多，系统间协作的效率很低，例如：</p><ul><li><p>用户发一条微博后，微博子系统需要通知审核子系统进行审核，然后通知统计子系统进行统计，再通知广告子系统进行广告预测，接着通知消息子系统进行消息推送……一条微博有十几个通知，目前都是系统间通过接口调用的。每通知一个新系统，微博子系统就要设计接口、进行测试，效率很低，问题定位很麻烦，经常和其他子系统的技术人员产生分岐，微博子系统的开发人员不胜其烦。</p></li><li><p>用户等级达到VIP后，等级子系统要通知福利子系统进行奖品发放，要通知客服子系统安排专属服务人员，要通知商品子系统进行商品打折处理……等级子系统的开发人员也是不胜其烦。</p></li></ul><p>新来的架构师在梳理这些问题时，结合自己的经验，敏锐地发现了这些问题背后的根源在于架构上各业务子系统强耦合，而消息队列系统正好可以完成子系统的解耦，于是提议要引入消息队列系统。经过一分析二讨论三开会四汇报五审批等一系列操作后，消息队列系统终于立项了。其他背景信息还有：</p><ul><li><p>中间件团队规模不大，大约6人左右。</p></li><li><p>中间件团队熟悉Java语言，但有一个新同事C&#x2F;C++很牛。</p></li><li><p>开发平台是Linux，数据库是MySQL。</p></li><li><p>目前整个业务系统是单机房部署，没有双机房。</p></li></ul><p>针对前浪微博的消息队列系统，采用“排查法”来分析复杂度，具体分析过程是：</p><ul><li>这个消息队列是否需要高性能</li></ul><p>我们假设前浪微博系统用户每天发送1000万条微博，那么微博子系统一天会产生1000万条消息，我们再假设平均一条消息有10个子系统读取，那么其他子系统读取的消息大约是1亿次。</p><p>1000万和1亿看起来很吓人，但对于架构师来说，关注的不是一天的数据，而是1秒的数据，即TPS和QPS。我们将数据按照秒来计算，一天内平均每秒写入消息数为115条，每秒读取的消息数是1150条；再考虑系统的读写并不是完全平均的，设计的目标应该以峰值来计算。峰值一般取平均值的3倍，那么消息队列系统的TPS是345，QPS是3450，这个量级的数据意味着并不要求高性能。</p><p>虽然根据当前业务规模计算的性能要求并不高，但业务会增长，因此系统设计需要考虑一定的性能余量。由于现在的基数较低，为了预留一定的系统容量应对后续业务的发展，我们将设计目标设定为峰值的4倍，因此最终的性能要求是：TPS为1380，QPS为13800。TPS为1380并不高，但QPS为13800已经比较高了，因此高性能读取是复杂度之一。注意，这里的设计目标设定为峰值的4倍是根据业务发展速度来预估的，不是固定为4倍，不同的业务可以是2倍，也可以是8倍，但一般不要设定在10倍以上，更不要一上来就按照100倍预估。</p><ul><li>这个消息队列是否需要高可用性</li></ul><p>对于微博子系统来说，如果消息丢了，导致没有审核，然后触犯了国家法律法规，则是非常严重的事情；对于等级子系统来说，如果用户达到相应等级后，系统没有给他奖品和专属服务，则VIP用户会很不满意，导致用户流失从而损失收入，虽然也比较关键，但没有审核子系统丢消息那么严重。</p><p>综合来看，消息队列需要高可用性，包括消息写入、消息存储、消息读取都需要保证高可用性。</p><ul><li>这个消息队列是否需要高可扩展性</li></ul><p>消息队列的功能很明确，基本无须扩展，因此可扩展性不是这个消息队列的复杂度关键。</p><p>为了方便理解，这里我只排查“高性能”“高可用”“扩展性”这3个复杂度，在实际应用中，不同的公司或者团队，可能还有一些其他方面的复杂度分析。例如，金融系统可能需要考虑安全性，有的公司会考虑成本等。</p><p>综合分析下来，消息队列的复杂性主要体现在这几个方面：高性能消息读取、高可用消息写入、高可用消息存储、高可用消息读取。</p><p>“前浪微博”的消息队列设计才刚完成第1步，专栏下一期会根据今天识别的复杂度设计备选方案，前面提到的场景在下一期还会用到哦。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了架构设计流程的第一个步骤“识别复杂度”，并且通过一个模拟的场景讲述了“排查法”的具体分析方式，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧。尝试用排查法分析一下你参与过或者研究过的系统的复杂度，然后与你以前的理解对比一下，看看是否有什么新发现？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/7563" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;10-架构设计流程：识别复杂度&quot;&gt;&lt;a href=&quot;#10-架构设计流程：识别复杂度&quot; class=&quot;headerlink&quot; title=&quot;10 | 架构设计流程：识别复杂度&quot;&gt;&lt;/a&gt;10 | 架构设计流程：识别复杂度&lt;/h1&gt;&lt;p&gt;从今天开始，我将分4期，结合</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>08 | 架构设计三原则</title>
    <link href="https://zhuansun.github.io/geekbang/posts/2918773531.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/2918773531.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.863Z</updated>
    
    <content type="html"><![CDATA[<h1 id="08-架构设计三原则"><a href="#08-架构设计三原则" class="headerlink" title="08 | 架构设计三原则"></a>08 | 架构设计三原则</h1><p>前面几期专栏，我跟你系统的聊了架构设计的主要目的是为了解决软件系统复杂度带来的问题，并分析了复杂度的来源。从今天开始，我会分两期讲讲架构设计的3个原则，以及架构设计原则的案例。</p><p>成为架构师是每个程序员的梦想，但并不意味着把编程做好就能够自然而然地成为一个架构师，优秀程序员和架构师之间还有一个明显的鸿沟需要跨越，这个鸿沟就是“ <strong>不确定性</strong>”。</p><p>对于编程来说，本质上是不能存在不确定的，对于同样一段代码，不管是谁写的，不管什么时候执行，执行的结果应该都是确定的（注意：“确定的”并不等于“正确的”，有bug也是确定的）。而对于架构设计来说，本质上是不确定的，同样的一个系统，A公司和B公司做出来的架构可能差异很大，但最后都能正常运转；同样一个方案，A设计师认为应该这样做，B设计师认为应该那样做，看起来好像都有道理……相比编程来说，架构设计并没有像编程语言那样的语法来进行约束，更多的时候是面对多种可能性时进行选择。</p><p>可是一旦涉及“选择”，就很容易让架构师陷入两难的境地，例如：</p><ul><li><p>是要选择业界最先进的技术，还是选择团队目前最熟悉的技术？如果选了最先进的技术后出了问题怎么办？如果选了目前最熟悉的技术，后续技术演进怎么办？</p></li><li><p>是要选择Google的Angular的方案来做，还是选择Facebook的React来做？Angular看起来更强大，但React看起来更灵活？</p></li><li><p>是要选MySQL还是MongoDB？团队对MySQL很熟悉，但是MongoDB更加适合业务场景？</p></li><li><p>淘宝的电商网站架构很完善，我们新做一个电商网站，是否简单地照搬淘宝就可以了？</p></li></ul><p>还有很多类似的问题和困惑，关键原因在于架构设计领域并没有一套通用的规范来指导架构师进行架构设计，更多是依赖架构师的经验和直觉，因此架构设计有时候也会被看作一项比较神秘的工作。</p><p>业务千变万化，技术层出不穷，设计理念也是百花齐放，看起来似乎很难有一套通用的规范来适用所有的架构设计场景。但是在研究了架构设计的发展历史、多个公司的架构发展过程（QQ、淘宝、Facebook等）、众多的互联网公司架构设计后，我发现有几个共性的原则隐含其中，这就是： <strong>合适原则、简单原则、演化原则</strong>，架构设计时遵循这几个原则，有助于你做出最好的选择。</p><h2 id="合适原则"><a href="#合适原则" class="headerlink" title="合适原则"></a>合适原则</h2><p><strong>合适原则宣言：“合适优于业界领先”。</strong></p><p>优秀的技术人员都有很强的技术情结，当他们做方案或者架构时，总想不断地挑战自己，想达到甚至优于业界领先水平是其中一个典型表现，因为这样才能够展现自己的优秀，才能在年终KPI绩效总结里面骄傲地写上“设计了XX方案，达到了和Google相同的技术水平”“XX方案的性能测试结果大大优于阿里集团的YY方案”。</p><p>但现实是，大部分这样想和这样做的架构，最后可能都以失败告终！我在互联网行业见过“亿级用户平台”的失败案例，2011年的时候，某个几个人规模的业务团队，雄心勃勃的提出要做一个和腾讯QQ（那时候微信还没起来）一拼高下的“亿级用户平台”，最后结果当然是不出所料的失败了。</p><p>为什么会这样呢？</p><p>再好的梦想，也需要脚踏实地实现！这里的“脚踏实地”主要体现在下面几个方面。</p><p>1.将军难打无兵之仗</p><p>大公司的分工比较细，一个小系统可能就是一个小组负责，比如说某个通信大厂，做一个OM管理系统就有十几个人，阿里的中间件团队有几十个人，而大部分公司，整个研发团队可能就100多人，某个业务团队可能就十几个人。十几个人的团队，想做几十个人的团队的事情，而且还要做得更好，不能说绝对不可能，但难度是可想而知的。</p><p><strong>没那么多人，却想干那么多活，是失败的第一个主要原因。</strong></p><p>2.罗马不是一天建成的</p><p>业界领先的很多方案，其实并不是一堆天才某个时期灵机一动，然后加班加点就做出来的，而是经过几年时间的发展才逐步完善和初具规模的。阿里中间件团队2008年成立，发展到现在已经有十年了。我们只知道他们抗住了多少次“双11”，做了多少优秀的系统，但经历了什么样的挑战、踩了什么样的坑，只有他们自己知道！这些挑战和踩坑，都是架构设计非常关键的促进因素，单纯靠拍脑袋或者头脑风暴，是不可能和真正实战相比的。</p><p><strong>没有那么多积累，却想一步登天，是失败的第二个主要原因。</strong></p><p>3.冰山下面才是关键</p><p>可能有人认为，业界领先的方案都是天才创造出来的，所以自己也要造一个业界领先的方案，以此来证明自己也是天才。确实有这样的天才，但更多的时候，业界领先的方案其实都是“逼”出来的！简单来说，“业务”发展到一定阶段，量变导致了质变，出现了新的问题，已有的方式已经不能应对这些问题，需要用一种新的方案来解决，通过创新和尝试，才有了业界领先的方案。GFS为何在Google诞生，而不是在Microsoft诞生？我认为Google有那么庞大的数据是一个主要的因素，而不是因为Google的工程师比Microsoft的工程师更加聪明。</p><p><strong>没有那么卓越的业务场景，却幻想灵光一闪成为天才，是失败的第三个主要原因。</strong></p><p>回到我前面提到的“亿级用户平台”失败的例子，分析一下原因。没有腾讯那么多的人（当然钱差得更多），没有QQ那样海量用户的积累，没有QQ那样的业务，这个项目失败其实是在一开始就注定的。注意这里的失败不是说系统做不出来，而是系统没有按照最初的目标来实现，上面提到的3个失败原因也全占了。</p><p>所以，真正优秀的架构都是在企业当前人力、条件、业务等各种约束下设计出来的，能够合理地将资源整合在一起并发挥出最大功效，并且能够快速落地。这也是很多BAT出来的架构师到了小公司或者创业团队反而做不出成绩的原因，因为没有了大公司的平台、资源、积累，只是生搬硬套大公司的做法，失败的概率非常高。</p><h2 id="简单原则"><a href="#简单原则" class="headerlink" title="简单原则"></a>简单原则</h2><p><strong>简单原则宣言：“简单优于复杂”。</strong></p><p>软件架构设计是一门技术活。所谓技术活，从历史上看，无论是瑞士的钟表，还是瓦特的蒸汽机；无论是莱特兄弟发明的飞机，还是摩托罗拉发明的手机，无一不是越来越精细、越来越复杂。因此当我们进行架构设计时，会自然而然地想把架构做精美、做复杂，这样才能体现我们的技术实力，也才能够将架构做成一件艺术品。</p><p>由于软件架构和建筑架构表面上的相似性，我们也会潜意识地将对建筑的审美观点移植到软件架构上面。我们惊叹于长城的宏伟、泰姬陵的精美、悉尼歌剧院的艺术感、迪拜帆船酒店的豪华感，因此，对于我们自己亲手打造的软件架构，我们也希望它宏伟、精美、艺术、豪华……总之就是不能寒酸、不能简单。</p><p>团队的压力有时也会有意无意地促进我们走向复杂的方向，因为大部分人在评价一个方案水平高低的时候，复杂性是其中一个重要的参考指标。例如设计一个主备方案，如果你用心跳来实现，可能大家都认为这太简单了。但如果你引入ZooKeeper来做主备决策，可能很多人会认为这个方案更加“高大上”一些，毕竟ZooKeeper使用的是ZAB协议，而ZAB协议本身就很复杂。其实，真正理解ZAB协议的人很少（我也不懂），但并不妨碍我们都知道ZAB协议很优秀。</p><p>刚才我聊的这些原因，会在潜意识层面促使初出茅庐的架构师，不自觉地追求架构的复杂性。然而，“复杂”在制造领域代表先进，在建筑领域代表领先，但在软件领域，却恰恰相反，代表的是“问题”。</p><p>软件领域的复杂性体现在两个方面：</p><p>1.结构的复杂性</p><p>结构复杂的系统几乎毫无例外具备两个特点：</p><ul><li><p>组成复杂系统的组件数量更多；</p></li><li><p>同时这些组件之间的关系也更加复杂。</p></li></ul><p>我以图形的方式来说明复杂性：</p><p>2个组件组成的系统：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/2dca583c9634ffcc224852adab208d9c.png"></p><p>3个组件组成的系统：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/3f4181e7166570077c3a4701129c5274.png"></p><p>4个组件组成的系统：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e5716355fb4a84a464e14c2fc3289a8c.png"></p><p>5个组件组成的系统：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/a14cf5f4dba6fca660dea0aa56ce5486.png"></p><p>结构上的复杂性存在的第一个问题是， <strong>组件越多，就越有可能其中某个组件出现故障</strong>，从而导致系统故障。这个概率可以算出来，假设组件的故障率是10%（有10%的时间不可用），那么有3个组件的系统可用性是（1-10%）×（1-10%）×（1-10%）&#x3D; 72.9%，有5个组件的系统可用性是（1-10%）×（1-10%）×（1-10%）×（1-10%）×（1-10%）&#x3D;59%，两者的可用性相差13%。</p><p>结构上的复杂性存在的第二个问题是， <strong>某个组件改动，会影响关联的所有组件</strong>，这些被影响的组件同样会继续递归影响更多的组件。还以上面图中5个组件组成的系统为例，组件A修改或者异常时，会影响组件B&#x2F;C&#x2F;E，D又会影响E。这个问题会影响整个系统的开发效率，因为一旦变更涉及外部系统，需要协调各方统一进行方案评估、资源协调、上线配合。</p><p>结构上的复杂性存在的第三个问题是， <strong>定位一个复杂系统中的问题总是比简单系统更加困难</strong>。首先是组件多，每个组件都有嫌疑，因此要逐一排查；其次组件间的关系复杂，有可能表现故障的组件并不是真正问题的根源。</p><p>2.逻辑的复杂性</p><p>意识到结构的复杂性后，我们的第一反应可能就是“降低组件数量”，毕竟组件数量越少，系统结构越简。最简单的结构当然就是整个系统只有一个组件，即系统本身，所有的功能和逻辑都在这一个组件中实现。</p><p>不幸的是，这样做是行不通的，原因在于除了结构的复杂性，还有逻辑的复杂性，即如果某个组件的逻辑太复杂，一样会带来各种问题。</p><p>逻辑复杂的组件，一个典型特征就是单个组件承担了太多的功能。以电商业务为例，常见的功能有：商品管理、商品搜索、商品展示、订单管理、用户管理、支付、发货、客服……把这些功能全部在一个组件中实现，就是典型的逻辑复杂性。</p><p>逻辑复杂几乎会导致软件工程的每个环节都有问题，假设现在淘宝将这些功能全部在单一的组件中实现，可以想象一下这个恐怖的场景：</p><ul><li><p>系统会很庞大，可能是上百万、上千万的代码规模，“clone”一次代码要30分钟。</p></li><li><p>几十、上百人维护这一套代码，某个“菜鸟”不小心改了一行代码，导致整站崩溃。</p></li><li><p>需求像雪片般飞来，为了应对，开几十个代码分支，然后各种分支合并、各种分支覆盖。</p></li><li><p>产品、研发、测试、项目管理不停地开会讨论版本计划，协调资源，解决冲突。</p></li><li><p>版本太多，每天都要上线几十个版本，系统每隔1个小时重启一次。</p></li><li><p>线上运行出现故障，几十个人扑上去定位和处理，一间小黑屋都装不下所有人，整个办公区闹翻天。</p></li><li><p>……</p></li></ul><p>不用多说，肯定谁都无法忍受这样的场景。</p><p>但是，为什么复杂的电路就意味更强大的功能，而复杂的架构却有很多问题呢？根本原因在于电路一旦设计好后进入生产，就不会再变，复杂性只是在设计时带来影响；而一个软件系统在投入使用后，后续还有源源不断的需求要实现，因此要不断地修改系统，复杂性在整个系统生命周期中都有很大影响。</p><p>功能复杂的组件，另外一个典型特征就是采用了复杂的算法。复杂算法导致的问题主要是难以理解，进而导致难以实现、难以修改，并且出了问题难以快速解决。</p><p>以ZooKeeper为例，ZooKeeper本身的功能主要就是选举，为了实现分布式下的选举，采用了ZAB协议，所以ZooKeeper功能虽然相对简单，但系统实现却比较复杂。相比之下，etcd就要简单一些，因为etcd采用的是Raft算法，相比ZAB协议，Raft算法更加容易理解，更加容易实现。</p><p>综合前面的分析，我们可以看到，无论是结构的复杂性，还是逻辑的复杂性，都会存在各种问题，所以架构设计时如果简单的方案和复杂的方案都可以满足需求，最好选择简单的方案。《UNIX编程艺术》总结的KISS（Keep It Simple, Stupid!）原则一样适应于架构设计。</p><h2 id="演化原则"><a href="#演化原则" class="headerlink" title="演化原则"></a>演化原则</h2><p><strong>演化原则宣言：“演化优于一步到位”。</strong></p><p>软件架构从字面意思理解和建筑结构非常类似，事实上“架构”这个词就是建筑领域的专业名词，维基百科对“软件架构”的定义中有一段话描述了这种相似性：</p><blockquote><p>从和目的、主题、材料和结构的联系上来说，软件架构可以和建筑物的架构相比拟。</p></blockquote><p>例如，软件架构描述的是一个软件系统的结构，包括各个模块，以及这些模块的关系；建筑架构描述的是一幢建筑的结构，包括各个部件，以及这些部件如何有机地组成成一幢完美的建筑。</p><p>然而，字面意思上的相似性却掩盖了一个本质上的差异：建筑一旦完成（甚至一旦开建）就不可再变，而软件却需要根据业务的发展不断地变化！</p><ul><li><p>古埃及的吉萨大金字塔，4000多年前完成的，到现在还是当初的架构。</p></li><li><p>中国的明长城，600多年前完成的，现在保存下来的长城还是当年的结构。</p></li><li><p>美国白宫，1800年建成，200年来进行了几次扩展，但整体结构并无变化，只是在旁边的空地扩建或者改造内部的布局。</p></li></ul><p>对比一下，我们来看看软件架构。</p><p>Windows系统的发展历史：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/83a8089855470db9d2a4449bb032d8bc.png"></p><p>如果对比Windows 8的架构和Windows 1.0的架构，就会发现它们其实是两个不同的系统了！</p><p>Android的发展历史：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/671adc5fb5ed7c4fcf89b23ac5612cc4.png"></p><p>（ <a href="http://www.dappworld.com/wp-content/uploads/2015/09/Android-History-Dappworld.jpg%EF%BC%89">http://www.dappworld.com/wp-content/uploads/2015/09/Android-History-Dappworld.jpg）</a></p><p>同样，Android 6.0和Android 1.6的差异也很大。</p><p><strong>对于建筑来说，永恒是主题；而对于软件来说，变化才是主题</strong>。软件架构需要根据业务的发展而不断变化。设计Windows和Android的人都是顶尖的天才，即便如此，他们也不可能在1985年设计出Windows 8，不可能在2009年设计出Android 6.0。</p><p>如果没有把握“软件架构需要根据业务发展不断变化”这个本质，在做架构设计的时候就很容易陷入一个误区：试图一步到位设计一个软件架构，期望不管业务如何变化，架构都稳如磐石。</p><p>为了实现这样的目标，要么照搬业界大公司公开发表的方案；要么投入庞大的资源和时间来做各种各样的预测、分析、设计。无论哪种做法，后果都很明显：投入巨大，落地遥遥无期。更让人沮丧的是，就算跌跌撞撞拼死拼活终于落地，却发现很多预测和分析都是不靠谱的。</p><p>考虑到软件架构需要根据业务发展不断变化这个本质特点， <strong>软件架构设计其实更加类似于大自然“设计”一个生物，通过演化让生物适应环境，逐步变得更加强大：</strong></p><ul><li><p>首先，生物要适应当时的环境。</p></li><li><p>其次，生物需要不断地繁殖，将有利的基因传递下去，将不利的基因剔除或者修复。</p></li><li><p>第三，当环境变化时，生物要能够快速改变以适应环境变化；如果生物无法调整就被自然淘汰；新的生物会保留一部分原来被淘汰生物的基因。</p></li></ul><p>软件架构设计同样是类似的过程：</p><ul><li><p>首先，设计出来的架构要满足当时的业务需要。</p></li><li><p>其次，架构要不断地在实际应用过程中迭代，保留优秀的设计，修复有缺陷的设计，改正错误的设计，去掉无用的设计，使得架构逐渐完善。</p></li><li><p>第三，当业务发生变化时，架构要扩展、重构，甚至重写；代码也许会重写，但有价值的经验、教训、逻辑、设计等（类似生物体内的基因）却可以在新架构中延续。</p></li></ul><p>架构师在进行架构设计时需要牢记这个原则，时刻提醒自己不要贪大求全，或者盲目照搬大公司的做法。应该认真分析当前业务的特点，明确业务面临的主要问题，设计合理的架构，快速落地以满足业务需要，然后在运行过程中不断完善架构，不断随着业务演化架构。</p><p>即使是大公司的团队，在设计一个新系统的架构时，也需要遵循演化的原则，而不应该认为团队人员多、资源多，不管什么系统上来就要一步到位，因为业务的发展和变化是很快的，不管多牛的团队，也不可能完美预测所有的业务发展和变化路径。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了面对“不确定性”时架构设计的三原则，分别是合适优于业界领先、简单优于复杂、演化优于一步到位，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧。我讲的这三条架构设计原则是否每次都要全部遵循？是否有优先级？谈谈你的理解，并说说为什么。</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/7071" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;08-架构设计三原则&quot;&gt;&lt;a href=&quot;#08-架构设计三原则&quot; class=&quot;headerlink&quot; title=&quot;08 | 架构设计三原则&quot;&gt;&lt;/a&gt;08 | 架构设计三原则&lt;/h1&gt;&lt;p&gt;前面几期专栏，我跟你系统的聊了架构设计的主要目的是为了解决软件系统复</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>11 | 架构设计流程：设计备选方案</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1937874405.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1937874405.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.872Z</updated>
    
    <content type="html"><![CDATA[<h1 id="11-架构设计流程：设计备选方案"><a href="#11-架构设计流程：设计备选方案" class="headerlink" title="11 | 架构设计流程：设计备选方案"></a>11 | 架构设计流程：设计备选方案</h1><p>上一期我讲了架构设计流程第1步识别复杂度，确定了系统面临的主要复杂度问题后，方案设计就有了明确的目标，我们就可以开始真正进行架构方案设计了。今天我来讲讲架构设计流程第2步：设计备选方案，同样还会结合上期“前浪微博”的场景，谈谈消息队列设计备选方案的实战。</p><h2 id="架构设计第2步：设计备选方案"><a href="#架构设计第2步：设计备选方案" class="headerlink" title="架构设计第2步：设计备选方案"></a>架构设计第2步：设计备选方案</h2><p>架构师的工作并不神秘，成熟的架构师需要对已经存在的技术非常熟悉，对已经经过验证的架构模式烂熟于心，然后根据自己对业务的理解，挑选合适的架构模式进行组合，再对组合后的方案进行修改和调整。</p><p>虽然软件技术经过几十年的发展，新技术层出不穷，但是经过时间考验，已经被各种场景验证过的成熟技术其实更多。例如，高可用的主备方案、集群方案，高性能的负载均衡、多路复用，可扩展的分层、插件化等技术，绝大部分时候我们有了明确的目标后，按图索骥就能够找到可选的解决方案。</p><p>只有当这种方式完全无法满足需求的时候，才会考虑进行方案的创新，而事实上方案的创新绝大部分情况下也都是基于已有的成熟技术。</p><ul><li><p>NoSQL：Key-Value的存储和数据库的索引其实是类似的，Memcache只是把数据库的索引独立出来做成了一个缓存系统。</p></li><li><p>Hadoop大文件存储方案，基础其实是集群方案+ 数据复制方案。</p></li><li><p>Docker虚拟化，基础是LXC（Linux Containers）。</p></li><li><p>LevelDB的文件存储结构是Skip List。</p></li></ul><p>在《技术的本质》一书中，对技术的组合有清晰的阐述：</p><blockquote><p>新技术都是在现有技术的基础上发展起来的，现有技术又来源于先前的技术。将技术进行功能性分组，可以大大简化设计过程，这是技术“模块化”的首要原因。技术的“组合”和“递归”特征，将彻底改变我们对技术本质的认识。</p></blockquote><p>虽说基于已有的技术或者架构模式进行组合，然后调整，大部分情况下就能够得到我们需要的方案，但并不意味着架构设计是一件很简单的事情。因为可选的模式有很多，组合的方案更多，往往一个问题的解决方案有很多个；如果再在组合的方案上进行一些创新，解决方案会更多。因此，如何设计最终的方案，并不是一件容易的事情，这个阶段也是很多架构师容易犯错的地方。</p><p>第一种常见的错误：设计最优秀的方案。</p><p>很多架构师在设计架构方案时，心里会默认有一种技术情结：我要设计一个优秀的架构，才能体现我的技术能力！例如，高可用的方案中，集群方案明显比主备方案要优秀和强大；高性能的方案中，淘宝的XX方案是业界领先的方案……</p><p>根据架构设计原则中“合适原则”和“简单原则“的要求，挑选合适自己业务、团队、技术能力的方案才是好方案；否则要么浪费大量资源开发了无用的系统（例如，之前提过的“亿级用户平台”的案例，设计了TPS 50000的系统，实际TPS只有500），要么根本无法实现（例如，10个人的团队要开发现在的整个淘宝系统）。</p><p>第二种常见的错误：只做一个方案。</p><p>很多架构师在做方案设计时，可能心里会简单地对几个方案进行初步的设想，再简单地判断哪个最好，然后就基于这个判断开始进行详细的架构设计了。</p><p>这样做有很多弊端：</p><ul><li><p>心里评估过于简单，可能没有想得全面，只是因为某一个缺点就把某个方案给否决了，而实际上没有哪个方案是完美的，某个地方有缺点的方案可能是综合来看最好的方案。</p></li><li><p>架构师再怎么牛，经验知识和技能也有局限，有可能某个评估的标准或者经验是不正确的，或者是老的经验不适合新的情况，甚至有的评估标准是架构师自己原来就理解错了。</p></li><li><p>单一方案设计会出现过度辩护的情况，即架构评审时，针对方案存在的问题和疑问，架构师会竭尽全力去为自己的设计进行辩护，经验不足的设计人员可能会强词夺理。</p></li></ul><p>因此，架构师需要设计多个备选方案，但方案的数量可以说是无穷无尽的，架构师也不可能穷举所有方案，那合理的做法应该是什么样的呢？</p><ul><li><p><strong>备选方案的数量以3 ~ 5个为最佳</strong>。少于3个方案可能是因为思维狭隘，考虑不周全；多于5个则需要耗费大量的精力和时间，并且方案之间的差别可能不明显。</p></li><li><p><strong>备选方案的差异要比较明显</strong>。例如，主备方案和集群方案差异就很明显，或者同样是主备方案，用ZooKeeper做主备决策和用Keepalived做主备决策的差异也很明显。但是都用ZooKeeper做主备决策，一个检测周期是1分钟，一个检测周期是5分钟，这就不是架构上的差异，而是细节上的差异了，不适合做成两个方案。</p></li><li><p><strong>备选方案的技术不要只局限于已经熟悉的技术</strong>。设计架构时，架构师需要将视野放宽，考虑更多可能性。很多架构师或者设计师积累了一些成功的经验，出于快速完成任务和降低风险的目的，可能自觉或者不自觉地倾向于使用自己已经熟悉的技术，对于新的技术有一种不放心的感觉。就像那句俗语说的：“如果你手里有一把锤子，所有的问题在你看来都是钉子”。例如，架构师对MySQL很熟悉，因此不管什么存储都基于MySQL去设计方案，系统性能不够了，首先考虑的就是MySQL分库分表，而事实上也许引入一个Memcache缓存就能够解决问题。</p></li></ul><p>第三种常见的错误：备选方案过于详细。</p><p>有的架构师或者设计师在写备选方案时，错误地将备选方案等同于最终的方案，每个备选方案都写得很细。这样做的弊端显而易见：</p><ul><li><p>耗费了大量的时间和精力。</p></li><li><p>将注意力集中到细节中，忽略了整体的技术设计，导致备选方案数量不够或者差异不大。</p></li><li><p>评审的时候其他人会被很多细节给绕进去，评审效果很差。例如，评审的时候针对某个定时器应该是1分钟还是30秒，争论得不可开交。</p></li></ul><p>正确的做法是备选阶段关注的是技术选型，而不是技术细节，技术选型的差异要比较明显。例如，采用ZooKeeper和Keepalived两种不同的技术来实现主备，差异就很大；而同样都采用ZooKeeper，一个方案的节点设计是&#x2F;service&#x2F;node&#x2F;master，另一个方案的节点设计是&#x2F;company&#x2F;service&#x2F;master，这两个方案并无明显差异，无须在备选方案设计阶段作为两个不同的备选方案，至于节点路径究竟如何设计，只要在最终的方案中挑选一个进行细化即可。</p><h2 id="设计备选方案实战"><a href="#设计备选方案实战" class="headerlink" title="设计备选方案实战"></a>设计备选方案实战</h2><p>还是回到“前浪微博”的场景，上期我们通过“排查法”识别了消息队列的复杂性主要体现在：高性能消息读取、高可用消息写入、高可用消息存储、高可用消息读取。接下来进行第2步，设计备选方案。</p><p>1.备选方案1：采用开源的Kafka</p><p>Kafka是成熟的开源消息队列方案，功能强大，性能非常高，而且已经比较成熟，很多大公司都在使用。</p><p>2.备选方案2：集群 + MySQL存储</p><p>首先考虑单服务器高性能。高性能消息读取属于“计算高可用”的范畴，单服务器高性能备选方案有很多种。考虑到团队的开发语言是Java，虽然有人觉得C&#x2F;C++语言更加适合写高性能的中间件系统，但架构师综合来看，认为无须为了语言的性能优势而让整个团队切换语言，消息队列系统继续用Java开发。由于Netty是Java领域成熟的高性能网络库，因此架构师选择基于Netty开发消息队列系统。</p><p>由于系统设计的QPS是13800，即使单机采用Netty来构建高性能系统，单台服务器支撑这么高的QPS还是有很大风险的，因此架构师选择采取集群方式来满足高性能消息读取，集群的负载均衡算法采用简单的轮询即可。</p><p>同理，“高可用写入”和“高性能读取”一样，可以采取集群的方式来满足。因为消息只要写入集群中一台服务器就算成功写入，因此“高可用写入”的集群分配算法和“高性能读取”也一样采用轮询，即正常情况下，客户端将消息依次写入不同的服务器；某台服务器异常的情况下，客户端直接将消息写入下一台正常的服务器即可。</p><p>整个系统中最复杂的是“高可用存储”和“高可用读取”，“高可用存储”要求已经写入的消息在单台服务器宕机的情况下不丢失；“高可用读取”要求已经写入的消息在单台服务器宕机的情况下可以继续读取。架构师第一时间想到的就是可以利用MySQL的主备复制功能来达到“高可用存储“的目的，通过服务器的主备方案来达到“高可用读取”的目的。</p><p>具体方案：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/7b224715dc8efe67faa2af94922f948a.png"></p><p>简单描述一下方案：</p><ul><li><p>采用数据分散集群的架构，集群中的服务器进行分组，每个分组存储一部分消息数据。</p></li><li><p>每个分组包含一台主MySQL和一台备MySQL，分组内主备数据复制，分组间数据不同步。</p></li><li><p>正常情况下，分组内的主服务器对外提供消息写入和消息读取服务，备服务器不对外提供服务；主服务器宕机的情况下，备服务器对外提供消息读取的服务。</p></li><li><p>客户端采取轮询的策略写入和读取消息。</p></li></ul><p>3.备选方案3：集群 + 自研存储方案</p><p>在备选方案2的基础上，将MySQL存储替换为自研实现存储方案，因为MySQL的关系型数据库的特点并不是很契合消息队列的数据特点，参考Kafka的做法，可以自己实现一套文件存储和复制方案（此处省略具体的方案描述，实际设计时需要给出方案）。</p><p>可以看出，高性能消息读取单机系统设计这部分时并没有多个备选方案可选，备选方案2和备选方案3都采取基于Netty的网络库，用Java语言开发，原因就在于团队的Java背景约束了备选的范围。通常情况下，成熟的团队不会轻易改变技术栈，反而是新成立的技术团队更加倾向于采用新技术。</p><p>上面简单地给出了3个备选方案用来示范如何操作，实践中要比上述方案复杂一些。架构师的技术储备越丰富、经验越多，备选方案也会更多，从而才能更好地设计备选方案。例如，开源方案选择可能就包括Kafka、ActiveMQ、RabbitMQ；集群方案的存储既可以考虑用MySQL，也可以考虑用HBase，还可以考虑用Redis与MySQL结合等；自研文件系统也可以有多个，可以参考Kafka，也可以参考LevelDB，还可以参考HBase等。限于篇幅，这里就不一一展开了。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了架构设计流程的第二个步骤：设计备选方案，基于我们模拟的“前浪微博”消息系统，给出了备选方案的设计样例，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧，除了这三个备选方案，如果让你来设计第四个备选方案，你的方案是什么？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/7800" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;11-架构设计流程：设计备选方案&quot;&gt;&lt;a href=&quot;#11-架构设计流程：设计备选方案&quot; class=&quot;headerlink&quot; title=&quot;11 | 架构设计流程：设计备选方案&quot;&gt;&lt;/a&gt;11 | 架构设计流程：设计备选方案&lt;/h1&gt;&lt;p&gt;上一期我讲了架构设计</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>13 | 架构设计流程：详细方案设计</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1395192220.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1395192220.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="13-架构设计流程：详细方案设计"><a href="#13-架构设计流程：详细方案设计" class="headerlink" title="13 | 架构设计流程：详细方案设计"></a>13 | 架构设计流程：详细方案设计</h1><p>完成备选方案的设计和选择后，我们终于可以长出一口气，因为整个架构设计最难的一步已经完成了，但整体方案尚未完成，架构师还需继续努力。接下来我们需要再接再励，将最终确定的备选方案进行细化，使得备选方案变成一个可以落地的设计方案。所以今天我来讲讲架构设计流程第4步：详细方案设计。</p><h2 id="架构设计第4步：详细方案设计"><a href="#架构设计第4步：详细方案设计" class="headerlink" title="架构设计第4步：详细方案设计"></a>架构设计第4步：详细方案设计</h2><p>简单来说，详细方案设计就是将方案涉及的关键技术细节给确定下来。</p><ul><li><p>假如我们确定使用Elasticsearch来做全文搜索，那么就需要确定Elasticsearch的索引是按照业务划分，还是一个大索引就可以了；副本数量是2个、3个还是4个，集群节点数量是3个还是6个等。</p></li><li><p>假如我们确定使用MySQL分库分表，那么就需要确定哪些表要分库分表，按照什么维度来分库分表，分库分表后联合查询怎么处理等。</p></li><li><p>假如我们确定引入Nginx来做负载均衡，那么Nginx的主备怎么做，Nginx的负载均衡策略用哪个（权重分配？轮询？ip_hash？）等。</p></li></ul><p>可以看到，详细设计方案里面其实也有一些技术点和备选方案类似。例如，Nginx的负载均衡策略，备选有轮询、权重分配、ip_hash、fair、url_hash五个，具体选哪个呢？看起来和备选方案阶段面临的问题类似，但实际上这里的技术方案选择是 <strong>很轻量级的</strong>，我们无须像备选方案阶段那样操作，而只需要简单根据这些技术的适用场景选择就可以了。</p><p>例如，Nginx的负载均衡策略，简单按照下面的规则选择就可以了。</p><ul><li>轮询（默认）</li></ul><p>每个请求按时间顺序逐一分配到不同的后端服务器，后端服务器分配的请求数基本一致，如果后端服务器“down掉”，能自动剔除。</p><ul><li>加权轮询</li></ul><p>根据权重来进行轮询，权重高的服务器分配的请求更多，主要适应于后端服务器性能不均的情况，如新老服务器混用。</p><ul><li>ip_hash</li></ul><p>每个请求按访问IP的hash结果分配，这样每个访客固定访问一个后端服务器，主要用于解决session的问题，如购物车类的应用。</p><ul><li>fair</li></ul><p>按后端服务器的响应时间来分配请求，响应时间短的优先分配，能够最大化地平衡各后端服务器的压力，可以适用于后端服务器性能不均衡的情况，也可以防止某台后端服务器性能不足的情况下还继续接收同样多的请求从而造成雪崩效应。</p><ul><li>url_hash</li></ul><p>按访问URL的hash结果来分配请求，每个URL定向到同一个后端服务器，适用于后端服务器能够将URL的响应结果缓存的情况。</p><p>这几个策略的适用场景区别还是比较明显的，根据我们的业务需要，挑选一个合适的即可。例如，比如一个电商架构，由于和session比较强相关，因此如果用Nginx来做集群负载均衡，那么选择ip_hash策略是比较合适的。</p><p><strong>详细设计方案阶段可能遇到的一种极端情况就是在详细设计阶段发现备选方案不可行，一般情况下主要的原因是备选方案设计时遗漏了某个关键技术点或者关键的质量属性。</strong> 例如，我曾经参与过一个项目，在备选方案阶段确定是可行的，但在详细方案设计阶段，发现由于细节点太多，方案非常庞大，整个项目可能要开发长达1年时间，最后只得废弃原来的备选方案，重新调整项目目标、计划和方案。这个项目的主要失误就是在备选方案评估时忽略了开发周期这个质量属性。</p><p>幸运的是，这种情况可以通过下面方式有效地避免：</p><ul><li><p><strong>架构师不但要进行备选方案设计和选型，还需要对备选方案的关键细节有较深入的理解。</strong> 例如，架构师选择了Elasticsearch作为全文搜索解决方案，前提必须是架构师自己对Elasticsearch的设计原理有深入的理解，比如索引、副本、集群等技术点；而不能道听途说Elasticsearch很牛，所以选择它，更不能成为把“细节我们不讨论”这句话挂在嘴边的“PPT架构师”。</p></li><li><p><strong>通过分步骤、分阶段、分系统等方式，尽量降低方案复杂度</strong>，方案本身的复杂度越高，某个细节推翻整个方案的可能性就越高，适当降低复杂性，可以减少这种风险。</p></li><li><p>如果方案本身就很复杂，那就采取 <strong>设计团队</strong> 的方式来进行设计，博采众长，汇集大家的智慧和经验，防止只有1~2个架构师可能出现的思维盲点或者经验盲区。</p></li></ul><h2 id="详细方案设计实战"><a href="#详细方案设计实战" class="headerlink" title="详细方案设计实战"></a>详细方案设计实战</h2><p>虽然我们上期在“前浪微博”消息队列的架构设计挑选了备选方案2作为最终方案，但备选方案设计阶段的方案粒度还比较粗，无法真正指导开发人员进行后续的设计和开发，因此需要在备选方案的基础上进一步细化。</p><p>下面我列出一些备选方案2典型的需要细化的点供参考，有兴趣的同学可以自己尝试细化更多的设计点。</p><p>1.细化设计点1：数据库表如何设计？</p><ul><li><p>数据库设计两类表，一类是日志表，用于消息写入时快速存储到MySQL中；另一类是消息表，每个消息队列一张表。</p></li><li><p>业务系统发布消息时，首先写入到日志表，日志表写入成功就代表消息写入成功；后台线程再从日志表中读取消息写入记录，将消息内容写入到消息表中。</p></li><li><p>业务系统读取消息时，从消息表中读取。</p></li><li><p>日志表表名为MQ_LOG，包含的字段：日志ID、发布者信息、发布时间、队列名称、消息内容。</p></li><li><p>消息表表名就是队列名称，包含的字段：消息ID（递增生成）、消息内容、消息发布时间、消息发布者。</p></li><li><p>日志表需要及时清除已经写入消息表的日志数据，消息表最多保存30天的消息数据。</p></li></ul><p>2.细化设计点2：数据如何复制？</p><p>直接采用MySQL主从复制即可，只复制消息存储表，不复制日志表。</p><p>3.细化设计点3：主备服务器如何倒换？</p><p>采用ZooKeeper来做主备决策，主备服务器都连接到ZooKeeper建立自己的节点，主服务器的路径规则为“&#x2F;MQ&#x2F;server&#x2F;分区编号&#x2F;master”，备机为“&#x2F;MQ&#x2F;server&#x2F;分区编号&#x2F;slave”，节点类型为EPHEMERAL。</p><p>备机监听主机的节点消息，当发现主服务器节点断连后，备服务器修改自己的状态，对外提供消息读取服务。</p><p>4.细化设计点4：业务服务器如何写入消息？</p><ul><li><p>消息队列系统设计两个角色：生产者和消费者，每个角色都有唯一的名称。</p></li><li><p>消息队列系统提供SDK供各业务系统调用，SDK从配置中读取所有消息队列系统的服务器信息，SDK采取轮询算法发起消息写入请求给主服务器。如果某个主服务器无响应或者返回错误，SDK将发起请求发送到下一台服务器。</p></li></ul><p>5.细化设计点5：业务服务器如何读取消息？</p><ul><li><p>消息队列系统提供SDK供各业务系统调用，SDK从配置中读取所有消息队列系统的服务器信息，轮流向所有服务器发起消息读取请求。</p></li><li><p>消息队列服务器需要记录每个消费者的消费状态，即当前消费者已经读取到了哪条消息，当收到消息读取请求时，返回下一条未被读取的消息给消费者。</p></li></ul><p>6.细化设计点6：业务服务器和消息队列服务器之间的通信协议如何设计？</p><p>考虑到消息队列系统后续可能会对接多种不同编程语言编写的系统，为了提升兼容性，传输协议用TCP，数据格式为ProtocolBuffer。</p><p>当然还有更多设计细节就不再一一列举，因此这还不是一个完整的设计方案，我希望可以通过这些具体实例来说明细化方案具体如何去做。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了架构设计流程的第四个步骤：详细方案设计，并且基于模拟的“前浪微博”消息队列系统，给出了具体的详细设计示例，希望对你有所帮助。这个示例并不完整，有兴趣的同学可以自己再详细思考一下还有哪些细节可以继续完善。</p><p>这就是今天的全部内容，留一道思考题给你吧，你见过“PPT架构师”么？他们一般都具备什么特点？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/7885" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;13-架构设计流程：详细方案设计&quot;&gt;&lt;a href=&quot;#13-架构设计流程：详细方案设计&quot; class=&quot;headerlink&quot; title=&quot;13 | 架构设计流程：详细方案设计&quot;&gt;&lt;/a&gt;13 | 架构设计流程：详细方案设计&lt;/h1&gt;&lt;p&gt;完成备选方案的设计和</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>12 | 架构设计流程：评估和选择备选方案</title>
    <link href="https://zhuansun.github.io/geekbang/posts/4171523783.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/4171523783.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.876Z</updated>
    
    <content type="html"><![CDATA[<h1 id="12-架构设计流程：评估和选择备选方案"><a href="#12-架构设计流程：评估和选择备选方案" class="headerlink" title="12 | 架构设计流程：评估和选择备选方案"></a>12 | 架构设计流程：评估和选择备选方案</h1><p>上一期我讲了设计备选方案，在完成备选方案设计后，如何挑选出最终的方案也是一个很大的挑战，主要原因有：</p><ul><li><p>每个方案都是可行的，如果方案不可行就根本不应该作为备选方案。</p></li><li><p>没有哪个方案是完美的。例如，A方案有性能的缺点，B方案有成本的缺点，C方案有新技术不成熟的风险。</p></li><li><p>评价标准主观性比较强，比如设计师说A方案比B方案复杂，但另外一个设计师可能会认为差不多，因为比较难将“复杂”一词进行量化。因此，方案评审的时候我们经常会遇到几个设计师针对某个方案或者某个技术点争论得面红耳赤。</p></li></ul><p>正因为选择备选方案存在这些困难，所以实践中很多设计师或者架构师就采取了下面几种指导思想：</p><ul><li>最简派</li></ul><p>设计师挑选一个看起来最简单的方案。例如，我们要做全文搜索功能，方案1基于MySQL，方案2基于Elasticsearch。MySQL的查询功能比较简单，而Elasticsearch的倒排索引设计要复杂得多，写入数据到Elasticsearch，要设计Elasticsearch的索引，要设计Elasticsearch的分布式……全套下来复杂度很高，所以干脆就挑选MySQL来做吧。</p><ul><li>最牛派</li></ul><p>最牛派的做法和最简派正好相反，设计师会倾向于挑选技术上看起来最牛的方案。例如，性能最高的、可用性最好的、功能最强大的，或者淘宝用的、微信开源的、Google出品的等。</p><p>我们以缓存方案中的Memcache和Redis为例，假如我们要挑选一个搭配MySQL使用的缓存，Memcache是纯内存缓存，支持基于一致性hash的集群；而Redis同时支持持久化、支持数据字典、支持主备、支持集群，看起来比Memcache好很多啊，所以就选Redis好了。</p><ul><li>最熟派</li></ul><p>设计师基于自己的过往经验，挑选自己最熟悉的方案。我以编程语言为例，假如设计师曾经是一个C++经验丰富的开发人员，现在要设计一个运维管理系统，由于对Python或者Ruby on Rails不熟悉，因此继续选择C++来做运维管理系统。</p><ul><li>领导派</li></ul><p>领导派就更加聪明了，列出备选方案，设计师自己拿捏不定，然后就让领导来定夺，反正最后方案选的对那是领导厉害，方案选的不对怎么办？那也是领导“背锅”。</p><p>其实这些不同的做法本身并不存在绝对的正确或者绝对的错误，关键是不同的场景应该采取不同的方式。也就是说，有时候我们要挑选最简单的方案，有时候要挑选最优秀的方案，有时候要挑选最熟悉的方案，甚至有时候真的要领导拍板。因此关键问题是：这里的“有时候”到底应该怎么判断？今天我就来讲讲架构设计流程的第3步：评估和选择备选方案。</p><h2 id="架构设计第3步：评估和选择备选方案"><a href="#架构设计第3步：评估和选择备选方案" class="headerlink" title="架构设计第3步：评估和选择备选方案"></a>架构设计第3步：评估和选择备选方案</h2><p>前面提到了那么多指导思想，真正应该选择哪种方法来评估和选择备选方案呢？我的答案就是“ <strong>360度环评</strong>”！具体的操作方式为： <strong>列出我们需要关注的质量属性点，然后分别从这些质量属性的维度去评估每个方案，再综合挑选适合当时情况的最优方案</strong>。</p><p>常见的方案质量属性点有：性能、可用性、硬件成本、项目投入、复杂度、安全性、可扩展性等。在评估这些质量属性时，需要遵循架构设计原则1“合适原则”和原则2“简单原则”，避免贪大求全，基本上某个质量属性能够满足一定时期内业务发展就可以了。</p><p>假如我们做一个购物网站，现在的TPS是1000，如果我们预期1年内能够发展到TPS 2000（业务一年翻倍已经是很好的情况了），在评估方案的性能时，只要能超过2000的都是合适的方案，而不是说淘宝的网站TPS是每秒10万，我们的购物网站就要按照淘宝的标准也实现TPS 10万。</p><p>有的设计师会有这样的担心：如果我们运气真的很好，业务直接一年翻了10倍，TPS从1000上升到10000，那岂不是按照TPS 2000做的方案不合适了，又要重新做方案？</p><p>这种情况确实有可能存在，但概率很小，如果每次做方案都考虑这种小概率事件，我们的方案会出现过度设计，导致投入浪费。考虑这个问题的时候，需要遵循架构设计原则3“演化原则”，避免过度设计、一步到位的想法。按照原则3的思想，即使真的出现这种情况，那就算是重新做方案，代价也是可以接受的，因为业务如此迅猛发展，钱和人都不是问题。例如，淘宝和微信的发展历程中，有过多次这样大规模重构系统的经历。</p><p>通常情况下，如果某个质量属性评估和业务发展有关系（例如，性能、硬件成本等），需要评估未来业务发展的规模时，一种简单的方式是将当前的业务规模乘以2 ~4即可，如果现在的基数较低，可以乘以4；如果现在基数较高，可以乘以2。例如，现在的TPS是1000，则按照TPS 4000来设计方案；如果现在TPS是10000，则按照TPS 20000来设计方案。</p><p>当然，最理想的情况是设计一个方案，能够简单地扩容就能够跟上业务的发展。例如，我们设计一个方案，TPS 2000的时候只要2台机器，TPS 20000的时候只需要简单地将机器扩展到20台即可。但现实往往没那么理想，因为量变会引起质变，具体哪些地方质变，是很难提前很长时间能预判到的。举一个最简单的例子：一个开发团队5个人开发了一套系统，能够从TPS 2000平滑扩容到TPS 20000，但是当业务规模真的达到TPS 20000的时候，团队规模已经扩大到了20个人，此时系统发生了两个质变：</p><ul><li><p>首先是团队规模扩大，20个人的团队在同一个系统上开发，开发效率变将很低，系统迭代速度很慢，经常出现某个功能开发完了要等另外的功能开发完成才能一起测试上线，此时如果要解决问题，就需要将系统拆分为更多子系统。</p></li><li><p>其次是原来单机房的集群设计不满足业务需求了，需要升级为异地多活的架构。</p></li></ul><p>如果团队一开始就预测到这两个问题，系统架构提前就拆分为多个子系统并且支持异地多活呢？这种“事后诸葛亮”也是不行的，因为最开始的时候团队只有5个人，5个人在有限的时间内要完成后来20个人才能完成的高性能、异地多活、可扩展的架构，项目时间会遥遥无期，业务很难等待那么长的时间。</p><p>完成方案的360度环评后，我们可以基于评估结果整理出360度环评表，一目了然地看到各个方案的优劣点。但是360度环评表也只能帮助我们分析各个备选方案，还是没有告诉我们具体选哪个方案，原因就在于没有哪个方案是完美的，极少出现某个方案在所有对比维度上都是最优的。例如：引入开源方案工作量小，但是可运维性和可扩展性差；自研工作量大，但是可运维和可维护性好；使用C语言开发性能高，但是目前团队C语言技术积累少；使用Java技术积累多，但是性能没有C语言开发高，成本会高一些……诸如此类。</p><p>面临这种选择上的困难，有几种看似正确但实际错误的做法。</p><ul><li>数量对比法：简单地看哪个方案的优点多就选哪个。例如，总共5个质量属性的对比，其中A方案占优的有3个，B方案占优的有2个，所以就挑选A方案。</li></ul><p>这种方案主要的问题在于把所有质量属性的重要性等同，而没有考虑质量属性的优先级。例如，对于BAT这类公司来说，方案的成本都不是问题，可用性和可扩展性比成本要更重要得多；但对于创业公司来说，成本可能就会变得很重要。</p><p>其次，有时候会出现两个方案的优点数量是一样的情况。例如，我们对比6个质量属性，很可能出现两个方案各有3个优点，这种情况下也没法选；如果为了数量上的不对称，强行再增加一个质量属性进行对比，这个最后增加的不重要的属性反而成了影响方案选择的关键因素，这又犯了没有区分质量属性的优先级的问题。</p><ul><li>加权法：每个质量属性给一个权重。例如，性能的权重高中低分别得10分、5分、3分，成本权重高中低分别是5分、3分、1分，然后将每个方案的权重得分加起来，最后看哪个方案的权重得分最高就选哪个。</li></ul><p>这种方案主要的问题是无法客观地给出每个质量属性的权重得分。例如，性能权重得分为何是10分、5分、3分，而不是5分、3分、1分，或者是100分、80分、60分？这个分数是很难确定的，没有明确的标准，甚至会出现为了选某个方案，设计师故意将某些权重分值调高而降低另外一些权重分值，最后方案的选择就变成了一个数字游戏了。</p><p>正确的做法是 <strong>按优先级选择</strong>，即架构师综合当前的业务发展情况、团队人员规模和技能、业务发展预测等因素，将质量属性按照优先级排序，首先挑选满足第一优先级的，如果方案都满足，那就再看第二优先级……以此类推。那会不会出现两个或者多个方案，每个质量属性的优缺点都一样的情况呢？理论上是可能的，但实际上是不可能的。前面我提到，在做备选方案设计时，不同的备选方案之间的差异要比较明显，差异明显的备选方案不可能所有的优缺点都是一样的。</p><h2 id="评估和选择备选方案实战"><a href="#评估和选择备选方案实战" class="headerlink" title="评估和选择备选方案实战"></a>评估和选择备选方案实战</h2><p>再回到我们设计的场景“前浪微博”。针对上期提出的3个备选方案，架构师组织了备选方案评审会议，参加的人有研发、测试、运维、还有几个核心业务的主管。</p><p>1.备选方案1：采用开源Kafka方案</p><ul><li><p>业务主管倾向于采用Kafka方案，因为Kafka已经比较成熟，各个业务团队或多或少都了解过Kafka。</p></li><li><p>中间件团队部分研发人员也支持使用Kafka，因为使用Kafka能节省大量的开发投入；但部分人员认为Kafka可能并不适合我们的业务场景，因为Kafka的设计目的是为了支撑大容量的日志消息传输，而我们的消息队列是为了业务数据的可靠传输。</p></li><li><p>运维代表提出了强烈的反对意见：首先，Kafka是Scala语言编写的，运维团队没有维护Scala语言开发的系统的经验，出问题后很难快速处理；其次，目前运维团队已经有一套成熟的运维体系，包括部署、监控、应急等，使用Kafka无法融入这套体系，需要单独投入运维人力。</p></li><li><p>测试代表也倾向于引入Kafka，因为Kafka比较成熟，无须太多测试投入。</p></li></ul><p>2.备选方案2：集群 + MySQL存储</p><ul><li><p>中间件团队的研发人员认为这个方案比较简单，但部分研发人员对于这个方案的性能持怀疑态度，毕竟使用MySQL来存储消息数据，性能肯定不如使用文件系统；并且有的研发人员担心做这样的方案是否会影响中间件团队的技术声誉，毕竟用MySQL来做消息队列，看起来比较“土”、比较另类。</p></li><li><p>运维代表赞同这个方案，因为这个方案可以融入到现有的运维体系中，而且使用MySQL存储数据，可靠性有保证，运维团队也有丰富的MySQL运维经验；但运维团队认为这个方案的成本比较高，一个数据分组就需要4台机器（2台服务器 + 2台数据库）。</p></li><li><p>测试代表认为这个方案测试人力投入较大，包括功能测试、性能测试、可靠性测试等都需要大量地投入人力。</p></li><li><p>业务主管对这个方案既不肯定也不否定，因为反正都不是业务团队来投入人力来开发，系统维护也是中间件团队负责，对业务团队来说，只要保证消息队列系统稳定和可靠即可。</p></li></ul><p>3.备选方案3：集群 + 自研存储系统</p><ul><li><p>中间件团队部分研发人员认为这是一个很好的方案，既能够展现中间件团队的技术实力，性能上相比MySQL也要高；但另外的研发人员认为这个方案复杂度太高，按照目前的团队人力和技术实力，要做到稳定可靠的存储系统，需要耗时较长的迭代，这个过程中消息队列系统可能因为存储出现严重问题，例如文件损坏导致丢失大量数据。</p></li><li><p>运维代表不太赞成这个方案，因为运维之前遇到过几次类似的存储系统故障导致数据丢失的问题，损失惨重。例如，MongoDB丢数据、Tokyo Tyrant丢数据无法恢复等。运维团队并不相信目前的中间件团队的技术实力足以支撑自己研发一个存储系统（这让中间件团队的人员感觉有点不爽）。</p></li><li><p>测试代表赞同运维代表的意见，并且自研存储系统的测试难度也很高，投入也很大。</p></li><li><p>业务主管对自研存储系统也持保留意见，因为从历史经验来看，新系统上线肯定有bug，而存储系统出bug是最严重的，一旦出bug导致大量消息丢失，对系统的影响会严重。</p></li></ul><p>针对3个备选方案的讨论初步完成后，架构师列出了3个方案的360度环评表：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/7de80a7501627b02ba0288f8f725a68c.jpg"></p><p>列出这个表格后，无法一眼看出具体哪个方案更合适，于是大家都把目光投向架构师，决策的压力现在集中在架构师身上了。</p><p>架构师经过思考后，给出了最终选择备选方案2，原因有：</p><ul><li><p>排除备选方案1的主要原因是可运维性，因为再成熟的系统，上线后都可能出问题，如果出问题无法快速解决，则无法满足业务的需求；并且Kafka的主要设计目标是高性能日志传输，而我们的消息队列设计的主要目标是业务消息的可靠传输。</p></li><li><p>排除备选方案3的主要原因是复杂度，目前团队技术实力和人员规模（总共6人，还有其他中间件系统需要开发和维护）无法支撑自研存储系统（参考架构设计原则2：简单原则）。</p></li><li><p>备选方案2的优点就是复杂度不高，也可以很好地融入现有运维体系，可靠性也有保障。</p></li></ul><p>针对备选方案2的缺点，架构师解释是：</p><ul><li><p>备选方案2的第一个缺点是性能，业务目前需要的性能并不是非常高，方案2能够满足，即使后面性能需求增加，方案2的数据分组方案也能够平行扩展进行支撑（参考架构设计原则3：演化原则）。</p></li><li><p>备选方案2的第二个缺点是成本，一个分组就需要4台机器，支撑目前的业务需求可能需要12台服务器，但实际上备机（包括服务器和数据库）主要用作备份，可以和其他系统并行部署在同一台机器上。</p></li><li><p>备选方案2的第三个缺点是技术上看起来并不很优越，但我们的设计目的不是为了证明自己（参考架构设计原则1：合适原则），而是更快更好地满足业务需求。</p></li></ul><p>最后，大家针对一些细节再次讨论后，确定了选择备选方案2。</p><p>通过“前浪微博”这个案例我们可以看出，备选方案的选择和很多因素相关，并不单单考虑性能高低、技术是否优越这些纯技术因素。业务的需求特点、运维团队的经验、已有的技术体系、团队人员的技术水平都会影响备选方案的选择。因此，同样是上述3个备选方案，有的团队会选择引入Kafka（例如，很多创业公司的初创团队，人手不够，需要快速上线支撑业务），有的会选择自研存储系统（例如，阿里开发了RocketMQ，人多力量大，业务复杂是主要原因）。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了架构设计流程的第三个步骤：评估和选择备选方案，并且基于模拟的“前浪微博”消息队列系统，给出了具体的评估和选择示例，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧，RocketMQ和Kafka有什么区别，阿里为何选择了自己开发RocketMQ？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/7832" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;12-架构设计流程：评估和选择备选方案&quot;&gt;&lt;a href=&quot;#12-架构设计流程：评估和选择备选方案&quot; class=&quot;headerlink&quot; title=&quot;12 | 架构设计流程：评估和选择备选方案&quot;&gt;&lt;/a&gt;12 | 架构设计流程：评估和选择备选方案&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>15 | 高性能数据库集群：分库分表</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1360063623.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1360063623.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.884Z</updated>
    
    <content type="html"><![CDATA[<h1 id="15-高性能数据库集群：分库分表"><a href="#15-高性能数据库集群：分库分表" class="headerlink" title="15 | 高性能数据库集群：分库分表"></a>15 | 高性能数据库集群：分库分表</h1><p>上期我讲了“读写分离”，读写分离分散了数据库读写操作的压力，但没有分散存储压力，当数据量达到千万甚至上亿条的时候，单台数据库服务器的存储能力会成为系统的瓶颈，主要体现在这几个方面：</p><ul><li><p>数据量太大，读写的性能会下降，即使有索引，索引也会变得很大，性能同样会下降。</p></li><li><p>数据文件会变得很大，数据库备份和恢复需要耗费很长时间。</p></li><li><p>数据文件越大，极端情况下丢失数据的风险越高（例如，机房火灾导致数据库主备机都发生故障）。</p></li></ul><p>基于上述原因，单个数据库服务器存储的数据量不能太大，需要控制在一定的范围内。为了满足业务数据存储的需求，就需要将存储分散到多台数据库服务器上。</p><p>今天我来介绍常见的分散存储的方法“分库分表”，其中包括“分库”和“分表”两大类。</p><h2 id="业务分库"><a href="#业务分库" class="headerlink" title="业务分库"></a>业务分库</h2><p><strong>业务分库指的是按照业务模块将数据分散到不同的数据库服务器。</strong> 例如，一个简单的电商网站，包括用户、商品、订单三个业务模块，我们可以将用户数据、商品数据、订单数据分开放到三台不同的数据库服务器上，而不是将所有数据都放在一台数据库服务器上。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/71f41d46cc5c0405f4d4dc944b4350c9.jpg"></p><p>虽然业务分库能够分散存储和访问压力，但同时也带来了新的问题，接下来我进行详细分析。</p><p>1.join操作问题</p><p>业务分库后，原本在同一个数据库中的表分散到不同数据库中，导致无法使用SQL的join查询。</p><p>例如：“查询购买了化妆品的用户中女性用户的列表”这个功能，虽然订单数据中有用户的ID信息，但是用户的性别数据在用户数据库中，如果在同一个库中，简单的join查询就能完成；但现在数据分散在两个不同的数据库中，无法做join查询，只能采取先从订单数据库中查询购买了化妆品的用户ID列表，然后再到用户数据库中查询这批用户ID中的女性用户列表，这样实现就比简单的join查询要复杂一些。</p><p>2.事务问题</p><p>原本在同一个数据库中不同的表可以在同一个事务中修改，业务分库后，表分散到不同的数据库中，无法通过事务统一修改。虽然数据库厂商提供了一些分布式事务的解决方案（例如，MySQL的XA），但性能实在太低，与高性能存储的目标是相违背的。</p><p>例如，用户下订单的时候需要扣商品库存，如果订单数据和商品数据在同一个数据库中，我们可以使用事务来保证扣减商品库存和生成订单的操作要么都成功要么都失败，但分库后就无法使用数据库事务了，需要业务程序自己来模拟实现事务的功能。例如，先扣商品库存，扣成功后生成订单，如果因为订单数据库异常导致生成订单失败，业务程序又需要将商品库存加上；而如果因为业务程序自己异常导致生成订单失败，则商品库存就无法恢复了，需要人工通过日志等方式来手工修复库存异常。</p><p>3.成本问题</p><p>业务分库同时也带来了成本的代价，本来1台服务器搞定的事情，现在要3台，如果考虑备份，那就是2台变成了6台。</p><p>基于上述原因，对于小公司初创业务，并不建议一开始就这样拆分，主要有几个原因：</p><ul><li><p>初创业务存在很大的不确定性，业务不一定能发展起来，业务开始的时候并没有真正的存储和访问压力，业务分库并不能为业务带来价值。</p></li><li><p>业务分库后，表之间的join查询、数据库事务无法简单实现了。</p></li><li><p>业务分库后，因为不同的数据要读写不同的数据库，代码中需要增加根据数据类型映射到不同数据库的逻辑，增加了工作量。而业务初创期间最重要的是快速实现、快速验证，业务分库会拖慢业务节奏。</p></li></ul><p>有的架构师可能会想：如果业务真的发展很快，岂不是很快就又要进行业务分库了？那为何不一开始就设计好呢？</p><p>其实这个问题很好回答，按照我前面提到的“架构设计三原则”，简单分析一下。</p><p>首先，这里的“如果”事实上发生的概率比较低，做10个业务有1个业务能活下去就很不错了，更何况快速发展，和中彩票的概率差不多。如果我们每个业务上来就按照淘宝、微信的规模去做架构设计，不但会累死自己，还会害死业务。</p><p>其次，如果业务真的发展很快，后面进行业务分库也不迟。因为业务发展好，相应的资源投入就会加大，可以投入更多的人和更多的钱，那业务分库带来的代码和业务复杂的问题就可以通过增加人来解决，成本问题也可以通过增加资金来解决。</p><p>第三，单台数据库服务器的性能其实也没有想象的那么弱，一般来说，单台数据库服务器能够支撑10万用户量量级的业务，初创业务从0发展到10万级用户，并不是想象得那么快。</p><p>而对于业界成熟的大公司来说，由于已经有了业务分库的成熟解决方案，并且即使是尝试性的新业务，用户规模也是海量的， <strong>这与前面提到的初创业务的小公司有本质区别</strong>，因此最好在业务开始设计时就考虑业务分库。例如，在淘宝上做一个新的业务，由于已经有成熟的数据库解决方案，用户量也很大，需要在一开始就设计业务分库甚至接下来介绍的分表方案。</p><h2 id="分表"><a href="#分表" class="headerlink" title="分表"></a>分表</h2><p>将不同业务数据分散存储到不同的数据库服务器，能够支撑百万甚至千万用户规模的业务，但如果业务继续发展，同一业务的单表数据也会达到单台数据库服务器的处理瓶颈。例如，淘宝的几亿用户数据，如果全部存放在一台数据库服务器的一张表中，肯定是无法满足性能要求的，此时就需要对单表数据进行拆分。</p><p>单表数据拆分有两种方式： <strong>垂直分表</strong> 和 <strong>水平分表</strong>。示意图如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/136bc2f01919edcb8271df6f7e71af40.jpg"></p><p>为了形象地理解垂直拆分和水平拆分的区别，可以想象你手里拿着一把刀，面对一个蛋糕切一刀：</p><ul><li><p>从上往下切就是垂直切分，因为刀的运行轨迹与蛋糕是垂直的，这样可以把蛋糕切成高度相等（面积可以相等也可以不相等）的两部分，对应到表的切分就是表记录数相同但包含不同的列。例如，示意图中的垂直切分，会把表切分为两个表，一个表包含ID、name、age、sex列，另外一个表包含ID、nickname、description列。</p></li><li><p>从左往右切就是水平切分，因为刀的运行轨迹与蛋糕是平行的，这样可以把蛋糕切成面积相等（高度可以相等也可以不相等）的两部分，对应到表的切分就是表的列相同但包含不同的行数据。例如，示意图中的水平切分，会把表分为两个表，两个表都包含ID、name、age、sex、nickname、description列，但是一个表包含的是ID从1到999999的行数据，另一个表包含的是ID从1000000到9999999的行数据。</p></li></ul><p>上面这个示例比较简单，只考虑了一次切分的情况，实际架构设计过程中并不局限切分的次数，可以切两次，也可以切很多次，就像切蛋糕一样，可以切很多刀。</p><p>单表进行切分后，是否要将切分后的多个表分散在不同的数据库服务器中，可以根据实际的切分效果来确定，并不强制要求单表切分为多表后一定要分散到不同数据库中。原因在于单表切分为多表后，新的表即使在同一个数据库服务器中，也可能带来可观的性能提升，如果性能能够满足业务要求，是可以不拆分到多台数据库服务器的，毕竟我们在上面业务分库的内容看到业务分库也会引入很多复杂性的问题；如果单表拆分为多表后，单台服务器依然无法满足性能要求，那就不得不再次进行业务分库的设计了。</p><p>分表能够有效地分散存储压力和带来性能提升，但和分库一样，也会引入各种复杂性。</p><p>1.垂直分表</p><p>垂直分表适合将表中某些不常用且占了大量空间的列拆分出去。例如，前面示意图中的nickname和description字段，假设我们是一个婚恋网站，用户在筛选其他用户的时候，主要是用age和sex两个字段进行查询，而nickname和description两个字段主要用于展示，一般不会在业务查询中用到。description本身又比较长，因此我们可以将这两个字段独立到另外一张表中，这样在查询age和sex时，就能带来一定的性能提升。</p><p>垂直分表引入的复杂性主要体现在表操作的数量要增加。例如，原来只要一次查询就可以获取name、age、sex、nickname、description，现在需要两次查询，一次查询获取name、age、sex，另外一次查询获取nickname、description。</p><p>不过相比接下来要讲的水平分表，这个复杂性就是小巫见大巫了。</p><p>2.水平分表</p><p>水平分表适合表行数特别大的表，有的公司要求单表行数超过5000万就必须进行分表，这个数字可以作为参考，但并不是绝对标准，关键还是要看表的访问性能。对于一些比较复杂的表，可能超过1000万就要分表了；而对于一些简单的表，即使存储数据超过1亿行，也可以不分表。但不管怎样，当看到表的数据量达到千万级别时，作为架构师就要警觉起来，因为这很可能是架构的性能瓶颈或者隐患。</p><p>水平分表相比垂直分表，会引入更多的复杂性，主要表现在下面几个方面：</p><ul><li>路由</li></ul><p>水平分表后，某条数据具体属于哪个切分后的子表，需要增加路由算法进行计算，这个算法会引入一定的复杂性。</p><p>常见的路由算法有：</p><p><strong>范围路由：</strong> 选取有序的数据列（例如，整形、时间戳等）作为路由的条件，不同分段分散到不同的数据库表中。以最常见的用户ID为例，路由算法可以按照1000000的范围大小进行分段，1 ~ 999999放到数据库1的表中，1000000 ~ 1999999放到数据库2的表中，以此类推。</p><p>范围路由设计的复杂点主要体现在分段大小的选取上，分段太小会导致切分后子表数量过多，增加维护复杂度；分段太大可能会导致单表依然存在性能问题，一般建议分段大小在100万至2000万之间，具体需要根据业务选取合适的分段大小。</p><p>范围路由的优点是可以随着数据的增加平滑地扩充新的表。例如，现在的用户是100万，如果增加到1000万，只需要增加新的表就可以了，原有的数据不需要动。</p><p>范围路由的一个比较隐含的缺点是分布不均匀，假如按照1000万来进行分表，有可能某个分段实际存储的数据量只有1000条，而另外一个分段实际存储的数据量有900万条。</p><p><strong>Hash路由：</strong> 选取某个列（或者某几个列组合也可以）的值进行Hash运算，然后根据Hash结果分散到不同的数据库表中。同样以用户ID为例，假如我们一开始就规划了10个数据库表，路由算法可以简单地用user_id % 10的值来表示数据所属的数据库表编号，ID为985的用户放到编号为5的子表中，ID为10086的用户放到编号为6的字表中。</p><p>Hash路由设计的复杂点主要体现在初始表数量的选取上，表数量太多维护比较麻烦，表数量太少又可能导致单表性能存在问题。而用了Hash路由后，增加子表数量是非常麻烦的，所有数据都要重分布。</p><p>Hash路由的优缺点和范围路由基本相反，Hash路由的优点是表分布比较均匀，缺点是扩充新的表很麻烦，所有数据都要重分布。</p><p><strong>配置路由：</strong> 配置路由就是路由表，用一张独立的表来记录路由信息。同样以用户ID为例，我们新增一张user_router表，这个表包含user_id和table_id两列，根据user_id就可以查询对应的table_id。</p><p>配置路由设计简单，使用起来非常灵活，尤其是在扩充表的时候，只需要迁移指定的数据，然后修改路由表就可以了。</p><p>配置路由的缺点就是必须多查询一次，会影响整体性能；而且路由表本身如果太大（例如，几亿条数据），性能同样可能成为瓶颈，如果我们再次将路由表分库分表，则又面临一个死循环式的路由算法选择问题。</p><ul><li>join操作</li></ul><p>水平分表后，数据分散在多个表中，如果需要与其他表进行join查询，需要在业务代码或者数据库中间件中进行多次join查询，然后将结果合并。</p><ul><li>count()操作</li></ul><p>水平分表后，虽然物理上数据分散到多个表中，但某些业务逻辑上还是会将这些表当作一个表来处理。例如，获取记录总数用于分页或者展示，水平分表前用一个count()就能完成的操作，在分表后就没那么简单了。常见的处理方式有下面两种：</p><p><strong>count()相加：</strong> 具体做法是在业务代码或者数据库中间件中对每个表进行count()操作，然后将结果相加。这种方式实现简单，缺点就是性能比较低。例如，水平分表后切分为20张表，则要进行20次count(*)操作，如果串行的话，可能需要几秒钟才能得到结果。</p><p><strong>记录数表：</strong> 具体做法是新建一张表，假如表名为“记录数表”，包含table_name、row_count两个字段，每次插入或者删除子表数据成功后，都更新“记录数表”。</p><p>这种方式获取表记录数的性能要大大优于count()相加的方式，因为只需要一次简单查询就可以获取数据。缺点是复杂度增加不少，对子表的操作要同步操作“记录数表”，如果有一个业务逻辑遗漏了，数据就会不一致；且针对“记录数表”的操作和针对子表的操作无法放在同一事务中进行处理，异常的情况下会出现操作子表成功了而操作记录数表失败，同样会导致数据不一致。</p><p>此外，记录数表的方式也增加了数据库的写压力，因为每次针对子表的insert和delete操作都要update记录数表，所以对于一些不要求记录数实时保持精确的业务，也可以通过后台定时更新记录数表。定时更新实际上就是“count()相加”和“记录数表”的结合，即定时通过count()相加计算表的记录数，然后更新记录数表中的数据。</p><ul><li>order by操作</li></ul><p>水平分表后，数据分散到多个子表中，排序操作无法在数据库中完成，只能由业务代码或者数据库中间件分别查询每个子表中的数据，然后汇总进行排序。</p><h2 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h2><p>和数据库读写分离类似，分库分表具体的实现方式也是“程序代码封装”和“中间件封装”，但实现会更复杂。读写分离实现时只要识别SQL操作是读操作还是写操作，通过简单的判断SELECT、UPDATE、INSERT、DELETE几个关键字就可以做到，而分库分表的实现除了要判断操作类型外，还要判断SQL中具体需要操作的表、操作函数（例如count函数)、order by、group by操作等，然后再根据不同的操作进行不同的处理。例如order by操作，需要先从多个库查询到各个库的数据，然后再重新order by才能得到最终的结果。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了高性能数据库集群的分库分表架构，包括业务分库产生的问题和分表的两种方式及其带来的复杂度，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧，你认为什么时候引入分库分表是合适的？是数据库性能不够的时候就开始分库分表么？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/8373" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;15-高性能数据库集群：分库分表&quot;&gt;&lt;a href=&quot;#15-高性能数据库集群：分库分表&quot; class=&quot;headerlink&quot; title=&quot;15 | 高性能数据库集群：分库分表&quot;&gt;&lt;/a&gt;15 | 高性能数据库集群：分库分表&lt;/h1&gt;&lt;p&gt;上期我讲了“读写分离</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>14 | 高性能数据库集群：读写分离</title>
    <link href="https://zhuansun.github.io/geekbang/posts/213319958.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/213319958.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.881Z</updated>
    
    <content type="html"><![CDATA[<h1 id="14-高性能数据库集群：读写分离"><a href="#14-高性能数据库集群：读写分离" class="headerlink" title="14 | 高性能数据库集群：读写分离"></a>14 | 高性能数据库集群：读写分离</h1><p>“从0开始学架构”专栏已经更新了13期，从各个方面阐述了架构设计相关的理论和流程，包括架构设计起源、架构设计的目的、常见架构复杂度分析、架构设计原则、架构设计流程等，掌握这些知识是做好架构设计的基础。</p><p>在具体的实践过程中，为了更快、更好地设计出优秀的架构，除了掌握这些基础知识外，还需要掌握业界已经成熟的各种架构模式。大部分情况下，我们做架构设计主要都是基于已有的成熟模式，结合业务和团队的具体情况，进行一定的优化或者调整；即使少部分情况我们需要进行较大的创新，前提也是需要对已有的各种架构模式和技术非常熟悉。</p><p>接下来，我将逐一介绍最常见的“高性能架构模式”“高可用架构模式”“可扩展架构模式”，这些模式可能你之前大概了解过，但其实每个方案里面都有很多细节，只有深入的理解这些细节才能理解常见的架构模式，进而设计出优秀的架构。</p><p>虽然近十年来各种存储技术飞速发展，但关系数据库由于其ACID的特性和功能强大的SQL查询，目前还是各种业务系统中关键和核心的存储系统，很多场景下高性能的设计最核心的部分就是关系数据库的设计。</p><p>不管是为了满足业务发展的需要，还是为了提升自己的竞争力，关系数据库厂商（Oracle、DB2、MySQL等）在优化和提升单个数据库服务器的性能方面也做了非常多的技术优化和改进。但业务发展速度和数据增长速度，远远超出数据库厂商的优化速度，尤其是互联网业务兴起之后，海量用户加上海量数据的特点，单个数据库服务器已经难以满足业务需要，必须考虑数据库集群的方式来提升性能。</p><p>从今天开始，我会分几期来介绍高性能数据库集群。高性能数据库集群的第一种方式是“读写分离”，其本质是将访问压力分散到集群中的多个节点，但是没有分散存储压力；第二种方式是“分库分表”，既可以分散访问压力，又可以分散存储压力。先来看看“读写分离”，下一期我再介绍“分库分表”。</p><h2 id="读写分离原理"><a href="#读写分离原理" class="headerlink" title="读写分离原理"></a>读写分离原理</h2><p><strong>读写分离的基本原理是将数据库读写操作分散到不同的节点上</strong>，下面是其基本架构图。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/362d22168bf344687ec0c206aa115807.jpg"></p><p>读写分离的基本实现是：</p><ul><li><p>数据库服务器搭建主从集群，一主一从、一主多从都可以。</p></li><li><p>数据库主机负责读写操作，从机只负责读操作。</p></li><li><p>数据库主机通过复制将数据同步到从机，每台数据库服务器都存储了所有的业务数据。</p></li><li><p>业务服务器将写操作发给数据库主机，将读操作发给数据库从机。</p></li></ul><p>需要注意的是，这里用的是“主从集群”，而不是“主备集群”。“从机”的“从”可以理解为“仆从”，仆从是要帮主人干活的，“从机”是需要提供读数据的功能的；而“备机”一般被认为仅仅提供备份功能，不提供访问功能。所以使用“主从”还是“主备”，是要看场景的，这两个词并不是完全等同的。</p><p>读写分离的实现逻辑并不复杂，但有两个细节点将引入设计复杂度： <strong>主从复制延迟</strong> 和 <strong>分配机制</strong>。</p><h2 id="复制延迟"><a href="#复制延迟" class="headerlink" title="复制延迟"></a>复制延迟</h2><p>以MySQL为例，主从复制延迟可能达到1秒，如果有大量数据同步，延迟1分钟也是有可能的。主从复制延迟会带来一个问题：如果业务服务器将数据写入到数据库主服务器后立刻（1秒内）进行读取，此时读操作访问的是从机，主机还没有将数据复制过来，到从机读取数据是读不到最新数据的，业务上就可能出现问题。例如，用户刚注册完后立刻登录，业务服务器会提示他“你还没有注册”，而用户明明刚才已经注册成功了。</p><p>解决主从复制延迟有几种常见的方法：</p><p>1.写操作后的读操作指定发给数据库主服务器</p><p>例如，注册账号完成后，登录时读取账号的读操作也发给数据库主服务器。这种方式和业务强绑定，对业务的侵入和影响较大，如果哪个新来的程序员不知道这样写代码，就会导致一个bug。</p><p>2.读从机失败后再读一次主机</p><p>这就是通常所说的“二次读取”，二次读取和业务无绑定，只需要对底层数据库访问的API进行封装即可，实现代价较小，不足之处在于如果有很多二次读取，将大大增加主机的读操作压力。例如，黑客暴力破解账号，会导致大量的二次读取操作，主机可能顶不住读操作的压力从而崩溃。</p><p>3.关键业务读写操作全部指向主机，非关键业务采用读写分离</p><p>例如，对于一个用户管理系统来说，注册+登录的业务读写操作全部访问主机，用户的介绍、爱好、等级等业务，可以采用读写分离，因为即使用户改了自己的自我介绍，在查询时却看到了自我介绍还是旧的，业务影响与不能登录相比就小很多，还可以忍受。</p><h2 id="分配机制"><a href="#分配机制" class="headerlink" title="分配机制"></a>分配机制</h2><p>将读写操作区分开来，然后访问不同的数据库服务器，一般有两种方式： <strong>程序代码封装</strong> 和 <strong>中间件封装</strong>。</p><p>1.程序代码封装</p><p>程序代码封装指在代码中抽象一个数据访问层（所以有的文章也称这种方式为“中间层封装”），实现读写操作分离和数据库服务器连接的管理。例如，基于Hibernate进行简单封装，就可以实现读写分离，基本架构是：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f8d538f9201e3ebee37dfdcd1922e9df.jpg"></p><p>程序代码封装的方式具备几个特点：</p><ul><li><p>实现简单，而且可以根据业务做较多定制化的功能。</p></li><li><p>每个编程语言都需要自己实现一次，无法通用，如果一个业务包含多个编程语言写的多个子系统，则重复开发的工作量比较大。</p></li><li><p>故障情况下，如果主从发生切换，则可能需要所有系统都修改配置并重启。</p></li></ul><p>目前开源的实现方案中，淘宝的TDDL（Taobao Distributed Data Layer，外号:头都大了）是比较有名的。它是一个通用数据访问层，所有功能封装在jar包中提供给业务代码调用。其基本原理是一个基于集中式配置的 jdbc datasource实现，具有主备、读写分离、动态数据库配置等功能，基本架构是：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/3b87f6ce297c4af219fa316d29eb5507.jpg"></p><p>2.中间件封装</p><p>中间件封装指的是独立一套系统出来，实现读写操作分离和数据库服务器连接的管理。中间件对业务服务器提供SQL兼容的协议，业务服务器无须自己进行读写分离。对于业务服务器来说，访问中间件和访问数据库没有区别，事实上在业务服务器看来，中间件就是一个数据库服务器。其基本架构是：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/2a2dba7f07581fd055d9cd5a3aa8388e.jpg"></p><p>数据库中间件的方式具备的特点是：</p><ul><li><p>能够支持多种编程语言，因为数据库中间件对业务服务器提供的是标准SQL接口。</p></li><li><p>数据库中间件要支持完整的SQL语法和数据库服务器的协议（例如，MySQL客户端和服务器的连接协议），实现比较复杂，细节特别多，很容易出现bug，需要较长的时间才能稳定。</p></li><li><p>数据库中间件自己不执行真正的读写操作，但所有的数据库操作请求都要经过中间件，中间件的性能要求也很高。</p></li><li><p>数据库主从切换对业务服务器无感知，数据库中间件可以探测数据库服务器的主从状态。例如，向某个测试表写入一条数据，成功的就是主机，失败的就是从机。</p></li></ul><p>由于数据库中间件的复杂度要比程序代码封装高出一个数量级，一般情况下建议采用程序语言封装的方式，或者使用成熟的开源数据库中间件。如果是大公司，可以投入人力去实现数据库中间件，因为这个系统一旦做好，接入的业务系统越多，节省的程序开发投入就越多，价值也越大。</p><p>目前的开源数据库中间件方案中，MySQL官方先是提供了MySQL Proxy，但MySQL Proxy一直没有正式GA，现在MySQL官方推荐MySQL Router。MySQL Router的主要功能有读写分离、故障自动切换、负载均衡、连接池等，其基本架构如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c9c7a3f3602a05d428484c571c1d4faf.jpg"></p><p>奇虎360公司也开源了自己的数据库中间件Atlas，Atlas是基于MySQL Proxy实现的，基本架构如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/75058a4145bb78880faa4e9c74d9d031.png"></p><p>以下是官方介绍，更多内容你可以参考 <a href="https://github.com/Qihoo360/Atlas/wiki/Atlas%E7%9A%84%E6%9E%B6%E6%9E%84">这里</a>。</p><blockquote><p>Atlas是一个位于应用程序与MySQL之间中间件。在后端DB看来，Atlas相当于连接它的客户端，在前端应用看来，Atlas相当于一个DB。Atlas作为服务端与应用程序通信，它实现了MySQL的客户端和服务端协议，同时作为客户端与MySQL通信。它对应用程序屏蔽了DB的细节，同时为了降低MySQL负担，它还维护了连接池。</p></blockquote><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了读写分离方式的原理，以及两个设计复杂度：复制延迟和分配机制，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧，数据库读写分离一般应用于什么场景？能支撑多大的业务规模？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/8269" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;14-高性能数据库集群：读写分离&quot;&gt;&lt;a href=&quot;#14-高性能数据库集群：读写分离&quot; class=&quot;headerlink&quot; title=&quot;14 | 高性能数据库集群：读写分离&quot;&gt;&lt;/a&gt;14 | 高性能数据库集群：读写分离&lt;/h1&gt;&lt;p&gt;“从0开始学架构”专</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>16 | 高性能NoSQL</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1946696691.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1946696691.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.887Z</updated>
    
    <content type="html"><![CDATA[<h1 id="16-高性能NoSQL"><a href="#16-高性能NoSQL" class="headerlink" title="16 | 高性能NoSQL"></a>16 | 高性能NoSQL</h1><p>关系数据库经过几十年的发展后已经非常成熟，强大的SQL功能和ACID的属性，使得关系数据库广泛应用于各式各样的系统中，但这并不意味着关系数据库是完美的，关系数据库存在如下缺点。</p><ul><li>关系数据库存储的是行记录，无法存储数据结构</li></ul><p>以微博的关注关系为例，“我关注的人”是一个用户ID列表，使用关系数据库存储只能将列表拆成多行，然后再查询出来组装，无法直接存储一个列表。</p><ul><li>关系数据库的schema扩展很不方便</li></ul><p>关系数据库的表结构schema是强约束，操作不存在的列会报错，业务变化时扩充列也比较麻烦，需要执行DDL（data definition language，如CREATE、ALTER、DROP等）语句修改，而且修改时可能会长时间锁表（例如，MySQL可能将表锁住1个小时）。</p><ul><li>关系数据库在大数据场景下I&#x2F;O较高</li></ul><p>如果对一些大量数据的表进行统计之类的运算，关系数据库的I&#x2F;O会很高，因为即使只针对其中某一列进行运算，关系数据库也会将整行数据从存储设备读入内存。</p><ul><li>关系数据库的全文搜索功能比较弱</li></ul><p>关系数据库的全文搜索只能使用like进行整表扫描匹配，性能非常低，在互联网这种搜索复杂的场景下无法满足业务要求。</p><p>针对上述问题，分别诞生了不同的NoSQL解决方案，这些方案与关系数据库相比，在某些应用场景下表现更好。但世上没有免费的午餐，NoSQL方案带来的优势，本质上是牺牲ACID中的某个或者某几个特性， <strong>因此我们不能盲目地迷信NoSQL是银弹，而应该将NoSQL作为SQL的一个有力补充</strong>，NoSQL !&#x3D; No SQL，而是NoSQL &#x3D; Not Only SQL。</p><p>常见的NoSQL方案分为4类。</p><ul><li><p>K-V存储：解决关系数据库无法存储数据结构的问题，以Redis为代表。</p></li><li><p>文档数据库：解决关系数据库强schema约束的问题，以MongoDB为代表。</p></li><li><p>列式数据库：解决关系数据库大数据场景下的I&#x2F;O问题，以HBase为代表。</p></li><li><p>全文搜索引擎：解决关系数据库的全文搜索性能问题，以Elasticsearch为代表。</p></li></ul><p>今天，我来介绍一下各种高性能NoSQL方案的典型特征和应用场景。</p><h2 id="K-V存储"><a href="#K-V存储" class="headerlink" title="K-V存储"></a>K-V存储</h2><p>K-V存储的全称是Key-Value存储，其中Key是数据的标识，和关系数据库中的主键含义一样，Value就是具体的数据。</p><p>Redis是K-V存储的典型代表，它是一款开源（基于BSD许可）的高性能K-V缓存和存储系统。Redis的Value是具体的数据结构，包括string、hash、list、set、sorted set、bitmap和hyperloglog，所以常常被称为数据结构服务器。</p><p>以List数据结构为例，Redis提供了下面这些典型的操作（更多请参考链接： <a href="http://redis.cn/commands.html#list">http://redis.cn/commands.html#list</a>）：</p><ul><li><p>LPOP key从队列的左边出队一个元素。</p></li><li><p>LINDEX key index获取一个元素，通过其索引列表。</p></li><li><p>LLEN key获得队列（List）的长度。</p></li><li><p>RPOP key从队列的右边出队一个元素。</p></li></ul><p>以上这些功能，如果用关系数据库来实现，就会变得很复杂。例如，LPOP操作是移除并返回 key对应的list的第一个元素。如果用关系数据库来存储，为了达到同样目的，需要进行下面的操作：</p><ul><li><p>每条数据除了数据编号（例如，行ID），还要有位置编号，否则没有办法判断哪条数据是第一条。注意这里不能用行ID作为位置编号，因为我们会往列表头部插入数据。</p></li><li><p>查询出第一条数据。</p></li><li><p>删除第一条数据。</p></li><li><p>更新从第二条开始的所有数据的位置编号。</p></li></ul><p>可以看出关系数据库的实现很麻烦，而且需要进行多次SQL操作，性能很低。</p><p>Redis的缺点主要体现在并不支持完整的ACID事务，Redis虽然提供事务功能，但Redis的事务和关系数据库的事务不可同日而语，Redis的事务只能保证隔离性和一致性（I和C），无法保证原子性和持久性（A和D）。</p><p>虽然Redis并没有严格遵循ACID原则，但实际上大部分业务也不需要严格遵循ACID原则。以上面的微博关注操作为例，即使系统没有将A加入B的粉丝列表，其实业务影响也非常小，因此我们在设计方案时，需要根据业务特性和要求来确定是否可以用Redis，而不能因为Redis不遵循ACID原则就直接放弃。</p><h2 id="文档数据库"><a href="#文档数据库" class="headerlink" title="文档数据库"></a>文档数据库</h2><p>为了解决关系数据库schema带来的问题，文档数据库应运而生。文档数据库最大的特点就是no-schema，可以存储和读取任意的数据。目前绝大部分文档数据库存储的数据格式是JSON（或者BSON），因为JSON数据是自描述的，无须在使用前定义字段，读取一个JSON中不存在的字段也不会导致SQL那样的语法错误。</p><p>文档数据库的no-schema特性，给业务开发带来了几个明显的优势。</p><p>1.新增字段简单</p><p>业务上增加新的字段，无须再像关系数据库一样要先执行DDL语句修改表结构，程序代码直接读写即可。</p><p>2.历史数据不会出错</p><p>对于历史数据，即使没有新增的字段，也不会导致错误，只会返回空值，此时代码进行兼容处理即可。</p><p>3.可以很容易存储复杂数据</p><p>JSON是一种强大的描述语言，能够描述复杂的数据结构。例如，我们设计一个用户管理系统，用户的信息有ID、姓名、性别、爱好、邮箱、地址、学历信息。其中爱好是列表（因为可以有多个爱好）；地址是一个结构，包括省市区楼盘地址；学历包括学校、专业、入学毕业年份信息等。如果我们用关系数据库来存储，需要设计多张表，包括基本信息（列：ID、姓名、性别、邮箱）、爱好（列：ID、爱好）、地址（列：省、市、区、详细地址）、学历（列：入学时间、毕业时间、学校名称、专业），而使用文档数据库，一个JSON就可以全部描述。</p><pre class="line-numbers language-none"><code class="language-none">&#123;   &quot;id&quot;: 10000,   &quot;name&quot;: &quot;James&quot;,   &quot;sex&quot;: &quot;male&quot;,   &quot;hobbies&quot;: [       &quot;football&quot;,       &quot;playing&quot;,       &quot;singing&quot;   ],   &quot;email&quot;: &quot;user@google.com&quot;,   &quot;address&quot;: &#123;       &quot;province&quot;: &quot;GuangDong&quot;,       &quot;city&quot;: &quot;GuangZhou&quot;,       &quot;district&quot;: &quot;Tianhe&quot;,       &quot;detail&quot;: &quot;PingYun Road 163&quot;   &#125;,   &quot;education&quot;: [       &#123;           &quot;begin&quot;: &quot;2000-09-01&quot;,           &quot;end&quot;: &quot;2004-07-01&quot;,           &quot;school&quot;: &quot;UESTC&quot;,           &quot;major&quot;: &quot;Computer Science &amp; Technology&quot;       &#125;,       &#123;           &quot;begin&quot;: &quot;2004-09-01&quot;,           &quot;end&quot;: &quot;2007-07-01&quot;,           &quot;school&quot;: &quot;SCUT&quot;,           &quot;major&quot;: &quot;Computer Science &amp; Technology&quot;       &#125;   ]&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过这个样例我们看到，使用JSON来描述数据，比使用关系型数据库表来描述数据方便和容易得多，而且更加容易理解。</p><p>文档数据库的这个特点，特别适合电商和游戏这类的业务场景。以电商为例，不同商品的属性差异很大。例如，冰箱的属性和笔记本电脑的属性差异非常大，如下图所示。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/81c57d42e269521ba4b671cac345066e.jpg"><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/83614dfae6106ae3d08yy5a8b3bda5e7.jpg"></p><p>即使是同类商品也有不同的属性。例如，LCD和LED显示器，两者有不同的参数指标。这种业务场景如果使用关系数据库来存储数据，就会很麻烦，而使用文档数据库，会简单、方便许多，扩展新的属性也更加容易。</p><p>文档数据库no-schema的特性带来的这些优势也是有代价的，最主要的代价就是不支持事务。例如，使用MongoDB来存储商品库存，系统创建订单的时候首先需要减扣库存，然后再创建订单。这是一个事务操作，用关系数据库来实现就很简单，但如果用MongoDB来实现，就无法做到事务性。异常情况下可能出现库存被扣减了，但订单没有创建的情况。因此某些对事务要求严格的业务场景是不能使用文档数据库的。</p><p>文档数据库另外一个缺点就是无法实现关系数据库的join操作。例如，我们有一个用户信息表和一个订单表，订单表中有买家用户id。如果要查询“购买了苹果笔记本用户中的女性用户”，用关系数据库来实现，一个简单的join操作就搞定了；而用文档数据库是无法进行join查询的，需要查两次：一次查询订单表中购买了苹果笔记本的用户，然后再查询这些用户哪些是女性用户。</p><h2 id="列式数据库"><a href="#列式数据库" class="headerlink" title="列式数据库"></a>列式数据库</h2><p>顾名思义，列式数据库就是按照列来存储数据的数据库，与之对应的传统关系数据库被称为“行式数据库”，因为关系数据库是按照行来存储数据的。</p><p>关系数据库按照行式来存储数据，主要有以下几个优势：</p><ul><li><p>业务同时读取多个列时效率高，因为这些列都是按行存储在一起的，一次磁盘操作就能够把一行数据中的各个列都读取到内存中。</p></li><li><p>能够一次性完成对一行中的多个列的写操作，保证了针对行数据写操作的原子性和一致性；否则如果采用列存储，可能会出现某次写操作，有的列成功了，有的列失败了，导致数据不一致。</p></li></ul><p>我们可以看到，行式存储的优势是在特定的业务场景下才能体现，如果不存在这样的业务场景，那么行式存储的优势也将不复存在，甚至成为劣势，典型的场景就是海量数据进行统计。例如，计算某个城市体重超重的人员数据，实际上只需要读取每个人的体重这一列并进行统计即可，而行式存储即使最终只使用一列，也会将所有行数据都读取出来。如果单行用户信息有1KB，其中体重只有4个字节，行式存储还是会将整行1KB数据全部读取到内存中，这是明显的浪费。而如果采用列式存储，每个用户只需要读取4字节的体重数据即可，I&#x2F;O将大大减少。</p><p>除了节省I&#x2F;O，列式存储还具备更高的存储压缩比，能够节省更多的存储空间。普通的行式数据库一般压缩率在3:1到5:1左右，而列式数据库的压缩率一般在8:1到30:1左右，因为单个列的数据相似度相比行来说更高，能够达到更高的压缩率。</p><p>同样，如果场景发生变化，列式存储的优势又会变成劣势。典型的场景是需要频繁地更新多个列。因为列式存储将不同列存储在磁盘上不连续的空间，导致更新多个列时磁盘是随机写操作；而行式存储时同一行多个列都存储在连续的空间，一次磁盘写操作就可以完成，列式存储的随机写效率要远远低于行式存储的写效率。此外，列式存储高压缩率在更新场景下也会成为劣势，因为更新时需要将存储数据解压后更新，然后再压缩，最后写入磁盘。</p><p>基于上述列式存储的优缺点，一般将列式存储应用在离线的大数据分析和统计场景中，因为这种场景主要是针对部分列单列进行操作，且数据写入后就无须再更新删除。</p><h2 id="全文搜索引擎"><a href="#全文搜索引擎" class="headerlink" title="全文搜索引擎"></a>全文搜索引擎</h2><p>传统的关系型数据库通过索引来达到快速查询的目的，但是在全文搜索的业务场景下，索引也无能为力，主要体现在：</p><ul><li><p>全文搜索的条件可以随意排列组合，如果通过索引来满足，则索引的数量会非常多。</p></li><li><p>全文搜索的模糊匹配方式，索引无法满足，只能用like查询，而like查询是整表扫描，效率非常低。</p></li></ul><p>我举一个具体的例子来看看关系型数据库为何无法满足全文搜索的要求。假设我们做一个婚恋网站，其主要目的是帮助程序员找朋友，但模式与传统婚恋网站不同，是“程序员发布自己的信息，用户来搜索程序员”。程序员的信息表设计如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d93121cecabc2182edb68bebfc467f39.jpg"></p><p>我们来看一下这个简单业务的搜索场景：</p><ul><li>美女1：听说PHP是世界上最好的语言，那么PHP的程序员肯定是钱最多的，而且我妈一定要我找一个上海的。</li></ul><p>美女1的搜索条件是“性别 + PHP + 上海”，其中“PHP”要用模糊匹配查询“语言”列，“上海”要查询“地点”列，如果用索引支撑，则需要建立“地点”这个索引。</p><ul><li>美女2：我好崇拜这些技术哥哥啊，要是能找一个鹅厂技术哥哥陪我旅游就更好了。</li></ul><p>美女2的搜索条件是“性别 + 鹅厂 + 旅游”，其中“旅游”要用模糊匹配查询“爱好”列，“鹅厂”需要查询“单位”列，如果要用索引支撑，则需要建立“单位”索引。</p><ul><li>美女3：我是一个“女程序员”，想在北京找一个猫厂的Java技术专家。</li></ul><p>美女3的搜索条件是“性别 + 猫厂 + 北京 + Java + 技术专家”，其中“猫厂 + 北京”可以通过索引来查询，但“Java”“技术专家”都只能通过模糊匹配来查询。</p><ul><li>帅哥4：程序员妹子有没有漂亮的呢？试试看看。</li></ul><p>帅哥4的搜索条件是“性别 + 美丽 + 美女”，只能通过模糊匹配搜索“自我介绍”列。</p><p>以上只是简单举个例子，实际上搜索条件是无法列举完全的，各种排列组合非常多，通过这个简单的样例我们就可以看出关系数据库在支撑全文搜索时的不足。</p><p>1.全文搜索基本原理</p><p>全文搜索引擎的技术原理被称为“倒排索引”（Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，其基本原理是建立单词到文档的索引。之所以被称为“倒排”索引，是和“正排“索引相对的，“正排索引”的基本原理是建立文档到单词的索引。我们通过一个简单的样例来说明这两种索引的差异。</p><p>假设我们有一个技术文章的网站，里面收集了各种技术文章，用户可以在网站浏览或者搜索文章。</p><p>正排索引示例：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/5fe73007957ecfcca009fd81f673df87.jpg"></p><p>正排索引适用于根据文档名称来查询文档内容。例如，用户在网站上单击了“面向对象葵花宝典是什么”，网站根据文章标题查询文章的内容展示给用户。</p><p>倒排索引示例：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ea5dc300ec9c556dc13790b69f4d60f6.jpg"></p><p>倒排索引适用于根据关键词来查询文档内容。例如，用户只是想看“设计”相关的文章，网站需要将文章内容中包含“设计”一词的文章都搜索出来展示给用户。</p><p>2.全文搜索的使用方式</p><p>全文搜索引擎的索引对象是单词和文档，而关系数据库的索引对象是键和行，两者的术语差异很大，不能简单地等同起来。因此，为了让全文搜索引擎支持关系型数据的全文搜索，需要做一些转换操作，即将关系型数据转换为文档数据。</p><p>目前常用的转换方式是将关系型数据按照对象的形式转换为JSON文档，然后将JSON文档输入全文搜索引擎进行索引。我同样以程序员的基本信息表为例，看看如何转换。</p><p>将前面样例中的程序员表格转换为JSON文档，可以得到3个程序员信息相关的文档，我以程序员1为例：</p><pre class="line-numbers language-none"><code class="language-none">&#123; &quot;id&quot;: 1, &quot;姓名&quot;: &quot;多隆&quot;, &quot;性别&quot;: &quot;男&quot;, &quot;地点&quot;: &quot;北京&quot;, &quot;单位&quot;: &quot;猫厂&quot;, &quot;爱好&quot;: &quot;写代码，旅游，马拉松&quot;, &quot;语言&quot;: &quot;Java、C++、PHP&quot;, &quot;自我介绍&quot;: &quot;技术专家，简单，为人热情&quot;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>全文搜索引擎能够基于JSON文档建立全文索引，然后快速进行全文搜索。以Elasticsearch为例，其索引基本原理如下：</p><blockquote><p>Elastcisearch是分布式的文档存储方式。它能存储和检索复杂的数据结构——序列化成为JSON文档——以实时的方式。</p></blockquote><blockquote><p>在Elasticsearch中，每个字段的所有数据都是默认被索引的。即每个字段都有为了快速检索设置的专用倒排索引。而且，不像其他多数的数据库，它能在相同的查询中使用所有倒排索引，并以惊人的速度返回结果。</p></blockquote><p>（ <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/data-in-data-out.html">https://www.elastic.co/guide/cn/elasticsearch/guide/current/data-in-data-out.html</a>）</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了为了弥补关系型数据库缺陷而产生的NoSQL技术，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧，因为NoSQL的方案功能都很强大，有人认为NoSQL &#x3D; No SQL，架构设计的时候无需再使用关系数据库，对此你怎么看？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/8377" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;16-高性能NoSQL&quot;&gt;&lt;a href=&quot;#16-高性能NoSQL&quot; class=&quot;headerlink&quot; title=&quot;16 | 高性能NoSQL&quot;&gt;&lt;/a&gt;16 | 高性能NoSQL&lt;/h1&gt;&lt;p&gt;关系数据库经过几十年的发展后已经非常成熟，强大的SQL功能和</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>18 | 单服务器高性能模式：PPC与TPC</title>
    <link href="https://zhuansun.github.io/geekbang/posts/3704927571.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/3704927571.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.894Z</updated>
    
    <content type="html"><![CDATA[<h1 id="18-单服务器高性能模式：PPC与TPC"><a href="#18-单服务器高性能模式：PPC与TPC" class="headerlink" title="18 | 单服务器高性能模式：PPC与TPC"></a>18 | 单服务器高性能模式：PPC与TPC</h1><p>高性能是每个程序员的追求，无论我们是做一个系统还是写一行代码，都希望能够达到高性能的效果，而高性能又是最复杂的一环，磁盘、操作系统、CPU、内存、缓存、网络、编程语言、架构等，每个都有可能影响系统达到高性能，一行不恰当的debug日志，就可能将服务器的性能从TPS 30000降低到8000；一个tcp_nodelay参数，就可能将响应时间从2毫秒延长到40毫秒。因此，要做到高性能计算是一件很复杂很有挑战的事情，软件系统开发过程中的不同阶段都关系着高性能最终是否能够实现。</p><p>站在架构师的角度，当然需要特别关注高性能架构的设计。高性能架构设计主要集中在两方面：</p><ul><li><p>尽量提升单服务器的性能，将单服务器的性能发挥到极致。</p></li><li><p>如果单服务器无法支撑性能，设计服务器集群方案。</p></li></ul><p>除了以上两点，最终系统能否实现高性能，还和具体的实现及编码相关。但架构设计是高性能的基础，如果架构设计没有做到高性能，则后面的具体实现和编码能提升的空间是有限的。形象地说，架构设计决定了系统性能的上限，实现细节决定了系统性能的下限。</p><p>单服务器高性能的关键之一就是 <strong>服务器采取的并发模型</strong>，并发模型有如下两个关键设计点：</p><ul><li><p>服务器如何管理连接。</p></li><li><p>服务器如何处理请求。</p></li></ul><p>以上两个设计点最终都和操作系统的I&#x2F;O模型及进程模型相关。</p><ul><li><p>I&#x2F;O模型：阻塞、非阻塞、同步、异步。</p></li><li><p>进程模型：单进程、多进程、多线程。</p></li></ul><p>在下面详细介绍并发模型时会用到上面这些基础的知识点，所以我建议你先检测一下对这些基础知识的掌握情况，更多内容你可以参考《UNIX网络编程》三卷本。今天，我们先来看看单服务器高性能模式：PPC与TPC。</p><h2 id="PPC"><a href="#PPC" class="headerlink" title="PPC"></a>PPC</h2><p>PPC是Process Per Connection的缩写，其含义是指每次有新的连接就新建一个进程去专门处理这个连接的请求，这是传统的UNIX网络服务器所采用的模型。基本的流程图是：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/53b17d63a31c6b551d3a039a2568daba.jpg"></p><ul><li><p>父进程接受连接（图中accept）。</p></li><li><p>父进程“fork”子进程（图中fork）。</p></li><li><p>子进程处理连接的读写请求（图中子进程read、业务处理、write）。</p></li><li><p>子进程关闭连接（图中子进程中的close）。</p></li></ul><p>注意，图中有一个小细节，父进程“fork”子进程后，直接调用了close，看起来好像是关闭了连接，其实只是将连接的文件描述符引用计数减一，真正的关闭连接是等子进程也调用close后，连接对应的文件描述符引用计数变为0后，操作系统才会真正关闭连接，更多细节请参考《UNIX网络编程：卷一》。</p><p>PPC模式实现简单，比较适合服务器的连接数没那么多的情况，例如数据库服务器。对于普通的业务服务器，在互联网兴起之前，由于服务器的访问量和并发量并没有那么大，这种模式其实运作得也挺好，世界上第一个web服务器CERN httpd就采用了这种模式（具体你可以参考 <a href="https://en.wikipedia.org/wiki/CERN_httpd">https://en.wikipedia.org/wiki/CERN_httpd</a>）。互联网兴起后，服务器的并发和访问量从几十剧增到成千上万，这种模式的弊端就凸显出来了，主要体现在这几个方面：</p><ul><li><p>fork代价高：站在操作系统的角度，创建一个进程的代价是很高的，需要分配很多内核资源，需要将内存映像从父进程复制到子进程。即使现在的操作系统在复制内存映像时用到了Copy on Write（写时复制）技术，总体来说创建进程的代价还是很大的。</p></li><li><p>父子进程通信复杂：父进程“fork”子进程时，文件描述符可以通过内存映像复制从父进程传到子进程，但“fork”完成后，父子进程通信就比较麻烦了，需要采用IPC（Interprocess Communication）之类的进程通信方案。例如，子进程需要在close之前告诉父进程自己处理了多少个请求以支撑父进程进行全局的统计，那么子进程和父进程必须采用IPC方案来传递信息。</p></li><li><p>支持的并发连接数量有限：如果每个连接存活时间比较长，而且新的连接又源源不断的进来，则进程数量会越来越多，操作系统进程调度和切换的频率也越来越高，系统的压力也会越来越大。因此，一般情况下，PPC方案能处理的并发连接数量最大也就几百。</p></li></ul><h2 id="prefork"><a href="#prefork" class="headerlink" title="prefork"></a>prefork</h2><p>PPC模式中，当连接进来时才fork新进程来处理连接请求，由于fork进程代价高，用户访问时可能感觉比较慢，prefork模式的出现就是为了解决这个问题。</p><p>顾名思义，prefork就是提前创建进程（pre-fork）。系统在启动的时候就预先创建好进程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去fork进程的操作，让用户访问更快、体验更好。prefork的基本示意图是：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/3c931b04d3372ebcebe4f2c2cf59d42f.jpg"></p><p>prefork的实现关键就是多个子进程都accept同一个socket，当有新的连接进入时，操作系统保证只有一个进程能最后accept成功。但这里也存在一个小小的问题：“惊群”现象，就是指虽然只有一个子进程能accept成功，但所有阻塞在accept上的子进程都会被唤醒，这样就导致了不必要的进程调度和上下文切换了。幸运的是，操作系统可以解决这个问题，例如Linux 2.6版本后内核已经解决了accept惊群问题。</p><p>prefork模式和PPC一样，还是存在父子进程通信复杂、支持的并发连接数量有限的问题，因此目前实际应用也不多。Apache服务器提供了MPM prefork模式，推荐在需要可靠性或者与旧软件兼容的站点时采用这种模式，默认情况下最大支持256个并发连接。</p><h2 id="TPC"><a href="#TPC" class="headerlink" title="TPC"></a>TPC</h2><p>TPC是Thread Per Connection的缩写，其含义是指每次有新的连接就新建一个线程去专门处理这个连接的请求。与进程相比，线程更轻量级，创建线程的消耗比进程要少得多；同时多线程是共享进程内存空间的，线程通信相比进程通信更简单。因此，TPC实际上是解决或者弱化了PPC fork代价高的问题和父子进程通信复杂的问题。</p><p>TPC的基本流程是：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/25b3910c8c5fb0055e184c5c186eece7.jpg"></p><ul><li><p>父进程接受连接（图中accept）。</p></li><li><p>父进程创建子线程（图中pthread）。</p></li><li><p>子线程处理连接的读写请求（图中子线程read、业务处理、write）。</p></li><li><p>子线程关闭连接（图中子线程中的close）。</p></li></ul><p>注意，和PPC相比，主进程不用“close”连接了。原因是在于子线程是共享主进程的进程空间的，连接的文件描述符并没有被复制，因此只需要一次close即可。</p><p>TPC虽然解决了fork代价高和进程通信复杂的问题，但是也引入了新的问题，具体表现在：</p><ul><li><p>创建线程虽然比创建进程代价低，但并不是没有代价，高并发时（例如每秒上万连接）还是有性能问题。</p></li><li><p>无须进程间通信，但是线程间的互斥和共享又引入了复杂度，可能一不小心就导致了死锁问题。</p></li><li><p>多线程会出现互相影响的情况，某个线程出现异常时，可能导致整个进程退出（例如内存越界）。</p></li></ul><p>除了引入了新的问题，TPC还是存在CPU线程调度和切换代价的问题。因此，TPC方案本质上和PPC方案基本类似，在并发几百连接的场景下，反而更多地是采用PPC的方案，因为PPC方案不会有死锁的风险，也不会多进程互相影响，稳定性更高。</p><h2 id="prethread"><a href="#prethread" class="headerlink" title="prethread"></a>prethread</h2><p>TPC模式中，当连接进来时才创建新的线程来处理连接请求，虽然创建线程比创建进程要更加轻量级，但还是有一定的代价，而prethread模式就是为了解决这个问题。</p><p>和prefork类似，prethread模式会预先创建线程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去创建线程的操作，让用户感觉更快、体验更好。</p><p>由于多线程之间数据共享和通信比较方便，因此实际上prethread的实现方式相比prefork要灵活一些，常见的实现方式有下面几种：</p><ul><li><p>主进程accept，然后将连接交给某个线程处理。</p></li><li><p>子线程都尝试去accept，最终只有一个线程accept成功，方案的基本示意图如下：</p></li></ul><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/115308f686fe0bb1c93ec4b1728eda4d.jpg"></p><p>Apache服务器的MPM worker模式本质上就是一种prethread方案，但稍微做了改进。Apache服务器会首先创建多个进程，每个进程里面再创建多个线程，这样做主要是为了考虑稳定性，即：即使某个子进程里面的某个线程异常导致整个子进程退出，还会有其他子进程继续提供服务，不会导致整个服务器全部挂掉。</p><p>prethread理论上可以比prefork支持更多的并发连接，Apache服务器MPM worker模式默认支持16 × 25 &#x3D; 400 个并发处理线程。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了传统的单服务器高性能模式PPC与TPC，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧，什么样的系统比较适合本期所讲的高性能模式？原因是什么？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/8697" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;18-单服务器高性能模式：PPC与TPC&quot;&gt;&lt;a href=&quot;#18-单服务器高性能模式：PPC与TPC&quot; class=&quot;headerlink&quot; title=&quot;18 | 单服务器高性能模式：PPC与TPC&quot;&gt;&lt;/a&gt;18 | 单服务器高性能模式：PPC与TPC&lt;/</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>20 | 高性能负载均衡：分类及架构</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1890516548.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1890516548.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.899Z</updated>
    
    <content type="html"><![CDATA[<h1 id="20-高性能负载均衡：分类及架构"><a href="#20-高性能负载均衡：分类及架构" class="headerlink" title="20 | 高性能负载均衡：分类及架构"></a>20 | 高性能负载均衡：分类及架构</h1><p>单服务器无论如何优化，无论采用多好的硬件，总会有一个性能天花板，当单服务器的性能无法满足业务需求时，就需要设计高性能集群来提升系统整体的处理性能。</p><p>高性能集群的本质很简单，通过增加更多的服务器来提升系统整体的计算能力。由于计算本身存在一个特点：同样的输入数据和逻辑，无论在哪台服务器上执行，都应该得到相同的输出。因此高性能集群设计的复杂度主要体现在任务分配这部分，需要设计合理的任务分配策略，将计算任务分配到多台服务器上执行。</p><p><strong>高性能集群的复杂性主要体现在需要增加一个任务分配器，以及为任务选择一个合适的任务分配算法</strong>。对于任务分配器，现在更流行的通用叫法是“负载均衡器”。但这个名称有一定的误导性，会让人潜意识里认为任务分配的目的是要保持各个计算单元的负载达到均衡状态。而实际上任务分配并不只是考虑计算单元的负载均衡，不同的任务分配算法目标是不一样的，有的基于负载考虑，有的基于性能（吞吐量、响应时间）考虑，有的基于业务考虑。考虑到“负载均衡”已经成为了事实上的标准术语，这里我也用“负载均衡”来代替“任务分配”，但请你时刻记住， <strong>负载均衡不只是为了计算单元的负载达到均衡状态</strong>。</p><p>今天我先来讲讲负载均衡的分类及架构，下一期会讲负载均衡的算法。</p><h2 id="负载均衡分类"><a href="#负载均衡分类" class="headerlink" title="负载均衡分类"></a>负载均衡分类</h2><p>常见的负载均衡系统包括3种：DNS负载均衡、硬件负载均衡和软件负载均衡。</p><p><strong>DNS负载均衡</strong></p><p>DNS是最简单也是最常见的负载均衡方式，一般用来实现地理级别的均衡。例如，北方的用户访问北京的机房，南方的用户访问深圳的机房。DNS负载均衡的本质是DNS解析同一个域名可以返回不同的IP地址。例如，同样是<a href="http://www.baidu.com,北方用户解析后获取的地址是61.135.165.224(这是北京机房的ip),南方用户解析后获取的地址是14.215.177.38(这是深圳机房的ip)./">www.baidu.com，北方用户解析后获取的地址是61.135.165.224（这是北京机房的IP），南方用户解析后获取的地址是14.215.177.38（这是深圳机房的IP）。</a></p><p>下面是DNS负载均衡的简单示意图：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/dbb61acde016acb2f57212d627d2732f.jpg"></p><p>DNS负载均衡实现简单、成本低，但也存在粒度太粗、负载均衡算法少等缺点。仔细分析一下优缺点，其优点有：</p><ul><li><p>简单、成本低：负载均衡工作交给DNS服务器处理，无须自己开发或者维护负载均衡设备。</p></li><li><p>就近访问，提升访问速度：DNS解析时可以根据请求来源IP，解析成距离用户最近的服务器地址，可以加快访问速度，改善性能。</p></li></ul><p>缺点有：</p><ul><li><p>更新不及时：DNS缓存的时间比较长，修改DNS配置后，由于缓存的原因，还是有很多用户会继续访问修改前的IP，这样的访问会失败，达不到负载均衡的目的，并且也影响用户正常使用业务。</p></li><li><p>扩展性差：DNS负载均衡的控制权在域名商那里，无法根据业务特点针对其做更多的定制化功能和扩展特性。</p></li><li><p>分配策略比较简单：DNS负载均衡支持的算法少；不能区分服务器的差异（不能根据系统与服务的状态来判断负载）；也无法感知后端服务器的状态。</p></li></ul><p>针对DNS负载均衡的一些缺点，对于时延和故障敏感的业务，有一些公司自己实现了HTTP-DNS的功能，即使用HTTP协议实现一个私有的DNS系统。这样的方案和通用的DNS优缺点正好相反。</p><p><strong>硬件负载均衡</strong></p><p>硬件负载均衡是通过单独的硬件设备来实现负载均衡功能，这类设备和路由器、交换机类似，可以理解为一个用于负载均衡的基础网络设备。目前业界典型的硬件负载均衡设备有两款：F5和A10。这类设备性能强劲、功能强大，但价格都不便宜，一般只有“土豪”公司才会考虑使用此类设备。普通业务量级的公司一是负担不起，二是业务量没那么大，用这些设备也是浪费。</p><p>硬件负载均衡的优点是：</p><ul><li><p>功能强大：全面支持各层级的负载均衡，支持全面的负载均衡算法，支持全局负载均衡。</p></li><li><p>性能强大：对比一下，软件负载均衡支持到10万级并发已经很厉害了，硬件负载均衡可以支持100万以上的并发。</p></li><li><p>稳定性高：商用硬件负载均衡，经过了良好的严格测试，经过大规模使用，稳定性高。</p></li><li><p>支持安全防护：硬件均衡设备除具备负载均衡功能外，还具备防火墙、防DDoS攻击等安全功能。</p></li></ul><p>硬件负载均衡的缺点是：</p><ul><li><p>价格昂贵：最普通的一台F5就是一台“马6”，好一点的就是“Q7”了。</p></li><li><p>扩展能力差：硬件设备，可以根据业务进行配置，但无法进行扩展和定制。</p></li></ul><p><strong>软件负载均衡</strong></p><p>软件负载均衡通过负载均衡软件来实现负载均衡功能，常见的有Nginx和LVS，其中Nginx是软件的7层负载均衡，LVS是Linux内核的4层负载均衡。4层和7层的区别就在于 <strong>协议</strong> 和 <strong>灵活性</strong>，Nginx支持HTTP、E-mail协议；而LVS是4层负载均衡，和协议无关，几乎所有应用都可以做，例如，聊天、数据库等。</p><p>软件和硬件的最主要区别就在于性能，硬件负载均衡性能远远高于软件负载均衡性能。Nginx的性能是万级，一般的Linux服务器上装一个Nginx大概能到5万&#x2F;秒；LVS的性能是十万级，据说可达到80万&#x2F;秒；而F5性能是百万级，从200万&#x2F;秒到800万&#x2F;秒都有（数据来源网络，仅供参考，如需采用请根据实际业务场景进行性能测试）。当然，软件负载均衡的最大优势是便宜，一台普通的Linux服务器批发价大概就是1万元左右，相比F5的价格，那就是自行车和宝马的区别了。</p><p>除了使用开源的系统进行负载均衡，如果业务比较特殊，也可能基于开源系统进行定制（例如，Nginx插件），甚至进行自研。</p><p>下面是Nginx的负载均衡架构示意图：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/136afcb3b3bc964f2609127eb27a0235.jpg"></p><p>软件负载均衡的优点：</p><ul><li><p>简单：无论是部署还是维护都比较简单。</p></li><li><p>便宜：只要买个Linux服务器，装上软件即可。</p></li><li><p>灵活：4层和7层负载均衡可以根据业务进行选择；也可以根据业务进行比较方便的扩展，例如，可以通过Nginx的插件来实现业务的定制化功能。</p></li></ul><p>其实下面的缺点都是和硬件负载均衡相比的，并不是说软件负载均衡没法用。</p><ul><li><p>性能一般：一个Nginx大约能支撑5万并发。</p></li><li><p>功能没有硬件负载均衡那么强大。</p></li><li><p>一般不具备防火墙和防DDoS攻击等安全功能。</p></li></ul><h2 id="负载均衡典型架构"><a href="#负载均衡典型架构" class="headerlink" title="负载均衡典型架构"></a>负载均衡典型架构</h2><p>前面我们介绍了3种常见的负载均衡机制：DNS负载均衡、硬件负载均衡、软件负载均衡，每种方式都有一些优缺点，但并不意味着在实际应用中只能基于它们的优缺点进行非此即彼的选择，反而是基于它们的优缺点进行组合使用。具体来说，组合的 <strong>基本原则</strong> 为：DNS负载均衡用于实现地理级别的负载均衡；硬件负载均衡用于实现集群级别的负载均衡；软件负载均衡用于实现机器级别的负载均衡。</p><p>我以一个假想的实例来说明一下这种组合方式，如下图所示。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/79f371ecbf74818e2a34b4a31664668d.png"></p><p>整个系统的负载均衡分为三层。</p><ul><li><p>地理级别负载均衡：<a href="http://www.xxx.com部署在北京、广州、上海三个机房,当用户访问时,dns会根据用户的地理位置来决定返回哪个机房的ip,图中返回了广州机房的ip地址,这样用户就访问到广州机房了./">www.xxx.com部署在北京、广州、上海三个机房，当用户访问时，DNS会根据用户的地理位置来决定返回哪个机房的IP，图中返回了广州机房的IP地址，这样用户就访问到广州机房了。</a></p></li><li><p>集群级别负载均衡：广州机房的负载均衡用的是F5设备，F5收到用户请求后，进行集群级别的负载均衡，将用户请求发给3个本地集群中的一个，我们假设F5将用户请求发给了“广州集群2”。</p></li><li><p>机器级别的负载均衡：广州集群2的负载均衡用的是Nginx，Nginx收到用户请求后，将用户请求发送给集群里面的某台服务器，服务器处理用户的业务请求并返回业务响应。</p></li></ul><p>需要注意的是，上图只是一个示例，一般在大型业务场景下才会这样用，如果业务量没这么大，则没有必要严格照搬这套架构。例如，一个大学的论坛，完全可以不需要DNS负载均衡，也不需要F5设备，只需要用Nginx作为一个简单的负载均衡就足够了。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了负载均衡的常见分类以及典型架构，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧，假设你来设计一个日活跃用户1000万的论坛的负载均衡集群，你的方案是什么？设计理由是什么？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/8942" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;20-高性能负载均衡：分类及架构&quot;&gt;&lt;a href=&quot;#20-高性能负载均衡：分类及架构&quot; class=&quot;headerlink&quot; title=&quot;20 | 高性能负载均衡：分类及架构&quot;&gt;&lt;/a&gt;20 | 高性能负载均衡：分类及架构&lt;/h1&gt;&lt;p&gt;单服务器无论如何优化</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>21 | 高性能负载均衡：算法</title>
    <link href="https://zhuansun.github.io/geekbang/posts/4173720359.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/4173720359.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.902Z</updated>
    
    <content type="html"><![CDATA[<h1 id="21-高性能负载均衡：算法"><a href="#21-高性能负载均衡：算法" class="headerlink" title="21 | 高性能负载均衡：算法"></a>21 | 高性能负载均衡：算法</h1><p>负载均衡算法数量较多，而且可以根据一些业务特性进行定制开发，抛开细节上的差异，根据算法期望达到的目的，大体上可以分为下面几类。</p><ul><li><p>任务平分类：负载均衡系统将收到的任务平均分配给服务器进行处理，这里的“平均”可以是绝对数量的平均，也可以是比例或者权重上的平均。</p></li><li><p>负载均衡类：负载均衡系统根据服务器的负载来进行分配，这里的负载并不一定是通常意义上我们说的“CPU负载”，而是系统当前的压力，可以用CPU负载来衡量，也可以用连接数、I&#x2F;O使用率、网卡吞吐量等来衡量系统的压力。</p></li><li><p>性能最优类：负载均衡系统根据服务器的响应时间来进行任务分配，优先将新任务分配给响应最快的服务器。</p></li><li><p>Hash类：负载均衡系统根据任务中的某些关键信息进行Hash运算，将相同Hash值的请求分配到同一台服务器上。常见的有源地址Hash、目标地址Hash、session id hash、用户ID Hash等。</p></li></ul><p>接下来我介绍一下负载均衡算法以及它们的优缺点。</p><h2 id="轮询"><a href="#轮询" class="headerlink" title="轮询"></a>轮询</h2><p>负载均衡系统收到请求后，按照顺序轮流分配到服务器上。</p><p>轮询是最简单的一个策略，无须关注服务器本身的状态，例如：</p><ul><li><p>某个服务器当前因为触发了程序bug进入了死循环导致CPU负载很高，负载均衡系统是不感知的，还是会继续将请求源源不断地发送给它。</p></li><li><p>集群中有新的机器是32核的，老的机器是16核的，负载均衡系统也是不关注的，新老机器分配的任务数是一样的。</p></li></ul><p>需要注意的是负载均衡系统无须关注“服务器本身状态”，这里的关键词是“本身”。也就是说， <strong>只要服务器在运行，运行状态是不关注的</strong>。但如果服务器直接宕机了，或者服务器和负载均衡系统断连了，这时负载均衡系统是能够感知的，也需要做出相应的处理。例如，将服务器从可分配服务器列表中删除，否则就会出现服务器都宕机了，任务还不断地分配给它，这明显是不合理的。</p><p>总而言之，“简单”是轮询算法的优点，也是它的缺点。</p><h2 id="加权轮询"><a href="#加权轮询" class="headerlink" title="加权轮询"></a>加权轮询</h2><p>负载均衡系统根据服务器权重进行任务分配，这里的权重一般是根据硬件配置进行静态配置的，采用动态的方式计算会更加契合业务，但复杂度也会更高。</p><p>加权轮询是轮询的一种特殊形式，其主要目的就是为了 <strong>解决不同服务器处理能力有差异的问题</strong>。例如，集群中有新的机器是32核的，老的机器是16核的，那么理论上我们可以假设新机器的处理能力是老机器的2倍，负载均衡系统就可以按照2:1的比例分配更多的任务给新机器，从而充分利用新机器的性能。</p><p>加权轮询解决了轮询算法中无法根据服务器的配置差异进行任务分配的问题，但同样存在无法根据服务器的状态差异进行任务分配的问题。</p><h2 id="负载最低优先"><a href="#负载最低优先" class="headerlink" title="负载最低优先"></a>负载最低优先</h2><p>负载均衡系统将任务分配给当前负载最低的服务器，这里的负载根据不同的任务类型和业务场景，可以用不同的指标来衡量。例如：</p><ul><li><p>LVS这种4层网络负载均衡设备，可以以“连接数”来判断服务器的状态，服务器连接数越大，表明服务器压力越大。</p></li><li><p>Nginx这种7层网络负载系统，可以以“HTTP请求数”来判断服务器状态（Nginx内置的负载均衡算法不支持这种方式，需要进行扩展）。</p></li><li><p>如果我们自己开发负载均衡系统，可以根据业务特点来选择指标衡量系统压力。如果是CPU密集型，可以以“CPU负载”来衡量系统压力；如果是I&#x2F;O密集型，可以以“I&#x2F;O负载”来衡量系统压力。</p></li></ul><p>负载最低优先的算法解决了轮询算法中无法感知服务器状态的问题，由此带来的代价是复杂度要增加很多。例如：</p><ul><li><p>最少连接数优先的算法要求负载均衡系统统计每个服务器当前建立的连接，其应用场景仅限于负载均衡接收的任何连接请求都会转发给服务器进行处理，否则如果负载均衡系统和服务器之间是固定的连接池方式，就不适合采取这种算法。例如，LVS可以采取这种算法进行负载均衡，而一个通过连接池的方式连接MySQL集群的负载均衡系统就不适合采取这种算法进行负载均衡。</p></li><li><p>CPU负载最低优先的算法要求负载均衡系统以某种方式收集每个服务器的CPU负载，而且要确定是以1分钟的负载为标准，还是以15分钟的负载为标准，不存在1分钟肯定比15分钟要好或者差。不同业务最优的时间间隔是不一样的，时间间隔太短容易造成频繁波动，时间间隔太长又可能造成峰值来临时响应缓慢。</p></li></ul><p>负载最低优先算法基本上能够比较完美地解决轮询算法的缺点，因为采用这种算法后，负载均衡系统需要感知服务器当前的运行状态。当然，其代价是复杂度大幅上升。通俗来讲，轮询可能是5行代码就能实现的算法，而负载最低优先算法可能要1000行才能实现，甚至需要负载均衡系统和服务器都要开发代码。负载最低优先算法如果本身没有设计好，或者不适合业务的运行特点，算法本身就可能成为性能的瓶颈，或者引发很多莫名其妙的问题。所以负载最低优先算法虽然效果看起来很美好，但实际上真正应用的场景反而没有轮询（包括加权轮询）那么多。</p><h2 id="性能最优类"><a href="#性能最优类" class="headerlink" title="性能最优类"></a>性能最优类</h2><p>负载最低优先类算法是站在服务器的角度来进行分配的，而性能最优优先类算法则是站在客户端的角度来进行分配的，优先将任务分配给处理速度最快的服务器，通过这种方式达到最快响应客户端的目的。</p><p>和负载最低优先类算法类似，性能最优优先类算法本质上也是感知了服务器的状态，只是通过响应时间这个外部标准来衡量服务器状态而已。因此性能最优优先类算法存在的问题和负载最低优先类算法类似，复杂度都很高，主要体现在：</p><ul><li><p>负载均衡系统需要收集和分析每个服务器每个任务的响应时间，在大量任务处理的场景下，这种收集和统计本身也会消耗较多的性能。</p></li><li><p>为了减少这种统计上的消耗，可以采取采样的方式来统计，即不统计所有任务的响应时间，而是抽样统计部分任务的响应时间来估算整体任务的响应时间。采样统计虽然能够减少性能消耗，但使得复杂度进一步上升，因为要确定合适的 <strong>采样率</strong>，采样率太低会导致结果不准确，采样率太高会导致性能消耗较大，找到合适的采样率也是一件复杂的事情。</p></li><li><p>无论是全部统计还是采样统计，都需要选择合适的 <strong>周期</strong>：是10秒内性能最优，还是1分钟内性能最优，还是5分钟内性能最优……没有放之四海而皆准的周期，需要根据实际业务进行判断和选择，这也是一件比较复杂的事情，甚至出现系统上线后需要不断地调优才能达到最优设计。</p></li></ul><h2 id="Hash类"><a href="#Hash类" class="headerlink" title="Hash类"></a>Hash类</h2><p>负载均衡系统根据任务中的某些关键信息进行Hash运算，将相同Hash值的请求分配到同一台服务器上，这样做的目的主要是为了满足特定的业务需求。例如：</p><ul><li>源地址Hash</li></ul><p>将来源于同一个源IP地址的任务分配给同一个服务器进行处理，适合于存在事务、会话的业务。例如，当我们通过浏览器登录网上银行时，会生成一个会话信息，这个会话是临时的，关闭浏览器后就失效。网上银行后台无须持久化会话信息，只需要在某台服务器上临时保存这个会话就可以了，但需要保证用户在会话存在期间，每次都能访问到同一个服务器，这种业务场景就可以用源地址Hash来实现。</p><ul><li>ID Hash</li></ul><p>将某个ID标识的业务分配到同一个服务器中进行处理，这里的ID一般是临时性数据的ID（如session id）。例如，上述的网上银行登录的例子，用session id hash同样可以实现同一个会话期间，用户每次都是访问到同一台服务器的目的。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了常见负载均衡算法的优缺点和应用场景，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧，微信抢红包的高并发架构，应该采取什么样的负载均衡算法？谈谈你的分析和理解。</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/9055" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;21-高性能负载均衡：算法&quot;&gt;&lt;a href=&quot;#21-高性能负载均衡：算法&quot; class=&quot;headerlink&quot; title=&quot;21 | 高性能负载均衡：算法&quot;&gt;&lt;/a&gt;21 | 高性能负载均衡：算法&lt;/h1&gt;&lt;p&gt;负载均衡算法数量较多，而且可以根据一些业务特</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>23 | 想成为架构师，你必须掌握的CAP细节</title>
    <link href="https://zhuansun.github.io/geekbang/posts/2474301592.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/2474301592.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.908Z</updated>
    
    <content type="html"><![CDATA[<h1 id="23-想成为架构师，你必须掌握的CAP细节"><a href="#23-想成为架构师，你必须掌握的CAP细节" class="headerlink" title="23 | 想成为架构师，你必须掌握的CAP细节"></a>23 | 想成为架构师，你必须掌握的CAP细节</h1><p>理论的优点在于清晰简洁、易于理解，但缺点就是高度抽象化，省略了很多细节，导致在将理论应用到实践时，由于各种复杂情况，可能出现误解和偏差，CAP理论也不例外。如果我们没有意识到这些关键的细节点，那么在实践中应用CAP理论时，就可能发现方案很难落地。</p><p>而且当谈到数据一致性时，CAP、ACID、BASE难免会被我们拿出来讨论，原因在于这三者都是和数据一致性相关的理论，如果不仔细理解三者之间的差别，则可能会陷入一头雾水的状态，不知道应该用哪个才好。</p><p>今天，我来讲讲CAP的具体细节，简单对比一下ACID、BASE几个概念的关键区别点。</p><h2 id="CAP关键细节点"><a href="#CAP关键细节点" class="headerlink" title="CAP关键细节点"></a>CAP关键细节点</h2><p>埃里克·布鲁尔（Eric Brewer）在《CAP理论十二年回顾：“规则”变了》（ <a href="http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed">http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed</a>）一文中详细地阐述了理解和应用CAP的一些细节点，可能是由于作者写作风格的原因，对于一些非常关键的细节点一句话就带过了，这里我特别提炼出来重点阐述。</p><ul><li>CAP关注的粒度是 <strong>数据</strong>，而不是整个系统。</li></ul><p>原文就只有一句话：</p><blockquote><p>C与A之间的取舍可以在同一系统内以非常细小的粒度反复发生，而每一次的决策可能因为具体的操作，乃至因为牵涉到特定的数据或用户而有所不同。</p></blockquote><p>但这句话是理解和应用CAP理论非常关键的一点。CAP理论的定义和解释中，用的都是system、node这类系统级的概念，这就给很多人造成了很大的误导，认为我们在进行架构设计时，整个系统要么选择CP，要么选择AP。但在实际设计过程中，每个系统不可能只处理一种数据，而是包含多种类型的数据，有的数据必须选择CP，有的数据必须选择AP。而如果我们做设计时，从整个系统的角度去选择CP还是AP，就会发现顾此失彼，无论怎么做都是有问题的。</p><p>以一个最简单的用户管理系统为例，用户管理系统包含用户账号数据（用户ID、密码）、用户信息数据（昵称、兴趣、爱好、性别、自我介绍等）。通常情况下，用户账号数据会选择CP，而用户信息数据会选择AP，如果限定整个系统为CP，则不符合用户信息数据的应用场景；如果限定整个系统为AP，则又不符合用户账号数据的应用场景。</p><p>所以在CAP理论落地实践时，我们需要将系统内的数据按照不同的应用场景和要求进行分类，每类数据选择不同的策略（CP还是AP），而不是直接限定整个系统所有数据都是同一策略。</p><ul><li>CAP是忽略网络延迟的。</li></ul><p>这是一个非常隐含的假设，布鲁尔在定义一致性时，并没有将延迟考虑进去。也就是说，当事务提交时，数据能够瞬间复制到所有节点。但实际情况下，从节点A复制数据到节点B，总是需要花费一定时间的。如果是相同机房，耗费时间可能是几毫秒；如果是跨地域的机房，例如北京机房同步到广州机房，耗费的时间就可能是几十毫秒。这就意味着，CAP理论中的C在实践中是不可能完美实现的，在数据复制的过程中，节点A和节点B的数据并不一致。</p><p>不要小看了这几毫秒或者几十毫秒的不一致，对于某些严苛的业务场景，例如和金钱相关的用户余额，或者和抢购相关的商品库存，技术上是无法做到分布式场景下完美的一致性的。而业务上必须要求一致性，因此单个用户的余额、单个商品的库存，理论上要求选择CP而实际上CP都做不到，只能选择CA。也就是说，只能单点写入，其他节点做备份，无法做到分布式情况下多点写入。</p><p>需要注意的是，这并不意味着这类系统无法应用分布式架构，只是说“单个用户余额、单个商品库存”无法做分布式，但系统整体还是可以应用分布式架构的。例如，下面的架构图是常见的将用户分区的分布式架构。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/66476fd7ffd5d6f80f4f9ba0938d0443.png"></p><p>我们可以将用户id为0 ~ 100的数据存储在Node 1，将用户id为101 ~ 200的数据存储在Node 2，Client根据用户id来决定访问哪个Node。对于单个用户来说，读写操作都只能在某个节点上进行；对所有用户来说，有一部分用户的读写操作在Node 1上，有一部分用户的读写操作在Node 2上。</p><p>这样的设计有一个很明显的问题就是某个节点故障时，这个节点上的用户就无法进行读写操作了，但站在整体上来看，这种设计可以降低节点故障时受影响的用户的数量和范围，毕竟只影响20%的用户肯定要比影响所有用户要好。这也是为什么挖掘机挖断光缆后，支付宝只有一部分用户会出现业务异常，而不是所有用户业务异常的原因。</p><ul><li>正常运行情况下，不存在CP和AP的选择，可以同时满足CA。</li></ul><p>CAP理论告诉我们分布式系统只能选择CP或者AP，但其实这里的前提是系统发生了“分区”现象。如果系统没有发生分区现象，也就是说P不存在的时候（节点间的网络连接一切正常），我们没有必要放弃C或者A，应该C和A都可以保证，这就要求架构设计的时候 <strong>既要考虑分区发生时选择CP还是AP，也要考虑分区没有发生时如何保证CA</strong>。</p><p>同样以用户管理系统为例，即使是实现CA，不同的数据实现方式也可能不一样：用户账号数据可以采用“消息队列”的方式来实现CA，因为消息队列可以比较好地控制实时性，但实现起来就复杂一些；而用户信息数据可以采用“数据库同步”的方式来实现CA，因为数据库的方式虽然在某些场景下可能延迟较高，但使用起来简单。</p><ul><li>放弃并不等于什么都不做，需要为分区恢复后做准备。</li></ul><p>CAP理论告诉我们三者只能取两个，需要“牺牲”（sacrificed）另外一个，这里的“牺牲”是有一定误导作用的，因为“牺牲”让很多人理解成什么都不做。实际上，CAP理论的“牺牲”只是说在分区过程中我们无法保证C或者A，但并不意味着什么都不做。因为在系统整个运行周期中，大部分时间都是正常的，发生分区现象的时间并不长。例如，99.99%可用性（俗称4个9）的系统，一年运行下来，不可用的时间只有50分钟；99.999%（俗称5个9）可用性的系统，一年运行下来，不可用的时间只有5分钟。分区期间放弃C或者A，并不意味着永远放弃C和A，我们可以在分区期间进行一些操作，从而让分区故障解决后，系统能够重新达到CA的状态。</p><p>最典型的就是在分区期间记录一些日志，当分区故障解决后，系统根据日志进行数据恢复，使得重新达到CA状态。同样以用户管理系统为例，对于用户账号数据，假设我们选择了CP，则分区发生后，节点1可以继续注册新用户，节点2无法注册新用户（这里就是不符合A的原因，因为节点2收到注册请求后会返回error），此时节点1可以将新注册但未同步到节点2的用户记录到日志中。当分区恢复后，节点1读取日志中的记录，同步给节点2，当同步完成后，节点1和节点2就达到了同时满足CA的状态。</p><p>而对于用户信息数据，假设我们选择了AP，则分区发生后，节点1和节点2都可以修改用户信息，但两边可能修改不一样。例如，用户在节点1中将爱好改为“旅游、美食、跑步”，然后用户在节点2中将爱好改为“美食、游戏”，节点1和节点2都记录了未同步的爱好数据，当分区恢复后，系统按照某个规则来合并数据。例如，按照“最后修改优先规则”将用户爱好修改为“美食、游戏”，按照“字数最多优先规则”则将用户爱好修改为“旅游，美食、跑步”，也可以完全将数据冲突报告出来，由人工来选择具体应该采用哪一条。</p><h2 id="ACID"><a href="#ACID" class="headerlink" title="ACID"></a>ACID</h2><p>ACID是数据库管理系统为了保证事务的正确性而提出来的一个理论，ACID包含四个约束，下面我来解释一下。</p><p>1.Atomicity（原子性）</p><p>一个事务中的所有操作，要么全部完成，要么全部不完成，不会在中间某个环节结束。事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。</p><p>2.Consistency（一致性）</p><p>在事务开始之前和事务结束以后，数据库的完整性没有被破坏。</p><p>3.Isolation（隔离性）</p><p>数据库允许多个并发事务同时对数据进行读写和修改的能力。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。</p><p>4.Durability（持久性）</p><p>事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。</p><p>可以看到，ACID中的A（Atomicity）和CAP中的A（Availability）意义完全不同，而ACID中的C和CAP中的C名称虽然都是一致性，但含义也完全不一样。ACID中的C是指数据库的数据完整性，而CAP中的C是指分布式节点中的数据一致性。再结合ACID的应用场景是数据库事务，CAP关注的是分布式系统数据读写这个差异点来看，其实CAP和ACID的对比就类似关公战秦琼，虽然关公和秦琼都是武将，但其实没有太多可比性。</p><h2 id="BASE"><a href="#BASE" class="headerlink" title="BASE"></a>BASE</h2><p>BASE是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency），核心思想是即使无法做到强一致性（CAP的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性。</p><p>1.基本可用（Basically Available）</p><p>分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。</p><p>这里的关键词是“ <strong>部分</strong>”和“ <strong>核心</strong>”，具体选择哪些作为可以损失的业务，哪些是必须保证的业务，是一项有挑战的工作。例如，对于一个用户管理系统来说，“登录”是核心功能，而“注册”可以算作非核心功能。因为未注册的用户本来就还没有使用系统的业务，注册不了最多就是流失一部分用户，而且这部分用户数量较少。如果用户已经注册但无法登录，那就意味用户无法使用系统。例如，充了钱的游戏不能玩了、云存储不能用了……这些会对用户造成较大损失，而且登录用户数量远远大于新注册用户，影响范围更大。</p><p>2.软状态（Soft State）</p><p>允许系统存在中间状态，而该中间状态不会影响系统整体可用性。这里的中间状态就是CAP理论中的数据不一致。</p><p>3.最终一致性（Eventual Consistency）</p><p>系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。</p><p>这里的关键词是“一定时间” 和 “最终”，“一定时间”和数据的特性是强关联的，不同的数据能够容忍的不一致时间是不同的。举一个微博系统的例子，用户账号数据最好能在1分钟内就达到一致状态，因为用户在A节点注册或者登录后，1分钟内不太可能立刻切换到另外一个节点，但10分钟后可能就重新登录到另外一个节点了；而用户发布的最新微博，可以容忍30分钟内达到一致状态，因为对于用户来说，看不到某个明星发布的最新微博，用户是无感知的，会认为明星没有发布微博。“最终”的含义就是不管多长时间，最终还是要达到一致性的状态。</p><p>BASE理论本质上是对CAP的延伸和补充，更具体地说， <strong>是对CAP中AP方案的一个补充</strong>。前面在剖析CAP理论时，提到了其实和BASE相关的两点：</p><ul><li>CAP理论是忽略延时的，而实际应用中延时是无法避免的。</li></ul><p>这一点就意味着完美的CP场景是不存在的，即使是几毫秒的数据复制延迟，在这几毫秒时间间隔内，系统是不符合CP要求的。因此CAP中的CP方案，实际上也是实现了最终一致性，只是“一定时间”是指几毫秒而已。</p><ul><li>AP方案中牺牲一致性只是指分区期间，而不是永远放弃一致性。</li></ul><p>这一点其实就是BASE理论延伸的地方，分区期间牺牲一致性，但分区故障恢复后，系统应该达到最终一致性。</p><p>综合上面的分析，ACID是数据库事务完整性的理论，CAP是分布式系统设计理论，BASE是CAP理论中AP方案的延伸。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了深入理解CAP理论所需要特别关注的细节点，以及ACID和BASE两个相似的术语，这些技术细节在架构设计中非常关键，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧，假如你来设计电商网站的高可用系统，按照CAP理论的要求，你会如何设计？</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/9390" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;23-想成为架构师，你必须掌握的CAP细节&quot;&gt;&lt;a href=&quot;#23-想成为架构师，你必须掌握的CAP细节&quot; class=&quot;headerlink&quot; title=&quot;23 | 想成为架构师，你必须掌握的CAP细节&quot;&gt;&lt;/a&gt;23 | 想成为架构师，你必须掌握的CAP</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>24 | FMEA方法，排除架构可用性隐患的利器</title>
    <link href="https://zhuansun.github.io/geekbang/posts/1330967209.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/1330967209.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.911Z</updated>
    
    <content type="html"><![CDATA[<h1 id="24-FMEA方法，排除架构可用性隐患的利器"><a href="#24-FMEA方法，排除架构可用性隐患的利器" class="headerlink" title="24 | FMEA方法，排除架构可用性隐患的利器"></a>24 | FMEA方法，排除架构可用性隐患的利器</h1><p>我在前面的专栏分析高可用复杂度的时候提出了一个问题：高可用和高性能哪个更复杂，大部分同学都分析出了正确的答案：高可用更复杂一些，主要原因在于异常的场景很多，只要有一个场景遗漏，架构设计就存在可用性隐患，而根据墨菲定律“可能出错的事情最终都会出错”，架构隐患总有一天会导致系统故障。因此，我们在进行架构设计的时候必须全面分析系统的可用性，那么如何才能做到“全面”呢？</p><p>我今天介绍的FMEA方法，就是保证我们做到全面分析的一个非常简单但是非常有效的方法。</p><h2 id="FMEA介绍"><a href="#FMEA介绍" class="headerlink" title="FMEA介绍"></a>FMEA介绍</h2><p>FMEA（Failure mode and effects analysis，故障模式与影响分析）又称为失效模式与后果分析、失效模式与效应分析、故障模式与后果分析等，专栏采用“ <strong>故障模式与影响分析</strong>”，因为这个中文翻译更加符合可用性的语境。FMEA是一种在各行各业都有广泛应用的可用性分析方法，通过对系统范围内潜在的故障模式加以分析，并按照严重程度进行分类，以确定失效对于系统的最终影响。</p><p>FMEA最早是在美国军方开始应用的，20世纪40年代后期，美国空军正式采用了FMEA。尽管最初是在军事领域建立的方法，但FMEA方法现在已广泛应用于各种各样的行业，包括半导体加工、餐饮服务、塑料制造、软件及医疗保健行业。FMEA之所以能够在这些差异很大的领域都得到应用，根本原因在于FMEA是一套分析和思考的方法，而不是某个领域的技能或者工具。</p><p>回到软件架构设计领域，FMEA并不能指导我们如何做架构设计，而是当我们设计出一个架构后，再使用FMEA对这个架构进行分析，看看架构是否还存在某些可用性的隐患。</p><h2 id="FMEA方法"><a href="#FMEA方法" class="headerlink" title="FMEA方法"></a>FMEA方法</h2><p>在架构设计领域，FMEA的具体分析方法是：</p><ul><li><p>给出初始的架构设计图。</p></li><li><p>假设架构中某个部件发生故障。</p></li><li><p>分析此故障对系统功能造成的影响。</p></li><li><p>根据分析结果，判断架构是否需要进行优化。</p></li></ul><p>FMEA分析的方法其实很简单，就是一个FMEA分析表，常见的FMEA分析表格包含下面部分。</p><ol><li><strong>功能点</strong></li></ol><p>当前的FMEA分析涉及的功能点，注意这里的“功能点”指的是从用户角度来看的，而不是从系统各个模块功能点划分来看的。例如，对于一个用户管理系统，使用FMEA分析时 “登录”“注册”才是功能点，而用户管理系统中的数据库存储功能、Redis缓存功能不能作为FMEA分析的功能点。</p><ol start="2"><li><strong>故障模式</strong></li></ol><p>故障模式指的是系统会出现什么样的故障，包括故障点和故障形式。需要特别注意的是，这里的故障模式并不需要给出真正的故障原因，我们只需要假设出现某种故障现象即可，例如MySQL响应时间达到3秒。造成MySQL响应时间达到3秒可能的原因很多：磁盘坏道、慢查询、服务器到MySQL的连接网络故障、MySQL bug等，我们并不需要在故障模式中一一列出来，而是在后面的“故障原因”一节中列出来。因为在实际应用过程中，不管哪种原因，只要现象是一样的，对业务的影响就是一样的。</p><p>此外，故障模式的描述要尽量精确，多使用量化描述，避免使用泛化的描述。例如，推荐使用“MySQL响应时间达到3秒”，而不是“MySQL响应慢”。</p><ol start="3"><li><strong>故障影响</strong></li></ol><p>当发生故障模式中描述的故障时，功能点具体会受到什么影响。常见的影响有：功能点偶尔不可用、功能点完全不可用、部分用户功能点不可用、功能点响应缓慢、功能点出错等。</p><p>故障影响也需要尽量准确描述。例如，推荐使用“20%的用户无法登录”，而不是“大部分用户无法登录”。要注意这里的数字不需要完全精确，比如21.25%这样的数据其实是没有必要的，我们只需要预估影响是20%还是40%。</p><ol start="4"><li><strong>严重程度</strong></li></ol><p>严重程度指站在业务的角度故障的影响程度，一般分为“致命&#x2F;高&#x2F;中&#x2F;低&#x2F;无”五个档次。严重程度按照这个公式进行评估：严重程度 &#x3D; 功能点重要程度 × 故障影响范围 × 功能点受损程度。同样以用户管理系统为例：登录功能比修改用户资料要重要得多，80%的用户比20%的用户范围更大，完全无法登录比登录缓慢要更严重。因此我们可以得出如下故障模式的严重程度。</p><ul><li><p>致命：超过70%用户无法登录。</p></li><li><p>高：超过30%的用户无法登录。</p></li><li><p>中：所有用户登录时间超过5秒。</p></li><li><p>低：10%的用户登录时间超过5秒。</p></li><li><p>中：所有用户都无法修改资料。</p></li><li><p>低：20%的用户无法修改头像。</p></li></ul><p>对于某个故障的影响到底属于哪个档次，有时会出现一些争议。例如，“所有用户都无法修改资料”，有的人认为是高，有的人可能认为是中，这个没有绝对标准，一般建议相关人员讨论确定即可。也不建议花费太多时间争论，争执不下时架构师裁定即可。</p><ol start="5"><li><strong>故障原因</strong></li></ol><p>“故障模式”中只描述了故障的现象，并没有单独列出故障原因。主要原因在于不管什么故障原因，故障现象相同，对功能点的影响就相同。那为何这里还要单独将故障原因列出来呢？主要原因有这几个：</p><ul><li>不同的故障原因发生概率不相同</li></ul><p>例如，导致MySQL查询响应慢的原因可能是MySQL bug，也可能是没有索引。很明显“MySQL bug”的概率要远远低于“没有索引”；而不同的概率又会影响我们具体如何应对这个故障。</p><ul><li>不同的故障原因检测手段不一样</li></ul><p>例如，磁盘坏道导致MySQL响应慢，那我们需要增加机器的磁盘坏道检查，这个检查很可能不是当前系统本身去做，而是另外运维专门的系统；如果是慢查询导致MySQL慢，那我们只需要配置MySQL的慢查询日志即可。</p><ul><li>不同的故障原因的处理措施不一样</li></ul><p>例如，如果是MySQL bug，我们的应对措施只能是升级MySQL版本；如果是没有索引，我们的应对措施就是增加索引。</p><ol start="6"><li><strong>故障概率</strong></li></ol><p>这里的概率就是指某个具体故障原因发生的概率。例如，磁盘坏道的概率、MySQL bug的概率、没有索引的概率。一般分为“高&#x2F;中&#x2F;低”三档即可，具体评估的时候需要有以下几点需要重点关注。</p><ul><li>硬件</li></ul><p>硬件随着使用时间推移，故障概率会越来越高。例如，新的硬盘坏道几率很低，但使用了3年的硬盘，坏道几率就会高很多。</p><ul><li>开源系统</li></ul><p>成熟的开源系统bug率低，刚发布的开源系统bug率相比会高一些；自己已经有使用经验的开源系统bug率会低，刚开始尝试使用的开源系统bug率会高。</p><ul><li>自研系统</li></ul><p>和开源系统类似，成熟的自研系统故障概率会低，而新开发的系统故障概率会高。</p><p>高中低是相对的，只是为了确定优先级以决定后续的资源投入，没有必要绝对量化，因为绝对量化是需要成本的，而且很多时候都没法量化。例如，XX开源系统是3个月故障一次，还是6个月才故障一次，是无法评估的。</p><ol start="7"><li><strong>风险程度</strong></li></ol><p>风险程度就是综合严重程度和故障概率来一起判断某个故障的最终等级，风险程度 &#x3D; 严重程度 × 故障概率。因此可能出现某个故障影响非常严重，但其概率很低，最终来看风险程度就低。“某个机房业务瘫痪”对业务影响是致命的，但如果故障原因是“地震”，那概率就很低。例如，广州的地震概率就很低，5级以上地震的20世纪才1次（1940年）；如果故障的原因是“机房空调烧坏”，则概率就比地震高很多了，可能是2年1次；如果故障的原因是“系统所在机架掉电”，这个概率比机房空调又要高了，可能是1年1次。同样的故障影响，不同的故障原因有不同的概率，最终得到的风险级别就是不同的。</p><ol start="8"><li><strong>已有措施</strong></li></ol><p>针对具体的故障原因，系统现在是否提供了某些措施来应对，包括：检测告警、容错、自恢复等。</p><ul><li>检测告警</li></ul><p>最简单的措施就是检测故障，然后告警，系统自己不针对故障进行处理，需要人工干预。</p><ul><li>容错</li></ul><p>检测到故障后，系统能够通过备份手段应对。例如，MySQL主备机，当业务服务器检测到主机无法连接后，自动连接备机读取数据。</p><ul><li>自恢复</li></ul><p>检测到故障后，系统能够自己恢复。例如，Hadoop检测到某台机器故障后，能够将存储在这台机器的副本重新分配到其他机器。当然，这里的恢复主要还是指“业务”上的恢复，一般不太可能将真正的故障恢复。例如，Hadoop不可能将产生了磁盘坏道的磁盘修复成没有坏道的磁盘。</p><ol start="9"><li><strong>规避措施</strong></li></ol><p>规避措施指为了降低故障发生概率而做的一些事情，可以是技术手段，也可以是管理手段。例如：</p><ul><li><p>技术手段：为了避免新引入的MongoDB丢失数据，在MySQL中冗余一份。</p></li><li><p>管理手段：为了降低磁盘坏道的概率，强制统一更换服务时间超过2年的磁盘。</p></li></ul><ol start="10"><li><strong>解决措施</strong></li></ol><p>解决措施指为了能够解决问题而做的一些事情，一般都是技术手段。例如：</p><ul><li><p>为了解决密码暴力破解，增加密码重试次数限制。</p></li><li><p>为了解决拖库导致数据泄露，将数据库中的敏感数据加密保存。</p></li><li><p>为了解决非法访问，增加白名单控制。</p></li></ul><p>一般来说，如果某个故障既可以采取规避措施，又可以采取解决措施，那么我们会优先选择解决措施，毕竟能解决问题当然是最好的。但很多时候有些问题是系统自己无法解决的，例如磁盘坏道、开源系统bug，这类故障只能采取规避措施；系统能够自己解决的故障，大部分是和系统本身功能相关的。</p><ol start="11"><li><strong>后续规划</strong></li></ol><p>综合前面的分析，就可以看出哪些故障我们目前还缺乏对应的措施，哪些已有措施还不够，针对这些不足的地方，再结合风险程度进行排序，给出后续的改进规划。这些规划既可以是技术手段，也可以是管理手段；可以是规避措施，也可以是解决措施。同时需要考虑资源的投入情况，优先将风险程度高的系统隐患解决。</p><p>例如：</p><ul><li><p>地震导致机房业务中断：这个故障模式就无法解决，只能通过备份中心规避，尽量减少影响；而机柜断电导致机房业务中断：可以通过将业务机器分散在不同机柜来规避。</p></li><li><p>敏感数据泄露：这个故障模式可以通过数据库加密的技术手段来解决。</p></li><li><p>MongoDB断电丢数据：这个故障模式可以通过将数据冗余一份在MySQL中，在故障情况下重建数据来规避影响。</p></li></ul><h2 id="FMEA实战"><a href="#FMEA实战" class="headerlink" title="FMEA实战"></a>FMEA实战</h2><p>下面我以一个简单的样例来模拟一次FMEA分析。假设我们设计一个最简单的用户管理系统，包含登录和注册两个功能，其初始架构是：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f2e22565b6b60cd3cdce52fcc3711b5a.jpg"></p><p>初始架构很简单：MySQL负责存储，Memcache（以下简称MC）负责缓存，Server负责业务处理。我们来看看这个架构通过FMEA分析后，能够有什么样的发现，下表是分析的样例（注意，这个样例并不完整，感兴趣的同学可以自行尝试将这个案例补充完整）。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/583fd4cc24680d407e248a3e15fb138d.jpg"></p><p>经过上表的FMEA分析，将“后续规划”列的内容汇总一下，我们最终得到了下面几条需要改进的措施：</p><ul><li><p>MySQL增加备机。</p></li><li><p>MC从单机扩展为集群。</p></li><li><p>MySQL双网卡连接。</p></li></ul><p>改进后的架构如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/4250e8b6eb07023e2b55fa6fdbayyeab.png"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了FMEA高可用分析方法，并且给出了一个简单的案例描述如何操作。FMEA是高可用架构设计的一个非常有用的方法，能够发现架构中隐藏的高可用问题，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧，请使用FMEA方法分析一下HDFS系统的架构，看看HDFS是如何应对各种故障的，并且分析一下HDFS是否存在高可用问题。</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/9391" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;24-FMEA方法，排除架构可用性隐患的利器&quot;&gt;&lt;a href=&quot;#24-FMEA方法，排除架构可用性隐患的利器&quot; class=&quot;headerlink&quot; title=&quot;24 | FMEA方法，排除架构可用性隐患的利器&quot;&gt;&lt;/a&gt;24 | FMEA方法，排除架构可用</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>25 | 高可用存储架构：双机架构</title>
    <link href="https://zhuansun.github.io/geekbang/posts/341580218.html"/>
    <id>https://zhuansun.github.io/geekbang/posts/341580218.html</id>
    <published>2023-12-13T23:07:14.000Z</published>
    <updated>2023-12-15T14:43:23.913Z</updated>
    
    <content type="html"><![CDATA[<h1 id="25-高可用存储架构：双机架构"><a href="#25-高可用存储架构：双机架构" class="headerlink" title="25 | 高可用存储架构：双机架构"></a>25 | 高可用存储架构：双机架构</h1><p>存储高可用方案的本质都是通过将数据复制到多个存储设备，通过数据冗余的方式来实现高可用，其复杂性主要体现在如何应对复制延迟和中断导致的数据不一致问题。因此，对任何一个高可用存储方案，我们需要从以下几个方面去进行思考和分析：</p><ul><li><p>数据如何复制？</p></li><li><p>各个节点的职责是什么？</p></li><li><p>如何应对复制延迟？</p></li><li><p>如何应对复制中断？</p></li></ul><p>常见的高可用存储架构有主备、主从、主主、集群、分区，每一种又可以根据业务的需求进行一些特殊的定制化功能，由此衍生出更多的变种。由于不同业务的定制功能难以通用化，今天我将针对业界通用的方案，来分析常见的双机高可用架构：主备、主从、主备&#x2F;主从切换和主主。</p><h2 id="主备复制"><a href="#主备复制" class="headerlink" title="主备复制"></a>主备复制</h2><p>主备复制是最常见也是最简单的一种存储高可用方案，几乎所有的存储系统都提供了主备复制的功能，例如MySQL、Redis、MongoDB等。</p><p>1.基本实现</p><p>下面是标准的主备方案结构图：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/24646c3e5db8533186c9yy5ae2f870d2.jpg"></p><p>其整体架构比较简单，主备架构中的“备机”主要还是起到一个备份作用，并不承担实际的业务读写操作，如果要把备机改为主机，需要人工操作。</p><p>2.优缺点分析</p><p>主备复制架构的优点就是简单，表现有：</p><ul><li><p>对于客户端来说，不需要感知备机的存在，即使灾难恢复后，原来的备机被人工修改为主机后，对于客户端来说，只是认为主机的地址换了而已，无须知道是原来的备机升级为主机。</p></li><li><p>对于主机和备机来说，双方只需要进行数据复制即可，无须进行状态判断和主备切换这类复杂的操作。</p></li></ul><p>主备复制架构的缺点主要有：</p><ul><li><p>备机仅仅只为备份，并没有提供读写操作，硬件成本上有浪费。</p></li><li><p>故障后需要人工干预，无法自动恢复。人工处理的效率是很低的，可能打电话找到能够操作的人就耗费了10分钟，甚至如果是深更半夜，出了故障都没人知道。人工在执行恢复操作的过程中也容易出错，因为这类操作并不常见，可能1年就2、3次，实际操作的时候很可能遇到各种意想不到的问题。</p></li></ul><p>综合主备复制架构的优缺点，内部的后台管理系统使用主备复制架构的情况会比较多，例如学生管理系统、员工管理系统、假期管理系统等，因为这类系统的数据变更频率低，即使在某些场景下丢失数据，也可以通过人工的方式补全。</p><h2 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a>主从复制</h2><p>主从复制和主备复制只有一字之差，“从”意思是“随从、仆从”，“备”的意思是备份。我们可以理解为仆从是要帮主人干活的，这里的干活就是承担“读”的操作。也就是说，主机负责读写操作，从机只负责读操作，不负责写操作。</p><p>1.基本实现</p><p>下面是标准的主从复制架构：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/03715e39bde946f363643a70d4cb02b7.jpg"></p><p>与主备复制架构比较类似，主要的差别点在于从机正常情况下也是要提供读的操作。</p><p>2.优缺点分析</p><p>主从复制与主备复制相比，优点有：</p><ul><li><p>主从复制在主机故障时，读操作相关的业务可以继续运行。</p></li><li><p>主从复制架构的从机提供读操作，发挥了硬件的性能。</p></li></ul><p>缺点有：</p><ul><li><p>主从复制架构中，客户端需要感知主从关系，并将不同的操作发给不同的机器进行处理，复杂度比主备复制要高。</p></li><li><p>主从复制架构中，从机提供读业务，如果主从复制延迟比较大，业务会因为数据不一致出现问题。</p></li><li><p>故障时需要人工干预。</p></li></ul><p>综合主从复制的优缺点，一般情况下，写少读多的业务使用主从复制的存储架构比较多。例如，论坛、BBS、新闻网站这类业务，此类业务的读操作数量是写操作数量的10倍甚至100倍以上。</p><h2 id="双机切换"><a href="#双机切换" class="headerlink" title="双机切换"></a>双机切换</h2><p>1.设计关键</p><p>主备复制和主从复制方案存在两个共性的问题：</p><ul><li><p>主机故障后，无法进行写操作。</p></li><li><p>如果主机无法恢复，需要人工指定新的主机角色。</p></li></ul><p>双机切换就是为了解决这两个问题而产生的，包括主备切换和主从切换两种方案。简单来说，这两个方案就是在原有方案的基础上增加“切换”功能，即系统自动决定主机角色，并完成角色切换。由于主备切换和主从切换在切换的设计上没有差别，我接下来以主备切换为例，一起来看看双机切换架构是如何实现的。</p><p>要实现一个完善的切换方案，必须考虑这几个关键的设计点：</p><ul><li>主备间状态判断</li></ul><p>主要包括两方面：状态传递的渠道，以及状态检测的内容。</p><p><strong>状态传递的渠道</strong>：是相互间互相连接，还是第三方仲裁？</p><p><strong>状态检测的内容</strong>：例如机器是否掉电、进程是否存在、响应是否缓慢等。</p><ul><li>切换决策</li></ul><p>主要包括几方面：切换时机、切换策略、自动程度。</p><p><strong>切换时机</strong>：什么情况下备机应该升级为主机？是机器掉电后备机才升级，还是主机上的进程不存在就升级，还是主机响应时间超过2秒就升级，还是3分钟内主机连续重启3次就升级等。</p><p><strong>切换策略</strong>：原来的主机故障恢复后，要再次切换，确保原来的主机继续做主机，还是原来的主机故障恢复后自动成为新的备机？</p><p><strong>自动程度</strong>：切换是完全自动的，还是半自动的？例如，系统判断当前需要切换，但需要人工做最终的确认操作（例如，单击一下“切换”按钮）。</p><ul><li>数据冲突解决</li></ul><p>当原有故障的主机恢复后，新旧主机之间可能存在数据冲突。例如，用户在旧主机上新增了一条ID为100的数据，这个数据还没有复制到旧的备机，此时发生了切换，旧的备机升级为新的主机，用户又在新的主机上新增了一条ID为100的数据，当旧的故障主机恢复后，这两条ID都为100的数据，应该怎么处理？</p><p>以上设计点并没有放之四海而皆准的答案，不同的业务要求不一样，所以切换方案比复制方案不只是多了一个切换功能那么简单，而是复杂度上升了一个量级。形象点来说，如果复制方案的代码是1000行，那么切换方案的代码可能就是10000行，多出来的那9000行就是用于实现上面我所讲的3个设计点的。</p><p>2.常见架构</p><p>根据状态传递渠道的不同，常见的主备切换架构有三种形式：互连式、中介式和模拟式。</p><p><strong>互连式</strong></p><p>故名思议，互连式就是指主备机直接建立状态传递的渠道，架构图请注意与主备复制架构对比。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d5bed3e75be97013154e73003c151fb2.jpg"></p><p>你可以看到，在主备复制的架构基础上，主机和备机多了一个“状态传递”的通道，这个通道就是用来传递状态信息的。这个通道的具体实现可以有很多方式：</p><ul><li><p>可以是网络连接（例如，各开一个端口），也可以是非网络连接（用串口线连接）。</p></li><li><p>可以是主机发送状态给备机，也可以是备机到主机来获取状态信息。</p></li><li><p>可以和数据复制通道共用，也可以独立一条通道。</p></li><li><p>状态传递通道可以是一条，也可以是多条，还可以是不同类型的通道混合（例如，网络+串口）。</p></li></ul><p>为了充分利用切换方案能够自动决定主机这个优势，客户端这里也会有一些相应的改变，常见的方式有：</p><ul><li><p>为了切换后不影响客户端的访问，主机和备机之间共享一个对客户端来说唯一的地址。例如虚拟IP，主机需要绑定这个虚拟的IP。</p></li><li><p>客户端同时记录主备机的地址，哪个能访问就访问哪个；备机虽然能收到客户端的操作请求，但是会直接拒绝，拒绝的原因就是“备机不对外提供服务”。</p></li></ul><p>互连式主备切换主要的缺点在于：</p><ul><li><p>如果状态传递的通道本身有故障（例如，网线被人不小心踢掉了），那么备机也会认为主机故障了从而将自己升级为主机，而此时主机并没有故障，最终就可能出现两个主机。</p></li><li><p>虽然可以通过增加多个通道来增强状态传递的可靠性，但这样做只是降低了通道故障概率而已，不能从根本上解决这个缺点，而且通道越多，后续的状态决策会更加复杂，因为对备机来说，可能从不同的通道收到了不同甚至矛盾的状态信息。</p></li></ul><p><strong>中介式</strong></p><p>中介式指的是在主备两者之外引入第三方中介，主备机之间不直接连接，而都去连接中介，并且通过中介来传递状态信息，其架构图如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/f69d33c8d7d3d85e510d8eb54d5dd065.jpg"></p><p>对比一下互连式切换架构，我们可以看到，主机和备机不再通过互联通道传递状态信息，而是都将状态上报给中介这一角色。单纯从架构上看，中介式似乎比互连式更加复杂了，首先要引入中介，然后要各自上报状态。然而事实上，中介式架构在状态传递和决策上却更加简单了，这是为何呢？</p><p><strong>连接管理更简单</strong>：主备机无须再建立和管理多种类型的状态传递连接通道，只要连接到中介即可，实际上是降低了主备机的连接管理复杂度。</p><p>例如，互连式要求主机开一个监听端口，备机来获取状态信息；或者要求备机开一个监听端口，主机推送状态信息到备机；如果还采用了串口连接，则需要增加串口连接管理和数据读取。采用中介式后，主备机都只需要把状态信息发送给中介，或者从中介获取对方的状态信息。无论是发送还是获取，主备机都是作为中介的客户端去操作，复杂度会降低。</p><p><strong>状态决策更简单</strong>：主备机的状态决策简单了，无须考虑多种类型的连接通道获取的状态信息如何决策的问题，只需要按照下面简单的算法即可完成状态决策。</p><ul><li><p>无论是主机还是备机，初始状态都是备机，并且只要与中介断开连接，就将自己降级为备机，因此可能出现双备机的情况。</p></li><li><p>主机与中介断连后，中介能够立刻告知备机，备机将自己升级为主机。</p></li><li><p>如果是网络中断导致主机与中介断连，主机自己会降级为备机，网络恢复后，旧的主机以新的备机身份向中介上报自己的状态。</p></li><li><p>如果是掉电重启或者进程重启，旧的主机初始状态为备机，与中介恢复连接后，发现已经有主机了，保持自己备机状态不变。</p></li><li><p>主备机与中介连接都正常的情况下，按照实际的状态决定是否进行切换。例如，主机响应时间超过3秒就进行切换，主机降级为备机，备机升级为主机即可。</p></li></ul><p>虽然中介式架构在状态传递和状态决策上更加简单，但并不意味着这种优点是没有代价的，其关键代价就在于如何实现中介本身的高可用。如果中介自己宕机了，整个系统就进入了双备的状态，写操作相关的业务就不可用了。这就陷入了一个递归的陷阱：为了实现高可用，我们引入中介，但中介本身又要求高可用，于是又要设计中介的高可用方案……如此递归下去就无穷无尽了。</p><p>MongoDB的Replica Set采取的就是这种方式，其基本架构如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/1902b97ea2c6dda74dc0270f945725f9.jpg"></p><p>MongoDB(M)表示主节点，MongoDB(S)表示备节点，MongoDB(A)表示仲裁节点。主备节点存储数据，仲裁节点不存储数据。客户端同时连接主节点与备节点，不连接仲裁节点。</p><p>幸运的是，开源方案已经有比较成熟的中介式解决方案，例如ZooKeeper和Keepalived。ZooKeeper本身已经实现了高可用集群架构，因此已经帮我们解决了中介本身的可靠性问题，在工程实践中推荐基于ZooKeeper搭建中介式切换架构。</p><p><strong>模拟式</strong></p><p>模拟式指主备机之间并不传递任何状态数据，而是备机模拟成一个客户端，向主机发起模拟的读写操作，根据读写操作的响应情况来判断主机的状态。其基本架构如下：</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/fbd04e44f3ddab37b40a1e459af1af57.jpg"></p><p>对比一下互连式切换架构，我们可以看到，主备机之间只有数据复制通道，而没有状态传递通道，备机通过模拟的读写操作来探测主机的状态，然后根据读写操作的响应情况来进行状态决策。</p><p>模拟式切换与互连式切换相比，优点是实现更加简单，因为省去了状态传递通道的建立和管理工作。</p><p>简单既是优点，同时也是缺点。因为模拟式读写操作获取的状态信息只有响应信息（例如，HTTP 404，超时、响应时间超过3秒等），没有互连式那样多样（除了响应信息，还可以包含CPU负载、I&#x2F;O负载、吞吐量、响应时间等），基于有限的状态来做状态决策，可能出现偏差。</p><h2 id="主主复制"><a href="#主主复制" class="headerlink" title="主主复制"></a>主主复制</h2><p>主主复制指的是两台机器都是主机，互相将数据复制给对方，客户端可以任意挑选其中一台机器进行读写操作，下面是基本架构图。</p><p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/555c5714ec983ca5506191156430df22.jpg"></p><p>相比主备切换架构，主主复制架构具有如下特点：</p><ul><li><p>两台都是主机，不存在切换的概念。</p></li><li><p>客户端无须区分不同角色的主机，随便将读写操作发送给哪台主机都可以。</p></li></ul><p>从上面的描述来看，主主复制架构从总体上来看要简单很多，无须状态信息传递，也无须状态决策和状态切换。然而事实上主主复制架构也并不简单，而是有其独特的复杂性，具体表现在：如果采取主主复制架构，必须保证数据能够双向复制，而很多数据是不能双向复制的。例如：</p><ul><li><p>用户注册后生成的用户ID，如果按照数字增长，那就不能双向复制，否则就会出现X用户在主机A注册，分配的用户ID是100，同时Y用户在主机B注册，分配的用户ID也是100，这就出现了冲突。</p></li><li><p>库存不能双向复制。例如，一件商品库存100件，主机A上减了1件变成99，主机B上减了2件变成98，然后主机A将库存99复制到主机B，主机B原有的库存98被覆盖，变成了99，而实际上此时真正的库存是97。类似的还有余额数据。</p></li></ul><p>因此，主主复制架构对数据的设计有严格的要求，一般适合于那些临时性、可丢失、可覆盖的数据场景。例如，用户登录产生的session数据（可以重新登录生成）、用户行为的日志数据（可以丢失）、论坛的草稿数据（可以丢失）等。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天我为你讲了高可用存储架构中常见的双机架构，分析了每类架构的优缺点以及适应场景，希望对你有所帮助。</p><p>这就是今天的全部内容，留一道思考题给你吧，如果你来设计一个政府信息公开网站的信息存储系统，你会采取哪种架构？谈谈你的分析和理由。</p><p>欢迎你把答案写到留言区，和我一起讨论。相信经过深度思考的回答，也会让你对知识的理解更加深刻。（编辑乱入：精彩的留言有机会获得丰厚福利哦！）</p><hr><iframe width="100%" frameborder=1 height=500px src="https://time.geekbang.org/comment/nice/9399" > </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;25-高可用存储架构：双机架构&quot;&gt;&lt;a href=&quot;#25-高可用存储架构：双机架构&quot; class=&quot;headerlink&quot; title=&quot;25 | 高可用存储架构：双机架构&quot;&gt;&lt;/a&gt;25 | 高可用存储架构：双机架构&lt;/h1&gt;&lt;p&gt;存储高可用方案的本质都是通过</summary>
      
    
    
    
    <category term="从0开始学架构" scheme="https://zhuansun.github.io/geekbang/categories/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%9E%B6%E6%9E%84/"/>
    
    
  </entry>
  
</feed>
