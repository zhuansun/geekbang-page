<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>10｜CLIP：让AI绘画模型乖乖听你的话 | geekbang</title><meta name="author" content="码农张三"><meta name="copyright" content="码农张三"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="10｜CLIP：让AI绘画模型乖乖听你的话你好，我是南柯。 今天我们要学习的是OpenAI在2021年提出的CLIP算法。在AI绘画的过程中，CLIP的作用是理解我们给到模型的prompt指令，将prompt指令编码为模型能理解的“语言”。 但你可能不知道，最早提出CLIP模型并不是帮助AI绘画模型理解prompt指令，而是用于连接图像和文本这两种模态。如今，随着AIGC技术的大爆发，CLIP模型">
<meta property="og:type" content="article">
<meta property="og:title" content="10｜CLIP：让AI绘画模型乖乖听你的话">
<meta property="og:url" content="https://zhuansun.github.io/geekbang/posts/3769512864.html">
<meta property="og:site_name" content="geekbang">
<meta property="og:description" content="10｜CLIP：让AI绘画模型乖乖听你的话你好，我是南柯。 今天我们要学习的是OpenAI在2021年提出的CLIP算法。在AI绘画的过程中，CLIP的作用是理解我们给到模型的prompt指令，将prompt指令编码为模型能理解的“语言”。 但你可能不知道，最早提出CLIP模型并不是帮助AI绘画模型理解prompt指令，而是用于连接图像和文本这两种模态。如今，随着AIGC技术的大爆发，CLIP模型">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg">
<meta property="article:published_time" content="2024-02-29T12:01:07.000Z">
<meta property="article:modified_time" content="2024-03-21T11:14:53.273Z">
<meta property="article:author" content="码农张三">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhuansun.github.io/geekbang/posts/3769512864"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '10｜CLIP：让AI绘画模型乖乖听你的话',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-21 11:14:53'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="geekbang" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic.imgdb.cn/item/653470a0c458853aef5813f1.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1342</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">geekbang</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">10｜CLIP：让AI绘画模型乖乖听你的话</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2024-02-29T12:01:07.000Z" title="发表于 2024-02-29 12:01:07">2024-02-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI%E7%BB%98%E7%94%BB%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98/">AI绘画核心技术与实战</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="10｜CLIP：让AI绘画模型乖乖听你的话"><a href="#10｜CLIP：让AI绘画模型乖乖听你的话" class="headerlink" title="10｜CLIP：让AI绘画模型乖乖听你的话"></a>10｜CLIP：让AI绘画模型乖乖听你的话</h1><p>你好，我是南柯。</p>
<p>今天我们要学习的是OpenAI在2021年提出的CLIP算法。在AI绘画的过程中，CLIP的作用是理解我们给到模型的prompt指令，将prompt指令编码为模型能理解的“语言”。</p>
<p>但你可能不知道，最早提出CLIP模型并不是帮助AI绘画模型理解prompt指令，而是用于连接图像和文本这两种模态。如今，随着AIGC技术的大爆发，CLIP模型又在AI绘画、多模态大模型等方向发挥了巨大价值。</p>
<p>这一讲，我们就一起来搞清楚CLIP这个算法背后的奥秘！只有真正理解了CLIP，你才能知道为什么prompt可以控制AI绘画生成的内容。在设计你自己的AI绘画模型的时候，便可以根据你的需求选择各种CLIP模型或者其变体模型，得到更好的绘画效果。</p>
<h2 id="追本溯源：CLIP的提出背景"><a href="#追本溯源：CLIP的提出背景" class="headerlink" title="追本溯源：CLIP的提出背景"></a>追本溯源：CLIP的提出背景</h2><p>在学习CLIP之前，我们有必要先理解模态（Modality）的概念。在深度学习领域，模态可以用于描述输入数据的不同形式，比如图像、文本、音频等。不同的模态可以提供不同的特征，使模型能够从更多的角度理解和处理数据。在实践中，通过整合多种模态的信息，通常能够帮助模型获得更好的性能。</p>
<p>我们常说的NLP（Natural Language Processing），即自然语言处理，解决的就是文本模态的任务，比如文本问答、文本对话、文本情绪分析等任务。</p>
<p>和它并列的是CV（Computer Vision），即计算机视觉，解决的是图像模态的任务，当然广义上也包括视频、红外图像等信息输入。CV算法解决的问题包括图像分类、图像目标检测、图像生成等。音频处理算法，比如语音识别、语音合成、声音情感分析等能力解决的便是音频模态的任务。</p>
<p>我们最近常常讨论的AI绘画和ChatGPT，就分别是CV领域和NLP领域的明星技术。而GPT-4，则是同时使用了文本模态和图像模态两种输入信息。了解了模态概念，我们再来梳理一下CLIP是在怎样的背景下被提出的。</p>
<p>在NLP领域，早在2020年，OpenAI便已经发布了GPT-3这个技术，证明了使用海量互联网数据得到的预训练模型可以用于各种文本类任务，比如文本分类、机器翻译、文本情感分析、文本问答等，GPT-3的工作直接衍生出后来大火的ChatGPT。</p>
<p>那时在CV领域里，最常见的模式还是使用各种各样既定任务的数据集，通过标注员的标注获得训练样本，针对特定任务来训练。比如我们熟知的图像分类数据集ImageNet，就包括1000个类别和超过100万图像样本。</p>
<p>CV任务千千万，便催生了各式各样的数据集，比如图像分类、目标检测、图像分割、图像生成等。不过，在每个训练集上得到的模型通常只能完成特定的任务，无法在其他任务上推广。</p>
<p>我们来总结下，CLIP被提出之前主要有这样两个痛点。</p>
<p>第一，CV数据集标注是个劳动密集型任务，标注成本高昂。</p>
<p>第二，每个CV模型通常只能胜任一个任务，无法轻易迁移到新的任务。</p>
<h2 id="CLIP解决方案"><a href="#CLIP解决方案" class="headerlink" title="CLIP解决方案"></a>CLIP解决方案</h2><p>能否将GPT-3的经验迁移到图像领域，使用海量互联网数据做一个大一统的模型，同时能够很好地支持各种图像任务，比如图像分类、文字识别、视频理解等等？这就是CLIP工作的初衷！</p>
<p>要达成这个目的，有两个关键点，一是怎么利用海量的互联网数据，二是如何训练这样一个模型。</p>
<h3 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h3><p>首先，为了解决数据的问题，OpenAI选定了50万条不同的查询请求，从互联网上获取到4亿图像-文本对，来源包括Google这类通用搜索引擎和Twitter这类垂直领域社区。</p>
<p>这些数据不需要人工标注，比如我们在任意搜索引擎搜索图像，这些图像都会自带文本描述。下面的图中我展示了在搜索引擎中搜索柯基犬的结果截图。可以看到，图像自带的文本描述与图像具有较强的语义一致性，说白了就是图文对应。这种关联信息就是用于训练的监督信号！</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/699e9bf506be36a92e30e5a344df7643.png"></p>
<p>也就是说，互联网上天然就存在已经标注好的CV数据集，而且每天还在飞速新增。此外，使用互联网数据的另一个优势是它的数据非常多样，包含各种各样的图像内容，因此训练得到的模型自然就可以迁移到各种各样的场景。</p>
<p>放在当时，4亿图文对是个很大的数据量，但技术发展到今天，用于训练各种多模态模型的数据早已突破10亿大关。比如人们常说的LAION 5B数据集，包括50亿图文对。</p>
<h3 id="监督信号"><a href="#监督信号" class="headerlink" title="监督信号"></a>监督信号</h3><p>好，现在我们拥有了海量图文数据，文本信息成为我们要用的“监督信号”，还得解决第二个问题——这些数据如何用于模型训练呢？</p>
<p>CLIP通过巧妙的设计利用了图像模态和文本模态的对应关系。CLIP分别构造了一个图像编码器和一个文本编码器，将图像及其文本描述映射到一个特征空间，比如我们可以映射到 512 维度的特征空间。简言之，一张图或者一个文本描述，经过映射都是 512 个浮点数。</p>
<p>那么此时，我们需要设计一个监督信号，利用图文成对的关系，驱动两个编码器模型学习到有效的特征提取能力。该怎么做呢？</p>
<p>答案是对比学习。具体思路是这样的。我们可以计算图像特征向量和文本特征向量之间的余弦距离，余弦距离的范围是-1 到 1，越大表示距离越接近。CLIP 的训练目标是让对应的图像、文本得到的特征向量靠近，也就是余弦距离越大越好，让不对应的图像、文本得到的特征向量远离，也就是余弦距离尽可能小。</p>
<p>你可以结合后面的图片来理解这个过程。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/8eb3da98237df983fe6a70d1aed1c683.png"></p>
<p>下面这张图是这个过程的伪代码，可以帮你进一步强化理解。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># image_encoder - 图像编码器可以使用ResNet或者Vision Transformer结构</span>
<span class="token comment"># text_encoder - 文本编码器可以使用CBOW或者Text Transformer结构</span>
<span class="token comment"># I[n, h, w, c] - 一个训练批次的图像</span>
<span class="token comment"># T[n, l] - 一个训练批次的对应文本图像</span>
<span class="token comment"># W_i[d_i, d_e] - 可学习的图像特征投影层权重</span>
<span class="token comment"># W_t[d_t, d_e] - 可学习的文本特征投影层权重</span>
<span class="token comment"># t - 一个可学习的温度系数</span>

<span class="token comment"># 第一步，提取图像和文本模态的表征</span>
I_f <span class="token operator">=</span> image_encoder<span class="token punctuation">(</span>I<span class="token punctuation">)</span> <span class="token comment">#[n, d_i]</span>
T_f <span class="token operator">=</span> text_encoder<span class="token punctuation">(</span>T<span class="token punctuation">)</span> <span class="token comment">#[n, d_t]</span>

<span class="token comment"># 图像表征和文本表征分别映射到共同的多模态空间 [n, d_e]</span>
I_e <span class="token operator">=</span> l2_normalize<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>I_f<span class="token punctuation">,</span> W_i<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
T_e <span class="token operator">=</span> l2_normalize<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>T_f<span class="token punctuation">,</span> W_t<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 计算余弦相似度 [n, n]</span>
logits <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>I_e<span class="token punctuation">,</span> T_e<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>t<span class="token punctuation">)</span>

<span class="token comment"># 计算损失值</span>
labels <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>n<span class="token punctuation">)</span>
loss_i <span class="token operator">=</span> cross_entropy_loss<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
loss_t <span class="token operator">=</span> cross_entropy_loss<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> <span class="token punctuation">(</span>loss_i <span class="token operator">+</span> loss_t<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到，训练过程分为四步：提取表征、映射和归一化、计算距离和更新模型权重。</p>
<p>首先，使用图像编码器提取图像表征I_f，使用文本编码器提取文本表征T_f。</p>
<p>然后，分别引入一个线性投影（Linear Projection），将图像表征和文本表征分别映射到共同的多模态空间。这里线性投影的参数对应伪代码中的W_i和W_t，然后将投影后的特征向量分别进行归一化，归一化后的表征平方和等于1。</p>
<p>之后，我们要计算这批图文归一化表征两两之间的距离，距离范围是-1到1，然后再乘以一个温度系数相关的数值项（伪代码第18行），这里的温度系数是一个可学习的参数。</p>
<p>最后，通过交叉熵损失函数进行模型监督，匹配的图文对距离拉近、不匹配的图文对距离拉远。</p>
<p>你可以结合后面的动态图来理解CLIP完整的训练过程。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/17befe1b7beaea9a5b0fbc08de6cb8bc.gif"></p>
<h2 id="CLIP进阶探索"><a href="#CLIP进阶探索" class="headerlink" title="CLIP进阶探索"></a>CLIP进阶探索</h2><p>了解了CLIP的海量数据获取和对比学习的训练方式，相信你一定也会感叹这个算法设计的巧妙。</p>
<p>那么，CLIP都有哪些应用呢？在众多的CLIP模型的版本里，该如何选择合适的模型呢？我们这就来探讨这些问题。</p>
<h3 id="CLIP应用"><a href="#CLIP应用" class="headerlink" title="CLIP应用"></a>CLIP应用</h3><p>在4亿图文数据上完成训练后，我们便得到了一个图像编码器和一个文本编码器。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/8eb3da98237df983fe6a70d1aed1c683.png"></p>
<p>回顾下CLIP设计的初衷，是为了解决CV模型的效果问题。所以，CLIP工作在最开始主要是将训练得到的图像编码器用于处理各种图像下游任务。之后，在AI绘画过程中，CLIP文本编码器用于理解我们给出的prompt。</p>
<p>图像中最常见的下游任务便是图像分类任务。经典的图像分类任务通常需要使用人工标注的标签数据来训练，训练完成后只能区分训练时限定的类别。由于CLIP见过4亿图文，拥有海量的知识，我们便可以直接通过跨模态检索的方式直接进行分类。具体来说，我们可以设计后面这样的文本。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">A photo of a <span class="token operator">&lt;</span><span class="token keyword">class</span><span class="token operator">></span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>这里的class可以是ImageNet的1000个既定类别，也可以是你自己设计的目标类别。以ImageNet的1000类别为例，我们复用上面模板，得到1000个不同的prompt。这1000个prompt经过预训练得到的文本编码器之后，便得到1000个文本表征；对于输入图像，我们经过预训练的图像编码器可以得到1个图像表征。</p>
<p>将图像表征和1000个文本表征线性投影、归一化之后计算余弦距离，余弦距离最大的prompt对应的类别便是CLIP模型预测的类别。</p>
<p>细心的你可能已经发现了，这并不是经典的分类方案，更像是检索的方案。是的，这种方法我们可以称之为 <strong>跨模态检索</strong>。检索方案的扩展性要强于分类方案，比如上面这个任务的候选类别，你可以随意设计。</p>
<p>再举个人脸识别的例子，如果我们把人脸识别当做是分类任务，那么每次系统中录入一个新人，都需要将分类类别数加一，然后重新训练模型；如果我们将人脸识别建模为检索任务，我们只需要像CLIP这样，对每个人脸提一个特征，然后通过检索的方式进行身份定位即可。这样就不需要重新训练模型了。</p>
<p>关于CLIP通过检索为图像分类的整体过程，你可以参考后面这张图来加强理解。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/fb0e612e647408896fcf51b1d701f434.png"></p>
<p>对于我们AI绘画这门课，CLIP模型便是让AI绘画模型听我们话的关键！在CLIP的训练过程中，图像表征和文本表征被线性投影到共同的多模态空间，在文本生图的过程中，prompt信息便可以通过CLIP抽取特征，然后指导模型作画。</p>
<p>以Stable Diffusion为例，我们前面已经了解了Stable Diffusion模型的UNet和Transformer结构设计，CLIP提取得到的文本表征经过交叉注意力（Cross Attention）的方式进行信息注入，便可以将我们的指令传递给模型。关于交叉注意力机制的技术原理，你可以回看 <a target="_blank" rel="noopener" href="https://time.geekbang.org/column/article/682762">第7讲</a>。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/c920eb4143b09f25a1468c87014d69d9.png"></p>
<p>后面的课程里，我们还会学习DALL-E 2模型，它在CLIP的基础上进一步扩展，提出unCLIP结构，不仅能够用文本指导图像生成，还能输入图像生成多个相似变体。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/91d92cb67e8fab75a3ca16b67eaed705.jpg"></p>
<h3 id="CLIP增强版"><a href="#CLIP增强版" class="headerlink" title="CLIP增强版"></a>CLIP增强版</h3><p>OpenAI只是开源了CLIP模型的权重，并没有开源对应的4亿图文对。后来的学者便开始复现OpenAI的工作。比较有代表性的工作包括OpenCLIP、ChineseCLIP和EVA-CLIP。</p>
<p>OpenCLIP基于LAION公司收集的 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.02114">4亿开源图文</a> 数据训练而成，相当于是对OpenAI的CLIP模型的复现。公开的LAION 5B数据集和开源的OpenCLIP代码库，打破了此前OpenAI的“数据垄断”。你可以点开 <a target="_blank" rel="noopener" href="https://github.com/mlfoundations/open_clip">这个链接</a> 了解更多细节。</p>
<p>ChineseCLIP的目标是用中文的图文数据完成CLIP训练，强化文本编码器的中文理解能力。比如说，ChineseCLIP模型可以帮助AI绘画模型更好地理解中文。ChineseCLIP使用大约2亿中文图文对数据进行训练，你可以点开 <a target="_blank" rel="noopener" href="https://github.com/OFA-Sys/Chinese-CLIP">这个链接</a> 了解更多细节。</p>
<p>EVA-CLIP是2023年3月由北京智源研究院提出的模型，通过提高训练效率和优化模型设计，取得了比传统CLIP更好的性能。感兴趣的同学可以点开 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.15389">这个链接</a> 了解更多细节。</p>
<h2 id="如何使用CLIP"><a href="#如何使用CLIP" class="headerlink" title="如何使用CLIP"></a>如何使用CLIP</h2><p>了解了CLIP的原理和各种应用场景，我们不妨来动手用CLIP完成一个图像分类任务。</p>
<p>你可以点开我的 <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson10/CLIP_demo.ipynb">Colab链接</a> 运行一下现成的案例，按照猫和狗的目标类别给一张图片做分类，点进去以后的样子如下图所示。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/7d8dcd80fb02a4c4fbb9d5de5db057e0.png"></p>
<p>点击播放按钮，就能生成后面这样的图片分类结果。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/8347yy7c59a795ab9dc69a66yyb3ce21.png"></p>
<p>当然，我更推荐你自己用科学上网的方法注册一个Google账号，熟悉一下Colab的用法。我们的课程后面的实战项目也都会在Colab上完成。</p>
<p>注册好Google账号以后，你可以访问文稿后面 <a target="_blank" rel="noopener" href="https://research.google.com/colaboratory/">这个链接</a>。进入后点击文件菜单下面的新建笔记本。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/8a3ddbc7b4158395da004167c86b663b.png"></p>
<p>之后，我们需要连接GPU环境。点击左上方的修改选项，选择笔记本设置。使用GPU作为硬件加速器，并完成一系列确认操作。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/8ab37015003fb13352c56e4500ca5a24.png"></p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/a673069ae17bb046383ca89ac6f54f8f.png"></p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/f625099d23f24de9c0106543bc1ded67.png"></p>
<p>我们可以通过下面这行指令，确认是否已经获取到GPU资源。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token operator">!</span>nvidia-smi
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>点击播放按钮，我们可以看到，我们“获得”了一张T4显卡，显存大小为15GB。这样一张显卡足够我们完成本课程中的各种运算。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/6856ba6ae126d37e6bc11a865df26bbe.png"></p>
<p>接着在新建好的笔记本页面，输入后面的指令安装OpenAI的CLIP工具包。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token operator">!</span>pip <span class="token function">install</span> git+https://github.com/openai/CLIP.git
<span class="token comment"># 参考教程：https://github.com/openai/CLIP</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/8c87d87189db605556bf12df4bc0b097.png"></p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/72d4a355cdfff0d562a26577ffc5cc58.png"></p>
<p>稍等片刻，出现后面截图里安装成功的字样，就表示成功安装CLIP。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/c99eab21762846f66c6dcbebce691c6e.png"></p>
<p>然后在Colab中粘贴后面的代码，并点击播放按钮运行，对image_url中的图片进行分类。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> clip
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image
<span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># Load the CLIP model</span>
device <span class="token operator">=</span> <span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span>
model<span class="token punctuation">,</span> preprocess <span class="token operator">=</span> clip<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"ViT-B/32"</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>

<span class="token comment"># Define the target classes</span>
target_classes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"cat"</span><span class="token punctuation">,</span> <span class="token string">"dog"</span><span class="token punctuation">]</span>

<span class="token comment"># Load and preprocess the image</span>

image_url <span class="token operator">=</span> <span class="token string">"https://lh4.googleusercontent.com/sL5lctmCOd0nCQcaHAm9bd1Gi3xsmQrgRP3HEgvWZwNcBPkBIAhbh4_e8torbhL_bLV3T80Sq_kj1GQ7kS7GhYR1E9ETPsYR5uaC1ZEqi0kuSRijl7mzF6TM9wAjRhiofuDwfxE4"</span>
image_path <span class="token operator">=</span> <span class="token string">"test_image.png"</span>
urllib<span class="token punctuation">.</span>request<span class="token punctuation">.</span>urlretrieve<span class="token punctuation">(</span>image_url<span class="token punctuation">,</span> image_path<span class="token punctuation">)</span>
image <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>image_path<span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">"RGB"</span><span class="token punctuation">)</span>
image_input <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

<span class="token comment"># Encode the image</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    image_features <span class="token operator">=</span> model<span class="token punctuation">.</span>encode_image<span class="token punctuation">(</span>image_input<span class="token punctuation">)</span>

<span class="token comment"># Encode the target classes</span>
text_inputs <span class="token operator">=</span> clip<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>target_classes<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    text_features <span class="token operator">=</span> model<span class="token punctuation">.</span>encode_text<span class="token punctuation">(</span>text_inputs<span class="token punctuation">)</span>

<span class="token comment"># Compute the similarity scores</span>
similarity_scores <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">100.0</span> <span class="token operator">*</span> image_features @ text_features<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># Get the predicted class</span>
_<span class="token punctuation">,</span> predicted_class <span class="token operator">=</span> similarity_scores<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
predicted_class <span class="token operator">=</span> predicted_class<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Print the predicted class</span>
predicted_label <span class="token operator">=</span> target_classes<span class="token punctuation">[</span>predicted_class<span class="token punctuation">]</span>

plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>image<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Predicted class: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>predicted_label<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"prob: cat </span><span class="token interpolation"><span class="token punctuation">&#123;</span>similarity_scores<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string">, dog </span><span class="token interpolation"><span class="token punctuation">&#123;</span>similarity_scores<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>这里代码16行的图片可以换成你想测试的图片。不过要注意，Google只能读url图片，如果想测试你的本地图片，需要先通过图床（比如 <a target="_blank" rel="noopener" href="https://imgse.com/">这个链接</a>）上传该图片，获得url链接再替换。</p>
<p>运行之后，得到的结果也是后面这张图。可以看到，分类成表格和猫的置信度为0，而分类成狗的概率为1。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/290b20acce2373522d833f3d254a3929.png"></p>
<p>很多AI绘画的文本理解任务也会用到OpenCLIP，所以这里我们也体验下OpenCLIP的效果。</p>
<p>首先需要安装对应python包。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token operator">!</span>pip <span class="token function">install</span> open_clip_torch
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>同样是上面的柯基犬图片，我们可以通过下面这段代码提取CLIP的图像表征和文本表征。这里我们不妨提供三个类别选项，分别是图表、猫和狗。我们运行后面这段代码后，OpenCLIP模型预测为狗这一类别的概率是99.9%。你可以点开我的 <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson10/CLIP_demo.ipynb">Colab链接</a> 运行一下，也可以试试你自己想测试的图片。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image
<span class="token keyword">import</span> open_clip
<span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

model<span class="token punctuation">,</span> _<span class="token punctuation">,</span> preprocess <span class="token operator">=</span> open_clip<span class="token punctuation">.</span>create_model_and_transforms<span class="token punctuation">(</span><span class="token string">'ViT-B-32'</span><span class="token punctuation">,</span> pretrained<span class="token operator">=</span><span class="token string">'laion2b_s34b_b79k'</span><span class="token punctuation">)</span>
tokenizer <span class="token operator">=</span> open_clip<span class="token punctuation">.</span>get_tokenizer<span class="token punctuation">(</span><span class="token string">'ViT-B-32'</span><span class="token punctuation">)</span>
image_url <span class="token operator">=</span> <span class="token string">"https://lh4.googleusercontent.com/sL5lctmCOd0nCQcaHAm9bd1Gi3xsmQrgRP3HEgvWZwNcBPkBIAhbh4_e8torbhL_bLV3T80Sq_kj1GQ7kS7GhYR1E9ETPsYR5uaC1ZEqi0kuSRijl7mzF6TM9wAjRhiofuDwfxE4"</span>
image_path <span class="token operator">=</span> <span class="token string">"test_image.png"</span>
urllib<span class="token punctuation">.</span>request<span class="token punctuation">.</span>urlretrieve<span class="token punctuation">(</span>image_url<span class="token punctuation">,</span> image_path<span class="token punctuation">)</span>
image <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>image_path<span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">"RGB"</span><span class="token punctuation">)</span>
image <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
text <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"a diagram"</span><span class="token punctuation">,</span> <span class="token string">"a dog"</span><span class="token punctuation">,</span> <span class="token string">"a cat"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>amp<span class="token punctuation">.</span>autocast<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    image_features <span class="token operator">=</span> model<span class="token punctuation">.</span>encode_image<span class="token punctuation">(</span>image<span class="token punctuation">)</span>
    text_features <span class="token operator">=</span> model<span class="token punctuation">.</span>encode_text<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
    image_features <span class="token operator">/=</span> image_features<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    text_features <span class="token operator">/=</span> text_features<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    text_probs <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">100.0</span> <span class="token operator">*</span> image_features @ text_features<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>image_path<span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"prob: a diagram </span><span class="token interpolation"><span class="token punctuation">&#123;</span>text_probs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string">, a dog </span><span class="token interpolation"><span class="token punctuation">&#123;</span>text_probs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string">, a cat </span><span class="token interpolation"><span class="token punctuation">&#123;</span>text_probs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/054e040dfe016950ccc7b64bb1ddbcbc.png"></p>
<h2 id="总结时刻"><a href="#总结时刻" class="headerlink" title="总结时刻"></a>总结时刻</h2><p>总结一下，今天我们学习了OpenAI在2021年推出的CLIP模型。</p>
<p>CLIP模型使用4亿互联网图文数据，结合对比学习的方式进行训练，得到了一个通用的视觉特征模型。CLIP的训练目标是在多模态空间中，成对图文的图像文本特征尽可能靠近，不成对图文的图像文本特征尽可能远离。</p>
<p>然后，我们了解了CLIP模型的两种常用用法，分别是基于检索的方式为图像分类，以及实现AI绘画模型的prompt信息提取。</p>
<p>之后，在原始OpenAI CLIP模型的基础之上，我们认识了CLIP模型的系列变体，如OpenCLIP、ChineseCLIP、EVA-CLIP等工作，了解了这些模型该如何选用。</p>
<p>最后，我们通过一个实际的代码示例学习了如何基于CLIP的图像、文本表征进行跨模态检索。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/457f1003603abec01f84d18aa8128b3f.jpg"></p>
<p>掌握了CLIP模型的工作原理，对于我们理解之后的AI绘画模型、设计我们自己的AI绘画模型至关重要，相关概念和用法在之后的课程还会频繁被提到。非常鼓励你课后去读读 <a target="_blank" rel="noopener" href="https://readpaper.com/pdf-annotate/note?pdfId=4557522938392223745&noteId=1772676889632149504">原始论文</a>、看一些开源代码实现来加深对CLIP的理解。</p>
<h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>第一道题，除了今天我们提到的CLIP应用场景，还有哪些实际应用中，可以用到CLIP模型的图像和文本编码能力呢？</p>
<p>第二个是练习题，你可以试着用CLIP，给后面这两张图片分个类。图像你可以点击超链接获取： <a target="_blank" rel="noopener" href="https://s.yimg.com/ny/api/res/1.2/v2ics1Z_DbOFT6wrjTaxGw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTY0MDtoPTQyNw--/https://s.yimg.com/os/creatr-uploaded-images/2022-06/3757bb00-eca8-11ec-bf3f-7c2b69f1b53a">链接1</a>、 <a target="_blank" rel="noopener" href="https://images.chinatimes.com/newsphoto/2023-06-05/656/20230605002784.jpg">链接2</a>。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202402/5817b721e3494f6be09b99bf74a1d940.jpg"></p>
<p>欢迎你在留言区和我交流互动，如果这一讲对你有启发，别忘了分享给身边更多朋友。</p>
</article><div class="tag_share"><div class="post_share"></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#10%EF%BD%9CCLIP%EF%BC%9A%E8%AE%A9AI%E7%BB%98%E7%94%BB%E6%A8%A1%E5%9E%8B%E4%B9%96%E4%B9%96%E5%90%AC%E4%BD%A0%E7%9A%84%E8%AF%9D"><span class="toc-number">1.</span> <span class="toc-text">10｜CLIP：让AI绘画模型乖乖听你的话</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%BD%E6%9C%AC%E6%BA%AF%E6%BA%90%EF%BC%9ACLIP%E7%9A%84%E6%8F%90%E5%87%BA%E8%83%8C%E6%99%AF"><span class="toc-number">1.1.</span> <span class="toc-text">追本溯源：CLIP的提出背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CLIP%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">1.2.</span> <span class="toc-text">CLIP解决方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90"><span class="toc-number">1.2.1.</span> <span class="toc-text">数据来源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E4%BF%A1%E5%8F%B7"><span class="toc-number">1.2.2.</span> <span class="toc-text">监督信号</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CLIP%E8%BF%9B%E9%98%B6%E6%8E%A2%E7%B4%A2"><span class="toc-number">1.3.</span> <span class="toc-text">CLIP进阶探索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CLIP%E5%BA%94%E7%94%A8"><span class="toc-number">1.3.1.</span> <span class="toc-text">CLIP应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CLIP%E5%A2%9E%E5%BC%BA%E7%89%88"><span class="toc-number">1.3.2.</span> <span class="toc-text">CLIP增强版</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8CLIP"><span class="toc-number">1.4.</span> <span class="toc-text">如何使用CLIP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E6%97%B6%E5%88%BB"><span class="toc-number">1.5.</span> <span class="toc-text">总结时刻</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%80%83%E9%A2%98"><span class="toc-number">1.6.</span> <span class="toc-text">思考题</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By 码农张三</div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.4.0/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div></div></body></html>