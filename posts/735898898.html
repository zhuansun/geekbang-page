<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>10｜AI连接外部资料库，让Llama Index带你阅读一本书 | geekbang</title><meta name="author" content="码农张三"><meta name="copyright" content="码农张三"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="10｜AI连接外部资料库，让Llama Index带你阅读一本书你好，我是徐文浩。 有不少人在使用OpenAI提供的GPT系列模型的时候，都反馈效果并不好。这些反馈中有一大类问题，是回答不了一些简单的问题。比如当我们用中文问AI一些事实性的问题，AI很容易胡编乱造。而当你问它最近发生的新闻事件的时候，它就干脆告诉你它不知道21年之后的事情。 本来呢，我写到这里就可以了。不过到了3月24日，Open">
<meta property="og:type" content="article">
<meta property="og:title" content="10｜AI连接外部资料库，让Llama Index带你阅读一本书">
<meta property="og:url" content="https://zhuansun.github.io/geekbang/posts/735898898.html">
<meta property="og:site_name" content="geekbang">
<meta property="og:description" content="10｜AI连接外部资料库，让Llama Index带你阅读一本书你好，我是徐文浩。 有不少人在使用OpenAI提供的GPT系列模型的时候，都反馈效果并不好。这些反馈中有一大类问题，是回答不了一些简单的问题。比如当我们用中文问AI一些事实性的问题，AI很容易胡编乱造。而当你问它最近发生的新闻事件的时候，它就干脆告诉你它不知道21年之后的事情。 本来呢，我写到这里就可以了。不过到了3月24日，Open">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg">
<meta property="article:published_time" content="2023-10-20T09:48:40.000Z">
<meta property="article:modified_time" content="2023-12-07T15:35:26.900Z">
<meta property="article:author" content="码农张三">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhuansun.github.io/geekbang/posts/735898898"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '10｜AI连接外部资料库，让Llama Index带你阅读一本书',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-07 15:35:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="geekbang" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic.imgdb.cn/item/653470a0c458853aef5813f1.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">585</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">geekbang</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">10｜AI连接外部资料库，让Llama Index带你阅读一本书</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2023-10-20T09:48:40.000Z" title="发表于 2023-10-20 09:48:40">2023-10-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/">AI大模型之美</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="10｜AI连接外部资料库，让Llama-Index带你阅读一本书"><a href="#10｜AI连接外部资料库，让Llama-Index带你阅读一本书" class="headerlink" title="10｜AI连接外部资料库，让Llama Index带你阅读一本书"></a>10｜AI连接外部资料库，让Llama Index带你阅读一本书</h1><p>你好，我是徐文浩。</p>
<p>有不少人在使用OpenAI提供的GPT系列模型的时候，都反馈效果并不好。这些反馈中有一大类问题，是回答不了一些简单的问题。比如当我们用中文问AI一些事实性的问题，AI很容易胡编乱造。而当你问它最近发生的新闻事件的时候，它就干脆告诉你它不知道21年之后的事情。</p>
<p>本来呢，我写到这里就可以了。不过到了3月24日，OpenAI推出了ChatGPT Plugins这个功能，可以让ChatGPT通过插件的形式链接外部的第三方应用。我自己也还在排队等waiting list，所以暂时也无法体验。不过，即使有了第三方应用，我们也不能确保自己想要知道的信息正好被其他人提供了。而且，有些信息和问题我们只想提供给自己公司的内部使用，并不想开放给所有人。这个时候，我们既希望能够利用OpenAI的大语言模型的能力，但是又需要这些能力仅仅在我们自己指定的数据上。那么这一讲，就是为了解决这个问题的。</p>
<h2 id="大型语言模型的不足之处"><a href="#大型语言模型的不足之处" class="headerlink" title="大型语言模型的不足之处"></a>大型语言模型的不足之处</h2><p>我们先来尝试问ChatGPT一个人尽皆知的常识，“鲁迅先生去日本学习医学的老师是谁”，结果它给出的答案是鲁迅的好友，内山书店的老板内山完造，而不是大家都学习过的藤野先生。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/891fc431e1cd46b1d45f60fe79c2e964.png" alt="图片"></p>
<p>之所以会出现这样的情况，和大模型的原理以及它使用训练的数据集是有关的。大语言模型的原理，就是利用训练样本里面出现的文本的前后关系，通过前面的文本对接下来出现的文本进行概率预测。如果类似的前后文本出现得越多，那么这个概率在训练过程里会收敛到少数正确答案上，回答就准确。如果这样的文本很少，那么训练过程里就会有一定的随机性，对应的答案就容易似是而非。而在GPT-3的模型里，虽然整体的训练语料很多，但是中文语料很少。只有不到1%的语料是中文的，所以如果问很多中文相关的知识性或者常识性问题，它的回答往往就很扯。</p>
<p>当然，你可以说我们有一个解决办法，就是多找一些高质量的中文语料训练一个新的模型。或者，对于我们想让AI能够回答出来的问题，找一些数据。然后利用OpenAI提供的“微调”（Fine-tune）接口，在原来的基础上训练一个新模型出来。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/47e11ff6d1534e8f53ea8da3ae5bcb9c.png" alt="图片"></p>
<p>这样当然是可以的，就是成本有点高。对于上面那个例子来说，只是缺少一些文本数据，还好说。如果是时效性要求比较强的资讯类的信息，就很难这么做。比如，我们想让AI告诉我们前一天足球赛的比分，我们不太可能每隔几个小时就单独训练或者微调一下模型，那样干的成本太高了。</p>
<h2 id="Bing的解法——先搜索，后提示"><a href="#Bing的解法——先搜索，后提示" class="headerlink" title="Bing的解法——先搜索，后提示"></a>Bing的解法——先搜索，后提示</h2><p>不过对这个领域比较关注的朋友可能就要问了。之前微软不是在Bing这个搜索引擎里，加上了ChatGPT的问答功能吗？效果似乎也还不错，那Bing是怎么做到的呢，是因为他们用了更加厉害的语言模型吗？</p>
<p>虽然我并没有什么内幕消息，不了解Bing是怎么做的。但是如果是我的话，会用这样一个解决办法——那就是先搜索，后提示（Prompt）。</p>
<ol>
<li>我们先通过搜索的方式，找到和询问的问题最相关的语料。这个搜索过程中，我们既可以用传统的基于关键词搜索的技术，也可以用 <a target="_blank" rel="noopener" href="http://time.geekbang.org/column/article/644795">第 9 讲</a> 我们刚刚介绍过的使用Embedding的相似度进行语义搜索的技术。</li>
<li>然后，我们将和问题语义最接近的前几条内容，作为提示语的一部分给到AI。然后请AI参考这些内容，再来回答这个问题。</li>
</ol>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/c8f7888bdfc3438934e784beayybeef2.png" alt="图片"></p>
<p>我在这里，也给了你一个例子的截图。当我们把《藤野先生》里的两个段落给到AI，然后请AI根据这两个段落，回答原来那个问题，就会得到正确的答案，你也可以去看一看。</p>
<p>这也是利用大语言模型的一个常见模式。因为大语言模型其实内含了两种能力。</p>
<p>第一种，是海量的语料中，本身已经包含了的知识信息。比如，我们前面问AI鱼香肉丝的做法，它能回答上来就是因为语料里已经有了充足的相关知识。我们一般称之为“世界知识”。</p>
<p>第二种，是根据你输入的内容，理解和推理的能力。这个能力，不需要训练语料里有一样的内容。而是大语言模型本身有“思维能力”，能够进行阅读理解。这个过程里，“知识”不是模型本身提供的，而是我们找出来，临时提供给模型的。如果不提供这个上下文，再问一次模型相同的问题，它还是答不上来的。</p>
<h2 id="通过llama-index封装“第二大脑”"><a href="#通过llama-index封装“第二大脑”" class="headerlink" title="通过llama_index封装“第二大脑”"></a>通过llama_index封装“第二大脑”</h2><p>我给上面这种先搜索、后提示的方式，取了一个名字，叫做AI的“第二大脑”模式。因为这个方法，需要提前把你希望AI能够回答的知识，建立一个外部的索引，这个索引就好像AI的“第二个大脑”。每次向AI提问的时候，它都会先去查询一下这个第二大脑里面的资料，找到相关资料之后，再通过自己的思维能力来回答问题。</p>
<p>实际上，你现在在网上看到的很多读论文、读书回答问题的应用，都是通过这个模式来实现的。那么，现在我们就来自己实现一下这个“第二大脑”模式。</p>
<p>不过，我们不必从0开始写代码。因为这个模式实在太过常用了，所以有人为它写了一个开源Python包，叫做llama-index。那么我们这里，可以直接利用这个软件包，用几行代码来试一试，它能不能回答上鲁迅先生写的《藤野先生》相关的问题。</p>
<p>llama-index还没有人做好Conda下的包，所以即使在Conda下还是要通过pip来安装。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install llama<span class="token operator">-</span>index
pip install langchain
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>我把从网上找到的《藤野先生》这篇文章变成了一个txt文件，放在了 data&#x2F;mr_fujino 这个目录下。我们的代码也非常简单，一共没有几行。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> openai<span class="token punctuation">,</span> os
<span class="token keyword">from</span> llama_index <span class="token keyword">import</span> GPTVectorStoreIndex<span class="token punctuation">,</span> SimpleDirectoryReader

openai<span class="token punctuation">.</span>api_key <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>

documents <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span><span class="token string">'./data/mr_fujino'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
index <span class="token operator">=</span> GPTSimpleVectorIndex<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>

index<span class="token punctuation">.</span>save_to_disk<span class="token punctuation">(</span><span class="token string">'index_mr_fujino.json'</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_nodes<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">0</span> tokens
INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_nodes<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">6763</span> tokens
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>注：日志中会打印出来我们通过Embedding消耗了多少个Token。</p>
<p>首先，我们通过一个叫做SimpleDirectoryReader的数据加载器，将整个.&#x2F;data&#x2F;mr_fujino 的目录给加载进来。这里面的每一个文件，都会被当成是一篇文档。</p>
<p>然后，我们将所有的文档交给了 GPTSimpleVectorIndex 构建索引。顾名思义，它会把文档分段转换成一个个向量，然后存储成一个索引。</p>
<p>最后，我们会把对应的索引存下来，存储的结果就是一个json文件。后面，我们就可以用这个索引来进行相应的问答。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">index <span class="token operator">=</span> GPTVectorStoreIndex<span class="token punctuation">.</span>load_from_disk<span class="token punctuation">(</span><span class="token string">'index_mr_fujino.json'</span><span class="token punctuation">)</span>
response <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"鲁迅先生在日本学习医学的老师是谁？"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>要进行问答也没有几行代码，我们通过 GPTSimpleVectorIndex 的 load_from_disk 函数，可以把刚才生成的索引加载到内存里面来。然后对着Index索引调用Query函数，就能够获得问题的答案。可以看到，通过外部的索引，我们可以正确地获得问题的答案。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">2984</span> tokens
INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">34</span> tokens

鲁迅先生在日本学习医学的老师是藤野严九郎先生。
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>这么一看，似乎问题特别简单，三行代码就搞定了。别着急，我们再看看别的问题它是不是也能答上来？这次我们来试着问问鲁迅先生是在哪里学习医学的。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">response <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"鲁迅先生去哪里学的医学？"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">></span> Got node text<span class="token punctuation">:</span> 藤野先生
东京也无非是这样。上野的樱花烂熳的时节，望去确也像绯红的轻云，但花下也缺不了成群结队的“清国留学生”的速成班，头顶上盘着大辫子，顶得学生制帽的顶上高高耸起，形成一座富士山。也有解散辫子，盘得平的，除下帽来，油光可鉴，宛如小姑娘的发髻一般，还要将脖子扭几扭。实在标致极了。
中国留学生会馆的门房里有几本书买，有时还值得去一转；倘在上午，里面的几间洋房里倒也还可以坐坐的。但到傍晚，有<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">2969</span> tokens
INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">26</span> tokens

鲁迅先生去仙台的医学专门学校学习医学。
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>它仍然正确回答了问题。那么，我们搜索到的内容，在这个过程里面是如何提交给OpenAI的呢？我们就来看看下面的这段代码就知道了。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> llama_index <span class="token keyword">import</span> QuestionAnswerPrompt
query_str <span class="token operator">=</span> <span class="token string">"鲁迅先生去哪里学的医学？"</span>
DEFAULT_TEXT_QA_PROMPT_TMPL <span class="token operator">=</span> <span class="token punctuation">(</span>
    <span class="token string">"Context information is below. \n"</span>
    <span class="token string">"---------------------\n"</span>
    <span class="token string">"&#123;context_str&#125;"</span>
    <span class="token string">"\n---------------------\n"</span>
    <span class="token string">"Given the context information and not prior knowledge, "</span>
    <span class="token string">"answer the question: &#123;query_str&#125;\n"</span>
<span class="token punctuation">)</span>
QA_PROMPT <span class="token operator">=</span> QuestionAnswerPrompt<span class="token punctuation">(</span>DEFAULT_TEXT_QA_PROMPT_TMPL<span class="token punctuation">)</span>

response <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span>query_str<span class="token punctuation">,</span> text_qa_template<span class="token operator">=</span>QA_PROMPT<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>这段代码里，我们定义了一个QA_PROMPT的对象，并且为它设计了一个模版。</p>
<ol>
<li>这个模版的开头，我们告诉AI，我们为AI提供了一些上下文信息（Context information）。</li>
<li>模版里面支持两个变量，一个叫做 context_str，另一个叫做query_str。context_str 的地方，在实际调用的时候，会被通过Embedding相似度找出来的内容填入。而 query_str 则是会被我们实际提的问题替换掉。</li>
<li>实际提问的时候，我们告诉AI，只考虑上下文信息，而不要根据自己已经有的先验知识（prior knowledge）来回答问题。</li>
</ol>
<p>我们就是这样，把搜索找到的相关内容以及问题，组合到一起变成一段提示语，让AI能够按照我们的要求来回答问题。那我们再问一次AI，看看答案是不是没有变。</p>
<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">鲁迅先生去仙台的医学专门学校学习医学。
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>这一次AI还是正确地回答出了鲁迅先生是去仙台的医学专门学校学习的。我们再试一试，问一些不相干的问题，会得到什么答案，比如我们问问红楼梦里林黛玉和贾宝玉的关系。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">QA_PROMPT_TMPL <span class="token operator">=</span> <span class="token punctuation">(</span>
    <span class="token string">"下面的“我”指的是鲁迅先生 \n"</span>
    <span class="token string">"---------------------\n"</span>
    <span class="token string">"&#123;context_str&#125;"</span>
    <span class="token string">"\n---------------------\n"</span>
    <span class="token string">"根据这些信息，请回答问题: &#123;query_str&#125;\n"</span>
    <span class="token string">"如果您不知道的话，请回答不知道\n"</span>
<span class="token punctuation">)</span>
QA_PROMPT <span class="token operator">=</span> QuestionAnswerPrompt<span class="token punctuation">(</span>QA_PROMPT_TMPL<span class="token punctuation">)</span>

response <span class="token operator">=</span> index<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"请问林黛玉和贾宝玉是什么关系？"</span><span class="token punctuation">,</span> text_qa_template<span class="token operator">=</span>QA_PROMPT<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">不知道
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>可以看到，AI的确按照我们的指令回答不知道，而不是胡答一气。</p>
<h2 id="通过llama-index对于文章进行小结"><a href="#通过llama-index对于文章进行小结" class="headerlink" title="通过llama_index对于文章进行小结"></a>通过llama_index对于文章进行小结</h2><p>还有一个常见的使用llama-index这样“第二大脑”的Python库的应用场景，就是生成文章的摘要。在前面教你如何进行文本聚类的时候，我们已经看到了可以通过合适的提示语（Prompt）做到这一点。不过，如果要总结一篇论文、甚至是一本书，每次最多只能支持4096个Token的API就不太够用了。</p>
<p>要解决这个问题也并不困难，我们只要进行分段小结，再对总结出来的内容再做一次小结就可以了。我们可以把一篇文章，乃至一本书，构建成一个树状的索引。每一个树里面的节点，就是它的子树下内容的摘要。最后，在整棵树的根节点，得到的就是整篇文章或者整本书的总结了。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/49489cc171aee72855930aaf68112b42.jpg" alt="图片"></p>
<p>事实上，llama-index本身就内置了这样的功能。下面我们就来看看要实现这个功能，我们的代码应该怎么写。</p>
<p>首先，我们先来安装一下 spaCy 这个Python库，并且下载一下对应的中文分词分句需要的模型。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install spacy
python <span class="token operator">-</span>m spacy download zh_core_web_sm
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>接下来的代码很简单，我们选用了GPTListIndex这个llama-index里最简单的索引结构。不过我们针对自身需求做了两点优化。</p>
<p>首先，在索引里面，我们指定了一个 LLMPredictor，让我们向OpenAI发起请求的时候，都使用ChatGPT的模型。因为这个模型比较快，也比较便宜。llama-index默认使用的模型是text-davinci-003，价格比gpt-3.5-turbo要贵上十倍。在我们前面只是简单进行几轮对话的时候，这个价格差异还不明显。而如果你要把几十本书都灌进去，那成本上就会差上不少了。我们在这里，设置了模型输出的内容都在1024个Token以内，这样可以确保我们的小结不会太长，不会把一大段不相关的内容都合并到一起去。</p>
<p>其次，我们定义了使用 SpacyTextSplitter来进行中文文本的分割。llama-index默认的设置对于中文的支持和效果都不太好。不过好在它可以让你自定义使用的文本分割方式。我们选用的文章是中文的，里面的标点符号也都是中文的，所以我们就用了中文的语言模型。我们也限制了分割出来的文本段，最长不要超过2048个Token，这些参数都可以根据你实际用来处理的文章内容和属性自己设置。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chat_models <span class="token keyword">import</span> ChatOpenAI
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>text_splitter <span class="token keyword">import</span> SpacyTextSplitter
<span class="token keyword">from</span> llama_index <span class="token keyword">import</span> GPTListIndex<span class="token punctuation">,</span> LLMPredictor<span class="token punctuation">,</span> ServiceContext
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>node_parser <span class="token keyword">import</span> SimpleNodeParser

<span class="token comment"># define LLM</span>
llm_predictor <span class="token operator">=</span> LLMPredictor<span class="token punctuation">(</span>llm<span class="token operator">=</span>ChatOpenAI<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> model_name<span class="token operator">=</span><span class="token string">"gpt-3.5-turbo"</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

text_splitter <span class="token operator">=</span> SpacyTextSplitter<span class="token punctuation">(</span>pipeline<span class="token operator">=</span><span class="token string">"zh_core_web_sm"</span><span class="token punctuation">,</span> chunk_size <span class="token operator">=</span> <span class="token number">2048</span><span class="token punctuation">)</span>
parser <span class="token operator">=</span> SimpleNodeParser<span class="token punctuation">(</span>text_splitter<span class="token operator">=</span>text_splitter<span class="token punctuation">)</span>
documents <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span><span class="token string">'./data/mr_fujino'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
nodes <span class="token operator">=</span> parser<span class="token punctuation">.</span>get_nodes_from_documents<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>

service_context <span class="token operator">=</span> ServiceContext<span class="token punctuation">.</span>from_defaults<span class="token punctuation">(</span>llm_predictor<span class="token operator">=</span>llm_predictor<span class="token punctuation">)</span>

list_index <span class="token operator">=</span> GPTListIndex<span class="token punctuation">(</span>nodes<span class="token operator">=</span>nodes<span class="token punctuation">,</span> service_context<span class="token operator">=</span>service_context<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">WARNING<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>llm_predictor<span class="token punctuation">.</span>base<span class="token punctuation">:</span>Unknown <span class="token builtin">max</span> <span class="token builtin">input</span> size <span class="token keyword">for</span> gpt<span class="token operator">-</span><span class="token number">3.5</span><span class="token operator">-</span>turbo<span class="token punctuation">,</span> using defaults<span class="token punctuation">.</span>
INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_nodes<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">0</span> tokens
INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>build_index_from_nodes<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">0</span> tokens
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>GPTListIndex在构建索引的时候，并不会创建Embedding，所以索引创建的时候很快，也不消耗Token数量。它只是根据你设置的索引结构和分割方式，建立了一个List的索引。</p>
<p>接着，我们就可以让AI帮我们去小结这篇文章了。同样的，提示语本身很重要，所以我们还是强调了文章内容是鲁迅先生以“我”这个第一人称写的。因为我们想要的是按照树状结构进行文章的小结，所以我们设定了一个参数，叫做 response_mode &#x3D; “tree_summarize”。这个参数，就会按照上面我们所说的树状结构把整个文章总结出来。</p>
<p>实际上，它就是将每一段文本分片，都通过query内的提示语小结。再对多个小结里的内容，再次通过query里的提示语继续小结。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">response <span class="token operator">=</span> list_index<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:"</span><span class="token punctuation">,</span> response_mode<span class="token operator">=</span><span class="token string">"tree_summarize"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>indices<span class="token punctuation">.</span>common_tree<span class="token punctuation">.</span>base<span class="token punctuation">:</span><span class="token operator">></span> Building index <span class="token keyword">from</span> nodes<span class="token punctuation">:</span> <span class="token number">2</span> chunks
INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">9787</span> tokens
INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">0</span> tokens
鲁迅先生回忆了自己在日本学医期间的经历，描述了自己在解剖实习中的经历，以及与教授藤野先生的交往。他还提到了一些不愉快的事情，比如遭到同学的诽谤和歧视，以及看到中国人被枪毙时的感受。最后，他告诉藤野先生自己将不再学医，而是想学生物学。他想起了一个人，这个人是他的老师，他对鲁迅很热心，给他很多鼓励和教诲。鲁迅现在只有他的照片，但是每次看到他的照片，都会让他感到勇气和良心发现。
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到，我们只用了几行代码就完成了整个文章的小结，返回的结果整体上来说也还算不错。</p>
<h2 id="引入多模态，让llamd-index能够识别小票"><a href="#引入多模态，让llamd-index能够识别小票" class="headerlink" title="引入多模态，让llamd-index能够识别小票"></a>引入多模态，让llamd-index能够识别小票</h2><p>llama_index不光能索引文本，很多书里面还有图片、插画这样的信息。llama_index一样可以索引起来，供你查询，这也就是所谓的多模态能力。当然，这个能力其实是通过一些多模态的模型，把文本和图片能够联系到一起做到的。在整个课程的第三部分，我们也会专门来看看这些图像的多模态模型是怎么样的。</p>
<p>这里我们就来看一个llama_index <a target="_blank" rel="noopener" href="https://github.com/jerryjliu/llama_index/blob/main/examples/multimodal/Multimodal.ipynb">官方样例库</a> 里面给到的例子，也就是把吃饭的小票都拍下来。然后询问哪天吃了什么，花了多少钱。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> llama_index <span class="token keyword">import</span> SimpleDirectoryReader<span class="token punctuation">,</span> GPTVectorStoreIndex
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>readers<span class="token punctuation">.</span><span class="token builtin">file</span><span class="token punctuation">.</span>base <span class="token keyword">import</span> DEFAULT_FILE_EXTRACTOR<span class="token punctuation">,</span> ImageParser
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>response<span class="token punctuation">.</span>notebook_utils <span class="token keyword">import</span> display_response<span class="token punctuation">,</span> display_image
<span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>indices<span class="token punctuation">.</span>query<span class="token punctuation">.</span>query_transform<span class="token punctuation">.</span>base <span class="token keyword">import</span> ImageOutputQueryTransform

image_parser <span class="token operator">=</span> ImageParser<span class="token punctuation">(</span>keep_image<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> parse_text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
file_extractor <span class="token operator">=</span> DEFAULT_FILE_EXTRACTOR
file_extractor<span class="token punctuation">.</span>update<span class="token punctuation">(</span>
<span class="token punctuation">&#123;</span>
    <span class="token string">".jpg"</span><span class="token punctuation">:</span> image_parser<span class="token punctuation">,</span>
    <span class="token string">".png"</span><span class="token punctuation">:</span> image_parser<span class="token punctuation">,</span>
    <span class="token string">".jpeg"</span><span class="token punctuation">:</span> image_parser<span class="token punctuation">,</span>
<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>

<span class="token comment"># NOTE: we add filename as metadata for all documents</span>
filename_fn <span class="token operator">=</span> <span class="token keyword">lambda</span> filename<span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token string">'file_name'</span><span class="token punctuation">:</span> filename<span class="token punctuation">&#125;</span>

receipt_reader <span class="token operator">=</span> SimpleDirectoryReader<span class="token punctuation">(</span>
    input_dir<span class="token operator">=</span><span class="token string">'./data/receipts'</span><span class="token punctuation">,</span>
    file_extractor<span class="token operator">=</span>file_extractor<span class="token punctuation">,</span>
    file_metadata<span class="token operator">=</span>filename_fn<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
receipt_documents <span class="token operator">=</span> receipt_reader<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>要能够索引图片，我们引入了ImageParser这个类，这个类背后，其实是一个基于OCR扫描的模型 <a target="_blank" rel="noopener" href="https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v2">Donut</a>。它通过一个视觉的Encoder和一个文本的Decoder，这样任何一个图片能够变成一个一段文本，然后我们再通过OpenAI的Embedding把这段文本变成了一个向量。</p>
<p>我们仍然只需要使用简单的SimpleDirectoryReader，我们通过指定FileExtractor，会把对应的图片通过ImageParser解析成为文本，并最终成为向量来用于检索。</p>
<p>然后，我们仍然只需要向我们的索引用自然语言提问，就能找到对应的图片了。在提问的时候，我们专门制定了一个ImageOutputQueryTransform，主要是为了在输出结果的时候，能够在图片外加上 <code>&lt;img&gt;</code> 的标签方便在Notebook里面显示。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">receipts_index <span class="token operator">=</span> GPTVectorStoreIndex<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>receipt_documents<span class="token punctuation">)</span>
receipts_response <span class="token operator">=</span> receipts_index<span class="token punctuation">.</span>query<span class="token punctuation">(</span>
    'When was the last time I went to McDonald\'s <span class="token keyword">and</span> how much did I spend<span class="token punctuation">.</span> \
    Also show me the receipt <span class="token keyword">from</span> my visit<span class="token punctuation">.</span>'<span class="token punctuation">,</span>
    query_transform<span class="token operator">=</span>ImageOutputQueryTransform<span class="token punctuation">(</span>width<span class="token operator">=</span><span class="token number">400</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>

display_response<span class="token punctuation">(</span>receipts_response<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total LLM token usage<span class="token punctuation">:</span> <span class="token number">1004</span> tokens
INFO<span class="token punctuation">:</span>llama_index<span class="token punctuation">.</span>token_counter<span class="token punctuation">.</span>token_counter<span class="token punctuation">:</span><span class="token operator">></span> <span class="token punctuation">[</span>query<span class="token punctuation">]</span> Total embedding token usage<span class="token punctuation">:</span> <span class="token number">30</span> tokens

Final Response<span class="token punctuation">:</span> The last time you went to McDonald's was on <span class="token number">03</span><span class="token operator">/</span><span class="token number">10</span><span class="token operator">/</span><span class="token number">2018</span> at <span class="token number">07</span><span class="token punctuation">:</span><span class="token number">39</span><span class="token punctuation">:</span><span class="token number">12</span> PM <span class="token keyword">and</span> you spent $<span class="token number">26.15</span><span class="token punctuation">.</span> Here <span class="token keyword">is</span> the receipt <span class="token keyword">from</span> your visit<span class="token punctuation">:</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/181747bf1e95664b3929db6ec4a46b08.jpg" alt="图片"></p>
<p>可以看到，答案中不仅显示出了对应的图片，也给出了正确的答案，这也要归功于OpenAI对于任意文本强大的处理能力。</p>
<p>我们可以单独解析一下图片，看看对应的文本内容是什么。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">output_image <span class="token operator">=</span> image_parser<span class="token punctuation">.</span>parse_file<span class="token punctuation">(</span><span class="token string">'./data/receipts/1100-receipt.jpg'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output_image<span class="token punctuation">.</span>text<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">&lt;s_menu>&lt;s_nm> Story&lt;/s_nm>&lt;s_num> 16725 Stony Platin Rd&lt;/s_nm>&lt;s_num> Store#:&lt;/s_nm>&lt;s_num> 3659&lt;/s_num>&lt;s_price> 700-418-8362&lt;/s_price>&lt;sep/>&lt;s_nm> Welcome to all day breakfast dormist O Md Donald's&lt;/s_nm>&lt;s_num> 192&lt;/s_num>&lt;s_price> 192&lt;/s_price>&lt;sep/>&lt;s_nm> QTY ITEM&lt;/s_nm>&lt;s_num> OTAL&lt;/s_num>&lt;s_unitprice> 03/10/2018&lt;/s_unitprice>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 07:39:12 PM&lt;/s_price>&lt;sep/>&lt;s_nm> Delivery&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 0.00&lt;/s_price>&lt;sep/>&lt;s_nm> 10 McNuggets EVM&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 10.29&lt;/s_price>&lt;sep/>&lt;s_nm> Barbeque Sauce&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 1&lt;/s_price>&lt;sep/>&lt;s_nm> Barbeque Sauce&lt;/s_nm>&lt;s_num> 1&lt;/s_cnt>&lt;s_price> 0.40&lt;/s_price>&lt;sep/>&lt;s_nm> L Coke&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 0.40&lt;/s_price>&lt;sep/>&lt;s_nm> M French Fries&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 3.99&lt;/s_price>&lt;sep/>&lt;s_nm> HM GrChS S-Fry Yog&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;sep/>&lt;s_nm> Smoonya&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;sep/>&lt;s_nm> HM Apple Juice&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 2.89&lt;/s_price>&lt;sep/>&lt;s_nm> Cookies&lt;/s_nm>&lt;s_cnt> 6&lt;/s_cnt>&lt;s_price> 2.89&lt;/s_price>&lt;sep/>&lt;s_nm> Choc Chip Cookie&lt;/s_nm>&lt;s_cnt> 6&lt;/s_cnt>&lt;s_price> 1.19&lt;/s_price>&lt;sep/>&lt;s_nm> Baked Apple Pie&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 3.29&lt;/s_price>&lt;sep/>&lt;s_nm> French Fries&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 2.99&lt;/s_price>&lt;sep/>&lt;s_nm> Iced Tea&lt;/s_nm>&lt;s_cnt> 1&lt;/s_cnt>&lt;s_price> 2.99&lt;/s_price>&lt;/s_menu>&lt;s_sub_total>&lt;s_subtotal_price> 25.04&lt;/s_subtotal_price>&lt;s_tax_price> 1.11&lt;/s_tax_price>&lt;/s_sub_total>&lt;s_total>&lt;s_total_price> 26.15&lt;/s_total_price>&lt;s_changeprice> 0.00&lt;/s_changeprice>&lt;s_creditcardprice> 26.15&lt;/s_creditcardprice>&lt;/s_total>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>可以看到，对应的就是OCR后的文本结果，里面的确有对应我们去的店铺的名字和时间，以及消费的金额。</p>
<p>围绕OpenAI以及整个大语言模型的生态还在快速发展中，所以llama-index这个库也在快速迭代。我自己在使用的过程中，也遇到各种各样的小Bug。对于中文的支持也有各种各样的小缺陷。不过，作为开源项目，它已经有一个很不错的生态了，特别是提供了大量的DataConnector，既包括PDF、ePub这样的电子书格式，也包括YouTube、Notion、MongoDB这样外部的数据源、API接入的数据，或者是本地数据库的数据。你可以在 <a target="_blank" rel="noopener" href="https://llamahub.ai/">llamahub.ai</a> 看到社区开发出来的读取各种不同数据源格式的DataConnector。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d19d1dd9888785c85bbc3ac5aec08191.png" alt="图片"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，相信经过这一讲，你已经能够上手使用llama-index这个Python包了。通过它，你可以快速将外部的资料库变成索引，并且通过它提供的query接口快速向文档提问，也能够通过将文本分片，并通过树状的方式管理索引并进行小结。</p>
<p>llama-index还有很多其他功能，这个Python库仍然在发展过程中，不过已经非常值得拿来使用，加速你开发大语言模型类的相关应用了。相关的文档，可以在 <a target="_blank" rel="noopener" href="https://gpt-index.readthedocs.io/en/latest/">官网</a> 看到。对应的代码也是开源的，遇到问题也可以直接去 <a target="_blank" rel="noopener" href="https://github.com/jerryjliu/gpt_index">源代码</a> 里一探究竟。</p>
<p>llama-index其实给出了一种使用大语言模型的设计模式，我称之为“第二大脑”模式。通过先将外部的资料库索引，然后每次提问的时候，先从资料库里通过搜索找到有相关性的材料，然后再通过AI的语义理解能力让AI基于搜索到的结果来回答问题。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e4ae3fbeaa82d82e317dfcef40679f6c.jpg" alt="图片"></p>
<p>其中，前两步的索引和搜索，我们可以使用OpenAI的Embedding接口，也可以使用其它的大语言模型的Embedding，或者传统的文本搜索技术。只有最后一步的问答，往往才必须使用OpenAI的接口。我们不仅可以索引文本信息，也可以通过其他的模型来把图片变成文本进行索引，实现所谓的多模态功能。</p>
<p>希望通过今天的这几个例子，你也能开始建立起自己的“第二大脑”资料库，能够将自己的数据集交给AI进行索引，获得一个专属于你自己的AI。</p>
<h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><ol>
<li>llama-index的生态，不仅支持各种各样的DataConnector去加载数据，后端也支持各种形式的索引，比如在语义搜索里面我们介绍过的 Faiss、Pinecone、Weaviate 它都是支持的。除了这些之外，你能看看 llama-index 还有哪些形式的索引吗？除了进行问答和文章概括之外，你觉得这个库还能帮助我们做什么事情？</li>
<li>现在有很多应用，在你把文档上传之后，还会给你一系列的提示，告诉你可以向对应的书或者论文问什么问题。比如 <a target="_blank" rel="noopener" href="https://scispace.com/">SCISPACE</a>，你能想想这些问题是怎么来的吗？</li>
</ol>
<p>期待能在评论区看到你的分享，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p>
<h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>llama-index的功能非常强大，并且源代码里也专门提供了示例部分。你可以去看一下它的官方文档以及示例，了解它可以用来干什么。</p>
<ol>
<li>官方文档： <a target="_blank" rel="noopener" href="https://gpt-index.readthedocs.io/en/latest/">https://gpt-index.readthedocs.io/en/latest/</a></li>
<li>源码以及示例： <a target="_blank" rel="noopener" href="https://github.com/jerryjliu/llama_index">https://github.com/jerryjliu/llama_index</a></li>
</ol>
</article><div class="tag_share"><div class="post_share"></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#10%EF%BD%9CAI%E8%BF%9E%E6%8E%A5%E5%A4%96%E9%83%A8%E8%B5%84%E6%96%99%E5%BA%93%EF%BC%8C%E8%AE%A9Llama-Index%E5%B8%A6%E4%BD%A0%E9%98%85%E8%AF%BB%E4%B8%80%E6%9C%AC%E4%B9%A6"><span class="toc-number">1.</span> <span class="toc-text">10｜AI连接外部资料库，让Llama Index带你阅读一本书</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3%E4%B9%8B%E5%A4%84"><span class="toc-number">1.1.</span> <span class="toc-text">大型语言模型的不足之处</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bing%E7%9A%84%E8%A7%A3%E6%B3%95%E2%80%94%E2%80%94%E5%85%88%E6%90%9C%E7%B4%A2%EF%BC%8C%E5%90%8E%E6%8F%90%E7%A4%BA"><span class="toc-number">1.2.</span> <span class="toc-text">Bing的解法——先搜索，后提示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E8%BF%87llama-index%E5%B0%81%E8%A3%85%E2%80%9C%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91%E2%80%9D"><span class="toc-number">1.3.</span> <span class="toc-text">通过llama_index封装“第二大脑”</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E8%BF%87llama-index%E5%AF%B9%E4%BA%8E%E6%96%87%E7%AB%A0%E8%BF%9B%E8%A1%8C%E5%B0%8F%E7%BB%93"><span class="toc-number">1.4.</span> <span class="toc-text">通过llama_index对于文章进行小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E5%A4%9A%E6%A8%A1%E6%80%81%EF%BC%8C%E8%AE%A9llamd-index%E8%83%BD%E5%A4%9F%E8%AF%86%E5%88%AB%E5%B0%8F%E7%A5%A8"><span class="toc-number">1.5.</span> <span class="toc-text">引入多模态，让llamd-index能够识别小票</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.6.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0"><span class="toc-number">1.7.</span> <span class="toc-text">课后练习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB"><span class="toc-number">1.8.</span> <span class="toc-text">推荐阅读</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By 码农张三</div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.4.0/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div></div></body></html>