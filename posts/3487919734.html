<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>04｜新时代模型性能大比拼，GPT-3到底胜在哪里？ | geekbang</title><meta name="author" content="码农张三"><meta name="copyright" content="码农张三"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="04｜新时代模型性能大比拼，GPT-3到底胜在哪里？你好，我是徐文浩。 前面两讲，我带你体验了OpenAI通过API提供的GPT-3.5系列模型的两个核心接口。一个是获取一段文本的Embedding向量，另一个则是根据提示语，直接生成一段补全的文本内容。我们用这两种方法，都可以实现零样本（zero-shot）或者少样本下的情感分析任务。不过，你可能会提出这样两个疑问。  Embedding不就是把">
<meta property="og:type" content="article">
<meta property="og:title" content="04｜新时代模型性能大比拼，GPT-3到底胜在哪里？">
<meta property="og:url" content="https://zhuansun.github.io/geekbang/posts/3487919734.html">
<meta property="og:site_name" content="geekbang">
<meta property="og:description" content="04｜新时代模型性能大比拼，GPT-3到底胜在哪里？你好，我是徐文浩。 前面两讲，我带你体验了OpenAI通过API提供的GPT-3.5系列模型的两个核心接口。一个是获取一段文本的Embedding向量，另一个则是根据提示语，直接生成一段补全的文本内容。我们用这两种方法，都可以实现零样本（zero-shot）或者少样本下的情感分析任务。不过，你可能会提出这样两个疑问。  Embedding不就是把">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg">
<meta property="article:published_time" content="2023-10-20T09:48:40.000Z">
<meta property="article:modified_time" content="2024-03-21T11:10:30.317Z">
<meta property="article:author" content="码农张三">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhuansun.github.io/geekbang/posts/3487919734"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '04｜新时代模型性能大比拼，GPT-3到底胜在哪里？',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-21 11:10:30'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="geekbang" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic.imgdb.cn/item/653470a0c458853aef5813f1.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1343</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">geekbang</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">04｜新时代模型性能大比拼，GPT-3到底胜在哪里？</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2023-10-20T09:48:40.000Z" title="发表于 2023-10-20 09:48:40">2023-10-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/">AI大模型之美</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="04｜新时代模型性能大比拼，GPT-3到底胜在哪里？"><a href="#04｜新时代模型性能大比拼，GPT-3到底胜在哪里？" class="headerlink" title="04｜新时代模型性能大比拼，GPT-3到底胜在哪里？"></a>04｜新时代模型性能大比拼，GPT-3到底胜在哪里？</h1><p>你好，我是徐文浩。</p>
<p>前面两讲，我带你体验了OpenAI通过API提供的GPT-3.5系列模型的两个核心接口。一个是获取一段文本的Embedding向量，另一个则是根据提示语，直接生成一段补全的文本内容。我们用这两种方法，都可以实现零样本（zero-shot）或者少样本下的情感分析任务。不过，你可能会提出这样两个疑问。</p>
<ol>
<li>Embedding不就是把文本变成向量吗？我也学过一些自然语言处理，直接用个开源模型，比如Word2Vec、Bert之类的就好了呀，何必要去调用OpenAI的API呢？</li>
<li>我们在这个情感分析里做了很多投机取巧的工作。一方面，我们把3分这种相对中性的评价分数排除掉了；另一方面，我们把1分2分和4分5分分别合并在了一起，把一个原本需要判断5个分类的问题简化了。那如果我们想要准确地预测多个分类，也会这么简单吗？</li>
</ol>
<p>那么，这一讲我们就先来回答第一个问题。我们还是拿代码和数据来说话，就拿常见的开源模型来试一试，看看能否通过零样本学习的方式来取得比较好的效果。第二个问题，我们下一讲再来探讨，看看能不能利用Embedding进一步通过一些机器学习的算法，来更好地处理情感分析问题。</p>
<h2 id="什么是预训练模型？"><a href="#什么是预训练模型？" class="headerlink" title="什么是预训练模型？"></a>什么是预训练模型？</h2><p>给出一段文本，OpenAI就能返回给你一个Embedding向量，这是因为它的背后是GPT-3这个超大规模的预训练模型（Pre-trained Model）。事实上，GPT的英文全称翻译过来就是“生成式预训练Transformer（Generative Pre-trained Transformer）”。</p>
<p>所谓预训练模型，就是虽然我们没有看过你想要解决的问题，比如这里我们在情感分析里看到的用户评论和评分。但是，我可以拿很多我能找到的文本，比如网页文章、维基百科里的文章，各种书籍的电子版等等， <strong>作为理解文本内容的一个学习资料</strong>。</p>
<p>我们不需要对这些数据进行人工标注，只根据这些文本前后的内容，来习得文本之间内在的关联。比如，网上的资料里，会有很多“小猫很可爱”、“小狗很可爱”这样的文本。小猫和小狗后面都会跟着“很可爱”，那么我们就会知道小猫和小狗应该是相似的词，都是宠物。同时，一般我们对于它们的情感也是正面的。这些隐含的内在信息，在我们做情感分析的时候，就带来了少量用户评论和评分数据里缺少的“常识”，这些“常识”也有助于我们更好地预测。</p>
<p>比如，文本里有“白日依山尽”，那么模型就知道后面应该跟“黄河入海流”。文本前面是“今天天气真”，后面跟着的大概率是“不错”，小概率是“糟糕”。这些文本关系，最后以一堆参数的形式体现出来。对于你输入的文本，它可以根据这些参数计算出一个向量，然后根据这个向量，来推算这个文本后面的内容。</p>
<p>可以这样来理解： <strong>用来训练的语料文本越丰富，模型中可以放的参数越多，那模型能够学到的关系也就越多。类似的情况在文本里出现得越多，那么将来模型猜得也就越准。</strong></p>
<p>预训练模型在自然语言处理领域并不是OpenAI的专利。早在2013年，就有一篇叫做Word2Vec的经典论文谈到过。它能够通过预训练，根据同一个句子里一个单词前后出现的单词，来得到每个单词的向量。而在2018年，Google关于BERT的论文发表之后，整个业界也都会使用BERT这样的预训练模型，把一段文本变成向量用来解决自己的自然语言处理任务。在GPT-3论文发表之前，大家普遍的结论是，BERT作为预训练的模型效果也是优于GPT的。</p>
<h2 id="Fasttext、T5、GPT-3模型效果大比拼"><a href="#Fasttext、T5、GPT-3模型效果大比拼" class="headerlink" title="Fasttext、T5、GPT-3模型效果大比拼"></a>Fasttext、T5、GPT-3模型效果大比拼</h2><p>今天我们就拿两个开源的预训练模型，来看看直接用它们对文本进行向量化，是不是也能取得和OpenAI的API一样好的效果。</p>
<p>第一个是来自Facebook的Fasttext，它继承了Word2Vec的思路，能够把一个个单词表示成向量。第二个是来自Google的T5，T5的全称是Text-to-Text Transfer Trasnformer，是适合做迁移学习的一个模型。所谓迁移学习，也就是它推理出来向量的结果，常常被拿来再进行机器学习，去解决其他自然语言处理问题。通常很多新发表的论文，会把T5作为预训练模型进行微调和训练，或者把它当作Benchmark来对比、评估。</p>
<h3 id="Fasttext效果测试"><a href="#Fasttext效果测试" class="headerlink" title="Fasttext效果测试"></a>Fasttext效果测试</h3><p>我们先来试一下Fasttext，在实际运行代码之前，我们需要先安装Fasttext和Gensim这两个Python包。我在下面列出了通过Conda安装对应Python包的代码，如果你使用的是PIP或者其他的Python包管理工具，你就换成对应的PIP命令就好了。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">conda <span class="token function">install</span> gensim
conda <span class="token function">install</span> fasttext
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>然后，我们要把Fasttext对应的模型下载到本地。因为这些开源库和对应的论文都是Facebook和Google这样的海外公司发布的，效果自然是在英语上比较好，所以我们就下载对应的英语模型，名字叫做 “cc.en.300.bin”。同样的，对应模型的下载链接，我也放在 <a target="_blank" rel="noopener" href="https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz">这里</a> 了。</p>
<p>下载之后解压，然后把文件放在和Notebook相同的目录下，方便我们接下来运行代码。</p>
<p>这里我们拿来测试效果的数据集还是和 <a target="_blank" rel="noopener" href="https://time.geekbang.org/column/article/642179">第 02 讲</a> 一样，用的是 2.5w 条亚马逊食物评论的数据。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/1288595dd50b5aa60250daa94f5fd346.png"></p>
<p>注：各种语言的 <a target="_blank" rel="noopener" href="https://fasttext.cc/docs/en/crawl-vectors.html">Fasttext模型</a>。</p>
<p>代码的逻辑也不复杂，我们先利用Gensim这个库，把Facebook预训练好的模型加载进来。然后，我们定义一个获取文本向量的函数。因为Fasttext学到的是单词的向量，而不是句子的向量。同时，因为我们想要测试一下零样本学习的效果，不能再根据拿到的评论数据进一步训练模型了。所以我们把一句话里每个单词的向量，加在一起平均一下，把得到的向量作为整段评论的向量。这个方法也是当年常用的 <strong>一种将一句话变成向量的办法</strong>。我们把这个操作定义成了 get_fasttext_vector 这个函数，供后面的程序使用。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> gensim
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token comment"># Load the FastText pre-trained model</span>
model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>fasttext<span class="token punctuation">.</span>load_facebook_model<span class="token punctuation">(</span><span class="token string">'cc.en.300.bin'</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">get_fasttext_vector</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">:</span>
    vec <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">)</span> <span class="token comment"># Initialize an empty 300-dimensional vector</span>
    <span class="token keyword">for</span> word <span class="token keyword">in</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        vec <span class="token operator">+=</span> model<span class="token punctuation">.</span>wv<span class="token punctuation">[</span>word<span class="token punctuation">]</span>
    vec <span class="token operator">/=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># Take the average over all words in the line</span>
    <span class="token keyword">return</span> vec
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>而对应的零样本学习，我们还是和 <a target="_blank" rel="noopener" href="https://time.geekbang.org/column/article/642179">第 02 讲</a> 一样，将需要进行情感判断的评论分别与 “An Amazon review with a positive sentiment.” 以及 “An Amazon review with a negative sentiment.” 这两句话进行向量计算，算出它们之间的余弦距离。</p>
<p><strong>离前一个近，我们就认为是正面情感，离后一个近就是负面情感。</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">positive_text <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Wanted to save some to bring to my Chicago family but my North Carolina family ate all 4 boxes before I could pack. These are excellent...could serve to anyone"""</span>
negative_text <span class="token operator">=</span> <span class="token triple-quoted-string string">"""First, these should be called Mac - Coconut bars, as Coconut is the #2 ingredient and Mango is #3.  Second, lots of people don't like coconut.  I happen to be allergic to it.  Word to Amazon that if you want happy customers to make things like this more prominent.  Thanks."""</span>

positive_example_in_fasttext <span class="token operator">=</span> get_fasttext_vector<span class="token punctuation">(</span>positive_text<span class="token punctuation">)</span>
negative_example_in_fasttext <span class="token operator">=</span> get_fasttext_vector<span class="token punctuation">(</span>negative_text<span class="token punctuation">)</span>

positive_review_in_fasttext <span class="token operator">=</span> get_fasttext_vector<span class="token punctuation">(</span><span class="token string">"An Amazon review with a positive sentiment."</span><span class="token punctuation">)</span>
negative_review_in_fasttext <span class="token operator">=</span> get_fasttext_vector<span class="token punctuation">(</span><span class="token string">'An Amazon review with a negative sentiment.'</span><span class="token punctuation">)</span>

<span class="token keyword">from</span> openai<span class="token punctuation">.</span>embeddings_utils <span class="token keyword">import</span> cosine_similarity

<span class="token keyword">def</span> <span class="token function">get_fasttext_score</span><span class="token punctuation">(</span>sample_embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">return</span> cosine_similarity<span class="token punctuation">(</span>sample_embedding<span class="token punctuation">,</span> positive_review_in_fasttext<span class="token punctuation">)</span> <span class="token operator">-</span> cosine_similarity<span class="token punctuation">(</span>sample_embedding<span class="token punctuation">,</span> negative_review_in_fasttext<span class="token punctuation">)</span>

positive_score <span class="token operator">=</span> get_fasttext_score<span class="token punctuation">(</span>positive_example_in_fasttext<span class="token punctuation">)</span>
negative_score <span class="token operator">=</span> get_fasttext_score<span class="token punctuation">(</span>negative_example_in_fasttext<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Fasttext好评例子的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>positive_score<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Fasttext差评例子的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>negative_score<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">Fasttext好评例子的评分 <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.000544</span>
Fasttext差评例子的评分 <span class="token punctuation">:</span> <span class="token number">0.000369</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>我们从亚马逊食物评论的数据集里，选取了一个用户打5分的正面例子和一个用户打1分的例子试了一下。结果非常不幸，通过这个零样本学习的方式，这两个例子，程序都判断错了。</p>
<p>不过，仔细想一下，这样的结果也正常。因为这里的整句向量就是把所有单词的向量平均了一下。这意味着， <strong>可能会出现我们之前说过的单词相同顺序不同的问题。</strong></p>
<p>“not good, really bad” 和 “not bad, really good”，在这个情况下，意思完全不同，但是向量完全相同。更何况，我们拿来做对比的正面情感和负面情感的两句话，只差了positive&#x2F;negative这样一个单词。不考虑单词的顺序，而只考虑出现了哪些单词，并且不同单词之间还平均一下。这种策略要是真的有很好的效果，你反而要担心是不是哪里有Bug。</p>
<h3 id="T5效果测试"><a href="#T5效果测试" class="headerlink" title="T5效果测试"></a>T5效果测试</h3><p>Fasttext出师不利，毕竟Word2Vec已经是10年前的技术了，可以理解。那么，我们来看看和GPT一样使用了现在最流行的Transformer结构的T5模型效果怎么样。</p>
<p>T5模型的全称是Text-to-Text Transfer Transformer，翻译成中文就是“文本到文本的迁移Transformer”，也就是说，这个模型就是为了方便预训练之后拿去“迁移”到别的任务上而创造出来的。当时发表的时候，它就在各种数据集的评测上高居榜首。</p>
<p>T5最大的模型也有110亿个参数，也是基于Transformer，虽然比起GPT-3的1750亿小了不少，但是对硬件的性能要求也不低。所以，我们先测试一下T5-Small这个小模型看看效果。</p>
<p>同样的，在实际运行代码之前，我们也需要安装对应的Python包。这里我们分别安装了SentencePiece和PyTorch。在安装PyTorch的时候，我一并安装了Torchvision，后面课程会用到。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">conda install transformers <span class="token operator">-</span>c conda<span class="token operator">-</span>forge
conda install pytorch torchvision <span class="token operator">-</span>c pytorch
conda install sentencepiece
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>代码也不复杂，我们先加载预训练好的T5模型的分词器（Tokenizer），还有对应的模型。然后，我们定义了一个get_t5_vector函数，它会接收一段你的文本输入，然后用分词器来分词把结果变成一个序列，然后让模型的编码器部分对其进行编码。编码后的结果，仍然是分词后的一个词一个向量，我们还是把这些向量平均一下，作为整段文本的向量。</p>
<p>不过要注意，虽然同样是平均，但是和前面Fasttext不一样的是，这里每个词的向量，随着位置以及前后词的不同，编码出来的结果是不一样的。所以 <strong>这个平均值里，仍然包含了顺序带来的语义信息。</strong></p>
<p>这段代码执行的过程可能会有点慢。因为第一次加载模型的时候，Transformer库会把模型下载到本地并缓存起来，整个下载过程会花一些时间。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> T5Tokenizer<span class="token punctuation">,</span> T5Model
<span class="token keyword">import</span> torch

<span class="token comment"># load the T5 tokenizer and model</span>
tokenizer <span class="token operator">=</span> T5Tokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'t5-small'</span><span class="token punctuation">,</span> model_max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> T5Model<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'t5-small'</span><span class="token punctuation">)</span>

<span class="token comment"># set the model to evaluation mode</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># encode the input sentence</span>
<span class="token keyword">def</span> <span class="token function">get_t5_vector</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>line<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">'pt'</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token comment"># generate the vector representation</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">)</span>
        vector <span class="token operator">=</span> outputs<span class="token punctuation">.</span>last_hidden_state<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> vector<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>有了模型和通过模型获取的向量数据，我们就可以再试一试前面的零样本学习的方式，来看看效果怎么样了。我们简单地把之前获取向量和计算向量的函数调用，都换成新的get_t5_vector，运行一下就能看到结果了。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">positive_review_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span><span class="token string">"An Amazon review with a positive sentiment."</span><span class="token punctuation">)</span>
negative_review_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span><span class="token string">'An Amazon review with a negative sentiment.'</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">test_t5</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  positive_example_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span>positive_text<span class="token punctuation">)</span>
  negative_example_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span>negative_text<span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">get_t5_score</span><span class="token punctuation">(</span>sample_embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> cosine_similarity<span class="token punctuation">(</span>sample_embedding<span class="token punctuation">,</span> positive_review_in_t5<span class="token punctuation">)</span> <span class="token operator">-</span> cosine_similarity<span class="token punctuation">(</span>sample_embedding<span class="token punctuation">,</span> negative_review_in_t5<span class="token punctuation">)</span>

  positive_score <span class="token operator">=</span> get_t5_score<span class="token punctuation">(</span>positive_example_in_t5<span class="token punctuation">)</span>
  negative_score <span class="token operator">=</span> get_t5_score<span class="token punctuation">(</span>negative_example_in_t5<span class="token punctuation">)</span>

  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"T5好评例子的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>positive_score<span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"T5差评例子的评分 : %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>negative_score<span class="token punctuation">)</span><span class="token punctuation">)</span>

test_t5<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">T5好评例子的评分 <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.010294</span>
T5差评例子的评分 <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.008990</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>不幸的是，结果还是不太好，两个例子都被判断成了负面情绪，而且好评的分数还更低一点。不过别着急，会不会是我们用的模型太小了呢？毕竟T5论文里霸占各个排行榜的是110亿个参数的大模型，我们这里用的是T5-Small这个同样架构下的小模型，参数数量只有6000万个。</p>
<p>110亿个参数要花太多时间了，我们不妨把模型放大一下，试试有2.2亿个参数的 T5-Base这个模型？试用起来也很简单，我们就直接把上面模型的名字从 T5-small 改成 T5-base 就好了，其他代码不需要动，重新运行一遍。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">tokenizer <span class="token operator">=</span> T5Tokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'t5-base'</span><span class="token punctuation">,</span> model_max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> T5Model<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'t5-base'</span><span class="token punctuation">)</span>

<span class="token comment"># set the model to evaluation mode</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># encode the input sentence</span>
<span class="token keyword">def</span> <span class="token function">get_t5_vector</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>line<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">'pt'</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token comment"># generate the vector representation</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">)</span>
        vector <span class="token operator">=</span> outputs<span class="token punctuation">.</span>last_hidden_state<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> vector<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

positive_review_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span><span class="token string">"An Amazon review with a positive sentiment."</span><span class="token punctuation">)</span>
negative_review_in_t5 <span class="token operator">=</span> get_t5_vector<span class="token punctuation">(</span><span class="token string">'An Amazon review with a negative sentiment.'</span><span class="token punctuation">)</span>

test_t5<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">T5好评例子的评分 <span class="token punctuation">:</span> <span class="token number">0.010347</span>
T5差评例子的评分 <span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">0.023935</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>这一次，结果似乎是我们想要的了，好评被判定为正面情感，而差评被判定为负面情感。不过，也许我们只是运气好，在这一两个例子上有效果呢？所以，接下来让我们把整个数据集里，1分2分的差评和4分5分的好评都拿出来看一看。在OpenAI的API拿到的Embedding里，它的准确率能够达到95%以上，我们看看用这个有2.2亿个参数的T5-base模型能有什么样的结果。</p>
<p>对应的代码也不复杂，基本上和 <a target="_blank" rel="noopener" href="https://time.geekbang.org/column/article/642179">第 02 讲</a> 里OpenAI给到的Embedding代码是类似的。无非是通过pandas，根据评论的Text字段，全部计算一遍T5下的Embedding，然后存到DataFrame的t5_embedding 里去。</p>
<p>同样的，我们还是要通过T5的模型，来获得 “An Amazon review with a positive sentiment.” 以及 “An Amazon review with a negative sentiment.” 这两句话的Embedding。然后，我们用刚刚计算的用户评论的Embedding和这两句话计算一下余弦距离，来判断这些评论是正面还是负面的。</p>
<p>最后，通过Scikit-learn的分类报告的类库把评估的报告结果打印出来。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> classification_report

datafile_path <span class="token operator">=</span> <span class="token string">"data/fine_food_reviews_with_embeddings_1k.csv"</span>

df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>datafile_path<span class="token punctuation">)</span>

df<span class="token punctuation">[</span><span class="token string">"t5_embedding"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">.</span>Text<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>get_t5_vector<span class="token punctuation">)</span>
<span class="token comment"># convert 5-star rating to binary sentiment</span>
df <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">.</span>Score <span class="token operator">!=</span> <span class="token number">3</span><span class="token punctuation">]</span>
df<span class="token punctuation">[</span><span class="token string">"sentiment"</span><span class="token punctuation">]</span> <span class="token operator">=</span> df<span class="token punctuation">.</span>Score<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">"negative"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span> <span class="token string">"negative"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span> <span class="token string">"positive"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">:</span> <span class="token string">"positive"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>

<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> PrecisionRecallDisplay
<span class="token keyword">def</span> <span class="token function">evaluate_embeddings_approach</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">label_score</span><span class="token punctuation">(</span>review_embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> cosine_similarity<span class="token punctuation">(</span>review_embedding<span class="token punctuation">,</span> positive_review_in_t5<span class="token punctuation">)</span> <span class="token operator">-</span> cosine_similarity<span class="token punctuation">(</span>review_embedding<span class="token punctuation">,</span> negative_review_in_t5<span class="token punctuation">)</span>

    probas <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"t5_embedding"</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> label_score<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
    preds <span class="token operator">=</span> probas<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token string">'positive'</span> <span class="token keyword">if</span> x<span class="token operator">></span><span class="token number">0</span> <span class="token keyword">else</span> <span class="token string">'negative'</span><span class="token punctuation">)</span>

    report <span class="token operator">=</span> classification_report<span class="token punctuation">(</span>df<span class="token punctuation">.</span>sentiment<span class="token punctuation">,</span> preds<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>report<span class="token punctuation">)</span>

    display <span class="token operator">=</span> PrecisionRecallDisplay<span class="token punctuation">.</span>from_predictions<span class="token punctuation">(</span>df<span class="token punctuation">.</span>sentiment<span class="token punctuation">,</span> probas<span class="token punctuation">,</span> pos_label<span class="token operator">=</span><span class="token string">'positive'</span><span class="token punctuation">)</span>
    _ <span class="token operator">=</span> display<span class="token punctuation">.</span>ax_<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"2-class Precision-Recall curve"</span><span class="token punctuation">)</span>

evaluate_embeddings_approach<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">              precision    recall  f1<span class="token operator">-</span>score   support
    negative       <span class="token number">0.60</span>      <span class="token number">0.90</span>      <span class="token number">0.72</span>       <span class="token number">136</span>
    positive       <span class="token number">0.98</span>      <span class="token number">0.90</span>      <span class="token number">0.94</span>       <span class="token number">789</span>
    accuracy                           <span class="token number">0.90</span>       <span class="token number">925</span>
   macro avg       <span class="token number">0.79</span>      <span class="token number">0.90</span>      <span class="token number">0.83</span>       <span class="token number">925</span>
weighted avg       <span class="token number">0.93</span>      <span class="token number">0.90</span>      <span class="token number">0.91</span>       <span class="token number">925</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/ac3b30d430fb855f1382bc845b6798af.png" alt="图片"></p>
<p>结果显示，使用T5的效果也还可以，考虑所有样本的准确率也能达到90%。但是，在比较困难的差评的判断里，它的表现要比直接用OpenAI给到的Embedding要差很多，整体的精度只有60%。我们去看整体模型的准确率的话，OpenAI的Embedding能够到达96%，还是比这里的90%要好上一些的。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">            precision    recall  f1<span class="token operator">-</span>score   support
    negative       <span class="token number">0.98</span>      <span class="token number">0.73</span>      <span class="token number">0.84</span>       <span class="token number">136</span>
    positive       <span class="token number">0.96</span>      <span class="token number">1.00</span>      <span class="token number">0.98</span>       <span class="token number">789</span>
    accuracy                           <span class="token number">0.96</span>       <span class="token number">925</span>
   macro avg       <span class="token number">0.97</span>      <span class="token number">0.86</span>      <span class="token number">0.91</span>       <span class="token number">925</span>
weighted avg       <span class="token number">0.96</span>      <span class="token number">0.96</span>      <span class="token number">0.96</span>       <span class="token number">925</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>这里我重新贴一下使用OpenAI的Embedding取得的效果，你可以做个对比。</p>
<p>当然，这个分数也还不错，也能作为一个合格的情感分析分类器的基准线了。毕竟，我们这里采用的是零样本分类的方法，没有对需要分类的数据做任何训练，使用的完全是预训练模型给出来的向量，直接根据距离做的判断。所以，看起来大一点的预训练模型的确有用，能够取得更好的效果。而且，当你因为成本或者网络延时的问题，不方便使用OpenAI的API的时候，如果只是要获取文本的Embedding向量，使用T5这样的开源模型其实效果也还不错。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>最后，我们来复习一下这一讲的内容。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/a2d8bc53ca126473543868e262d42baf.jpg"></p>
<p>这一讲我们一起使用 Fasttext、T5-small 和 T5-base 这三个预训练模型，做了零样本分类测试。在和之前相同的食物评论的数据集上，使用只学习了单词向量表示的Fasttext，效果很糟糕。当我们换用同样基于Transformer的T5模型的时候，T5-small这个6000万参数的小模型其实效果也不好。但是当我们用上2.2亿参数的T5-base模型的时候，结果还可以。不过，还是远远比不上直接使用OpenAI的API的效果。可见，模型的大小，即使是对情感分析这样简单的问题，也能产生明显的差距。</p>
<h2 id="课后练习"><a href="#课后练习" class="headerlink" title="课后练习"></a>课后练习</h2><ol>
<li>我们在尝试使用T5-base这个模型之后，下了个判断认为大一点的模型效果更好。不过，其实我们并没有在整个数据集上使用T5-small这个模型做评测，你能试着修改一下代码，用T5-small测试一下整个数据集吗？测试下来的效果又是怎样的呢？</li>
<li>我们使用Fasttext的时候，把所有的单词向量平均一下，用来做情感分析效果很糟糕。那么什么样的分类问题，可以使用这样的方式呢？给你一个小提示，你觉得什么样的文本分类，只关心出现的单词是什么，而不关心它们的顺序？</li>
</ol>
<p>期待你的思考，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p>
</article><div class="tag_share"><div class="post_share"></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#04%EF%BD%9C%E6%96%B0%E6%97%B6%E4%BB%A3%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E5%A4%A7%E6%AF%94%E6%8B%BC%EF%BC%8CGPT-3%E5%88%B0%E5%BA%95%E8%83%9C%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">04｜新时代模型性能大比拼，GPT-3到底胜在哪里？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">什么是预训练模型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fasttext%E3%80%81T5%E3%80%81GPT-3%E6%A8%A1%E5%9E%8B%E6%95%88%E6%9E%9C%E5%A4%A7%E6%AF%94%E6%8B%BC"><span class="toc-number">1.2.</span> <span class="toc-text">Fasttext、T5、GPT-3模型效果大比拼</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Fasttext%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95"><span class="toc-number">1.2.1.</span> <span class="toc-text">Fasttext效果测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#T5%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95"><span class="toc-number">1.2.2.</span> <span class="toc-text">T5效果测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.3.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0"><span class="toc-number">1.4.</span> <span class="toc-text">课后练习</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By 码农张三</div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.4.0/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div></div></body></html>