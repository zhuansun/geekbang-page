<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>22｜再探HuggingFace：一键部署自己的大模型 | geekbang</title><meta name="author" content="码农张三"><meta name="copyright" content="码农张三"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="22｜再探HuggingFace：一键部署自己的大模型你好，我是徐文浩。 过去几讲里，我们一起为AI加上了语音能力。而且相对于大语言模型，语音识别和语音合成都有完全可以用于商业应用的开源模型。事实上，Huggingface的火爆离不开他们开源的这个Transformers库。这个开源库里有数万个我们可以直接调用的模型。很多场景下，这个开源模型已经足够我们使用了。 不过，在使用这些开源模型的过程中，">
<meta property="og:type" content="article">
<meta property="og:title" content="22｜再探HuggingFace：一键部署自己的大模型">
<meta property="og:url" content="https://zhuansun.github.io/geekbang/posts/1838659324.html">
<meta property="og:site_name" content="geekbang">
<meta property="og:description" content="22｜再探HuggingFace：一键部署自己的大模型你好，我是徐文浩。 过去几讲里，我们一起为AI加上了语音能力。而且相对于大语言模型，语音识别和语音合成都有完全可以用于商业应用的开源模型。事实上，Huggingface的火爆离不开他们开源的这个Transformers库。这个开源库里有数万个我们可以直接调用的模型。很多场景下，这个开源模型已经足够我们使用了。 不过，在使用这些开源模型的过程中，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg">
<meta property="article:published_time" content="2023-10-20T09:48:40.000Z">
<meta property="article:modified_time" content="2024-03-21T11:04:49.939Z">
<meta property="article:author" content="码农张三">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhuansun.github.io/geekbang/posts/1838659324"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '22｜再探HuggingFace：一键部署自己的大模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-21 11:04:49'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="geekbang" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic.imgdb.cn/item/653470a0c458853aef5813f1.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1345</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">geekbang</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">22｜再探HuggingFace：一键部署自己的大模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2023-10-20T09:48:40.000Z" title="发表于 2023-10-20 09:48:40">2023-10-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/">AI大模型之美</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="22｜再探HuggingFace：一键部署自己的大模型"><a href="#22｜再探HuggingFace：一键部署自己的大模型" class="headerlink" title="22｜再探HuggingFace：一键部署自己的大模型"></a>22｜再探HuggingFace：一键部署自己的大模型</h1><p>你好，我是徐文浩。</p>
<p>过去几讲里，我们一起为AI加上了语音能力。而且相对于大语言模型，语音识别和语音合成都有完全可以用于商业应用的开源模型。事实上，Huggingface的火爆离不开他们开源的这个Transformers库。这个开源库里有数万个我们可以直接调用的模型。很多场景下，这个开源模型已经足够我们使用了。</p>
<p>不过，在使用这些开源模型的过程中，你会发现大部分模型都需要一块不错的显卡。而如果回到我们更早使用过的开源大语言模型，就更是这样了。</p>
<p>在课程里面，我们是通过用Colab免费的GPU资源来搞定的。但是如果我们想要投入生产环境使用，免费的Colab就远远不够用了。而且，Colab的GPU资源对于大语言模型来说还是太小了。我们在前面不得不使用小尺寸的T5-base和裁剪过的ChatGLM-6B-INT4，而不是FLAN-UL2或者ChatGLM-130B这样真正的大模型。</p>
<p>那么，这一讲我们就来看看，Transformers可以给我们提供哪些模型，以及如何在云端使用真正的大模型。而想要解决这两个问题啊，都少不了要使用HuggingFace这个目前最大的开源模型社区。</p>
<h2 id="Transformers-Pipeline"><a href="#Transformers-Pipeline" class="headerlink" title="Transformers Pipeline"></a>Transformers Pipeline</h2><h3 id="Pipeline的基本功能"><a href="#Pipeline的基本功能" class="headerlink" title="Pipeline的基本功能"></a>Pipeline的基本功能</h3><p>我们先来看看，Transformers这个开源库到底能干些什么。下面的代码都是直接使用开源模型，需要利用GPU的算力，所以你最好还是在Colab里运行，注意不要忘记把Runtime的类型修改为GPU。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

classifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span>task<span class="token operator">=</span><span class="token string">"sentiment-analysis"</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
preds <span class="token operator">=</span> classifier<span class="token punctuation">(</span><span class="token string">"I am really happy today!"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>preds<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-plain" data-language="plain"><code class="language-plain">No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
[&#123;'label': 'POSITIVE', 'score': 0.9998762607574463&#125;]
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>这个代码非常简单，第一行代码，我们定义了一个task是sentimental-analysis的Pipeline，也就是一个情感分析的分类器。里面device&#x3D;0的意思是我们指定让Transformer使用GPU资源。如果你想要让它使用CPU，你可以设置device&#x3D;-1。然后，调用这个分类器对一段文本进行情感分析。从输出结果看，它给出了正确的Positive预测，也给出了具体的预测分数。因为我们在这里没有指定任何模型，所以Transformers自动选择了默认的模型，也就是日志里看到的 distilbert-base-uncased-finetuned-sst-2-english 这个模型。</p>
<p>看名字我们可以知道，这个模型是一个针对英语的模型。如果想要支持中文，我们也可以换一个模型来试试。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">classifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">"uer/roberta-base-finetuned-jd-binary-chinese"</span><span class="token punctuation">,</span> task<span class="token operator">=</span><span class="token string">"sentiment-analysis"</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
preds <span class="token operator">=</span> classifier<span class="token punctuation">(</span><span class="token string">"这个餐馆太难吃了。"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>preds<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">[</span><span class="token punctuation">&#123;</span><span class="token string">'label'</span><span class="token punctuation">:</span> <span class="token string">'negative (stars 1, 2 and 3)'</span><span class="token punctuation">,</span> <span class="token string">'score'</span><span class="token punctuation">:</span> <span class="token number">0.934112012386322</span><span class="token punctuation">&#125;</span><span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>这里，我们指定模型的名称，就能换用另一个模型来进行情感分析了。这次我们选用的是roberta-base-finetuned-jd-binary-chinese 这个模型。RoBERTa这个模型是基于BERT做了一些设计上的修改而得来的。而后面的finetuned-jd-binary-chinese 是基于京东的数据进行微调过的一个模型。</p>
<p>Pipeline是Transformers库里面的一个核心功能，它封装了所有托管在HuggingFace上的模型推理预测的入口。你不需要关心具体每个模型的架构、输入数据格式是什么样子的。我们只要通过model参数指定使用的模型，通过task参数来指定任务类型，运行一下就能直接获得结果。</p>
<p>比如，我们现在不想做情感分析了，而是想要做英译中，我们只需要把task换成translation_en_to_zh，然后选用一个合适的模型就好了。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">translation <span class="token operator">=</span> pipeline<span class="token punctuation">(</span>task<span class="token operator">=</span><span class="token string">"translation_en_to_zh"</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">"Helsinki-NLP/opus-mt-en-zh"</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

text <span class="token operator">=</span> <span class="token string">"I like to learn data science and AI."</span>
translated_text <span class="token operator">=</span> translation<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>translated_text<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">[</span><span class="token punctuation">&#123;</span><span class="token string">'translation_text'</span><span class="token punctuation">:</span> <span class="token string">'我喜欢学习数据科学和人工智能'</span><span class="token punctuation">&#125;</span><span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>在这里，我们选用了赫尔辛基大学的opus-mt-en-zh这个模型来做英译中，运行一下就可以看到，我们输入的英文被翻译成了中文。不过，我们怎么知道应该选用哪个模型呢？这个如魔法一般的Helsinki-NLP&#x2F;opus-mt-en-zh模型名字从哪里可以找到呢？</p>
<h3 id="如何寻找自己需要的模型？"><a href="#如何寻找自己需要的模型？" class="headerlink" title="如何寻找自己需要的模型？"></a>如何寻找自己需要的模型？</h3><p>这个时候，我们就需要去HuggingFace的网站里找一找了。你点击网站的 <a target="_blank" rel="noopener" href="https://huggingface.co/models">Models 板块</a>，就可以看到一个界面，左侧是一系列的筛选器，而右侧则是筛选出来的模型。比如刚才的英译中的模型，我们就是先在左侧的筛选器里，选中Task下的Translation这种任务类型。然后再在Languages里面选择Chinese，就能找到所有能够翻译中文的模型。默认的模型排序是按照用户下载数量从高到低排序的。一般来说，下载的数量越多，往往也意味着大家觉得这个模型可能更加靠谱。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/33266fd79bfa69cc0eefa603c5c61522.png" alt="图片"></p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/b53ae4a89fddbeb1126d03b8a191bd1b.png" alt="图片"></p>
<p>我们点击Helsinki-NLP&#x2F;opus-mt-en-zh进入这个模型的卡片页，就能看到更详细的介绍。并且很多模型，都在右侧提供了对应的示例。不使用代码，你也可以直接体验一下模型的能力和效果。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/0cdd1a161cd7c1e9aeed3325e9c0d13f.png" alt="图片"></p>
<h3 id="Pipeline支持的自然语言处理任务"><a href="#Pipeline支持的自然语言处理任务" class="headerlink" title="Pipeline支持的自然语言处理任务"></a>Pipeline支持的自然语言处理任务</h3><p>Transformers的Pipeline模块，支持的task是非常丰富的。可以说大部分常见的自然语言处理任务都被囊括在内了，经常会用到的有这么几个。</p>
<ul>
<li>feature-extraction，其实它和OpenAI的Embedding差不多，也就是把文本变成一段向量。</li>
<li>fill-mask，也就是完形填空。你可以把一句话中的一部分遮盖掉，然后让模型预测遮盖掉的地方的词是什么。</li>
<li>ner，命名实体识别。我们常常用它来提取文本里面的时间、地点、人名、邮箱、电话号码、地址等信息，然后进一步用这些信息来处理其他任务。</li>
<li>question-answering和table-question-answering，专门针对问题进行自动问答，在客服的FAQ领域常常会用到这类任务。</li>
<li>sentiment-analysis和text-classification，也就是我们之前见过的情感分析，以及类目更自由的文本分类问题。</li>
<li>text-generation 和 text2text-generation，文本生成类型的任务。我们之前让AI写代码或者写故事，其实都是这一类的任务。</li>
</ul>
<p>剩下的还有 summarization文本摘要、translation机器翻译，以及zero-shot-classification，也就是我们课程一开始介绍的零样本分类。</p>
<p>看到这里，你有没有发现ChatGPT的强大之处？上面这些自然语言处理任务，常常需要切换使用不同的专有模型。但是 <strong>在ChatGPT里，我们只需要一个通用的模型，就能直接解决所有的问题</strong>。这也是很多人惊呼“通用人工智能”来了的原因。</p>
<h3 id="通过Pipeline进行语音识别"><a href="#通过Pipeline进行语音识别" class="headerlink" title="通过Pipeline进行语音识别"></a>通过Pipeline进行语音识别</h3><p>Pipeline不仅支持自然语言处理相关的任务，它还支持语音和视觉类的任务。比如，我们同样可以通过Pipeline使用OpenAI的Whisper模型来做语音识别。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

transcriber <span class="token operator">=</span> pipeline<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">"openai/whisper-medium"</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
result <span class="token operator">=</span> transcriber<span class="token punctuation">(</span><span class="token string">"./data/podcast_clip.mp3"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span><span class="token string">'text'</span><span class="token punctuation">:</span> <span class="token string">" Welcome to OnBoard, a real first-line experience, a new investment thinking. I'm Monica. I'm Gao Ning. Let's talk about how software changes the world."</span><span class="token punctuation">&#125;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>不过，这里你会发现一个小小的问题。我们原本中文的内容，在通过Pipeline调用Whisper模型之后，输出就变成了英文。这个是因为Pipeline对整个数据处理进行了封装。在实际调用Whisper模型的时候，它会在最终生成文本的过程里面，加入一个&lt;|en|&gt;，导致文本生成的时候强行被指定成了英文。我们可以修改一下这个decoder生成文本时的设置，让输出的内容变成中文。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> WhisperProcessor<span class="token punctuation">,</span> WhisperForConditionalGeneration
processor <span class="token operator">=</span> WhisperProcessor<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"openai/whisper-medium"</span><span class="token punctuation">)</span>
forced_decoder_ids <span class="token operator">=</span> processor<span class="token punctuation">.</span>get_decoder_prompt_ids<span class="token punctuation">(</span>language<span class="token operator">=</span><span class="token string">"zh"</span><span class="token punctuation">,</span> task<span class="token operator">=</span><span class="token string">"transcribe"</span><span class="token punctuation">)</span>

transcriber <span class="token operator">=</span> pipeline<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">"openai/whisper-medium"</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                       generate_kwargs<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"forced_decoder_ids"</span><span class="token punctuation">:</span> forced_decoder_ids<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
result <span class="token operator">=</span> transcriber<span class="token punctuation">(</span><span class="token string">"./data/podcast_clip.mp3"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span><span class="token string">'text'</span><span class="token punctuation">:</span> <span class="token string">'欢迎来到Onboard真实的一线经验走新的投资思考我是Monica我是高宁我们一起聊聊软件如何改变世界'</span><span class="token punctuation">&#125;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>不过，即使转录成了中文，也会有一些小小的问题。你会看到转录后的内容没有标点符号。目前，Transformers库的Pipeline还没有比较简单的方法给转录的内容加上Prompt。这也是Pipeline的抽象封装带来的一个缺点。如果你有兴趣，也可以看看是否可以为Transformers库贡献代码，让它能够为Whisper模型支持Prompt的功能。</p>
<p>除了语音之外，Transformers也支持图像类问题的处理。不过我们还没有讲到那一块，今天就先不介绍了。在课程后面的图像部分，我们再详细介绍。</p>
<h2 id="如何使用Inference-API？"><a href="#如何使用Inference-API？" class="headerlink" title="如何使用Inference API？"></a>如何使用Inference API？</h2><p>如果你实际运行了上面我们使用的Pipeline代码，你就会发现其实大量的时间，都被浪费在下载模型的过程里了。而且，因为Colab的内存和显存大小的限制，我们还没办法运行尺寸太大的模型。比如，flan-t5-xxl这样大尺寸的模型有110亿参数，Colab和一般的游戏显卡根本放不下。</p>
<p>但是这些模型的效果往往又比单机能够加载的小模型要好很多。那么这个时候，如果你想测试体验一下效果，就可以试试Inference API。它是HuggingFace免费提供的，让你可以通过API调用的方式先试用这些模型。</p>
<h3 id="尝试Inference-API"><a href="#尝试Inference-API" class="headerlink" title="尝试Inference API"></a>尝试Inference API</h3><p>首先，和其他的API Key一样，我们还是通过环境变量来设置一下Huggingface的Access Token。你可以在Huggingface的 <a target="_blank" rel="noopener" href="https://huggingface.co/settings/tokens">个人设置</a> 里面拿到这个Key，然后通过export设置到环境变量里就好了。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/be019ba96b54e41ca15a80ce2249cc20.png" alt="图片"></p>
<p>设置环境变量：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">export HUGGINGFACE_API_KEY<span class="token operator">=</span>YOUR_HUGGINGFACE_ACCESS_TOKEN
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>然后，我们就可以通过简单的HTTP请求，调用托管在Huggingace里的模型了。比如，我们可以通过下面的代码，直接用flan-t5-xxl这个模型来进行问答。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os<span class="token punctuation">,</span> requests<span class="token punctuation">,</span> json

API_TOKEN <span class="token operator">=</span> os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"HUGGINGFACE_API_KEY"</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> <span class="token string">"google/flan-t5-xxl"</span>
API_URL <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"https://api-inference.huggingface.co/models/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>model<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>
headers <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"Authorization"</span><span class="token punctuation">:</span> <span class="token string-interpolation"><span class="token string">f"Bearer </span><span class="token interpolation"><span class="token punctuation">&#123;</span>API_TOKEN<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">,</span> <span class="token string">"Content-Type"</span><span class="token punctuation">:</span> <span class="token string">"application/json"</span><span class="token punctuation">&#125;</span>

<span class="token keyword">def</span> <span class="token function">query</span><span class="token punctuation">(</span>payload<span class="token punctuation">,</span> api_url<span class="token operator">=</span>API_URL<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">)</span><span class="token punctuation">:</span>
    data <span class="token operator">=</span> json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span>payload<span class="token punctuation">)</span>
    response <span class="token operator">=</span> requests<span class="token punctuation">.</span>request<span class="token punctuation">(</span><span class="token string">"POST"</span><span class="token punctuation">,</span> api_url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">,</span> data<span class="token operator">=</span>data<span class="token punctuation">)</span>
    <span class="token keyword">return</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>response<span class="token punctuation">.</span>content<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

question <span class="token operator">=</span> <span class="token string">"Please answer the following question. What is the capital of France?"</span>
data <span class="token operator">=</span> query<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"inputs"</span> <span class="token punctuation">:</span> question<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">[</span><span class="token punctuation">&#123;</span><span class="token string">'generated_text'</span><span class="token punctuation">:</span> <span class="token string">'paris'</span><span class="token punctuation">&#125;</span><span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>上面的演示代码也很简单，需要做到三点。</p>
<ol>
<li>我们向HuggingFace的api-inference这个域名发起一个请求，在对应的路径里跟上模型的名字。</li>
<li>在请求头里，带上我们拿到的ACCESS TOKEN，来通过权限的校验。</li>
<li>通过一个以inputs为key的JSON，作为请求体发送过去就好了。</li>
</ol>
<p>运行这个例子你就可以看到，flan-t5-xxl这样的模型也有一定的知识和问答能力。在这个例子里，它就准确地回答出了法国的首都是巴黎。</p>
<h3 id="等待模型加载完毕"><a href="#等待模型加载完毕" class="headerlink" title="等待模型加载完毕"></a>等待模型加载完毕</h3><p>同样的，Inference API也支持各种各样的任务。我们在模型页的卡片里，如果能够看到一个带着闪电标记⚡️的Hosted Inference API字样，就代表着这个模型可以通过Inference API调用。并且下面可以让你测试的示例，就是这个Inference API支持的任务。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/d679ca67a21c7d25b3c099a0b4c8cffa.png" alt="图片"></p>
<p>比如上面截图里的 hfl&#x2F;chinese-pert-base 模型支持的就是 feature-extraction 的任务，它能够让你把自己的文本变成向量。我们不妨来试一试。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> <span class="token string">"hfl/chinese-pert-base"</span>
API_URL <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"https://api-inference.huggingface.co/models/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>model<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>

question <span class="token operator">=</span> <span class="token string">"今天天气真不错！"</span>
data <span class="token operator">=</span> query<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"inputs"</span> <span class="token punctuation">:</span> question<span class="token punctuation">&#125;</span><span class="token punctuation">,</span> api_url<span class="token operator">=</span>API_URL<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span><span class="token string">'error'</span><span class="token punctuation">:</span> <span class="token string">'Model hfl/chinese-pert-base is currently loading'</span><span class="token punctuation">,</span> <span class="token string">'estimated_time'</span><span class="token punctuation">:</span> <span class="token number">20.0</span><span class="token punctuation">&#125;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>第一次尝试去调用这个Inference API，我们得到了一个报错信息。这个消息说的是，模型还在加载，并且预计还需要20秒才会加载完。因为Inference API是Huggingface免费提供给大家的，它也没有足够的GPU资源把所有模型（约几万个）都随时加载到线上。所以实际上，很多模型在没有人调用的时候，就会把GPU资源释放出来。只有当我们调用的时候，它才会加载模型，运行对应的推理逻辑。</p>
<p>我们有两个选择，一个是等待一会儿，等模型加载完了再调用。或者，我们可以在调用的时候就直接加上一个参数 <strong>wait_for_model&#x3D;True</strong>。这个参数，会让服务端等待模型加载完成之后，再把结果返回给我们，而不是立刻返回一个模型正在加载的报错信息。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">data <span class="token operator">=</span> query<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"inputs"</span> <span class="token punctuation">:</span> question<span class="token punctuation">,</span> <span class="token string">"wait_for_model"</span> <span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> api_url<span class="token operator">=</span>API_URL<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.05410267040133476</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0140887051820755</span><span class="token punctuation">,</span> <span class="token number">0.017411280423402786</span><span class="token punctuation">,</span> <span class="token number">0.10337194055318832</span>……
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>我们在Pipeline里介绍的任务，基本都可以通过Inference API的方式来调用。如果你想深入了解每一个任务的API的参数，可以去看一下HuggingFace的 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/api-inference/detailed_parameters">官方文档</a>。</p>
<h2 id="如何部署自己的大模型"><a href="#如何部署自己的大模型" class="headerlink" title="如何部署自己的大模型?"></a>如何部署自己的大模型?</h2><p>不过，Inference API只能给你提供试用各个模型的接口。因为是免费的资源，自然不能无限使用，所以HuggingFace为它设置了限额（Rate Limit）。如果你觉得大模型真的好用，那么最好的办法，就是在云平台上找一些有GPU的机器，把自己需要的模型部署起来。</p>
<p>HuggingFace自己就提供了一个非常简便的部署开源模型的产品，叫做Inference Endpoint。你不需要自己去云平台申请服务器，搭建各种环境。只需要选择想要部署的模型、使用的服务器资源，一键就能把自己需要的模型部署到云平台上。</p>
<h3 id="把模型部署到Endpoint上"><a href="#把模型部署到Endpoint上" class="headerlink" title="把模型部署到Endpoint上"></a>把模型部署到Endpoint上</h3><p>其实GPT2的论文里，已经体现了大语言模型不少潜力了。那么，下面我们就试着来部署一下GPT2这个模型。</p>
<ol>
<li>首先，进入 <a target="_blank" rel="noopener" href="https://ui.endpoints.huggingface.co/new">创建 Endpoint 的界面</a>，你可以选择自己想要部署的模型，我们这里选择了GPT2这个模型。</li>
<li>Endpoint Name，你可以自己设置一个，也可以直接使用系统自动生成的。</li>
<li>系统默认会为你选择云服务商、对应的区域，以及需要的硬件资源。如果你选择的硬件资源不足以部署这个模型，页面上也会有对应的提示告诉你。GPT2的模型连GPU也不需要，有CPU就能运行起来。</li>
<li>最后你需要选择一下这个Endpoint的安全等级，一共有三种，分别是 Protected、Public和Private。</li>
</ol>
<ul>
<li>Public是指这个模型部署好之后，互联网上的任何人都能调用，不需要做任何权限验证。一般情况下，你不太会选择这一个安全等级。</li>
<li>Protected，需要HuggingFace的Access Token的验证。我们在这里就选用这个方式，这也是测试使用最常用的方式。</li>
<li>Private，不仅需要权限验证，还需要通过一个AWS或者Azure云里面的私有网络才能访问。如果你实际部署一个应用在线上，对应API访问都是通过自己在云上的服务器进行的，那么选择这个方式是最合理的。</li>
</ul>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/bbef38cb8602ce3bd759660bdc098ccb.png" alt="图片"></p>
<p>设置好了之后，你再点击最下面的 <strong>Create Endpoint</strong>，HuggingFace就会开始帮你创建机器资源，部署对应的模型了。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/035aac864b7beb60b2736265307061a1.png" alt="图片"></p>
<p>我们只要等待几分钟，模型就能部署起来。当然，这是因为GPT2的模型比较小。如果你尝试部署一些大尺寸的模型，可能需要1-2个小时才能完成。因为HuggingFace要完成模型下载、Docker镜像打包等一系列的工作，单个模型又很大，所以需要更长时间。</p>
<h3 id="测试体验一下大模型"><a href="#测试体验一下大模型" class="headerlink" title="测试体验一下大模型"></a>测试体验一下大模型</h3><p>部署完成之后，我们会自动进入对应的Endpoint详情页里。上面的Endpoint URL就表示你可以像调用Inference API一样调用模型的API_URL。而下面，也给出了一个测试输入框，这个测试输入框我们在HuggingFace模型卡片页面里也能够看到。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/35f473d6bb571d2ebb39b94dbe6c2e0e.png" alt="图片"></p>
<p>我们可以用这样一段简单的代码来测试一下GPT2模型对应的效果。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">API_URL <span class="token operator">=</span> <span class="token string">"https://abmlvcliaa98k9ct.us-east-1.aws.endpoints.huggingface.cloud"</span>

text <span class="token operator">=</span> <span class="token string">"My name is Lewis and I like to"</span>
data <span class="token operator">=</span> query<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">"inputs"</span> <span class="token punctuation">:</span> text<span class="token punctuation">&#125;</span><span class="token punctuation">,</span> api_url<span class="token operator">=</span>API_URL<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">[</span><span class="token punctuation">&#123;</span><span class="token string">'generated_text'</span><span class="token punctuation">:</span> <span class="token string">'My name is Lewis and I like to think I\'m a dog. It would mean my soul to you." The boy quickly gave up and returned to his studies, then began taking classes on the basics and the basics of English, then he\'d write'</span><span class="token punctuation">&#125;</span><span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>有了这样一个部署在线上的模型，你就可以完全根据自己的需求随时调用API来完成自己的任务了，唯一的限制就是你使用的硬件资源有多少。</p>
<h3 id="暂停、恢复以及删除Endpoint"><a href="#暂停、恢复以及删除Endpoint" class="headerlink" title="暂停、恢复以及删除Endpoint"></a>暂停、恢复以及删除Endpoint</h3><p>部署在Endpoint上的模型是按照在线的时长收费的。如果你暂时不用这个模型，可以选择 <strong>暂停</strong>（Pause）这个Endpoint。等到想使用的时候，再重新 <strong>恢复</strong>（Resume）这个Endpoint就好了。暂停期间的模型不会计费，这个功能的选项就在模型Overview标签页的右上角。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/9a373ea28f6749517905e035060c5876.png" alt="图片"></p>
<p>如果你彻底不需要使用这个模型了，你可以把对应的Endpoint删掉，你只需要在对应Endpoint的Setting页面里输入Endpoint的名称，然后选择删除就好了。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/574156a0da2c71b00ed769483b724841.png" alt="图片"></p>
<p>HuggingFace将部署一个开源模型到线上的成本基本降低到了0。不过，目前它只支持海外的AWS、Azure以及Google Cloud，并不支持阿里云或者腾讯云，对国内的用户算是一个小小的遗憾。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，这一讲到这里也就结束了。</p>
<p>今天，我带着你了解了如何利用HuggingFace以及开源模型，来实现各类大模型应用的推理任务。最简单的方式，是使用Transformers这个Python开源库里面的Pipeline模块，只需要指定Pipeline里的model和task，然后直接调用它们来处理我们给到的数据，就能拿到结果。我们不需要关心模型背后的结构、分词器，以及数据的处理方式，也能快速上手使用这些开源模型。Pipeline的任务，涵盖了常见的自然语言处理任务，同时也包括了音频和图像的功能。</p>
<p>而如果模型比较大，单个的GPU不足以加载这个模型，你可以尝试通过HuggingFace免费提供的Inference API来试用模型。只需要一个简单的HTTP请求，你就可以直接测试像 flan-t5-xxl 这样110亿参数的大模型。而如果你想要把这样的大模型应用到你的生产环境里，你就可以通过Inference Endpoint这个功能来把大模型部署到云端。当然，这需要花不少钱。</p>
<p>在了解了Pipeline、Inference API以及Inference Endpoint之后，相信你已经充分掌握利用Huggingface来完成各种常见的文本、音频任务的方法了。后面需要的就是多多实践。</p>
<h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>最后，我给你留一道思考题。</p>
<p>你能试着使用一些HuggingFace的feature-extraction任务，通过开源大模型来做一下情感分析吗？你可以拿一些数据，看看 flan-t5-xxl 这样的模型的效果怎么样。</p>
<p>欢迎你把你实践后的结果分享出来，我们一起学习，共同进步，你也可以把这一讲分享给你身边感兴趣的朋友，邀他一起学习。我们下一讲再见！</p>
<h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>HuggingFace的 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main_classes/pipelines">官方文档</a> 里，给出了通过Pipeline完成各种任务的详细示例。你可以对照着自己的需求看一下这个文档，相信能解决你90%以上的问题。</p>
</article><div class="tag_share"><div class="post_share"></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#22%EF%BD%9C%E5%86%8D%E6%8E%A2HuggingFace%EF%BC%9A%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">22｜再探HuggingFace：一键部署自己的大模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformers-Pipeline"><span class="toc-number">1.1.</span> <span class="toc-text">Transformers Pipeline</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Pipeline%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD"><span class="toc-number">1.1.1.</span> <span class="toc-text">Pipeline的基本功能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%AF%BB%E6%89%BE%E8%87%AA%E5%B7%B1%E9%9C%80%E8%A6%81%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">1.1.2.</span> <span class="toc-text">如何寻找自己需要的模型？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pipeline%E6%94%AF%E6%8C%81%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.1.3.</span> <span class="toc-text">Pipeline支持的自然语言处理任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87Pipeline%E8%BF%9B%E8%A1%8C%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB"><span class="toc-number">1.1.4.</span> <span class="toc-text">通过Pipeline进行语音识别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Inference-API%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">如何使用Inference API？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95Inference-API"><span class="toc-number">1.2.1.</span> <span class="toc-text">尝试Inference API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%89%E5%BE%85%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD%E5%AE%8C%E6%AF%95"><span class="toc-number">1.2.2.</span> <span class="toc-text">等待模型加载完毕</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%83%A8%E7%BD%B2%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.</span> <span class="toc-text">如何部署自己的大模型?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%8A%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%88%B0Endpoint%E4%B8%8A"><span class="toc-number">1.3.1.</span> <span class="toc-text">把模型部署到Endpoint上</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E4%BD%93%E9%AA%8C%E4%B8%80%E4%B8%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.2.</span> <span class="toc-text">测试体验一下大模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9A%82%E5%81%9C%E3%80%81%E6%81%A2%E5%A4%8D%E4%BB%A5%E5%8F%8A%E5%88%A0%E9%99%A4Endpoint"><span class="toc-number">1.3.3.</span> <span class="toc-text">暂停、恢复以及删除Endpoint</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.4.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%80%83%E9%A2%98"><span class="toc-number">1.5.</span> <span class="toc-text">思考题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB"><span class="toc-number">1.6.</span> <span class="toc-text">推荐阅读</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By 码农张三</div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.4.0/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div></div></body></html>