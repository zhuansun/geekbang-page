<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>20｜TTS与语音合成：让你的机器人拥有声音 | geekbang</title><meta name="author" content="码农张三"><meta name="copyright" content="码农张三"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="20｜TTS与语音合成：让你的机器人拥有声音你好，我是徐文浩。 上一讲里，我们通过Whisper模型，让AI“听懂”了我们在说什么。我们可以利用这个能力，让AI替我们听播客、做小结。不过，这只是我们和AI的单向沟通。那我们能不能更进一步，让AI不仅能“听懂”我们说的话，通过ChatGPT去回答我们问的问题，最后还能让AI把这些内容合成为语音，“说”给我们听呢？ 当然可以，这也是我们这一讲的主题，我">
<meta property="og:type" content="article">
<meta property="og:title" content="20｜TTS与语音合成：让你的机器人拥有声音">
<meta property="og:url" content="https://zhuansun.github.io/geekbang/posts/2231401771.html">
<meta property="og:site_name" content="geekbang">
<meta property="og:description" content="20｜TTS与语音合成：让你的机器人拥有声音你好，我是徐文浩。 上一讲里，我们通过Whisper模型，让AI“听懂”了我们在说什么。我们可以利用这个能力，让AI替我们听播客、做小结。不过，这只是我们和AI的单向沟通。那我们能不能更进一步，让AI不仅能“听懂”我们说的话，通过ChatGPT去回答我们问的问题，最后还能让AI把这些内容合成为语音，“说”给我们听呢？ 当然可以，这也是我们这一讲的主题，我">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg">
<meta property="article:published_time" content="2023-10-20T09:48:40.000Z">
<meta property="article:modified_time" content="2023-12-13T15:14:50.456Z">
<meta property="article:author" content="码农张三">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhuansun.github.io/geekbang/posts/2231401771"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '20｜TTS与语音合成：让你的机器人拥有声音',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-13 15:14:50'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="geekbang" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic.imgdb.cn/item/653470a0c458853aef5813f1.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">804</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">geekbang</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">20｜TTS与语音合成：让你的机器人拥有声音</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2023-10-20T09:48:40.000Z" title="发表于 2023-10-20 09:48:40">2023-10-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E/">AI大模型之美</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="20｜TTS与语音合成：让你的机器人拥有声音"><a href="#20｜TTS与语音合成：让你的机器人拥有声音" class="headerlink" title="20｜TTS与语音合成：让你的机器人拥有声音"></a>20｜TTS与语音合成：让你的机器人拥有声音</h1><p>你好，我是徐文浩。</p>
<p>上一讲里，我们通过Whisper模型，让AI“听懂”了我们在说什么。我们可以利用这个能力，让AI替我们听播客、做小结。不过，这只是我们和AI的单向沟通。那我们能不能更进一步，让AI不仅能“听懂”我们说的话，通过ChatGPT去回答我们问的问题，最后还能让AI把这些内容合成为语音，“说”给我们听呢？</p>
<p>当然可以，这也是我们这一讲的主题，我会带你一起来让AI说话。和上一讲一样，我不仅会教你如何使用云端API来做语音合成（Text-To-Speech），也会教你使用开源模型，给你一个用本地CPU就能实现的解决方案。这样，你也就不用担心数据安全的问题了。</p>
<h2 id="使用Azure云进行语音合成"><a href="#使用Azure云进行语音合成" class="headerlink" title="使用Azure云进行语音合成"></a>使用Azure云进行语音合成</h2><p>语音合成其实已经是一个非常成熟的技术了，现在在很多短视频平台里，你听到的很多配音其实都是通过语音合成技术完成的。国内外的各大公司都有类似的云服务，比如 <a target="_blank" rel="noopener" href="https://www.xfyun.cn/services/online_tts">科大讯飞</a>、 <a target="_blank" rel="noopener" href="https://ai.aliyun.com/nls/tts">阿里云</a>、 <a target="_blank" rel="noopener" href="https://ai.baidu.com/tech/speech/tts">百度</a>、 <a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/polly/">AWS Polly</a>、 <a target="_blank" rel="noopener" href="https://cloud.google.com/text-to-speech">Google Cloud</a> 等等。不过，今天我们先来体验一下微软Azure云的语音合成API。选用Azure，主要有两个原因。</p>
<ol>
<li>因为微软和OpenAI有合作，Azure还提供了OpenAI相关模型的托管。这样，我们在实际的生产环境使用的时候，只需要和一个云打交道就好了。</li>
<li>价格比较便宜，并且提供了免费的额度。如果你每个月的用量在50万个字符以内，那么就不用花钱。</li>
</ol>
<p>在运行代码之前，你需要先去注册一个Azure云的账号，并且开通 <a target="_blank" rel="noopener" href="https://azure.microsoft.com/zh-cn/products/cognitive-services/#overview">微软认知服务</a>，然后开启对应的认知服务资源，获得自己的API Key。我在这里放了对应文档的 <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice,anomaly-detector,language-service,computer-vision,macos#get-the-keys-for-your-resource">链接</a>，你照着文档一步步操作，就能完成。我在下面也放上了关键步骤的截图，具体注册过程，我就不一一介绍了。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/e5b476e56ec3358e0925cacac1ee463c.png" alt="图片"></p>
<p>注：我选择了 East US 区域，因为这个区域也可以部署OpenAI的ChatGPT服务。</p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/73eaa98c11abb722b0b08dd6377cd06f.png" alt="图片"></p>
<p><img src="https://note-1252548816.cos.ap-nanjing.myqcloud.com/uPic/202312/0855277b5b176c8c6f8bf5314c53eabf.png" alt="图片"></p>
<p>在拿到API Key之后，我还是建议你把API Key设置到环境变量里面。避免你使用Notebook或者撰写代码的时候，不小心把自己的Key暴露出去，被别人免费使用。同样的，我们也在环境变量里设置一下我们使用的Azure服务的区域eastus。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">export AZURE_SPEECH_KEY<span class="token operator">=</span>YOUR_API_KEY
export AZURE_SPEECH_REGION<span class="token operator">=</span>eastus
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>当然，也不要忘了安装对应的Python包。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install azure<span class="token operator">-</span>cognitiveservices<span class="token operator">-</span>speech
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h3 id="基本的语音合成"><a href="#基本的语音合成" class="headerlink" title="基本的语音合成"></a>基本的语音合成</h3><p>账号和环境都设置好了之后，我们就可以动手来试试Azure语音合成的效果了。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> azure<span class="token punctuation">.</span>cognitiveservices<span class="token punctuation">.</span>speech <span class="token keyword">as</span> speechsdk

<span class="token comment"># This example requires environment variables named "SPEECH_KEY" and "SPEECH_REGION"</span>
speech_config <span class="token operator">=</span> speechsdk<span class="token punctuation">.</span>SpeechConfig<span class="token punctuation">(</span>subscription<span class="token operator">=</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'AZURE_SPEECH_KEY'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> region<span class="token operator">=</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'AZURE_SPEECH_REGION'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
audio_config <span class="token operator">=</span> speechsdk<span class="token punctuation">.</span>audio<span class="token punctuation">.</span>AudioOutputConfig<span class="token punctuation">(</span>use_default_speaker<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># The language of the voice that speaks.</span>
speech_config<span class="token punctuation">.</span>speech_synthesis_voice_name<span class="token operator">=</span><span class="token string">'zh-CN-XiaohanNeural'</span>

speech_synthesizer <span class="token operator">=</span> speechsdk<span class="token punctuation">.</span>SpeechSynthesizer<span class="token punctuation">(</span>speech_config<span class="token operator">=</span>speech_config<span class="token punctuation">,</span> audio_config<span class="token operator">=</span>audio_config<span class="token punctuation">)</span>

text <span class="token operator">=</span> <span class="token string">"今天天气真不错，ChatGPT真好用。"</span>

speech_synthesizer<span class="token punctuation">.</span>speak_text_async<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行上面这个代码，你就会听到一个女声说：“今天天气真不错，ChatGPT真好用。”</p>
<p>这几行代码非常简单。</p>
<ul>
<li>我们先通过配置读取了API Key和Region。</li>
<li>然后通过 speech_synthesis_voice_name这个配置参数指定了我们合成语音所使用的声音。</li>
<li>通过 speak_text_async 这个函数，就能异步调用API服务，直接把合成的声音播放出来了。</li>
</ul>
<p>通过speech_synthesis_voice_name这个参数，我们还可以选用很多别的声音，包括不同语言和不同的人。对应的列表可以在Azure的 <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/language-support?tabs=tts#prebuilt-neural-voices">Language and voice support</a> 文档里面找到。我们换一个其他的voice_name，就可以把对应的语音换成男声。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">speech_config<span class="token punctuation">.</span>speech_synthesis_voice_name<span class="token operator">=</span><span class="token string">'zh-CN-YunfengNeural'</span>
speech_synthesizer <span class="token operator">=</span> speechsdk<span class="token punctuation">.</span>SpeechSynthesizer<span class="token punctuation">(</span>speech_config<span class="token operator">=</span>speech_config<span class="token punctuation">,</span> audio_config<span class="token operator">=</span>audio_config<span class="token punctuation">)</span>
speech_synthesizer<span class="token punctuation">.</span>speak_text_async<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h3 id="指定语音的风格与角色"><a href="#指定语音的风格与角色" class="headerlink" title="指定语音的风格与角色"></a>指定语音的风格与角色</h3><p>如果你仔细看了 <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/language-support?tabs=tts#prebuilt-neural-voices">Language and voice support</a> 的文档，你会发现它有很多很多voice_name。而且很多voice_name里，我们还有额外的两个参数可以选择，那就是Styles和Roles，它们分别代表了合成语音的语气和对应的角色。通过这两个参数，我们可以让AI把很多场景“演出来”。比如，下面的示例代码就演绎了一段母子之间关于买玩具的一段对话，你可以运行一下看看效果。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">ssml <span class="token operator">=</span> <span class="token triple-quoted-string string">"""&lt;speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis"
       xmlns:mstts="https://www.w3.org/2001/mstts" xml:lang="zh-CN">
    &lt;voice name="zh-CN-YunyeNeural">
        儿子看见母亲走了过来，说到：
        &lt;mstts:express-as role="Boy" style="cheerful">
            “妈妈，我想要买个新玩具”
        &lt;/mstts:express-as>
    &lt;/voice>
    &lt;voice name="zh-CN-XiaomoNeural">
        母亲放下包，说：
        &lt;mstts:express-as role="SeniorFemale" style="angry">
            “我看你长得像个玩具。”
        &lt;/mstts:express-as>
    &lt;/voice>
&lt;/speak>"""</span>

speech_synthesis_result <span class="token operator">=</span> speech_synthesizer<span class="token punctuation">.</span>speak_ssml_async<span class="token punctuation">(</span>ssml<span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Azure并不是通过让你在API里面配置一些参数来指定一段文本的角色和语气，而是通过一个叫做SSML格式的XML文件做到这一点的。这个SSML是Speech Synthesis Markup Language的首字母缩写，翻译过来就是 <strong>语音合成标记语言</strong>。它不是一个Azure云专属的格式，而是一个W3C的标准，所以同样的XML不仅可以用在Azure云里，也一样可以用在Google Cloud里。</p>
<p>通过SSML里面元素的属性配置，我们可以指定不同文本段的voice_name、role和style。比如，在上面的这个例子里面，我们就用两个voice元素，表示了两个不同的人的声音。voice 元素里面的name属性，指定了这段声音的voice_name。而在voice元素内部，你还可以内嵌mstss:express-as元素，在这个元素里我们可以指定role和style。这样一来，我们就可以让一个voice_name在不同的场景片段下，用不同的语气和角色来说话。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">ssml <span class="token operator">=</span> <span class="token triple-quoted-string string">"""&lt;speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis"
       xmlns:mstts="https://www.w3.org/2001/mstts" xml:lang="en-US">
    &lt;voice name="en-US-JennyNeural">
        &lt;mstts:express-as style="excited">
            That'd be just amazing!
        &lt;/mstts:express-as>
        &lt;mstts:express-as style="friendly">
            What's next?
        &lt;/mstts:express-as>
    &lt;/voice>
&lt;/speak>"""</span>

speech_synthesis_result <span class="token operator">=</span> speech_synthesizer<span class="token punctuation">.</span>speak_ssml_async<span class="token punctuation">(</span>ssml<span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>在我自己实际使用的体验里面，中文的语气和角色效果不算明显。但是英文的效果还是很明显的，你可以根据文档用不同的参数尝试一下。</p>
<p>SSML这个格式，不只支持style和role，还有更多丰富的参数可以配置，你可以去看看Azure <a target="_blank" rel="noopener" href="https://www.w3.org/TR/speech-synthesis/">文档的协议标准</a>。</p>
<h3 id="指定语音的输出方式"><a href="#指定语音的输出方式" class="headerlink" title="指定语音的输出方式"></a>指定语音的输出方式</h3><p>到目前为止，我们都是使用异步调用的方式，直接把语音播放出来了。但很多时候，我们可能需要把对应的语音存储下来。那下面的代码就可以做到这一点。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">speech_config<span class="token punctuation">.</span>speech_synthesis_language<span class="token operator">=</span><span class="token string">'zh-CN'</span>
speech_config<span class="token punctuation">.</span>speech_synthesis_voice_name<span class="token operator">=</span><span class="token string">'zh-CN-XiaohanNeural'</span>

audio_config <span class="token operator">=</span> speechsdk<span class="token punctuation">.</span>audio<span class="token punctuation">.</span>AudioOutputConfig<span class="token punctuation">(</span>filename<span class="token operator">=</span><span class="token string">"./data/tts.wav"</span><span class="token punctuation">)</span>

speech_synthesizer <span class="token operator">=</span> speechsdk<span class="token punctuation">.</span>SpeechSynthesizer<span class="token punctuation">(</span>speech_config<span class="token operator">=</span>speech_config<span class="token punctuation">,</span> audio_config<span class="token operator">=</span>audio_config<span class="token punctuation">)</span>

text <span class="token operator">=</span> <span class="token string">"今天天气真不错，ChatGPT真好用"</span>
speech_synthesizer<span class="token punctuation">.</span>speak_text_async<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>我们只需要把原先设置成 use_default_speaker&#x3D;True的AudioOutputConfig，改为设置成一个 .wav的输出文件就好了。我们之后调用 speak_text_async 的函数，就会把语音输出到相应的.wav文件里。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">audio_config <span class="token operator">=</span> speechsdk<span class="token punctuation">.</span>audio<span class="token punctuation">.</span>AudioOutputConfig<span class="token punctuation">(</span>use_default_speaker<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>当然，你可以把对应的语音，暂时放在内存里面，而不是存储到文件系统中，也可以把输出的内容通过我们习惯的MP3格式存储下来。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">speech_config<span class="token punctuation">.</span>set_speech_synthesis_output_format<span class="token punctuation">(</span>speechsdk<span class="token punctuation">.</span>SpeechSynthesisOutputFormat<span class="token punctuation">.</span>Audio48Khz192KBitRateMonoMp3<span class="token punctuation">)</span>

speech_synthesizer <span class="token operator">=</span> speechsdk<span class="token punctuation">.</span>SpeechSynthesizer<span class="token punctuation">(</span>speech_config<span class="token operator">=</span>speech_config<span class="token punctuation">,</span> audio_config<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
result <span class="token operator">=</span> speech_synthesizer<span class="token punctuation">.</span>speak_text_async<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
stream <span class="token operator">=</span>speechsdk<span class="token punctuation">.</span>AudioDataStream<span class="token punctuation">(</span>result<span class="token punctuation">)</span>

stream<span class="token punctuation">.</span>save_to_wav_file<span class="token punctuation">(</span><span class="token string">"./data/tts.mp3"</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>我们只需要给speech_config这个参数设定一个 synthesis_output_format 就好了。我们上面就是把输出格式设置成了一个48kHz采样、192K码率的MP3格式。然后，这一次我们把AudioConfig设置成了None。在speak_text_async函数被调用之后，我们又调用了一下get函数，拿到对应的SpeechSynthesisResult对象。然后把这个对象放到AudioDataStream里，之后我们就可以把这个AudioDataStream按照我们的需要进行处理了。这里，我们是直接把它存储成了一个MP3文件。</p>
<h2 id="使用开源模型进行语音合成"><a href="#使用开源模型进行语音合成" class="headerlink" title="使用开源模型进行语音合成"></a>使用开源模型进行语音合成</h2><p>虽然通过Azure云的API，我们可以很容易地进行语音合成，速度也很快。但很多时候因为数据安全的问题，我们还是希望能够直接在我们自己的服务器上进行语音合成。当然，这也是能够办到的，有很多开源项目都支持语音合成。</p>
<p>我们在这里，就不妨试一下百度开源的PaddleSpeech的语音合成功能，看看效果怎么样。</p>
<p>我们还是要先安装PaddleSpeech相关的Python包。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">%</span>pip install paddlepaddle
<span class="token operator">%</span>pip install paddlespeech
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>然后通过PaddleSpeech自带的TTSExecutor，可以将对应的文本内容转换成WAV文件。需要注意，这个过程中，PaddleSpeech 需要下载对应的模型，所以第一次运行的时候也要花费一定的时间。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> paddlespeech<span class="token punctuation">.</span>cli<span class="token punctuation">.</span>tts<span class="token punctuation">.</span>infer <span class="token keyword">import</span> TTSExecutor

tts_executor <span class="token operator">=</span> TTSExecutor<span class="token punctuation">(</span><span class="token punctuation">)</span>

text <span class="token operator">=</span> <span class="token string">"今天天气十分不错，百度也能做语音合成。"</span>
output_file <span class="token operator">=</span> <span class="token string">"./data/paddlespeech.wav"</span>
tts_executor<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">,</span> output<span class="token operator">=</span>output_file<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>PaddleSpeech的TTSExecutor，只是把你的文本输入转化成了一个WAV文件。要在Python里面播放对应的声音，我们还要借助于PyAudio这个包。对应的，我们要先安装PyAudio依赖的portaudio库，然后再安装PyAudio包。</p>
<p>Mac下可以通过homebrew来安装portaudio。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"> brew install portaudio
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>如果在Unbuntu或者Debian下，你就可以通过apt-get来安装portaudio。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">sudo apt<span class="token operator">-</span>get install portaudio19<span class="token operator">-</span>dev
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>只有在portaudio安装成功之后，我们才能安装PyAudio包，不然会报缺少依赖的错误。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install pyaudio
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>通过PyAudio，我们可以直接播放WAV文件的内容了。对应的代码我放在下面了，其实我不太熟悉PyAudio库，但是这样简单的代码直接让ChatGPT帮我写，一次就能运行成功。如果你仔细读一下这段代码，也不难理解它的含义。实际就是打开了一个PyAudio的Stream，然后不断从我们的WAV文件里面读入数据，然后写入这个Stream，写入之后声音就播放出来了。如果你把 stream.write(data) 那一行去掉，那么你就会发现整个程序运行的过程里，是没有声音的。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> wave
<span class="token keyword">import</span> pyaudio

<span class="token keyword">def</span> <span class="token function">play_wav_audio</span><span class="token punctuation">(</span>wav_file<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># open the wave file</span>
    wf <span class="token operator">=</span> wave<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>wav_file<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span>

    <span class="token comment"># instantiate PyAudio</span>
    p <span class="token operator">=</span> pyaudio<span class="token punctuation">.</span>PyAudio<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># open a stream</span>
    stream <span class="token operator">=</span> p<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token builtin">format</span><span class="token operator">=</span>p<span class="token punctuation">.</span>get_format_from_width<span class="token punctuation">(</span>wf<span class="token punctuation">.</span>getsampwidth<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    channels<span class="token operator">=</span>wf<span class="token punctuation">.</span>getnchannels<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    rate<span class="token operator">=</span>wf<span class="token punctuation">.</span>getframerate<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    output<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token comment"># read data from the wave file and play it</span>
    data <span class="token operator">=</span> wf<span class="token punctuation">.</span>readframes<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">)</span>
    <span class="token keyword">while</span> data<span class="token punctuation">:</span>
        stream<span class="token punctuation">.</span>write<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
        data <span class="token operator">=</span> wf<span class="token punctuation">.</span>readframes<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">)</span>

    <span class="token comment"># close the stream and terminate PyAudio</span>
    stream<span class="token punctuation">.</span>stop_stream<span class="token punctuation">(</span><span class="token punctuation">)</span>
    stream<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
    p<span class="token punctuation">.</span>terminate<span class="token punctuation">(</span><span class="token punctuation">)</span>

play_wav_audio<span class="token punctuation">(</span>output_file<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>不过，我们调用的PaddleSpeech代码里的默认参数有一个小问题，就是它只支持中文的语音合成。如果你的文本带上英文运行一下，你会发现合成的语音里面只有中文，没有英文。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">tts_executor <span class="token operator">=</span> TTSExecutor<span class="token punctuation">(</span><span class="token punctuation">)</span>

text <span class="token operator">=</span> <span class="token string">"今天天气十分不错，Paddle Speech也能做语音合成。"</span>
output_file <span class="token operator">=</span> <span class="token string">"./data/paddlespeech_missing.wav"</span>
tts_executor<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">,</span> output<span class="token operator">=</span>output_file<span class="token punctuation">)</span>

play_wav_audio<span class="token punctuation">(</span>output_file<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行上面的代码，你会发现，PaddleSpeech在合成的语音里面丢失了。</p>
<p>这是因为，PaddleSpeech默认情况下使用的是一个只支持中文的模型。我们可以通过一些参数来指定使用的模型，一样能够做中英文混合的语音合成。</p>
<p>对应的代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">tts_executor <span class="token operator">=</span> TTSExecutor<span class="token punctuation">(</span><span class="token punctuation">)</span>

text <span class="token operator">=</span> <span class="token string">"早上好, how are you? 百度Paddle Speech一样能做中英文混合的语音合成。"</span>
output_file <span class="token operator">=</span> <span class="token string">"./data/paddlespeech_mix.wav"</span>
tts_executor<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">,</span> output<span class="token operator">=</span>output_file<span class="token punctuation">,</span>
             am<span class="token operator">=</span><span class="token string">"fastspeech2_mix"</span><span class="token punctuation">,</span> voc<span class="token operator">=</span><span class="token string">"hifigan_csmsc"</span><span class="token punctuation">,</span>
             lang<span class="token operator">=</span><span class="token string">"mix"</span><span class="token punctuation">,</span> spk_id<span class="token operator">=</span><span class="token number">174</span><span class="token punctuation">)</span>

play_wav_audio<span class="token punctuation">(</span>output_file<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到，和上面的代码相比，我们增加了4个参数。</p>
<ul>
<li>am，是acoustic model的缩写，也就是我们使用的声学模型。我们这里选用的是fastspeech2_mix。fastspeech2也是一个基于Transformer的语音合成模型，速度快、质量高。这里带了一个mix，代表这个模型是支持中英文混合生成的。</li>
<li>voc，是vocoder的缩写，叫做音码器。声学模型只是把我们的文本变成了一个声音波形的信号。我们还需要通过音码器，把声学模型给出的波形变成可以播放的音频。我们这里选择的HiFiGAN_csMSC，是一个高保真（HiFi）、基于对抗生成网络（GAN）技术的模型，它的训练数据用到了HiFiSinger和csMSC，而模型的名字就来自这些关键词的组合。</li>
<li>lang，代表我们模型支持的语言，这里我们自然应该选mix。</li>
<li>spk_id，类似于我们之前在Azure里看到的voice_name，不同的spk_id 听起来就是不同的人说的话。</li>
</ul>
<p>运行这个代码，一样能够正常地生成中英文混合的语音内容。如果你想要了解PaddleSpeech的语音合成功能，还有它所支持的各种模型和各种应用场景，可以参看GitHub上的 <a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/PaddleSpeech/blob/develop/demos/text_to_speech/README_cn.md">Demo 文档</a>。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，这一讲到这里也就结束了。</p>
<p>这一讲我们学会了两种语音合成的方式。一种是使用Azure云提供的API，另一种则是使用百度开源的PaddleSpeech。Azure云的语音合成，不仅仅是能简单地把文本变成人声，还能通过SSML这个W3C标准下的XML标记语言，指定不同的人声（voice_name）、语气（style）还有角色（role）。这些功能都是非常有实用价值的，能够帮助我们处理各种场景下的语音合成需求。</p>
<p>而PaddleSpeech则带给了我们一个开源方案，并且它也支持中英文混合在一起的语音生成。它背后可供选择的模型里，我们使用的也是基于Transformer的fastspeech2模型。可以看到，目前Transformer类型的模型在各个领域都已经占据了主流。</p>
<p>学到这里，我们的AI就拥有了声音。而在下一讲里，我会拿我们已经学到的知识，搭建一个可以通过语音和你聊天的机器人。并且更进一步地，我们还会为它配上你的虚拟形象，希望你和我一样对下一讲充满期待！</p>
<h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>最后，给你留一道思考题。PaddleSpeech不仅能拿来做语音合成，也能用来做语音识别。你能试试看用它做语音识别的效果吗？和OpenAI Whisper比起来，你觉得它们两个哪个效果更好？欢迎你把你体验之后的感受分享出来，也欢迎你把这一讲分享给需要的朋友，我们下一讲再见！</p>
<h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>PaddleSpeech的 <a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/PaddleSpeech/blob/develop/README_cn.md">中文文档</a> 里面，包含了大量使用PaddleSpeech这个开源库的场景和方法，你有兴趣可以去看一看。百度的PaddlePaddle的社区里面，也有专门的 <a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/education/group/info/25130">语音相关的课程</a>，如果你想深入了解的话，也可以去看一看。</p>
</article><div class="tag_share"><div class="post_share"></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#20%EF%BD%9CTTS%E4%B8%8E%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90%EF%BC%9A%E8%AE%A9%E4%BD%A0%E7%9A%84%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%8B%A5%E6%9C%89%E5%A3%B0%E9%9F%B3"><span class="toc-number">1.</span> <span class="toc-text">20｜TTS与语音合成：让你的机器人拥有声音</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Azure%E4%BA%91%E8%BF%9B%E8%A1%8C%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90"><span class="toc-number">1.1.</span> <span class="toc-text">使用Azure云进行语音合成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%9A%84%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90"><span class="toc-number">1.1.1.</span> <span class="toc-text">基本的语音合成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E5%AE%9A%E8%AF%AD%E9%9F%B3%E7%9A%84%E9%A3%8E%E6%A0%BC%E4%B8%8E%E8%A7%92%E8%89%B2"><span class="toc-number">1.1.2.</span> <span class="toc-text">指定语音的风格与角色</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E5%AE%9A%E8%AF%AD%E9%9F%B3%E7%9A%84%E8%BE%93%E5%87%BA%E6%96%B9%E5%BC%8F"><span class="toc-number">1.1.3.</span> <span class="toc-text">指定语音的输出方式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90"><span class="toc-number">1.2.</span> <span class="toc-text">使用开源模型进行语音合成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.3.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%80%83%E9%A2%98"><span class="toc-number">1.4.</span> <span class="toc-text">思考题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB"><span class="toc-number">1.5.</span> <span class="toc-text">推荐阅读</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://s2.loli.net/2023/10/21/vq13okXnTbxDG2R.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By 码农张三</div></div><script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.4.0/mermaid.min.js"></script></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div></div></body></html>